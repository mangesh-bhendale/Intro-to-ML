{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2623ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "import numpy # https://numpy.org/\n",
    "\n",
    "import pandas # https://pandas.pydata.org/\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt  # https://matplotlib.org/\n",
    "import seaborn as sns # https://seaborn.pydata.org/\n",
    "\n",
    "from sklearn import model_selection       \n",
    "from sklearn.metrics import classification_report   \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.tree import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error  \n",
    "from sklearn.metrics import max_error\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# data preprocessing / feature selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# combining\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac954142",
   "metadata": {},
   "source": [
    "1. a. Loading data from 'winequality_white.csv' and Summrizing the data\n",
    "   b. A table and graph summarizing the dataset statistics\n",
    "   c. Setting a classification problem for predicting the quality value based on the values of all the other variables in the file (acidity, alcohol, pH, etc.)\n",
    "   d. Splitting the dataset into separate training and test sets and stratify the training set into k-folds and setting 'f1_micro' as scoring method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae07d3bc-b488-41bd-80af-65f4ed227faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file ...\n",
      "done \n",
      "\n",
      "Removing rows with missing data ...\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading data from file ...')  # Loading the data\n",
    "dataset = pandas.read_csv('winequality_white.csv') # default is header=infer, change if column names are not in first row\n",
    "print('done \\n')\n",
    "\n",
    "print('Removing rows with missing data ...')  # Make things simple\n",
    "dataset = dataset.dropna()  # default is to drop any row that contains at least one missing value\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e07b8b5-c074-46bc-ac5f-e9de14a4e913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rows from the dataset (top ten):\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            6.2              0.66         0.48             1.2      0.029   \n",
      "1            6.2              0.66         0.48             1.2      0.029   \n",
      "2            6.8              0.26         0.42             1.7      0.049   \n",
      "3            6.7              0.23         0.31             2.1      0.046   \n",
      "4            6.7              0.23         0.31             2.1      0.046   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 29.0                  75.0   0.9892  3.33       0.39   \n",
      "1                 29.0                  75.0   0.9892  3.33       0.39   \n",
      "2                 41.0                 122.0   0.9930  3.47       0.48   \n",
      "3                 30.0                  96.0   0.9926  3.33       0.64   \n",
      "4                 30.0                  96.0   0.9926  3.33       0.64   \n",
      "\n",
      "   alcohol  quality  \n",
      "0     12.8        8  \n",
      "1     12.8        8  \n",
      "2     10.5        8  \n",
      "3     10.7        8  \n",
      "4     10.7        8  \n",
      "Sample rows from the dataset (bottom ten):\n",
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "4868            7.1             0.240         0.34            1.20      0.045   \n",
      "4869            6.0             0.590         0.00            0.80      0.037   \n",
      "4870            6.0             0.350         0.46            0.90      0.033   \n",
      "4871            5.2             0.405         0.15            1.45      0.038   \n",
      "4872            6.2             0.530         0.02            0.90      0.035   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "4868                  6.0                 132.0  0.99132  3.16       0.46   \n",
      "4869                 30.0                  95.0  0.99032  3.10       0.40   \n",
      "4870                  9.0                  65.0  0.98934  3.24       0.35   \n",
      "4871                 10.0                  44.0  0.99125  3.52       0.40   \n",
      "4872                  6.0                  81.0  0.99234  3.24       0.35   \n",
      "\n",
      "      alcohol  quality  \n",
      "4868     11.2        4  \n",
      "4869     10.9        4  \n",
      "4870     12.1        4  \n",
      "4871     11.6        4  \n",
      "4872      9.5        4  \n"
     ]
    }
   ],
   "source": [
    "#Printing data to check first 5 and last 5 lines\n",
    "print('Sample rows from the dataset (top ten):')  \n",
    "print(dataset.head(5))  # first 5 rows\n",
    "\n",
    "print('Sample rows from the dataset (bottom ten):')  \n",
    "print(dataset.tail(5))  # last 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c797aa6-9ded-4dad-93f1-c22b69ca4ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the dataset - Univariate statistics\n",
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    4873.000000       4873.000000  4873.000000     4873.000000   \n",
      "mean        6.851149          0.277995     0.334131        6.393741   \n",
      "std         0.837109          0.100592     0.121189        5.072535   \n",
      "min         3.800000          0.080000     0.000000        0.600000   \n",
      "25%         6.300000          0.210000     0.270000        1.700000   \n",
      "50%         6.800000          0.260000     0.320000        5.200000   \n",
      "75%         7.300000          0.320000     0.390000        9.900000   \n",
      "max        14.200000          1.100000     1.660000       65.800000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  4873.000000          4873.000000           4873.000000  4873.000000   \n",
      "mean      0.045756            35.236097            138.251283     0.994026   \n",
      "std       0.021695            16.445156             42.011360     0.002990   \n",
      "min       0.009000             2.000000              9.000000     0.987110   \n",
      "25%       0.036000            23.000000            108.000000     0.991730   \n",
      "50%       0.043000            34.000000            134.000000     0.993740   \n",
      "75%       0.050000            46.000000            167.000000     0.996100   \n",
      "max       0.346000           138.500000            344.000000     1.038980   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  4873.000000  4873.000000  4873.000000  4873.000000  \n",
      "mean      3.188147     0.489934    10.513253     5.886518  \n",
      "std       0.150752     0.114136     1.229856     0.862730  \n",
      "min       2.720000     0.220000     8.000000     4.000000  \n",
      "25%       3.090000     0.410000     9.500000     5.000000  \n",
      "50%       3.180000     0.470000    10.400000     6.000000  \n",
      "75%       3.280000     0.550000    11.400000     6.000000  \n",
      "max       3.820000     1.080000    14.200000     8.000000  \n"
     ]
    }
   ],
   "source": [
    "# A numerical summary table of the above data\n",
    "print('Summary of the dataset - Univariate statistics')   \n",
    "print(dataset.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2bf2a602-ea6e-4d22-a05e-df1aa7cfb50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading list of problem variables X and Y...\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicting the quality value (a single variable with five classes labeled 4, 5, â€¦, 8) based on the values of all the other variables in the file (acidity, alcohol, pH, etc.\n",
    "\n",
    "print('Reading list of problem variables X and Y...')\n",
    "X_name = [ 'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol' ] # names of columns to focus on as predictors\n",
    "X = dataset[X_name]   # only keep these columns as features\n",
    "y_name = 'quality'   # name of column to focus on as target\n",
    "y = dataset[y_name]   # only keep this column as label \n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35483c58-59fb-4922-a3e1-1c5b995cc6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of X - Univariate graphs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 11 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Summary of X - Univariate graphs')\n",
    "\n",
    "plt.figure()  \n",
    "X.hist(bins=50)  # histogram with number of bins ~= sqrt(N)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "X.plot(kind='box', subplots=True, sharex=False, sharey=False)  # boxplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2463c8e2-3c7f-417d-8e05-60404be2f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of X - Bivariate (column-pair) graphs:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 121 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# More graphical summaries\n",
    "print('Summary of X - Bivariate (column-pair) graphs:')\n",
    "\n",
    "#Plotting correlation matrix\n",
    "plt.figure()\n",
    "corr = X.corr()    # compute the correlation between the variables\n",
    "sns.heatmap( corr, cmap='coolwarm', vmax=1.0, vmin=-1.0 );   # plot it as a heat map\n",
    "plt.title('Correlation matrix for X')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "scatter_matrix(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d19f9dea-e160-4c09-82d2-8492f819f281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the distribution of y\n",
    "plt.figure(figsize=(12, 2))   # we can control the size of the figure\n",
    "sns.histplot(data=y)\n",
    "plt.title('Distribution of Quality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60a56051-8370-4e6a-be54-3c0ad00a2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up ML framework \n",
    "# Splitting data into training and testing datasets\n",
    "test_pct = 0.80   # reserving a random 20% of the data points for testing performance\n",
    "seed = 42          # setting the seed means that _the_same_ 20% will be split for testing every time we run this notebook - this allows for repeatability/reproducibility\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_pct, random_state=42)\n",
    "\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_minmax.fit_transform(X_train)\n",
    "#y_train_scaled = scaler_minmax.fit_transform(y_train)\n",
    "X_test_scaled = scaler_minmax.transform(X_test)\n",
    "#y_test_scaled = scaler_minmax.transform(y_test)\n",
    "\n",
    "X_train_scaled_std = scaler_std.fit_transform(X_train)\n",
    "#y_train_scaled_std = scaler_std.fit_transform(y_train)\n",
    "X_test_scaled_std = scaler_std.transform(X_test)\n",
    "#y_test_scaled_std = scaler_std.transform(y_test)\n",
    "\n",
    "# Chose the scoring method\n",
    "scoring_method = 'f1_micro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b7c4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training candidate models on the training data and checking their performance on unseen test data.\n",
    "hyperparameters = {'kernel':['linear', 'sigmoid', 'poly', 'rbf'],'class_weight':[None, 'balanced'] }    # specify the versions of the family members\n",
    "kfold = model_selection.StratifiedKFold(n_splits=5, random_state=seed, shuffle=True)  # create the k folds of the TRAINING data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b32eccd",
   "metadata": {},
   "source": [
    "2. a. Use cross-validation to train and tune an MLPClassifier model whose configurations all have a single hidden layer but different numbers of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e5eabe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.96777959\n",
      "Iteration 2, loss = 1.92975970\n",
      "Iteration 3, loss = 1.89277227\n",
      "Iteration 4, loss = 1.85783805\n",
      "Iteration 5, loss = 1.82375303\n",
      "Iteration 6, loss = 1.79197493\n",
      "Iteration 7, loss = 1.76110928\n",
      "Iteration 8, loss = 1.73145104\n",
      "Iteration 9, loss = 1.70328787\n",
      "Iteration 10, loss = 1.67563788\n",
      "Iteration 11, loss = 1.64931327\n",
      "Iteration 12, loss = 1.62527727\n",
      "Iteration 13, loss = 1.60164525\n",
      "Iteration 14, loss = 1.57894577\n",
      "Iteration 15, loss = 1.55817656\n",
      "Iteration 16, loss = 1.53813160\n",
      "Iteration 17, loss = 1.51891827\n",
      "Iteration 18, loss = 1.50140369\n",
      "Iteration 19, loss = 1.48434954\n",
      "Iteration 20, loss = 1.46829097\n",
      "Iteration 21, loss = 1.45349372\n",
      "Iteration 22, loss = 1.43934583\n",
      "Iteration 23, loss = 1.42593506\n",
      "Iteration 24, loss = 1.41363891\n",
      "Iteration 25, loss = 1.40187188\n",
      "Iteration 26, loss = 1.39061618\n",
      "Iteration 27, loss = 1.38030311\n",
      "Iteration 28, loss = 1.37033789\n",
      "Iteration 29, loss = 1.36140199\n",
      "Iteration 30, loss = 1.35274659\n",
      "Iteration 31, loss = 1.34474504\n",
      "Iteration 32, loss = 1.33745605\n",
      "Iteration 33, loss = 1.33014509\n",
      "Iteration 34, loss = 1.32366963\n",
      "Iteration 35, loss = 1.31765292\n",
      "Iteration 36, loss = 1.31163657\n",
      "Iteration 37, loss = 1.30633054\n",
      "Iteration 38, loss = 1.30126761\n",
      "Iteration 39, loss = 1.29661871\n",
      "Iteration 40, loss = 1.29233487\n",
      "Iteration 41, loss = 1.28808915\n",
      "Iteration 42, loss = 1.28445508\n",
      "Iteration 43, loss = 1.28067109\n",
      "Iteration 44, loss = 1.27733152\n",
      "Iteration 45, loss = 1.27436460\n",
      "Iteration 46, loss = 1.27125877\n",
      "Iteration 47, loss = 1.26857412\n",
      "Iteration 48, loss = 1.26594728\n",
      "Iteration 49, loss = 1.26354682\n",
      "Iteration 50, loss = 1.26140888\n",
      "Iteration 51, loss = 1.25921090\n",
      "Iteration 52, loss = 1.25724025\n",
      "Iteration 53, loss = 1.25539400\n",
      "Iteration 54, loss = 1.25362805\n",
      "Iteration 55, loss = 1.25190385\n",
      "Iteration 56, loss = 1.25043069\n",
      "Iteration 57, loss = 1.24894508\n",
      "Iteration 58, loss = 1.24759085\n",
      "Iteration 59, loss = 1.24634290\n",
      "Iteration 60, loss = 1.24514869\n",
      "Iteration 61, loss = 1.24398331\n",
      "Iteration 62, loss = 1.24290870\n",
      "Iteration 63, loss = 1.24189664\n",
      "Iteration 64, loss = 1.24092025\n",
      "Iteration 65, loss = 1.24007444\n",
      "Iteration 66, loss = 1.23914370\n",
      "Iteration 67, loss = 1.23844528\n",
      "Iteration 68, loss = 1.23766546\n",
      "Iteration 69, loss = 1.23698778\n",
      "Iteration 70, loss = 1.23631666\n",
      "Iteration 71, loss = 1.23562848\n",
      "Iteration 72, loss = 1.23507374\n",
      "Iteration 73, loss = 1.23454896\n",
      "Iteration 74, loss = 1.23394065\n",
      "Iteration 75, loss = 1.23344631\n",
      "Iteration 76, loss = 1.23305000\n",
      "Iteration 77, loss = 1.23249298\n",
      "Iteration 78, loss = 1.23208135\n",
      "Iteration 79, loss = 1.23167374\n",
      "Iteration 80, loss = 1.23132264\n",
      "Iteration 81, loss = 1.23091634\n",
      "Iteration 82, loss = 1.23056441\n",
      "Iteration 83, loss = 1.23023561\n",
      "Iteration 84, loss = 1.22997999\n",
      "Iteration 85, loss = 1.22969448\n",
      "Iteration 86, loss = 1.22933296\n",
      "Iteration 87, loss = 1.22910890\n",
      "Iteration 88, loss = 1.22884554\n",
      "Iteration 89, loss = 1.22861765\n",
      "Iteration 90, loss = 1.22850605\n",
      "Iteration 91, loss = 1.22819237\n",
      "Iteration 92, loss = 1.22797350\n",
      "Iteration 93, loss = 1.22777858\n",
      "Iteration 94, loss = 1.22760489\n",
      "Iteration 95, loss = 1.22739144\n",
      "Iteration 96, loss = 1.22724971\n",
      "Iteration 97, loss = 1.22708472\n",
      "Iteration 98, loss = 1.22691797\n",
      "Iteration 99, loss = 1.22678035\n",
      "Iteration 100, loss = 1.22664068\n",
      "Iteration 1, loss = 1.96909566\n",
      "Iteration 2, loss = 1.93126300\n",
      "Iteration 3, loss = 1.89419048\n",
      "Iteration 4, loss = 1.85927716\n",
      "Iteration 5, loss = 1.82513627\n",
      "Iteration 6, loss = 1.79316393\n",
      "Iteration 7, loss = 1.76222286\n",
      "Iteration 8, loss = 1.73253842\n",
      "Iteration 9, loss = 1.70405594\n",
      "Iteration 10, loss = 1.67649262\n",
      "Iteration 11, loss = 1.65038815\n",
      "Iteration 12, loss = 1.62622824\n",
      "Iteration 13, loss = 1.60278345\n",
      "Iteration 14, loss = 1.57978010\n",
      "Iteration 15, loss = 1.55891661\n",
      "Iteration 16, loss = 1.53874753\n",
      "Iteration 17, loss = 1.51955616\n",
      "Iteration 18, loss = 1.50167380\n",
      "Iteration 19, loss = 1.48479369\n",
      "Iteration 20, loss = 1.46878263\n",
      "Iteration 21, loss = 1.45361275\n",
      "Iteration 22, loss = 1.43957468\n",
      "Iteration 23, loss = 1.42600461\n",
      "Iteration 24, loss = 1.41362595\n",
      "Iteration 25, loss = 1.40185954\n",
      "Iteration 26, loss = 1.39065853\n",
      "Iteration 27, loss = 1.38012036\n",
      "Iteration 28, loss = 1.37032574\n",
      "Iteration 29, loss = 1.36139658\n",
      "Iteration 30, loss = 1.35263040\n",
      "Iteration 31, loss = 1.34457567\n",
      "Iteration 32, loss = 1.33707237\n",
      "Iteration 33, loss = 1.32998345\n",
      "Iteration 34, loss = 1.32344765\n",
      "Iteration 35, loss = 1.31728613\n",
      "Iteration 36, loss = 1.31128202\n",
      "Iteration 37, loss = 1.30621951\n",
      "Iteration 38, loss = 1.30107880\n",
      "Iteration 39, loss = 1.29628803\n",
      "Iteration 40, loss = 1.29204942\n",
      "Iteration 41, loss = 1.28785105\n",
      "Iteration 42, loss = 1.28403449\n",
      "Iteration 43, loss = 1.28050745\n",
      "Iteration 44, loss = 1.27698104\n",
      "Iteration 45, loss = 1.27404848\n",
      "Iteration 46, loss = 1.27091360\n",
      "Iteration 47, loss = 1.26822559\n",
      "Iteration 48, loss = 1.26568130\n",
      "Iteration 49, loss = 1.26315561\n",
      "Iteration 50, loss = 1.26103986\n",
      "Iteration 51, loss = 1.25882473\n",
      "Iteration 52, loss = 1.25692541\n",
      "Iteration 53, loss = 1.25499992\n",
      "Iteration 54, loss = 1.25324687\n",
      "Iteration 55, loss = 1.25148765\n",
      "Iteration 56, loss = 1.25002442\n",
      "Iteration 57, loss = 1.24854758\n",
      "Iteration 58, loss = 1.24718911\n",
      "Iteration 59, loss = 1.24590138\n",
      "Iteration 60, loss = 1.24476807\n",
      "Iteration 61, loss = 1.24344882\n",
      "Iteration 62, loss = 1.24244462\n",
      "Iteration 63, loss = 1.24146840\n",
      "Iteration 64, loss = 1.24045707\n",
      "Iteration 65, loss = 1.23961073\n",
      "Iteration 66, loss = 1.23865942\n",
      "Iteration 67, loss = 1.23797122\n",
      "Iteration 68, loss = 1.23716320\n",
      "Iteration 69, loss = 1.23646924\n",
      "Iteration 70, loss = 1.23580600\n",
      "Iteration 71, loss = 1.23511277\n",
      "Iteration 72, loss = 1.23457684\n",
      "Iteration 73, loss = 1.23398030\n",
      "Iteration 74, loss = 1.23345186\n",
      "Iteration 75, loss = 1.23294390\n",
      "Iteration 76, loss = 1.23246708\n",
      "Iteration 77, loss = 1.23198704\n",
      "Iteration 78, loss = 1.23156747\n",
      "Iteration 79, loss = 1.23119532\n",
      "Iteration 80, loss = 1.23074331\n",
      "Iteration 81, loss = 1.23041553\n",
      "Iteration 82, loss = 1.23004280\n",
      "Iteration 83, loss = 1.22971946\n",
      "Iteration 84, loss = 1.22947392\n",
      "Iteration 85, loss = 1.22907238\n",
      "Iteration 86, loss = 1.22886532\n",
      "Iteration 87, loss = 1.22857823\n",
      "Iteration 88, loss = 1.22832705\n",
      "Iteration 89, loss = 1.22808064\n",
      "Iteration 90, loss = 1.22790015\n",
      "Iteration 91, loss = 1.22762225\n",
      "Iteration 92, loss = 1.22748814\n",
      "Iteration 93, loss = 1.22723635\n",
      "Iteration 94, loss = 1.22708399\n",
      "Iteration 95, loss = 1.22688133\n",
      "Iteration 96, loss = 1.22668078\n",
      "Iteration 97, loss = 1.22660793\n",
      "Iteration 98, loss = 1.22643137\n",
      "Iteration 99, loss = 1.22623267\n",
      "Iteration 100, loss = 1.22608109\n",
      "Iteration 1, loss = 1.96928773\n",
      "Iteration 2, loss = 1.93092895\n",
      "Iteration 3, loss = 1.89398458\n",
      "Iteration 4, loss = 1.85900573\n",
      "Iteration 5, loss = 1.82554327\n",
      "Iteration 6, loss = 1.79332131\n",
      "Iteration 7, loss = 1.76223790\n",
      "Iteration 8, loss = 1.73279926\n",
      "Iteration 9, loss = 1.70412508\n",
      "Iteration 10, loss = 1.67639152\n",
      "Iteration 11, loss = 1.65024143\n",
      "Iteration 12, loss = 1.62583537\n",
      "Iteration 13, loss = 1.60284093\n",
      "Iteration 14, loss = 1.57940232\n",
      "Iteration 15, loss = 1.55838549\n",
      "Iteration 16, loss = 1.53846487\n",
      "Iteration 17, loss = 1.51965701\n",
      "Iteration 18, loss = 1.50137863\n",
      "Iteration 19, loss = 1.48462438\n",
      "Iteration 20, loss = 1.46859321\n",
      "Iteration 21, loss = 1.45337297\n",
      "Iteration 22, loss = 1.43929275\n",
      "Iteration 23, loss = 1.42581566\n",
      "Iteration 24, loss = 1.41339944\n",
      "Iteration 25, loss = 1.40148954\n",
      "Iteration 26, loss = 1.39040402\n",
      "Iteration 27, loss = 1.38007812\n",
      "Iteration 28, loss = 1.37021352\n",
      "Iteration 29, loss = 1.36099005\n",
      "Iteration 30, loss = 1.35241855\n",
      "Iteration 31, loss = 1.34456920\n",
      "Iteration 32, loss = 1.33676723\n",
      "Iteration 33, loss = 1.32978976\n",
      "Iteration 34, loss = 1.32329637\n",
      "Iteration 35, loss = 1.31716410\n",
      "Iteration 36, loss = 1.31132229\n",
      "Iteration 37, loss = 1.30624990\n",
      "Iteration 38, loss = 1.30088232\n",
      "Iteration 39, loss = 1.29619346\n",
      "Iteration 40, loss = 1.29194287\n",
      "Iteration 41, loss = 1.28779698\n",
      "Iteration 42, loss = 1.28390969\n",
      "Iteration 43, loss = 1.28047118\n",
      "Iteration 44, loss = 1.27682003\n",
      "Iteration 45, loss = 1.27395670\n",
      "Iteration 46, loss = 1.27087041\n",
      "Iteration 47, loss = 1.26811979\n",
      "Iteration 48, loss = 1.26564036\n",
      "Iteration 49, loss = 1.26310275\n",
      "Iteration 50, loss = 1.26103416\n",
      "Iteration 51, loss = 1.25873534\n",
      "Iteration 52, loss = 1.25677157\n",
      "Iteration 53, loss = 1.25491603\n",
      "Iteration 54, loss = 1.25322474\n",
      "Iteration 55, loss = 1.25144841\n",
      "Iteration 56, loss = 1.24995087\n",
      "Iteration 57, loss = 1.24851431\n",
      "Iteration 58, loss = 1.24709760\n",
      "Iteration 59, loss = 1.24584319\n",
      "Iteration 60, loss = 1.24479611\n",
      "Iteration 61, loss = 1.24340974\n",
      "Iteration 62, loss = 1.24247360\n",
      "Iteration 63, loss = 1.24137204\n",
      "Iteration 64, loss = 1.24046522\n",
      "Iteration 65, loss = 1.23956554\n",
      "Iteration 66, loss = 1.23864467\n",
      "Iteration 67, loss = 1.23785536\n",
      "Iteration 68, loss = 1.23708404\n",
      "Iteration 69, loss = 1.23644767\n",
      "Iteration 70, loss = 1.23573005\n",
      "Iteration 71, loss = 1.23504904\n",
      "Iteration 72, loss = 1.23458176\n",
      "Iteration 73, loss = 1.23394693\n",
      "Iteration 74, loss = 1.23335567\n",
      "Iteration 75, loss = 1.23286676\n",
      "Iteration 76, loss = 1.23244521\n",
      "Iteration 77, loss = 1.23198519\n",
      "Iteration 78, loss = 1.23151385\n",
      "Iteration 79, loss = 1.23127434\n",
      "Iteration 80, loss = 1.23071777\n",
      "Iteration 81, loss = 1.23032665\n",
      "Iteration 82, loss = 1.23001372\n",
      "Iteration 83, loss = 1.22969188\n",
      "Iteration 84, loss = 1.22944124\n",
      "Iteration 85, loss = 1.22905901\n",
      "Iteration 86, loss = 1.22886694\n",
      "Iteration 87, loss = 1.22853819\n",
      "Iteration 88, loss = 1.22828875\n",
      "Iteration 89, loss = 1.22801317\n",
      "Iteration 90, loss = 1.22793342\n",
      "Iteration 91, loss = 1.22762452\n",
      "Iteration 92, loss = 1.22742432\n",
      "Iteration 93, loss = 1.22716906\n",
      "Iteration 94, loss = 1.22707872\n",
      "Iteration 95, loss = 1.22686853\n",
      "Iteration 96, loss = 1.22666508\n",
      "Iteration 97, loss = 1.22653510\n",
      "Iteration 98, loss = 1.22636138\n",
      "Iteration 99, loss = 1.22625991\n",
      "Iteration 100, loss = 1.22607789\n",
      "Iteration 1, loss = 1.96859080\n",
      "Iteration 2, loss = 1.93039489\n",
      "Iteration 3, loss = 1.89335763\n",
      "Iteration 4, loss = 1.85839697\n",
      "Iteration 5, loss = 1.82496174\n",
      "Iteration 6, loss = 1.79233660\n",
      "Iteration 7, loss = 1.76159247\n",
      "Iteration 8, loss = 1.73204623\n",
      "Iteration 9, loss = 1.70353986\n",
      "Iteration 10, loss = 1.67605794\n",
      "Iteration 11, loss = 1.64989881\n",
      "Iteration 12, loss = 1.62537987\n",
      "Iteration 13, loss = 1.60249589\n",
      "Iteration 14, loss = 1.57892143\n",
      "Iteration 15, loss = 1.55870379\n",
      "Iteration 16, loss = 1.53811680\n",
      "Iteration 17, loss = 1.51911318\n",
      "Iteration 18, loss = 1.50106260\n",
      "Iteration 19, loss = 1.48420141\n",
      "Iteration 20, loss = 1.46832182\n",
      "Iteration 21, loss = 1.45324766\n",
      "Iteration 22, loss = 1.43904994\n",
      "Iteration 23, loss = 1.42547993\n",
      "Iteration 24, loss = 1.41314054\n",
      "Iteration 25, loss = 1.40116632\n",
      "Iteration 26, loss = 1.39018273\n",
      "Iteration 27, loss = 1.37958346\n",
      "Iteration 28, loss = 1.37000820\n",
      "Iteration 29, loss = 1.36086607\n",
      "Iteration 30, loss = 1.35200986\n",
      "Iteration 31, loss = 1.34416259\n",
      "Iteration 32, loss = 1.33644780\n",
      "Iteration 33, loss = 1.32947877\n",
      "Iteration 34, loss = 1.32282914\n",
      "Iteration 35, loss = 1.31681021\n",
      "Iteration 36, loss = 1.31100931\n",
      "Iteration 37, loss = 1.30566066\n",
      "Iteration 38, loss = 1.30047720\n",
      "Iteration 39, loss = 1.29590459\n",
      "Iteration 40, loss = 1.29140455\n",
      "Iteration 41, loss = 1.28738965\n",
      "Iteration 42, loss = 1.28344628\n",
      "Iteration 43, loss = 1.27992431\n",
      "Iteration 44, loss = 1.27642070\n",
      "Iteration 45, loss = 1.27342313\n",
      "Iteration 46, loss = 1.27038964\n",
      "Iteration 47, loss = 1.26763571\n",
      "Iteration 48, loss = 1.26511289\n",
      "Iteration 49, loss = 1.26260586\n",
      "Iteration 50, loss = 1.26048252\n",
      "Iteration 51, loss = 1.25820039\n",
      "Iteration 52, loss = 1.25631553\n",
      "Iteration 53, loss = 1.25435777\n",
      "Iteration 54, loss = 1.25263748\n",
      "Iteration 55, loss = 1.25100183\n",
      "Iteration 56, loss = 1.24948507\n",
      "Iteration 57, loss = 1.24805541\n",
      "Iteration 58, loss = 1.24658045\n",
      "Iteration 59, loss = 1.24537058\n",
      "Iteration 60, loss = 1.24430186\n",
      "Iteration 61, loss = 1.24295533\n",
      "Iteration 62, loss = 1.24191274\n",
      "Iteration 63, loss = 1.24096336\n",
      "Iteration 64, loss = 1.24001887\n",
      "Iteration 65, loss = 1.23907466\n",
      "Iteration 66, loss = 1.23824649\n",
      "Iteration 67, loss = 1.23745098\n",
      "Iteration 68, loss = 1.23661600\n",
      "Iteration 69, loss = 1.23598437\n",
      "Iteration 70, loss = 1.23531256\n",
      "Iteration 71, loss = 1.23466779\n",
      "Iteration 72, loss = 1.23417852\n",
      "Iteration 73, loss = 1.23348592\n",
      "Iteration 74, loss = 1.23307417\n",
      "Iteration 75, loss = 1.23245365\n",
      "Iteration 76, loss = 1.23204755\n",
      "Iteration 77, loss = 1.23160143\n",
      "Iteration 78, loss = 1.23113463\n",
      "Iteration 79, loss = 1.23096112\n",
      "Iteration 80, loss = 1.23039714\n",
      "Iteration 81, loss = 1.22998820\n",
      "Iteration 82, loss = 1.22965315\n",
      "Iteration 83, loss = 1.22933528\n",
      "Iteration 84, loss = 1.22903200\n",
      "Iteration 85, loss = 1.22872792\n",
      "Iteration 86, loss = 1.22854010\n",
      "Iteration 87, loss = 1.22819803\n",
      "Iteration 88, loss = 1.22797679\n",
      "Iteration 89, loss = 1.22783326\n",
      "Iteration 90, loss = 1.22766634\n",
      "Iteration 91, loss = 1.22737848\n",
      "Iteration 92, loss = 1.22713165\n",
      "Iteration 93, loss = 1.22691979\n",
      "Iteration 94, loss = 1.22674692\n",
      "Iteration 95, loss = 1.22658270\n",
      "Iteration 96, loss = 1.22643005\n",
      "Iteration 97, loss = 1.22624496\n",
      "Iteration 98, loss = 1.22609133\n",
      "Iteration 99, loss = 1.22598416\n",
      "Iteration 100, loss = 1.22586180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.96858019\n",
      "Iteration 2, loss = 1.93029357\n",
      "Iteration 3, loss = 1.89339793\n",
      "Iteration 4, loss = 1.85810611\n",
      "Iteration 5, loss = 1.82436512\n",
      "Iteration 6, loss = 1.79206982\n",
      "Iteration 7, loss = 1.76117426\n",
      "Iteration 8, loss = 1.73145800\n",
      "Iteration 9, loss = 1.70269459\n",
      "Iteration 10, loss = 1.67565425\n",
      "Iteration 11, loss = 1.65008122\n",
      "Iteration 12, loss = 1.62499270\n",
      "Iteration 13, loss = 1.60126474\n",
      "Iteration 14, loss = 1.57931835\n",
      "Iteration 15, loss = 1.55783802\n",
      "Iteration 16, loss = 1.53798363\n",
      "Iteration 17, loss = 1.51890069\n",
      "Iteration 18, loss = 1.50113711\n",
      "Iteration 19, loss = 1.48413540\n",
      "Iteration 20, loss = 1.46836054\n",
      "Iteration 21, loss = 1.45302781\n",
      "Iteration 22, loss = 1.43915556\n",
      "Iteration 23, loss = 1.42553821\n",
      "Iteration 24, loss = 1.41333900\n",
      "Iteration 25, loss = 1.40133466\n",
      "Iteration 26, loss = 1.39029769\n",
      "Iteration 27, loss = 1.37993424\n",
      "Iteration 28, loss = 1.37012641\n",
      "Iteration 29, loss = 1.36097939\n",
      "Iteration 30, loss = 1.35242217\n",
      "Iteration 31, loss = 1.34441409\n",
      "Iteration 32, loss = 1.33670328\n",
      "Iteration 33, loss = 1.32973048\n",
      "Iteration 34, loss = 1.32332772\n",
      "Iteration 35, loss = 1.31699984\n",
      "Iteration 36, loss = 1.31154372\n",
      "Iteration 37, loss = 1.30583225\n",
      "Iteration 38, loss = 1.30077826\n",
      "Iteration 39, loss = 1.29620879\n",
      "Iteration 40, loss = 1.29178770\n",
      "Iteration 41, loss = 1.28774655\n",
      "Iteration 42, loss = 1.28395516\n",
      "Iteration 43, loss = 1.28029890\n",
      "Iteration 44, loss = 1.27677427\n",
      "Iteration 45, loss = 1.27368353\n",
      "Iteration 46, loss = 1.27076825\n",
      "Iteration 47, loss = 1.26801298\n",
      "Iteration 48, loss = 1.26544963\n",
      "Iteration 49, loss = 1.26303389\n",
      "Iteration 50, loss = 1.26075314\n",
      "Iteration 51, loss = 1.25870901\n",
      "Iteration 52, loss = 1.25659195\n",
      "Iteration 53, loss = 1.25485277\n",
      "Iteration 54, loss = 1.25299021\n",
      "Iteration 55, loss = 1.25143847\n",
      "Iteration 56, loss = 1.24978145\n",
      "Iteration 57, loss = 1.24842045\n",
      "Iteration 58, loss = 1.24705778\n",
      "Iteration 59, loss = 1.24577350\n",
      "Iteration 60, loss = 1.24460503\n",
      "Iteration 61, loss = 1.24345834\n",
      "Iteration 62, loss = 1.24227811\n",
      "Iteration 63, loss = 1.24135733\n",
      "Iteration 64, loss = 1.24039757\n",
      "Iteration 65, loss = 1.23944106\n",
      "Iteration 66, loss = 1.23862061\n",
      "Iteration 67, loss = 1.23783916\n",
      "Iteration 68, loss = 1.23707921\n",
      "Iteration 69, loss = 1.23633510\n",
      "Iteration 70, loss = 1.23571767\n",
      "Iteration 71, loss = 1.23506909\n",
      "Iteration 72, loss = 1.23445955\n",
      "Iteration 73, loss = 1.23391170\n",
      "Iteration 74, loss = 1.23341201\n",
      "Iteration 75, loss = 1.23290090\n",
      "Iteration 76, loss = 1.23235462\n",
      "Iteration 77, loss = 1.23190902\n",
      "Iteration 78, loss = 1.23146850\n",
      "Iteration 79, loss = 1.23110187\n",
      "Iteration 80, loss = 1.23071462\n",
      "Iteration 81, loss = 1.23030673\n",
      "Iteration 82, loss = 1.23001006\n",
      "Iteration 83, loss = 1.22967072\n",
      "Iteration 84, loss = 1.22932974\n",
      "Iteration 85, loss = 1.22903434\n",
      "Iteration 86, loss = 1.22872659\n",
      "Iteration 87, loss = 1.22848409\n",
      "Iteration 88, loss = 1.22820681\n",
      "Iteration 89, loss = 1.22804525\n",
      "Iteration 90, loss = 1.22777749\n",
      "Iteration 91, loss = 1.22755688\n",
      "Iteration 92, loss = 1.22736049\n",
      "Iteration 93, loss = 1.22716763\n",
      "Iteration 94, loss = 1.22693407\n",
      "Iteration 95, loss = 1.22678358\n",
      "Iteration 96, loss = 1.22663144\n",
      "Iteration 97, loss = 1.22645801\n",
      "Iteration 98, loss = 1.22628644\n",
      "Iteration 99, loss = 1.22618716\n",
      "Iteration 100, loss = 1.22604202\n",
      "Iteration 1, loss = 15.82780762\n",
      "Iteration 2, loss = 10.32700481\n",
      "Iteration 3, loss = 5.87431933\n",
      "Iteration 4, loss = 3.43493257\n",
      "Iteration 5, loss = 2.36675085\n",
      "Iteration 6, loss = 2.03180856\n",
      "Iteration 7, loss = 1.89514076\n",
      "Iteration 8, loss = 1.80875006\n",
      "Iteration 9, loss = 1.75198671\n",
      "Iteration 10, loss = 1.70642212\n",
      "Iteration 11, loss = 1.66595091\n",
      "Iteration 12, loss = 1.63007809\n",
      "Iteration 13, loss = 1.59796542\n",
      "Iteration 14, loss = 1.56858128\n",
      "Iteration 15, loss = 1.54236673\n",
      "Iteration 16, loss = 1.51793325\n",
      "Iteration 17, loss = 1.49520320\n",
      "Iteration 18, loss = 1.47503050\n",
      "Iteration 19, loss = 1.45634369\n",
      "Iteration 20, loss = 1.43891275\n",
      "Iteration 21, loss = 1.42261077\n",
      "Iteration 22, loss = 1.40814146\n",
      "Iteration 23, loss = 1.39435584\n",
      "Iteration 24, loss = 1.38178521\n",
      "Iteration 25, loss = 1.37024230\n",
      "Iteration 26, loss = 1.35974374\n",
      "Iteration 27, loss = 1.34970536\n",
      "Iteration 28, loss = 1.34064929\n",
      "Iteration 29, loss = 1.33245080\n",
      "Iteration 30, loss = 1.32488085\n",
      "Iteration 31, loss = 1.31760421\n",
      "Iteration 32, loss = 1.31115045\n",
      "Iteration 33, loss = 1.30516355\n",
      "Iteration 34, loss = 1.29958085\n",
      "Iteration 35, loss = 1.29430097\n",
      "Iteration 36, loss = 1.28948827\n",
      "Iteration 37, loss = 1.28530817\n",
      "Iteration 38, loss = 1.28107442\n",
      "Iteration 39, loss = 1.27750926\n",
      "Iteration 40, loss = 1.27416281\n",
      "Iteration 41, loss = 1.27084711\n",
      "Iteration 42, loss = 1.26798676\n",
      "Iteration 43, loss = 1.26534480\n",
      "Iteration 44, loss = 1.26284468\n",
      "Iteration 45, loss = 1.26075334\n",
      "Iteration 46, loss = 1.25851381\n",
      "Iteration 47, loss = 1.25652564\n",
      "Iteration 48, loss = 1.25477413\n",
      "Iteration 49, loss = 1.25316923\n",
      "Iteration 50, loss = 1.25142218\n",
      "Iteration 51, loss = 1.25001924\n",
      "Iteration 52, loss = 1.24866064\n",
      "Iteration 53, loss = 1.24723815\n",
      "Iteration 54, loss = 1.24606178\n",
      "Iteration 55, loss = 1.24497233\n",
      "Iteration 56, loss = 1.24383770\n",
      "Iteration 57, loss = 1.24293594\n",
      "Iteration 58, loss = 1.24190465\n",
      "Iteration 59, loss = 1.24104417\n",
      "Iteration 60, loss = 1.24025526\n",
      "Iteration 61, loss = 1.23939920\n",
      "Iteration 62, loss = 1.23864806\n",
      "Iteration 63, loss = 1.23799134\n",
      "Iteration 64, loss = 1.23723466\n",
      "Iteration 65, loss = 1.23668620\n",
      "Iteration 66, loss = 1.23607037\n",
      "Iteration 67, loss = 1.23557245\n",
      "Iteration 68, loss = 1.23501524\n",
      "Iteration 69, loss = 1.23447115\n",
      "Iteration 70, loss = 1.23403706\n",
      "Iteration 71, loss = 1.23361336\n",
      "Iteration 72, loss = 1.23319004\n",
      "Iteration 73, loss = 1.23280241\n",
      "Iteration 74, loss = 1.23247772\n",
      "Iteration 75, loss = 1.23211461\n",
      "Iteration 76, loss = 1.23174581\n",
      "Iteration 77, loss = 1.23150911\n",
      "Iteration 78, loss = 1.23119953\n",
      "Iteration 79, loss = 1.23090386\n",
      "Iteration 80, loss = 1.23066723\n",
      "Iteration 81, loss = 1.23042034\n",
      "Iteration 82, loss = 1.23020084\n",
      "Iteration 83, loss = 1.22995980\n",
      "Iteration 84, loss = 1.22973216\n",
      "Iteration 85, loss = 1.22956732\n",
      "Iteration 86, loss = 1.22937098\n",
      "Iteration 87, loss = 1.22916586\n",
      "Iteration 88, loss = 1.22903458\n",
      "Iteration 89, loss = 1.22885340\n",
      "Iteration 90, loss = 1.22873559\n",
      "Iteration 91, loss = 1.22855132\n",
      "Iteration 92, loss = 1.22840331\n",
      "Iteration 93, loss = 1.22828775\n",
      "Iteration 94, loss = 1.22816199\n",
      "Iteration 95, loss = 1.22801567\n",
      "Iteration 96, loss = 1.22793570\n",
      "Iteration 97, loss = 1.22782687\n",
      "Iteration 98, loss = 1.22768033\n",
      "Iteration 99, loss = 1.22755787\n",
      "Iteration 100, loss = 1.22748188\n",
      "Iteration 1, loss = 1.33948443\n",
      "Iteration 2, loss = 1.31241464\n",
      "Iteration 3, loss = 1.29168546\n",
      "Iteration 4, loss = 1.27433702\n",
      "Iteration 5, loss = 1.26051179\n",
      "Iteration 6, loss = 1.24891792\n",
      "Iteration 7, loss = 1.24008979\n",
      "Iteration 8, loss = 1.23275602\n",
      "Iteration 9, loss = 1.22476949\n",
      "Iteration 10, loss = 1.21856277\n",
      "Iteration 11, loss = 1.21207658\n",
      "Iteration 12, loss = 1.20653120\n",
      "Iteration 13, loss = 1.19988652\n",
      "Iteration 14, loss = 1.19373419\n",
      "Iteration 15, loss = 1.18703430\n",
      "Iteration 16, loss = 1.18077126\n",
      "Iteration 17, loss = 1.17411152\n",
      "Iteration 18, loss = 1.16746510\n",
      "Iteration 19, loss = 1.16088752\n",
      "Iteration 20, loss = 1.15408976\n",
      "Iteration 21, loss = 1.14717946\n",
      "Iteration 22, loss = 1.14027624\n",
      "Iteration 23, loss = 1.13355618\n",
      "Iteration 24, loss = 1.12763985\n",
      "Iteration 25, loss = 1.12105692\n",
      "Iteration 26, loss = 1.11560642\n",
      "Iteration 27, loss = 1.11039068\n",
      "Iteration 28, loss = 1.10522998\n",
      "Iteration 29, loss = 1.10040500\n",
      "Iteration 30, loss = 1.09695518\n",
      "Iteration 31, loss = 1.09356974\n",
      "Iteration 32, loss = 1.08936116\n",
      "Iteration 33, loss = 1.08655125\n",
      "Iteration 34, loss = 1.08378872\n",
      "Iteration 35, loss = 1.08189332\n",
      "Iteration 36, loss = 1.07939540\n",
      "Iteration 37, loss = 1.07789913\n",
      "Iteration 38, loss = 1.07542555\n",
      "Iteration 39, loss = 1.07408988\n",
      "Iteration 40, loss = 1.07241397\n",
      "Iteration 41, loss = 1.07155110\n",
      "Iteration 42, loss = 1.06948015\n",
      "Iteration 43, loss = 1.06810372\n",
      "Iteration 44, loss = 1.06684724\n",
      "Iteration 45, loss = 1.06719359\n",
      "Iteration 46, loss = 1.06506110\n",
      "Iteration 47, loss = 1.06373658\n",
      "Iteration 48, loss = 1.06287738\n",
      "Iteration 49, loss = 1.06222449\n",
      "Iteration 50, loss = 1.06143802\n",
      "Iteration 51, loss = 1.06079305\n",
      "Iteration 52, loss = 1.06004148\n",
      "Iteration 53, loss = 1.05924813\n",
      "Iteration 54, loss = 1.05883325\n",
      "Iteration 55, loss = 1.05771044\n",
      "Iteration 56, loss = 1.05735417\n",
      "Iteration 57, loss = 1.05660951\n",
      "Iteration 58, loss = 1.05753804\n",
      "Iteration 59, loss = 1.05576491\n",
      "Iteration 60, loss = 1.05540227\n",
      "Iteration 61, loss = 1.05485099\n",
      "Iteration 62, loss = 1.05472009\n",
      "Iteration 63, loss = 1.05381971\n",
      "Iteration 64, loss = 1.05419005\n",
      "Iteration 65, loss = 1.05338212\n",
      "Iteration 66, loss = 1.05355713\n",
      "Iteration 67, loss = 1.05256806\n",
      "Iteration 68, loss = 1.05246083\n",
      "Iteration 69, loss = 1.05155194\n",
      "Iteration 70, loss = 1.05140775\n",
      "Iteration 71, loss = 1.05134731\n",
      "Iteration 72, loss = 1.05055783\n",
      "Iteration 73, loss = 1.05064517\n",
      "Iteration 74, loss = 1.04884861\n",
      "Iteration 75, loss = 1.04945085\n",
      "Iteration 76, loss = 1.04818702\n",
      "Iteration 77, loss = 1.04798195\n",
      "Iteration 78, loss = 1.04769924\n",
      "Iteration 79, loss = 1.04714419\n",
      "Iteration 80, loss = 1.04713736\n",
      "Iteration 81, loss = 1.04628216\n",
      "Iteration 82, loss = 1.04619696\n",
      "Iteration 83, loss = 1.04562381\n",
      "Iteration 84, loss = 1.04565280\n",
      "Iteration 85, loss = 1.04497756\n",
      "Iteration 86, loss = 1.04480464\n",
      "Iteration 87, loss = 1.04442924\n",
      "Iteration 88, loss = 1.04454213\n",
      "Iteration 89, loss = 1.04413789\n",
      "Iteration 90, loss = 1.04336903\n",
      "Iteration 91, loss = 1.04354646\n",
      "Iteration 92, loss = 1.04369603\n",
      "Iteration 93, loss = 1.04211234\n",
      "Iteration 94, loss = 1.04366690\n",
      "Iteration 95, loss = 1.04154590\n",
      "Iteration 96, loss = 1.04125591\n",
      "Iteration 97, loss = 1.04168108\n",
      "Iteration 98, loss = 1.04110547\n",
      "Iteration 99, loss = 1.04114295\n",
      "Iteration 100, loss = 1.04051831\n",
      "Iteration 1, loss = 1.34122765\n",
      "Iteration 2, loss = 1.31610879\n",
      "Iteration 3, loss = 1.29547945\n",
      "Iteration 4, loss = 1.27750853\n",
      "Iteration 5, loss = 1.26369587\n",
      "Iteration 6, loss = 1.25291999\n",
      "Iteration 7, loss = 1.24385042\n",
      "Iteration 8, loss = 1.23570807\n",
      "Iteration 9, loss = 1.22773067\n",
      "Iteration 10, loss = 1.22108128\n",
      "Iteration 11, loss = 1.21419279\n",
      "Iteration 12, loss = 1.20745171\n",
      "Iteration 13, loss = 1.20025456\n",
      "Iteration 14, loss = 1.19351939\n",
      "Iteration 15, loss = 1.18571622\n",
      "Iteration 16, loss = 1.17867634\n",
      "Iteration 17, loss = 1.17130019\n",
      "Iteration 18, loss = 1.16372798\n",
      "Iteration 19, loss = 1.15630615\n",
      "Iteration 20, loss = 1.14872977\n",
      "Iteration 21, loss = 1.14116564\n",
      "Iteration 22, loss = 1.13363284\n",
      "Iteration 23, loss = 1.12669801\n",
      "Iteration 24, loss = 1.11973961\n",
      "Iteration 25, loss = 1.11313573\n",
      "Iteration 26, loss = 1.10736821\n",
      "Iteration 27, loss = 1.10306831\n",
      "Iteration 28, loss = 1.09651268\n",
      "Iteration 29, loss = 1.09167847\n",
      "Iteration 30, loss = 1.08830303\n",
      "Iteration 31, loss = 1.08460825\n",
      "Iteration 32, loss = 1.08142539\n",
      "Iteration 33, loss = 1.07827920\n",
      "Iteration 34, loss = 1.07546064\n",
      "Iteration 35, loss = 1.07385103\n",
      "Iteration 36, loss = 1.07186480\n",
      "Iteration 37, loss = 1.06992649\n",
      "Iteration 38, loss = 1.06779825\n",
      "Iteration 39, loss = 1.06627728\n",
      "Iteration 40, loss = 1.06447459\n",
      "Iteration 41, loss = 1.06289184\n",
      "Iteration 42, loss = 1.06101956\n",
      "Iteration 43, loss = 1.05960620\n",
      "Iteration 44, loss = 1.05821478\n",
      "Iteration 45, loss = 1.05783613\n",
      "Iteration 46, loss = 1.05621160\n",
      "Iteration 47, loss = 1.05473531\n",
      "Iteration 48, loss = 1.05370073\n",
      "Iteration 49, loss = 1.05267478\n",
      "Iteration 50, loss = 1.05156389\n",
      "Iteration 51, loss = 1.05039414\n",
      "Iteration 52, loss = 1.04971064\n",
      "Iteration 53, loss = 1.04913871\n",
      "Iteration 54, loss = 1.04803335\n",
      "Iteration 55, loss = 1.04686366\n",
      "Iteration 56, loss = 1.04629634\n",
      "Iteration 57, loss = 1.04507386\n",
      "Iteration 58, loss = 1.04460675\n",
      "Iteration 59, loss = 1.04522921\n",
      "Iteration 60, loss = 1.04331369\n",
      "Iteration 61, loss = 1.04326706\n",
      "Iteration 62, loss = 1.04218714\n",
      "Iteration 63, loss = 1.04173891\n",
      "Iteration 64, loss = 1.04159358\n",
      "Iteration 65, loss = 1.04060402\n",
      "Iteration 66, loss = 1.04038202\n",
      "Iteration 67, loss = 1.03950972\n",
      "Iteration 68, loss = 1.03930034\n",
      "Iteration 69, loss = 1.03871052\n",
      "Iteration 70, loss = 1.03934401\n",
      "Iteration 71, loss = 1.03905697\n",
      "Iteration 72, loss = 1.03820601\n",
      "Iteration 73, loss = 1.03831295\n",
      "Iteration 74, loss = 1.03674787\n",
      "Iteration 75, loss = 1.03698465\n",
      "Iteration 76, loss = 1.03715678\n",
      "Iteration 77, loss = 1.03649521\n",
      "Iteration 78, loss = 1.03619774\n",
      "Iteration 79, loss = 1.03548201\n",
      "Iteration 80, loss = 1.03533613\n",
      "Iteration 81, loss = 1.03467691\n",
      "Iteration 82, loss = 1.03511006\n",
      "Iteration 83, loss = 1.03412341\n",
      "Iteration 84, loss = 1.03403487\n",
      "Iteration 85, loss = 1.03343957\n",
      "Iteration 86, loss = 1.03341413\n",
      "Iteration 87, loss = 1.03326336\n",
      "Iteration 88, loss = 1.03309067\n",
      "Iteration 89, loss = 1.03344264\n",
      "Iteration 90, loss = 1.03269493\n",
      "Iteration 91, loss = 1.03245140\n",
      "Iteration 92, loss = 1.03268036\n",
      "Iteration 93, loss = 1.03199586\n",
      "Iteration 94, loss = 1.03218394\n",
      "Iteration 95, loss = 1.03154682\n",
      "Iteration 96, loss = 1.03204171\n",
      "Iteration 97, loss = 1.03165044\n",
      "Iteration 98, loss = 1.03139615\n",
      "Iteration 99, loss = 1.03132190\n",
      "Iteration 100, loss = 1.03113237\n",
      "Iteration 1, loss = 1.33970900\n",
      "Iteration 2, loss = 1.31431485\n",
      "Iteration 3, loss = 1.29371846\n",
      "Iteration 4, loss = 1.27611603\n",
      "Iteration 5, loss = 1.26184785\n",
      "Iteration 6, loss = 1.25126963\n",
      "Iteration 7, loss = 1.24251504\n",
      "Iteration 8, loss = 1.23462349\n",
      "Iteration 9, loss = 1.22732451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 1.22093638\n",
      "Iteration 11, loss = 1.21411300\n",
      "Iteration 12, loss = 1.20757945\n",
      "Iteration 13, loss = 1.20087511\n",
      "Iteration 14, loss = 1.19377606\n",
      "Iteration 15, loss = 1.18598934\n",
      "Iteration 16, loss = 1.17823133\n",
      "Iteration 17, loss = 1.17124215\n",
      "Iteration 18, loss = 1.16346551\n",
      "Iteration 19, loss = 1.15540999\n",
      "Iteration 20, loss = 1.14750036\n",
      "Iteration 21, loss = 1.14008867\n",
      "Iteration 22, loss = 1.13146056\n",
      "Iteration 23, loss = 1.12405080\n",
      "Iteration 24, loss = 1.11753769\n",
      "Iteration 25, loss = 1.11082894\n",
      "Iteration 26, loss = 1.10452617\n",
      "Iteration 27, loss = 1.09921885\n",
      "Iteration 28, loss = 1.09448913\n",
      "Iteration 29, loss = 1.09029886\n",
      "Iteration 30, loss = 1.08689448\n",
      "Iteration 31, loss = 1.08388816\n",
      "Iteration 32, loss = 1.08050073\n",
      "Iteration 33, loss = 1.07775688\n",
      "Iteration 34, loss = 1.07571483\n",
      "Iteration 35, loss = 1.07422822\n",
      "Iteration 36, loss = 1.07214356\n",
      "Iteration 37, loss = 1.07062990\n",
      "Iteration 38, loss = 1.06892685\n",
      "Iteration 39, loss = 1.06767748\n",
      "Iteration 40, loss = 1.06720214\n",
      "Iteration 41, loss = 1.06541705\n",
      "Iteration 42, loss = 1.06493589\n",
      "Iteration 43, loss = 1.06396850\n",
      "Iteration 44, loss = 1.06265646\n",
      "Iteration 45, loss = 1.06200633\n",
      "Iteration 46, loss = 1.06093013\n",
      "Iteration 47, loss = 1.06095532\n",
      "Iteration 48, loss = 1.05945964\n",
      "Iteration 49, loss = 1.05905180\n",
      "Iteration 50, loss = 1.05812682\n",
      "Iteration 51, loss = 1.05774691\n",
      "Iteration 52, loss = 1.05689091\n",
      "Iteration 53, loss = 1.05632780\n",
      "Iteration 54, loss = 1.05600695\n",
      "Iteration 55, loss = 1.05500145\n",
      "Iteration 56, loss = 1.05439512\n",
      "Iteration 57, loss = 1.05351017\n",
      "Iteration 58, loss = 1.05276641\n",
      "Iteration 59, loss = 1.05283816\n",
      "Iteration 60, loss = 1.05201201\n",
      "Iteration 61, loss = 1.05107766\n",
      "Iteration 62, loss = 1.05051608\n",
      "Iteration 63, loss = 1.05022152\n",
      "Iteration 64, loss = 1.04996430\n",
      "Iteration 65, loss = 1.04923195\n",
      "Iteration 66, loss = 1.04872316\n",
      "Iteration 67, loss = 1.04794592\n",
      "Iteration 68, loss = 1.04792709\n",
      "Iteration 69, loss = 1.04780743\n",
      "Iteration 70, loss = 1.04717606\n",
      "Iteration 71, loss = 1.04744231\n",
      "Iteration 72, loss = 1.04608185\n",
      "Iteration 73, loss = 1.04560597\n",
      "Iteration 74, loss = 1.04560315\n",
      "Iteration 75, loss = 1.04614511\n",
      "Iteration 76, loss = 1.04808456\n",
      "Iteration 77, loss = 1.04587665\n",
      "Iteration 78, loss = 1.04427334\n",
      "Iteration 79, loss = 1.04561428\n",
      "Iteration 80, loss = 1.04392984\n",
      "Iteration 81, loss = 1.04384568\n",
      "Iteration 82, loss = 1.04381962\n",
      "Iteration 83, loss = 1.04311841\n",
      "Iteration 84, loss = 1.04296923\n",
      "Iteration 85, loss = 1.04276775\n",
      "Iteration 86, loss = 1.04240394\n",
      "Iteration 87, loss = 1.04222585\n",
      "Iteration 88, loss = 1.04193533\n",
      "Iteration 89, loss = 1.04270714\n",
      "Iteration 90, loss = 1.04127324\n",
      "Iteration 91, loss = 1.04111878\n",
      "Iteration 92, loss = 1.04140798\n",
      "Iteration 93, loss = 1.04077104\n",
      "Iteration 94, loss = 1.04076978\n",
      "Iteration 95, loss = 1.04132808\n",
      "Iteration 96, loss = 1.04039764\n",
      "Iteration 97, loss = 1.04028513\n",
      "Iteration 98, loss = 1.04007423\n",
      "Iteration 99, loss = 1.04039637\n",
      "Iteration 100, loss = 1.04047857\n",
      "Iteration 1, loss = 1.34076481\n",
      "Iteration 2, loss = 1.31669845\n",
      "Iteration 3, loss = 1.29705124\n",
      "Iteration 4, loss = 1.28107237\n",
      "Iteration 5, loss = 1.26590673\n",
      "Iteration 6, loss = 1.25531752\n",
      "Iteration 7, loss = 1.24725089\n",
      "Iteration 8, loss = 1.23950770\n",
      "Iteration 9, loss = 1.23215052\n",
      "Iteration 10, loss = 1.22648782\n",
      "Iteration 11, loss = 1.22035364\n",
      "Iteration 12, loss = 1.21374770\n",
      "Iteration 13, loss = 1.20710661\n",
      "Iteration 14, loss = 1.20098343\n",
      "Iteration 15, loss = 1.19351441\n",
      "Iteration 16, loss = 1.18610634\n",
      "Iteration 17, loss = 1.17908017\n",
      "Iteration 18, loss = 1.17233117\n",
      "Iteration 19, loss = 1.16441830\n",
      "Iteration 20, loss = 1.15653160\n",
      "Iteration 21, loss = 1.14956312\n",
      "Iteration 22, loss = 1.14190323\n",
      "Iteration 23, loss = 1.13539890\n",
      "Iteration 24, loss = 1.12871806\n",
      "Iteration 25, loss = 1.12246235\n",
      "Iteration 26, loss = 1.11742584\n",
      "Iteration 27, loss = 1.11325285\n",
      "Iteration 28, loss = 1.10802887\n",
      "Iteration 29, loss = 1.10391660\n",
      "Iteration 30, loss = 1.10127239\n",
      "Iteration 31, loss = 1.09828936\n",
      "Iteration 32, loss = 1.09572974\n",
      "Iteration 33, loss = 1.09319638\n",
      "Iteration 34, loss = 1.09091607\n",
      "Iteration 35, loss = 1.08923412\n",
      "Iteration 36, loss = 1.08815643\n",
      "Iteration 37, loss = 1.08614482\n",
      "Iteration 38, loss = 1.08433538\n",
      "Iteration 39, loss = 1.08284548\n",
      "Iteration 40, loss = 1.08152056\n",
      "Iteration 41, loss = 1.08042793\n",
      "Iteration 42, loss = 1.08039038\n",
      "Iteration 43, loss = 1.07856928\n",
      "Iteration 44, loss = 1.07748889\n",
      "Iteration 45, loss = 1.07807431\n",
      "Iteration 46, loss = 1.07570517\n",
      "Iteration 47, loss = 1.07494966\n",
      "Iteration 48, loss = 1.07470672\n",
      "Iteration 49, loss = 1.07380076\n",
      "Iteration 50, loss = 1.07288784\n",
      "Iteration 51, loss = 1.07252415\n",
      "Iteration 52, loss = 1.07201428\n",
      "Iteration 53, loss = 1.07132732\n",
      "Iteration 54, loss = 1.07037030\n",
      "Iteration 55, loss = 1.07017311\n",
      "Iteration 56, loss = 1.06953564\n",
      "Iteration 57, loss = 1.06863949\n",
      "Iteration 58, loss = 1.06822591\n",
      "Iteration 59, loss = 1.06860752\n",
      "Iteration 60, loss = 1.06817037\n",
      "Iteration 61, loss = 1.06786971\n",
      "Iteration 62, loss = 1.06628608\n",
      "Iteration 63, loss = 1.06656019\n",
      "Iteration 64, loss = 1.06584770\n",
      "Iteration 65, loss = 1.06495477\n",
      "Iteration 66, loss = 1.06469556\n",
      "Iteration 67, loss = 1.06444193\n",
      "Iteration 68, loss = 1.06382718\n",
      "Iteration 69, loss = 1.06364415\n",
      "Iteration 70, loss = 1.06346506\n",
      "Iteration 71, loss = 1.06373963\n",
      "Iteration 72, loss = 1.06255256\n",
      "Iteration 73, loss = 1.06242688\n",
      "Iteration 74, loss = 1.06282315\n",
      "Iteration 75, loss = 1.06158190\n",
      "Iteration 76, loss = 1.06328366\n",
      "Iteration 77, loss = 1.06441563\n",
      "Iteration 78, loss = 1.06145717\n",
      "Iteration 79, loss = 1.06151981\n",
      "Iteration 80, loss = 1.06084690\n",
      "Iteration 81, loss = 1.06065018\n",
      "Iteration 82, loss = 1.06063826\n",
      "Iteration 83, loss = 1.06037294\n",
      "Iteration 84, loss = 1.06066238\n",
      "Iteration 85, loss = 1.05963086\n",
      "Iteration 86, loss = 1.05951821\n",
      "Iteration 87, loss = 1.05972073\n",
      "Iteration 88, loss = 1.05900127\n",
      "Iteration 89, loss = 1.05946468\n",
      "Iteration 90, loss = 1.05864249\n",
      "Iteration 91, loss = 1.05913757\n",
      "Iteration 92, loss = 1.05859787\n",
      "Iteration 93, loss = 1.05821287\n",
      "Iteration 94, loss = 1.05822122\n",
      "Iteration 95, loss = 1.05887688\n",
      "Iteration 96, loss = 1.05824400\n",
      "Iteration 97, loss = 1.05782228\n",
      "Iteration 98, loss = 1.05774721\n",
      "Iteration 99, loss = 1.05741716\n",
      "Iteration 100, loss = 1.05762130\n",
      "Iteration 1, loss = 1.33782729\n",
      "Iteration 2, loss = 1.31302736\n",
      "Iteration 3, loss = 1.29197613\n",
      "Iteration 4, loss = 1.27415473\n",
      "Iteration 5, loss = 1.25996755\n",
      "Iteration 6, loss = 1.24851429\n",
      "Iteration 7, loss = 1.23883504\n",
      "Iteration 8, loss = 1.22963941\n",
      "Iteration 9, loss = 1.22097639\n",
      "Iteration 10, loss = 1.21313363\n",
      "Iteration 11, loss = 1.20527113\n",
      "Iteration 12, loss = 1.19713574\n",
      "Iteration 13, loss = 1.18837359\n",
      "Iteration 14, loss = 1.18009720\n",
      "Iteration 15, loss = 1.17109419\n",
      "Iteration 16, loss = 1.16196535\n",
      "Iteration 17, loss = 1.15277086\n",
      "Iteration 18, loss = 1.14324045\n",
      "Iteration 19, loss = 1.13370030\n",
      "Iteration 20, loss = 1.12574962\n",
      "Iteration 21, loss = 1.11585561\n",
      "Iteration 22, loss = 1.10773651\n",
      "Iteration 23, loss = 1.09957183\n",
      "Iteration 24, loss = 1.09269496\n",
      "Iteration 25, loss = 1.08654826\n",
      "Iteration 26, loss = 1.07985099\n",
      "Iteration 27, loss = 1.07494019\n",
      "Iteration 28, loss = 1.07040042\n",
      "Iteration 29, loss = 1.06698699\n",
      "Iteration 30, loss = 1.06310678\n",
      "Iteration 31, loss = 1.06057858\n",
      "Iteration 32, loss = 1.05749187\n",
      "Iteration 33, loss = 1.05491803\n",
      "Iteration 34, loss = 1.05307740\n",
      "Iteration 35, loss = 1.05119728\n",
      "Iteration 36, loss = 1.04944919\n",
      "Iteration 37, loss = 1.04751496\n",
      "Iteration 38, loss = 1.04582058\n",
      "Iteration 39, loss = 1.04590071\n",
      "Iteration 40, loss = 1.04477328\n",
      "Iteration 41, loss = 1.04229861\n",
      "Iteration 42, loss = 1.04050462\n",
      "Iteration 43, loss = 1.03957741\n",
      "Iteration 44, loss = 1.03881140\n",
      "Iteration 45, loss = 1.03713930\n",
      "Iteration 46, loss = 1.03609061\n",
      "Iteration 47, loss = 1.03536816\n",
      "Iteration 48, loss = 1.03423606\n",
      "Iteration 49, loss = 1.03396380\n",
      "Iteration 50, loss = 1.03261959\n",
      "Iteration 51, loss = 1.03153029\n",
      "Iteration 52, loss = 1.03122283\n",
      "Iteration 53, loss = 1.03017736\n",
      "Iteration 54, loss = 1.02909763\n",
      "Iteration 55, loss = 1.02798346\n",
      "Iteration 56, loss = 1.02779404\n",
      "Iteration 57, loss = 1.02653176\n",
      "Iteration 58, loss = 1.02570423\n",
      "Iteration 59, loss = 1.02475379\n",
      "Iteration 60, loss = 1.02505702\n",
      "Iteration 61, loss = 1.02340582\n",
      "Iteration 62, loss = 1.02326527\n",
      "Iteration 63, loss = 1.02289088\n",
      "Iteration 64, loss = 1.02191239\n",
      "Iteration 65, loss = 1.02279277\n",
      "Iteration 66, loss = 1.02132420\n",
      "Iteration 67, loss = 1.02034522\n",
      "Iteration 68, loss = 1.02023915\n",
      "Iteration 69, loss = 1.02068010\n",
      "Iteration 70, loss = 1.01977075\n",
      "Iteration 71, loss = 1.01939884\n",
      "Iteration 72, loss = 1.01841742\n",
      "Iteration 73, loss = 1.01818977\n",
      "Iteration 74, loss = 1.01786634\n",
      "Iteration 75, loss = 1.01818266\n",
      "Iteration 76, loss = 1.01720790\n",
      "Iteration 77, loss = 1.01657391\n",
      "Iteration 78, loss = 1.01620163\n",
      "Iteration 79, loss = 1.01597231\n",
      "Iteration 80, loss = 1.01571694\n",
      "Iteration 81, loss = 1.01530353\n",
      "Iteration 82, loss = 1.01608691\n",
      "Iteration 83, loss = 1.01458815\n",
      "Iteration 84, loss = 1.01563632\n",
      "Iteration 85, loss = 1.01442071\n",
      "Iteration 86, loss = 1.01456697\n",
      "Iteration 87, loss = 1.01417211\n",
      "Iteration 88, loss = 1.01406343\n",
      "Iteration 89, loss = 1.01393675\n",
      "Iteration 90, loss = 1.01325773\n",
      "Iteration 91, loss = 1.01390465\n",
      "Iteration 92, loss = 1.01295875\n",
      "Iteration 93, loss = 1.01240071\n",
      "Iteration 94, loss = 1.01225699\n",
      "Iteration 95, loss = 1.01223600\n",
      "Iteration 96, loss = 1.01202534\n",
      "Iteration 97, loss = 1.01264155\n",
      "Iteration 98, loss = 1.01159865\n",
      "Iteration 99, loss = 1.01147517\n",
      "Iteration 100, loss = 1.01198994\n",
      "Iteration 1, loss = 11.70100718\n",
      "Iteration 2, loss = 7.35644379\n",
      "Iteration 3, loss = 3.93795657\n",
      "Iteration 4, loss = 2.01601494\n",
      "Iteration 5, loss = 1.49947701\n",
      "Iteration 6, loss = 1.41314272\n",
      "Iteration 7, loss = 1.37860453\n",
      "Iteration 8, loss = 1.36469800\n",
      "Iteration 9, loss = 1.34989428\n",
      "Iteration 10, loss = 1.33555317\n",
      "Iteration 11, loss = 1.32043126\n",
      "Iteration 12, loss = 1.30685053\n",
      "Iteration 13, loss = 1.29350132\n",
      "Iteration 14, loss = 1.28216781\n",
      "Iteration 15, loss = 1.27259942\n",
      "Iteration 16, loss = 1.26389795\n",
      "Iteration 17, loss = 1.25613651\n",
      "Iteration 18, loss = 1.24977852\n",
      "Iteration 19, loss = 1.24450971\n",
      "Iteration 20, loss = 1.24034198\n",
      "Iteration 21, loss = 1.23660145\n",
      "Iteration 22, loss = 1.23332921\n",
      "Iteration 23, loss = 1.23056099\n",
      "Iteration 24, loss = 1.22864304\n",
      "Iteration 25, loss = 1.22671800\n",
      "Iteration 26, loss = 1.22526233\n",
      "Iteration 27, loss = 1.22401687\n",
      "Iteration 28, loss = 1.22314114\n",
      "Iteration 29, loss = 1.22222619\n",
      "Iteration 30, loss = 1.22150153\n",
      "Iteration 31, loss = 1.22080426\n",
      "Iteration 32, loss = 1.22023901\n",
      "Iteration 33, loss = 1.21969136\n",
      "Iteration 34, loss = 1.21919087\n",
      "Iteration 35, loss = 1.21862897\n",
      "Iteration 36, loss = 1.21817949\n",
      "Iteration 37, loss = 1.21767968\n",
      "Iteration 38, loss = 1.21722643\n",
      "Iteration 39, loss = 1.21671887\n",
      "Iteration 40, loss = 1.21628908\n",
      "Iteration 41, loss = 1.21581475\n",
      "Iteration 42, loss = 1.21542163\n",
      "Iteration 43, loss = 1.21492135\n",
      "Iteration 44, loss = 1.21459070\n",
      "Iteration 45, loss = 1.21428676\n",
      "Iteration 46, loss = 1.21389550\n",
      "Iteration 47, loss = 1.21360037\n",
      "Iteration 48, loss = 1.21319912\n",
      "Iteration 49, loss = 1.21288035\n",
      "Iteration 50, loss = 1.21251802\n",
      "Iteration 51, loss = 1.21217300\n",
      "Iteration 52, loss = 1.21190038\n",
      "Iteration 53, loss = 1.21151420\n",
      "Iteration 54, loss = 1.21123386\n",
      "Iteration 55, loss = 1.21090393\n",
      "Iteration 56, loss = 1.21052177\n",
      "Iteration 57, loss = 1.21019738\n",
      "Iteration 58, loss = 1.20965006\n",
      "Iteration 59, loss = 1.20904213\n",
      "Iteration 60, loss = 1.20837621\n",
      "Iteration 61, loss = 1.20782035\n",
      "Iteration 62, loss = 1.20686903\n",
      "Iteration 63, loss = 1.20612395\n",
      "Iteration 64, loss = 1.20527727\n",
      "Iteration 65, loss = 1.20432376\n",
      "Iteration 66, loss = 1.20385105\n",
      "Iteration 67, loss = 1.20319414\n",
      "Iteration 68, loss = 1.20292074\n",
      "Iteration 69, loss = 1.20237246\n",
      "Iteration 70, loss = 1.20202680\n",
      "Iteration 71, loss = 1.20163434\n",
      "Iteration 72, loss = 1.20140562\n",
      "Iteration 73, loss = 1.20112109\n",
      "Iteration 74, loss = 1.20090566\n",
      "Iteration 75, loss = 1.20039926\n",
      "Iteration 76, loss = 1.20015286\n",
      "Iteration 77, loss = 1.19987431\n",
      "Iteration 78, loss = 1.19966369\n",
      "Iteration 79, loss = 1.19945558\n",
      "Iteration 80, loss = 1.19916919\n",
      "Iteration 81, loss = 1.19895196\n",
      "Iteration 82, loss = 1.19895600\n",
      "Iteration 83, loss = 1.19855561\n",
      "Iteration 84, loss = 1.19835443\n",
      "Iteration 85, loss = 1.19823112\n",
      "Iteration 86, loss = 1.19783753\n",
      "Iteration 87, loss = 1.19772051\n",
      "Iteration 88, loss = 1.19754076\n",
      "Iteration 89, loss = 1.19746431\n",
      "Iteration 90, loss = 1.19734586\n",
      "Iteration 91, loss = 1.19715589\n",
      "Iteration 92, loss = 1.19714163\n",
      "Iteration 93, loss = 1.19702674\n",
      "Iteration 94, loss = 1.19675194\n",
      "Iteration 95, loss = 1.19680584\n",
      "Iteration 96, loss = 1.19654808\n",
      "Iteration 97, loss = 1.19661041\n",
      "Iteration 98, loss = 1.19673517\n",
      "Iteration 99, loss = 1.19635022\n",
      "Iteration 100, loss = 1.19627529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.80646278\n",
      "Iteration 2, loss = 1.67621095\n",
      "Iteration 3, loss = 1.55074378\n",
      "Iteration 4, loss = 1.43151241\n",
      "Iteration 5, loss = 1.33591841\n",
      "Iteration 6, loss = 1.27096587\n",
      "Iteration 7, loss = 1.24169818\n",
      "Iteration 8, loss = 1.22554295\n",
      "Iteration 9, loss = 1.21120260\n",
      "Iteration 10, loss = 1.20238729\n",
      "Iteration 11, loss = 1.19591719\n",
      "Iteration 12, loss = 1.18723824\n",
      "Iteration 13, loss = 1.17760742\n",
      "Iteration 14, loss = 1.17009501\n",
      "Iteration 15, loss = 1.16520600\n",
      "Iteration 16, loss = 1.15774556\n",
      "Iteration 17, loss = 1.15183559\n",
      "Iteration 18, loss = 1.14551507\n",
      "Iteration 19, loss = 1.13980406\n",
      "Iteration 20, loss = 1.13494957\n",
      "Iteration 21, loss = 1.12962116\n",
      "Iteration 22, loss = 1.12492841\n",
      "Iteration 23, loss = 1.12026297\n",
      "Iteration 24, loss = 1.11619068\n",
      "Iteration 25, loss = 1.11263166\n",
      "Iteration 26, loss = 1.10843756\n",
      "Iteration 27, loss = 1.10474287\n",
      "Iteration 28, loss = 1.10165541\n",
      "Iteration 29, loss = 1.09850483\n",
      "Iteration 30, loss = 1.09506018\n",
      "Iteration 31, loss = 1.09164499\n",
      "Iteration 32, loss = 1.08855640\n",
      "Iteration 33, loss = 1.08597886\n",
      "Iteration 34, loss = 1.08163019\n",
      "Iteration 35, loss = 1.07797739\n",
      "Iteration 36, loss = 1.07465684\n",
      "Iteration 37, loss = 1.07143562\n",
      "Iteration 38, loss = 1.06889010\n",
      "Iteration 39, loss = 1.06558785\n",
      "Iteration 40, loss = 1.06449478\n",
      "Iteration 41, loss = 1.06180840\n",
      "Iteration 42, loss = 1.05963448\n",
      "Iteration 43, loss = 1.05849810\n",
      "Iteration 44, loss = 1.05538851\n",
      "Iteration 45, loss = 1.05351786\n",
      "Iteration 46, loss = 1.05069642\n",
      "Iteration 47, loss = 1.04945666\n",
      "Iteration 48, loss = 1.04733933\n",
      "Iteration 49, loss = 1.04575777\n",
      "Iteration 50, loss = 1.04430356\n",
      "Iteration 51, loss = 1.04244705\n",
      "Iteration 52, loss = 1.04238454\n",
      "Iteration 53, loss = 1.03988830\n",
      "Iteration 54, loss = 1.03871935\n",
      "Iteration 55, loss = 1.03722149\n",
      "Iteration 56, loss = 1.03603639\n",
      "Iteration 57, loss = 1.03510381\n",
      "Iteration 58, loss = 1.03399144\n",
      "Iteration 59, loss = 1.03025229\n",
      "Iteration 60, loss = 1.03001995\n",
      "Iteration 61, loss = 1.02588629\n",
      "Iteration 62, loss = 1.02543857\n",
      "Iteration 63, loss = 1.02330858\n",
      "Iteration 64, loss = 1.02109406\n",
      "Iteration 65, loss = 1.02173029\n",
      "Iteration 66, loss = 1.02026075\n",
      "Iteration 67, loss = 1.01930293\n",
      "Iteration 68, loss = 1.01593264\n",
      "Iteration 69, loss = 1.01503756\n",
      "Iteration 70, loss = 1.01413701\n",
      "Iteration 71, loss = 1.01330494\n",
      "Iteration 72, loss = 1.01040390\n",
      "Iteration 73, loss = 1.01049892\n",
      "Iteration 74, loss = 1.00888597\n",
      "Iteration 75, loss = 1.00609459\n",
      "Iteration 76, loss = 1.00640507\n",
      "Iteration 77, loss = 1.00477988\n",
      "Iteration 78, loss = 1.00309719\n",
      "Iteration 79, loss = 1.00331442\n",
      "Iteration 80, loss = 1.00460292\n",
      "Iteration 81, loss = 1.00283743\n",
      "Iteration 82, loss = 1.00031063\n",
      "Iteration 83, loss = 1.00009520\n",
      "Iteration 84, loss = 0.99900253\n",
      "Iteration 85, loss = 0.99794146\n",
      "Iteration 86, loss = 0.99611992\n",
      "Iteration 87, loss = 0.99648097\n",
      "Iteration 88, loss = 0.99657828\n",
      "Iteration 89, loss = 0.99539207\n",
      "Iteration 90, loss = 0.99327522\n",
      "Iteration 91, loss = 0.99264049\n",
      "Iteration 92, loss = 0.99193707\n",
      "Iteration 93, loss = 0.99169975\n",
      "Iteration 94, loss = 0.99162464\n",
      "Iteration 95, loss = 0.98975831\n",
      "Iteration 96, loss = 0.98977282\n",
      "Iteration 97, loss = 0.98881536\n",
      "Iteration 98, loss = 0.98762410\n",
      "Iteration 99, loss = 0.98864726\n",
      "Iteration 100, loss = 0.98765180\n",
      "Iteration 1, loss = 1.80804927\n",
      "Iteration 2, loss = 1.67842046\n",
      "Iteration 3, loss = 1.54999175\n",
      "Iteration 4, loss = 1.43004801\n",
      "Iteration 5, loss = 1.33030124\n",
      "Iteration 6, loss = 1.26048433\n",
      "Iteration 7, loss = 1.23248875\n",
      "Iteration 8, loss = 1.21679041\n",
      "Iteration 9, loss = 1.20332311\n",
      "Iteration 10, loss = 1.19380752\n",
      "Iteration 11, loss = 1.18737537\n",
      "Iteration 12, loss = 1.17994813\n",
      "Iteration 13, loss = 1.17067024\n",
      "Iteration 14, loss = 1.16299319\n",
      "Iteration 15, loss = 1.15759873\n",
      "Iteration 16, loss = 1.15114576\n",
      "Iteration 17, loss = 1.14524039\n",
      "Iteration 18, loss = 1.13942284\n",
      "Iteration 19, loss = 1.13400482\n",
      "Iteration 20, loss = 1.12806295\n",
      "Iteration 21, loss = 1.12443205\n",
      "Iteration 22, loss = 1.11885120\n",
      "Iteration 23, loss = 1.11501499\n",
      "Iteration 24, loss = 1.11079180\n",
      "Iteration 25, loss = 1.10561815\n",
      "Iteration 26, loss = 1.10334153\n",
      "Iteration 27, loss = 1.09865274\n",
      "Iteration 28, loss = 1.09483850\n",
      "Iteration 29, loss = 1.09169667\n",
      "Iteration 30, loss = 1.08786987\n",
      "Iteration 31, loss = 1.08491132\n",
      "Iteration 32, loss = 1.08101145\n",
      "Iteration 33, loss = 1.08000782\n",
      "Iteration 34, loss = 1.07557904\n",
      "Iteration 35, loss = 1.07388231\n",
      "Iteration 36, loss = 1.06967768\n",
      "Iteration 37, loss = 1.06515654\n",
      "Iteration 38, loss = 1.06295747\n",
      "Iteration 39, loss = 1.05944629\n",
      "Iteration 40, loss = 1.05692346\n",
      "Iteration 41, loss = 1.05498896\n",
      "Iteration 42, loss = 1.05219358\n",
      "Iteration 43, loss = 1.05015543\n",
      "Iteration 44, loss = 1.04792789\n",
      "Iteration 45, loss = 1.04902205\n",
      "Iteration 46, loss = 1.04471406\n",
      "Iteration 47, loss = 1.04256450\n",
      "Iteration 48, loss = 1.03987146\n",
      "Iteration 49, loss = 1.03886690\n",
      "Iteration 50, loss = 1.03659769\n",
      "Iteration 51, loss = 1.03532496\n",
      "Iteration 52, loss = 1.03459915\n",
      "Iteration 53, loss = 1.03293732\n",
      "Iteration 54, loss = 1.03164096\n",
      "Iteration 55, loss = 1.03051342\n",
      "Iteration 56, loss = 1.02916947\n",
      "Iteration 57, loss = 1.02764348\n",
      "Iteration 58, loss = 1.02672091\n",
      "Iteration 59, loss = 1.02508116\n",
      "Iteration 60, loss = 1.02495306\n",
      "Iteration 61, loss = 1.02443341\n",
      "Iteration 62, loss = 1.02415800\n",
      "Iteration 63, loss = 1.02294968\n",
      "Iteration 64, loss = 1.01979524\n",
      "Iteration 65, loss = 1.02093690\n",
      "Iteration 66, loss = 1.02220762\n",
      "Iteration 67, loss = 1.01895980\n",
      "Iteration 68, loss = 1.01720263\n",
      "Iteration 69, loss = 1.01796060\n",
      "Iteration 70, loss = 1.01442380\n",
      "Iteration 71, loss = 1.01741671\n",
      "Iteration 72, loss = 1.01347932\n",
      "Iteration 73, loss = 1.01291187\n",
      "Iteration 74, loss = 1.01221882\n",
      "Iteration 75, loss = 1.01087923\n",
      "Iteration 76, loss = 1.01113621\n",
      "Iteration 77, loss = 1.01066307\n",
      "Iteration 78, loss = 1.00958652\n",
      "Iteration 79, loss = 1.01042280\n",
      "Iteration 80, loss = 1.01022401\n",
      "Iteration 81, loss = 1.00901250\n",
      "Iteration 82, loss = 1.00793806\n",
      "Iteration 83, loss = 1.00708448\n",
      "Iteration 84, loss = 1.00545880\n",
      "Iteration 85, loss = 1.00566266\n",
      "Iteration 86, loss = 1.00637599\n",
      "Iteration 87, loss = 1.00469562\n",
      "Iteration 88, loss = 1.00438103\n",
      "Iteration 89, loss = 1.00355675\n",
      "Iteration 90, loss = 1.00337406\n",
      "Iteration 91, loss = 1.00207891\n",
      "Iteration 92, loss = 1.00190662\n",
      "Iteration 93, loss = 1.00111652\n",
      "Iteration 94, loss = 1.00067449\n",
      "Iteration 95, loss = 1.00022109\n",
      "Iteration 96, loss = 0.99978075\n",
      "Iteration 97, loss = 0.99934374\n",
      "Iteration 98, loss = 0.99898237\n",
      "Iteration 99, loss = 1.00004076\n",
      "Iteration 100, loss = 0.99972664\n",
      "Iteration 1, loss = 1.80436948\n",
      "Iteration 2, loss = 1.67385297\n",
      "Iteration 3, loss = 1.54424324\n",
      "Iteration 4, loss = 1.43049162\n",
      "Iteration 5, loss = 1.33311473\n",
      "Iteration 6, loss = 1.26893351\n",
      "Iteration 7, loss = 1.24629288\n",
      "Iteration 8, loss = 1.23132307\n",
      "Iteration 9, loss = 1.21834502\n",
      "Iteration 10, loss = 1.20989784\n",
      "Iteration 11, loss = 1.20496141\n",
      "Iteration 12, loss = 1.20258882\n",
      "Iteration 13, loss = 1.19742664\n",
      "Iteration 14, loss = 1.19241297\n",
      "Iteration 15, loss = 1.18737733\n",
      "Iteration 16, loss = 1.18318523\n",
      "Iteration 17, loss = 1.18018217\n",
      "Iteration 18, loss = 1.17463087\n",
      "Iteration 19, loss = 1.16999312\n",
      "Iteration 20, loss = 1.16630901\n",
      "Iteration 21, loss = 1.16176079\n",
      "Iteration 22, loss = 1.15716761\n",
      "Iteration 23, loss = 1.15308786\n",
      "Iteration 24, loss = 1.14889847\n",
      "Iteration 25, loss = 1.14474299\n",
      "Iteration 26, loss = 1.14123113\n",
      "Iteration 27, loss = 1.13743623\n",
      "Iteration 28, loss = 1.13380244\n",
      "Iteration 29, loss = 1.13174605\n",
      "Iteration 30, loss = 1.12767333\n",
      "Iteration 31, loss = 1.12397531\n",
      "Iteration 32, loss = 1.12093695\n",
      "Iteration 33, loss = 1.11937918\n",
      "Iteration 34, loss = 1.11560319\n",
      "Iteration 35, loss = 1.11287039\n",
      "Iteration 36, loss = 1.11024492\n",
      "Iteration 37, loss = 1.10751707\n",
      "Iteration 38, loss = 1.10537867\n",
      "Iteration 39, loss = 1.10295679\n",
      "Iteration 40, loss = 1.10116856\n",
      "Iteration 41, loss = 1.09795531\n",
      "Iteration 42, loss = 1.09619882\n",
      "Iteration 43, loss = 1.09396254\n",
      "Iteration 44, loss = 1.09190755\n",
      "Iteration 45, loss = 1.09113700\n",
      "Iteration 46, loss = 1.08858306\n",
      "Iteration 47, loss = 1.08586049\n",
      "Iteration 48, loss = 1.08392255\n",
      "Iteration 49, loss = 1.08152805\n",
      "Iteration 50, loss = 1.08041657\n",
      "Iteration 51, loss = 1.07749275\n",
      "Iteration 52, loss = 1.07567345\n",
      "Iteration 53, loss = 1.07058560\n",
      "Iteration 54, loss = 1.06676576\n",
      "Iteration 55, loss = 1.06316919\n",
      "Iteration 56, loss = 1.06096237\n",
      "Iteration 57, loss = 1.05636325\n",
      "Iteration 58, loss = 1.05534768\n",
      "Iteration 59, loss = 1.04971858\n",
      "Iteration 60, loss = 1.04758510\n",
      "Iteration 61, loss = 1.04429039\n",
      "Iteration 62, loss = 1.04133353\n",
      "Iteration 63, loss = 1.03833638\n",
      "Iteration 64, loss = 1.03664302\n",
      "Iteration 65, loss = 1.03363205\n",
      "Iteration 66, loss = 1.03381521\n",
      "Iteration 67, loss = 1.02866765\n",
      "Iteration 68, loss = 1.02769423\n",
      "Iteration 69, loss = 1.02615580\n",
      "Iteration 70, loss = 1.02517178\n",
      "Iteration 71, loss = 1.02451373\n",
      "Iteration 72, loss = 1.02145115\n",
      "Iteration 73, loss = 1.01949629\n",
      "Iteration 74, loss = 1.01897266\n",
      "Iteration 75, loss = 1.01693873\n",
      "Iteration 76, loss = 1.01655829\n",
      "Iteration 77, loss = 1.01594273\n",
      "Iteration 78, loss = 1.01770127\n",
      "Iteration 79, loss = 1.01479204\n",
      "Iteration 80, loss = 1.01222471\n",
      "Iteration 81, loss = 1.01169081\n",
      "Iteration 82, loss = 1.01007449\n",
      "Iteration 83, loss = 1.00979067\n",
      "Iteration 84, loss = 1.00840572\n",
      "Iteration 85, loss = 1.00849562\n",
      "Iteration 86, loss = 1.00900205\n",
      "Iteration 87, loss = 1.00762719\n",
      "Iteration 88, loss = 1.00745438\n",
      "Iteration 89, loss = 1.00612406\n",
      "Iteration 90, loss = 1.00511191\n",
      "Iteration 91, loss = 1.00524566\n",
      "Iteration 92, loss = 1.00395175\n",
      "Iteration 93, loss = 1.00378593\n",
      "Iteration 94, loss = 1.00370971\n",
      "Iteration 95, loss = 1.00308816\n",
      "Iteration 96, loss = 1.00228542\n",
      "Iteration 97, loss = 1.00225026\n",
      "Iteration 98, loss = 1.00160096\n",
      "Iteration 99, loss = 1.00305355\n",
      "Iteration 100, loss = 1.00296687\n",
      "Iteration 1, loss = 1.80660179\n",
      "Iteration 2, loss = 1.67674896\n",
      "Iteration 3, loss = 1.54656663\n",
      "Iteration 4, loss = 1.42982318\n",
      "Iteration 5, loss = 1.33537896\n",
      "Iteration 6, loss = 1.27428349\n",
      "Iteration 7, loss = 1.25208776\n",
      "Iteration 8, loss = 1.23960696\n",
      "Iteration 9, loss = 1.22654994\n",
      "Iteration 10, loss = 1.21843306\n",
      "Iteration 11, loss = 1.21357328\n",
      "Iteration 12, loss = 1.21261133\n",
      "Iteration 13, loss = 1.20745795\n",
      "Iteration 14, loss = 1.20281608\n",
      "Iteration 15, loss = 1.19951771\n",
      "Iteration 16, loss = 1.19564515\n",
      "Iteration 17, loss = 1.19277632\n",
      "Iteration 18, loss = 1.18913280\n",
      "Iteration 19, loss = 1.18511016\n",
      "Iteration 20, loss = 1.18177276\n",
      "Iteration 21, loss = 1.17759113\n",
      "Iteration 22, loss = 1.17363772\n",
      "Iteration 23, loss = 1.16981555\n",
      "Iteration 24, loss = 1.16572563\n",
      "Iteration 25, loss = 1.16143926\n",
      "Iteration 26, loss = 1.15688508\n",
      "Iteration 27, loss = 1.15245549\n",
      "Iteration 28, loss = 1.14691124\n",
      "Iteration 29, loss = 1.14284153\n",
      "Iteration 30, loss = 1.13791686\n",
      "Iteration 31, loss = 1.13268443\n",
      "Iteration 32, loss = 1.12829907\n",
      "Iteration 33, loss = 1.12401921\n",
      "Iteration 34, loss = 1.11912618\n",
      "Iteration 35, loss = 1.11608546\n",
      "Iteration 36, loss = 1.11112335\n",
      "Iteration 37, loss = 1.10611011\n",
      "Iteration 38, loss = 1.10269566\n",
      "Iteration 39, loss = 1.09696301\n",
      "Iteration 40, loss = 1.09501670\n",
      "Iteration 41, loss = 1.08623333\n",
      "Iteration 42, loss = 1.08511545\n",
      "Iteration 43, loss = 1.07942427\n",
      "Iteration 44, loss = 1.07729882\n",
      "Iteration 45, loss = 1.07347837\n",
      "Iteration 46, loss = 1.07107032\n",
      "Iteration 47, loss = 1.06894574\n",
      "Iteration 48, loss = 1.06573123\n",
      "Iteration 49, loss = 1.06176512\n",
      "Iteration 50, loss = 1.06137175\n",
      "Iteration 51, loss = 1.06011020\n",
      "Iteration 52, loss = 1.05807320\n",
      "Iteration 53, loss = 1.05516172\n",
      "Iteration 54, loss = 1.05250992\n",
      "Iteration 55, loss = 1.05103162\n",
      "Iteration 56, loss = 1.04791412\n",
      "Iteration 57, loss = 1.04804087\n",
      "Iteration 58, loss = 1.04582142\n",
      "Iteration 59, loss = 1.04446856\n",
      "Iteration 60, loss = 1.04244822\n",
      "Iteration 61, loss = 1.04142029\n",
      "Iteration 62, loss = 1.04090149\n",
      "Iteration 63, loss = 1.03874440\n",
      "Iteration 64, loss = 1.03759501\n",
      "Iteration 65, loss = 1.03611461\n",
      "Iteration 66, loss = 1.03593192\n",
      "Iteration 67, loss = 1.03594969\n",
      "Iteration 68, loss = 1.03270390\n",
      "Iteration 69, loss = 1.03237362\n",
      "Iteration 70, loss = 1.03072971\n",
      "Iteration 71, loss = 1.03055175\n",
      "Iteration 72, loss = 1.02869187\n",
      "Iteration 73, loss = 1.02885345\n",
      "Iteration 74, loss = 1.02910545\n",
      "Iteration 75, loss = 1.02688987\n",
      "Iteration 76, loss = 1.02553054\n",
      "Iteration 77, loss = 1.02659630\n",
      "Iteration 78, loss = 1.02544249\n",
      "Iteration 79, loss = 1.02469588\n",
      "Iteration 80, loss = 1.02374550\n",
      "Iteration 81, loss = 1.02268918\n",
      "Iteration 82, loss = 1.02177550\n",
      "Iteration 83, loss = 1.02174232\n",
      "Iteration 84, loss = 1.02096831\n",
      "Iteration 85, loss = 1.01993204\n",
      "Iteration 86, loss = 1.02107994\n",
      "Iteration 87, loss = 1.01941632\n",
      "Iteration 88, loss = 1.01932163\n",
      "Iteration 89, loss = 1.01834638\n",
      "Iteration 90, loss = 1.01725748\n",
      "Iteration 91, loss = 1.01695926\n",
      "Iteration 92, loss = 1.01693085\n",
      "Iteration 93, loss = 1.01541735\n",
      "Iteration 94, loss = 1.01449997\n",
      "Iteration 95, loss = 1.01414464\n",
      "Iteration 96, loss = 1.01409451\n",
      "Iteration 97, loss = 1.01343212\n",
      "Iteration 98, loss = 1.01347201\n",
      "Iteration 99, loss = 1.01400442\n",
      "Iteration 100, loss = 1.01314681\n",
      "Iteration 1, loss = 1.80467951\n",
      "Iteration 2, loss = 1.67332704\n",
      "Iteration 3, loss = 1.54645208\n",
      "Iteration 4, loss = 1.43040330\n",
      "Iteration 5, loss = 1.33552632\n",
      "Iteration 6, loss = 1.26740192\n",
      "Iteration 7, loss = 1.23850314\n",
      "Iteration 8, loss = 1.21969735\n",
      "Iteration 9, loss = 1.20524096\n",
      "Iteration 10, loss = 1.19482197\n",
      "Iteration 11, loss = 1.18645272\n",
      "Iteration 12, loss = 1.17823933\n",
      "Iteration 13, loss = 1.16969803\n",
      "Iteration 14, loss = 1.16066837\n",
      "Iteration 15, loss = 1.15349219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 1.14654056\n",
      "Iteration 17, loss = 1.14055118\n",
      "Iteration 18, loss = 1.13258384\n",
      "Iteration 19, loss = 1.12742510\n",
      "Iteration 20, loss = 1.12071864\n",
      "Iteration 21, loss = 1.11487629\n",
      "Iteration 22, loss = 1.10930178\n",
      "Iteration 23, loss = 1.10484335\n",
      "Iteration 24, loss = 1.09997217\n",
      "Iteration 25, loss = 1.09493668\n",
      "Iteration 26, loss = 1.09084000\n",
      "Iteration 27, loss = 1.08752475\n",
      "Iteration 28, loss = 1.08282743\n",
      "Iteration 29, loss = 1.07952876\n",
      "Iteration 30, loss = 1.07650232\n",
      "Iteration 31, loss = 1.07266404\n",
      "Iteration 32, loss = 1.07012454\n",
      "Iteration 33, loss = 1.06682483\n",
      "Iteration 34, loss = 1.06435386\n",
      "Iteration 35, loss = 1.06132010\n",
      "Iteration 36, loss = 1.05879010\n",
      "Iteration 37, loss = 1.05523990\n",
      "Iteration 38, loss = 1.05362681\n",
      "Iteration 39, loss = 1.05017010\n",
      "Iteration 40, loss = 1.04746145\n",
      "Iteration 41, loss = 1.04597743\n",
      "Iteration 42, loss = 1.04254786\n",
      "Iteration 43, loss = 1.04050736\n",
      "Iteration 44, loss = 1.03844350\n",
      "Iteration 45, loss = 1.03618479\n",
      "Iteration 46, loss = 1.03378287\n",
      "Iteration 47, loss = 1.03217852\n",
      "Iteration 48, loss = 1.03021957\n",
      "Iteration 49, loss = 1.02850027\n",
      "Iteration 50, loss = 1.02738526\n",
      "Iteration 51, loss = 1.02531485\n",
      "Iteration 52, loss = 1.02417618\n",
      "Iteration 53, loss = 1.02217767\n",
      "Iteration 54, loss = 1.02165889\n",
      "Iteration 55, loss = 1.02018542\n",
      "Iteration 56, loss = 1.01971830\n",
      "Iteration 57, loss = 1.01853934\n",
      "Iteration 58, loss = 1.01728856\n",
      "Iteration 59, loss = 1.01620417\n",
      "Iteration 60, loss = 1.01521018\n",
      "Iteration 61, loss = 1.01408845\n",
      "Iteration 62, loss = 1.01546313\n",
      "Iteration 63, loss = 1.01261694\n",
      "Iteration 64, loss = 1.01588590\n",
      "Iteration 65, loss = 1.01001527\n",
      "Iteration 66, loss = 1.01162242\n",
      "Iteration 67, loss = 1.01029846\n",
      "Iteration 68, loss = 1.00938313\n",
      "Iteration 69, loss = 1.00839255\n",
      "Iteration 70, loss = 1.00774050\n",
      "Iteration 71, loss = 1.00938340\n",
      "Iteration 72, loss = 1.00667898\n",
      "Iteration 73, loss = 1.00720737\n",
      "Iteration 74, loss = 1.00695506\n",
      "Iteration 75, loss = 1.00477593\n",
      "Iteration 76, loss = 1.00534147\n",
      "Iteration 77, loss = 1.00388005\n",
      "Iteration 78, loss = 1.00306809\n",
      "Iteration 79, loss = 1.00420651\n",
      "Iteration 80, loss = 1.00381297\n",
      "Iteration 81, loss = 1.00262702\n",
      "Iteration 82, loss = 1.00196308\n",
      "Iteration 83, loss = 1.00064388\n",
      "Iteration 84, loss = 1.00101764\n",
      "Iteration 85, loss = 1.00335967\n",
      "Iteration 86, loss = 0.99963297\n",
      "Iteration 87, loss = 0.99959953\n",
      "Iteration 88, loss = 0.99952160\n",
      "Iteration 89, loss = 0.99828247\n",
      "Iteration 90, loss = 0.99809387\n",
      "Iteration 91, loss = 0.99818363\n",
      "Iteration 92, loss = 0.99713998\n",
      "Iteration 93, loss = 0.99756466\n",
      "Iteration 94, loss = 0.99671024\n",
      "Iteration 95, loss = 0.99599721\n",
      "Iteration 96, loss = 0.99652599\n",
      "Iteration 97, loss = 0.99533852\n",
      "Iteration 98, loss = 0.99539992\n",
      "Iteration 99, loss = 0.99512314\n",
      "Iteration 100, loss = 0.99449996\n",
      "Iteration 1, loss = 15.59717244\n",
      "Iteration 2, loss = 4.19491646\n",
      "Iteration 3, loss = 3.90550932\n",
      "Iteration 4, loss = 3.05985580\n",
      "Iteration 5, loss = 2.63516773\n",
      "Iteration 6, loss = 2.60236268\n",
      "Iteration 7, loss = 2.26441183\n",
      "Iteration 8, loss = 2.16838591\n",
      "Iteration 9, loss = 2.07634333\n",
      "Iteration 10, loss = 1.90276324\n",
      "Iteration 11, loss = 1.80649884\n",
      "Iteration 12, loss = 1.66768715\n",
      "Iteration 13, loss = 1.56825987\n",
      "Iteration 14, loss = 1.42422339\n",
      "Iteration 15, loss = 1.38360969\n",
      "Iteration 16, loss = 1.35776023\n",
      "Iteration 17, loss = 1.31365830\n",
      "Iteration 18, loss = 1.29905302\n",
      "Iteration 19, loss = 1.25740256\n",
      "Iteration 20, loss = 1.23715200\n",
      "Iteration 21, loss = 1.22336952\n",
      "Iteration 22, loss = 1.21256416\n",
      "Iteration 23, loss = 1.20289516\n",
      "Iteration 24, loss = 1.21751855\n",
      "Iteration 25, loss = 1.20354687\n",
      "Iteration 26, loss = 1.16106205\n",
      "Iteration 27, loss = 1.16455423\n",
      "Iteration 28, loss = 1.20160106\n",
      "Iteration 29, loss = 1.28839970\n",
      "Iteration 30, loss = 1.22776399\n",
      "Iteration 31, loss = 1.20434559\n",
      "Iteration 32, loss = 1.16091301\n",
      "Iteration 33, loss = 1.14844429\n",
      "Iteration 34, loss = 1.14748347\n",
      "Iteration 35, loss = 1.15127254\n",
      "Iteration 36, loss = 1.14629189\n",
      "Iteration 37, loss = 1.14512398\n",
      "Iteration 38, loss = 1.14594346\n",
      "Iteration 39, loss = 1.15327195\n",
      "Iteration 40, loss = 1.17398633\n",
      "Iteration 41, loss = 1.17558695\n",
      "Iteration 42, loss = 1.14112992\n",
      "Iteration 43, loss = 1.14510311\n",
      "Iteration 44, loss = 1.15899026\n",
      "Iteration 45, loss = 1.16749136\n",
      "Iteration 46, loss = 1.15080926\n",
      "Iteration 47, loss = 1.16601326\n",
      "Iteration 48, loss = 1.13474414\n",
      "Iteration 49, loss = 1.11096429\n",
      "Iteration 50, loss = 1.12121979\n",
      "Iteration 51, loss = 1.11598617\n",
      "Iteration 52, loss = 1.13798503\n",
      "Iteration 53, loss = 1.16267907\n",
      "Iteration 54, loss = 1.12066869\n",
      "Iteration 55, loss = 1.10379608\n",
      "Iteration 56, loss = 1.11080898\n",
      "Iteration 57, loss = 1.16297371\n",
      "Iteration 58, loss = 1.14172559\n",
      "Iteration 59, loss = 1.11613693\n",
      "Iteration 60, loss = 1.13959600\n",
      "Iteration 61, loss = 1.13175918\n",
      "Iteration 62, loss = 1.10532831\n",
      "Iteration 63, loss = 1.11396951\n",
      "Iteration 64, loss = 1.15661033\n",
      "Iteration 65, loss = 1.17567182\n",
      "Iteration 66, loss = 1.16823281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.65813619\n",
      "Iteration 2, loss = 1.46697112\n",
      "Iteration 3, loss = 1.35512302\n",
      "Iteration 4, loss = 1.28056543\n",
      "Iteration 5, loss = 1.23516980\n",
      "Iteration 6, loss = 1.22791158\n",
      "Iteration 7, loss = 1.22675496\n",
      "Iteration 8, loss = 1.21936493\n",
      "Iteration 9, loss = 1.21161418\n",
      "Iteration 10, loss = 1.20603721\n",
      "Iteration 11, loss = 1.19942857\n",
      "Iteration 12, loss = 1.19193945\n",
      "Iteration 13, loss = 1.18357836\n",
      "Iteration 14, loss = 1.17619592\n",
      "Iteration 15, loss = 1.16862608\n",
      "Iteration 16, loss = 1.15960292\n",
      "Iteration 17, loss = 1.14951467\n",
      "Iteration 18, loss = 1.13870257\n",
      "Iteration 19, loss = 1.12740325\n",
      "Iteration 20, loss = 1.11976513\n",
      "Iteration 21, loss = 1.11154116\n",
      "Iteration 22, loss = 1.10440843\n",
      "Iteration 23, loss = 1.10030517\n",
      "Iteration 24, loss = 1.09180282\n",
      "Iteration 25, loss = 1.08876049\n",
      "Iteration 26, loss = 1.08348796\n",
      "Iteration 27, loss = 1.07941150\n",
      "Iteration 28, loss = 1.07394041\n",
      "Iteration 29, loss = 1.07068459\n",
      "Iteration 30, loss = 1.06694055\n",
      "Iteration 31, loss = 1.06424200\n",
      "Iteration 32, loss = 1.06065583\n",
      "Iteration 33, loss = 1.06038400\n",
      "Iteration 34, loss = 1.05537442\n",
      "Iteration 35, loss = 1.05347470\n",
      "Iteration 36, loss = 1.05179068\n",
      "Iteration 37, loss = 1.05096830\n",
      "Iteration 38, loss = 1.04952123\n",
      "Iteration 39, loss = 1.04683860\n",
      "Iteration 40, loss = 1.04448096\n",
      "Iteration 41, loss = 1.04321352\n",
      "Iteration 42, loss = 1.04123476\n",
      "Iteration 43, loss = 1.04044891\n",
      "Iteration 44, loss = 1.03821941\n",
      "Iteration 45, loss = 1.03690654\n",
      "Iteration 46, loss = 1.03565137\n",
      "Iteration 47, loss = 1.03521458\n",
      "Iteration 48, loss = 1.03331841\n",
      "Iteration 49, loss = 1.03212657\n",
      "Iteration 50, loss = 1.03137872\n",
      "Iteration 51, loss = 1.03021874\n",
      "Iteration 52, loss = 1.03032586\n",
      "Iteration 53, loss = 1.02926102\n",
      "Iteration 54, loss = 1.02719195\n",
      "Iteration 55, loss = 1.02746862\n",
      "Iteration 56, loss = 1.02527838\n",
      "Iteration 57, loss = 1.02532408\n",
      "Iteration 58, loss = 1.02383875\n",
      "Iteration 59, loss = 1.02284066\n",
      "Iteration 60, loss = 1.02357737\n",
      "Iteration 61, loss = 1.02192777\n",
      "Iteration 62, loss = 1.01981490\n",
      "Iteration 63, loss = 1.01954088\n",
      "Iteration 64, loss = 1.01792081\n",
      "Iteration 65, loss = 1.01668261\n",
      "Iteration 66, loss = 1.01614017\n",
      "Iteration 67, loss = 1.01488658\n",
      "Iteration 68, loss = 1.01451172\n",
      "Iteration 69, loss = 1.01281104\n",
      "Iteration 70, loss = 1.01444909\n",
      "Iteration 71, loss = 1.01236399\n",
      "Iteration 72, loss = 1.01188446\n",
      "Iteration 73, loss = 1.00947260\n",
      "Iteration 74, loss = 1.00918865\n",
      "Iteration 75, loss = 1.00809301\n",
      "Iteration 76, loss = 1.00872186\n",
      "Iteration 77, loss = 1.00692878\n",
      "Iteration 78, loss = 1.00601428\n",
      "Iteration 79, loss = 1.00649395\n",
      "Iteration 80, loss = 1.00502606\n",
      "Iteration 81, loss = 1.00374104\n",
      "Iteration 82, loss = 1.00352379\n",
      "Iteration 83, loss = 1.00220053\n",
      "Iteration 84, loss = 1.00272245\n",
      "Iteration 85, loss = 1.00233364\n",
      "Iteration 86, loss = 1.00105659\n",
      "Iteration 87, loss = 1.00138055\n",
      "Iteration 88, loss = 1.00037278\n",
      "Iteration 89, loss = 0.99981436\n",
      "Iteration 90, loss = 0.99935790\n",
      "Iteration 91, loss = 0.99815511\n",
      "Iteration 92, loss = 0.99694297\n",
      "Iteration 93, loss = 0.99625032\n",
      "Iteration 94, loss = 0.99557770\n",
      "Iteration 95, loss = 0.99566922\n",
      "Iteration 96, loss = 0.99412576\n",
      "Iteration 97, loss = 0.99473210\n",
      "Iteration 98, loss = 0.99328775\n",
      "Iteration 99, loss = 0.99207172\n",
      "Iteration 100, loss = 0.99246322\n",
      "Iteration 1, loss = 1.66494652\n",
      "Iteration 2, loss = 1.47628226\n",
      "Iteration 3, loss = 1.36241599\n",
      "Iteration 4, loss = 1.28228848\n",
      "Iteration 5, loss = 1.23208132\n",
      "Iteration 6, loss = 1.22834336\n",
      "Iteration 7, loss = 1.22646981\n",
      "Iteration 8, loss = 1.21813789\n",
      "Iteration 9, loss = 1.21057812\n",
      "Iteration 10, loss = 1.20407024\n",
      "Iteration 11, loss = 1.20045814\n",
      "Iteration 12, loss = 1.19412406\n",
      "Iteration 13, loss = 1.18827574\n",
      "Iteration 14, loss = 1.18189031\n",
      "Iteration 15, loss = 1.17465331\n",
      "Iteration 16, loss = 1.16354400\n",
      "Iteration 17, loss = 1.15231444\n",
      "Iteration 18, loss = 1.14010773\n",
      "Iteration 19, loss = 1.12858875\n",
      "Iteration 20, loss = 1.11887511\n",
      "Iteration 21, loss = 1.10782168\n",
      "Iteration 22, loss = 1.09845768\n",
      "Iteration 23, loss = 1.09090394\n",
      "Iteration 24, loss = 1.08301680\n",
      "Iteration 25, loss = 1.07658173\n",
      "Iteration 26, loss = 1.07103817\n",
      "Iteration 27, loss = 1.06592014\n",
      "Iteration 28, loss = 1.06262638\n",
      "Iteration 29, loss = 1.05807200\n",
      "Iteration 30, loss = 1.05466257\n",
      "Iteration 31, loss = 1.05299689\n",
      "Iteration 32, loss = 1.04892249\n",
      "Iteration 33, loss = 1.04703074\n",
      "Iteration 34, loss = 1.04397169\n",
      "Iteration 35, loss = 1.04224792\n",
      "Iteration 36, loss = 1.04062221\n",
      "Iteration 37, loss = 1.04054017\n",
      "Iteration 38, loss = 1.04108710\n",
      "Iteration 39, loss = 1.03664733\n",
      "Iteration 40, loss = 1.03493436\n",
      "Iteration 41, loss = 1.03430160\n",
      "Iteration 42, loss = 1.03282917\n",
      "Iteration 43, loss = 1.03238019\n",
      "Iteration 44, loss = 1.03166486\n",
      "Iteration 45, loss = 1.03048724\n",
      "Iteration 46, loss = 1.02924811\n",
      "Iteration 47, loss = 1.02857195\n",
      "Iteration 48, loss = 1.02884722\n",
      "Iteration 49, loss = 1.02719483\n",
      "Iteration 50, loss = 1.02677084\n",
      "Iteration 51, loss = 1.02568749\n",
      "Iteration 52, loss = 1.02617633\n",
      "Iteration 53, loss = 1.02669817\n",
      "Iteration 54, loss = 1.02416829\n",
      "Iteration 55, loss = 1.02308701\n",
      "Iteration 56, loss = 1.02294343\n",
      "Iteration 57, loss = 1.02151000\n",
      "Iteration 58, loss = 1.02193144\n",
      "Iteration 59, loss = 1.02051622\n",
      "Iteration 60, loss = 1.02036886\n",
      "Iteration 61, loss = 1.02038392\n",
      "Iteration 62, loss = 1.01949047\n",
      "Iteration 63, loss = 1.01888592\n",
      "Iteration 64, loss = 1.01809475\n",
      "Iteration 65, loss = 1.01838366\n",
      "Iteration 66, loss = 1.01690850\n",
      "Iteration 67, loss = 1.01792128\n",
      "Iteration 68, loss = 1.01604282\n",
      "Iteration 69, loss = 1.01616538\n",
      "Iteration 70, loss = 1.01567612\n",
      "Iteration 71, loss = 1.01511193\n",
      "Iteration 72, loss = 1.01484037\n",
      "Iteration 73, loss = 1.01467375\n",
      "Iteration 74, loss = 1.01266103\n",
      "Iteration 75, loss = 1.01335379\n",
      "Iteration 76, loss = 1.01343523\n",
      "Iteration 77, loss = 1.01209379\n",
      "Iteration 78, loss = 1.01230471\n",
      "Iteration 79, loss = 1.01310580\n",
      "Iteration 80, loss = 1.01105634\n",
      "Iteration 81, loss = 1.01117127\n",
      "Iteration 82, loss = 1.01169529\n",
      "Iteration 83, loss = 1.01066605\n",
      "Iteration 84, loss = 1.01118815\n",
      "Iteration 85, loss = 1.01143921\n",
      "Iteration 86, loss = 1.01281635\n",
      "Iteration 87, loss = 1.00904824\n",
      "Iteration 88, loss = 1.00921965\n",
      "Iteration 89, loss = 1.00719226\n",
      "Iteration 90, loss = 1.00747216\n",
      "Iteration 91, loss = 1.00665757\n",
      "Iteration 92, loss = 1.00662897\n",
      "Iteration 93, loss = 1.00606681\n",
      "Iteration 94, loss = 1.00727578\n",
      "Iteration 95, loss = 1.00653873\n",
      "Iteration 96, loss = 1.00611843\n",
      "Iteration 97, loss = 1.00590873\n",
      "Iteration 98, loss = 1.00443121\n",
      "Iteration 99, loss = 1.00626785\n",
      "Iteration 100, loss = 1.00412246\n",
      "Iteration 1, loss = 1.66046586\n",
      "Iteration 2, loss = 1.46956382\n",
      "Iteration 3, loss = 1.35594584\n",
      "Iteration 4, loss = 1.27981362\n",
      "Iteration 5, loss = 1.23381234\n",
      "Iteration 6, loss = 1.22673704\n",
      "Iteration 7, loss = 1.22501517\n",
      "Iteration 8, loss = 1.21790197\n",
      "Iteration 9, loss = 1.20914233\n",
      "Iteration 10, loss = 1.20323614\n",
      "Iteration 11, loss = 1.19958205\n",
      "Iteration 12, loss = 1.19471644\n",
      "Iteration 13, loss = 1.18925726\n",
      "Iteration 14, loss = 1.18022757\n",
      "Iteration 15, loss = 1.16782342\n",
      "Iteration 16, loss = 1.15547975\n",
      "Iteration 17, loss = 1.14101657\n",
      "Iteration 18, loss = 1.12673098\n",
      "Iteration 19, loss = 1.11476938\n",
      "Iteration 20, loss = 1.10366194\n",
      "Iteration 21, loss = 1.09321792\n",
      "Iteration 22, loss = 1.08499931\n",
      "Iteration 23, loss = 1.07821076\n",
      "Iteration 24, loss = 1.07196394\n",
      "Iteration 25, loss = 1.06478220\n",
      "Iteration 26, loss = 1.05930379\n",
      "Iteration 27, loss = 1.05602843\n",
      "Iteration 28, loss = 1.05136067\n",
      "Iteration 29, loss = 1.04934253\n",
      "Iteration 30, loss = 1.04658287\n",
      "Iteration 31, loss = 1.04326813\n",
      "Iteration 32, loss = 1.03993603\n",
      "Iteration 33, loss = 1.03866936\n",
      "Iteration 34, loss = 1.03647971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 1.03534378\n",
      "Iteration 36, loss = 1.03459231\n",
      "Iteration 37, loss = 1.03350720\n",
      "Iteration 38, loss = 1.03205953\n",
      "Iteration 39, loss = 1.03057702\n",
      "Iteration 40, loss = 1.02884574\n",
      "Iteration 41, loss = 1.02843930\n",
      "Iteration 42, loss = 1.02816929\n",
      "Iteration 43, loss = 1.02741839\n",
      "Iteration 44, loss = 1.02686553\n",
      "Iteration 45, loss = 1.02517427\n",
      "Iteration 46, loss = 1.02445180\n",
      "Iteration 47, loss = 1.02381208\n",
      "Iteration 48, loss = 1.02454884\n",
      "Iteration 49, loss = 1.02195023\n",
      "Iteration 50, loss = 1.02401833\n",
      "Iteration 51, loss = 1.02222694\n",
      "Iteration 52, loss = 1.02198022\n",
      "Iteration 53, loss = 1.02000724\n",
      "Iteration 54, loss = 1.02042251\n",
      "Iteration 55, loss = 1.01919210\n",
      "Iteration 56, loss = 1.01999310\n",
      "Iteration 57, loss = 1.01784005\n",
      "Iteration 58, loss = 1.01739426\n",
      "Iteration 59, loss = 1.01712362\n",
      "Iteration 60, loss = 1.01708668\n",
      "Iteration 61, loss = 1.01791958\n",
      "Iteration 62, loss = 1.01582120\n",
      "Iteration 63, loss = 1.01583277\n",
      "Iteration 64, loss = 1.01468126\n",
      "Iteration 65, loss = 1.01418959\n",
      "Iteration 66, loss = 1.01374960\n",
      "Iteration 67, loss = 1.01413727\n",
      "Iteration 68, loss = 1.01277054\n",
      "Iteration 69, loss = 1.01294365\n",
      "Iteration 70, loss = 1.01273905\n",
      "Iteration 71, loss = 1.01221516\n",
      "Iteration 72, loss = 1.01139646\n",
      "Iteration 73, loss = 1.01247074\n",
      "Iteration 74, loss = 1.01118610\n",
      "Iteration 75, loss = 1.01213236\n",
      "Iteration 76, loss = 1.01082379\n",
      "Iteration 77, loss = 1.01122475\n",
      "Iteration 78, loss = 1.01103104\n",
      "Iteration 79, loss = 1.01117525\n",
      "Iteration 80, loss = 1.01017797\n",
      "Iteration 81, loss = 1.00905655\n",
      "Iteration 82, loss = 1.01039725\n",
      "Iteration 83, loss = 1.00841179\n",
      "Iteration 84, loss = 1.00902351\n",
      "Iteration 85, loss = 1.00873389\n",
      "Iteration 86, loss = 1.01077391\n",
      "Iteration 87, loss = 1.00721439\n",
      "Iteration 88, loss = 1.00845905\n",
      "Iteration 89, loss = 1.00655497\n",
      "Iteration 90, loss = 1.00627840\n",
      "Iteration 91, loss = 1.00489906\n",
      "Iteration 92, loss = 1.00473575\n",
      "Iteration 93, loss = 1.00453078\n",
      "Iteration 94, loss = 1.00456337\n",
      "Iteration 95, loss = 1.00460701\n",
      "Iteration 96, loss = 1.00442337\n",
      "Iteration 97, loss = 1.00357963\n",
      "Iteration 98, loss = 1.00490451\n",
      "Iteration 99, loss = 1.00357177\n",
      "Iteration 100, loss = 1.00249981\n",
      "Iteration 1, loss = 1.66268246\n",
      "Iteration 2, loss = 1.47255410\n",
      "Iteration 3, loss = 1.36066452\n",
      "Iteration 4, loss = 1.28256272\n",
      "Iteration 5, loss = 1.23592036\n",
      "Iteration 6, loss = 1.23190068\n",
      "Iteration 7, loss = 1.22861697\n",
      "Iteration 8, loss = 1.22039279\n",
      "Iteration 9, loss = 1.21197030\n",
      "Iteration 10, loss = 1.20294822\n",
      "Iteration 11, loss = 1.19318232\n",
      "Iteration 12, loss = 1.18283212\n",
      "Iteration 13, loss = 1.17078640\n",
      "Iteration 14, loss = 1.15928933\n",
      "Iteration 15, loss = 1.14906921\n",
      "Iteration 16, loss = 1.13763206\n",
      "Iteration 17, loss = 1.12791178\n",
      "Iteration 18, loss = 1.11866495\n",
      "Iteration 19, loss = 1.11083851\n",
      "Iteration 20, loss = 1.10377569\n",
      "Iteration 21, loss = 1.09667997\n",
      "Iteration 22, loss = 1.09226827\n",
      "Iteration 23, loss = 1.08494841\n",
      "Iteration 24, loss = 1.08123858\n",
      "Iteration 25, loss = 1.07592437\n",
      "Iteration 26, loss = 1.07229896\n",
      "Iteration 27, loss = 1.06957010\n",
      "Iteration 28, loss = 1.06667900\n",
      "Iteration 29, loss = 1.06559217\n",
      "Iteration 30, loss = 1.06436490\n",
      "Iteration 31, loss = 1.06088945\n",
      "Iteration 32, loss = 1.05904428\n",
      "Iteration 33, loss = 1.05644063\n",
      "Iteration 34, loss = 1.05612179\n",
      "Iteration 35, loss = 1.05516818\n",
      "Iteration 36, loss = 1.05487191\n",
      "Iteration 37, loss = 1.05218715\n",
      "Iteration 38, loss = 1.05267548\n",
      "Iteration 39, loss = 1.04941501\n",
      "Iteration 40, loss = 1.04884054\n",
      "Iteration 41, loss = 1.04668820\n",
      "Iteration 42, loss = 1.04826230\n",
      "Iteration 43, loss = 1.04855529\n",
      "Iteration 44, loss = 1.04583018\n",
      "Iteration 45, loss = 1.04398134\n",
      "Iteration 46, loss = 1.04440297\n",
      "Iteration 47, loss = 1.04214123\n",
      "Iteration 48, loss = 1.04357351\n",
      "Iteration 49, loss = 1.04057278\n",
      "Iteration 50, loss = 1.04508265\n",
      "Iteration 51, loss = 1.04236060\n",
      "Iteration 52, loss = 1.04117926\n",
      "Iteration 53, loss = 1.03978692\n",
      "Iteration 54, loss = 1.03963798\n",
      "Iteration 55, loss = 1.03821996\n",
      "Iteration 56, loss = 1.03799259\n",
      "Iteration 57, loss = 1.03632867\n",
      "Iteration 58, loss = 1.03627056\n",
      "Iteration 59, loss = 1.03545053\n",
      "Iteration 60, loss = 1.03466364\n",
      "Iteration 61, loss = 1.03478674\n",
      "Iteration 62, loss = 1.03395242\n",
      "Iteration 63, loss = 1.03354788\n",
      "Iteration 64, loss = 1.03853650\n",
      "Iteration 65, loss = 1.03390477\n",
      "Iteration 66, loss = 1.03315807\n",
      "Iteration 67, loss = 1.03194384\n",
      "Iteration 68, loss = 1.03137593\n",
      "Iteration 69, loss = 1.03076454\n",
      "Iteration 70, loss = 1.02998572\n",
      "Iteration 71, loss = 1.03058849\n",
      "Iteration 72, loss = 1.02988155\n",
      "Iteration 73, loss = 1.02915855\n",
      "Iteration 74, loss = 1.02883667\n",
      "Iteration 75, loss = 1.02803721\n",
      "Iteration 76, loss = 1.02850414\n",
      "Iteration 77, loss = 1.02787520\n",
      "Iteration 78, loss = 1.02753092\n",
      "Iteration 79, loss = 1.02660356\n",
      "Iteration 80, loss = 1.02643580\n",
      "Iteration 81, loss = 1.02604032\n",
      "Iteration 82, loss = 1.02751598\n",
      "Iteration 83, loss = 1.02529410\n",
      "Iteration 84, loss = 1.02405216\n",
      "Iteration 85, loss = 1.02457551\n",
      "Iteration 86, loss = 1.02604595\n",
      "Iteration 87, loss = 1.02553409\n",
      "Iteration 88, loss = 1.02411828\n",
      "Iteration 89, loss = 1.02360952\n",
      "Iteration 90, loss = 1.02441081\n",
      "Iteration 91, loss = 1.02095980\n",
      "Iteration 92, loss = 1.02160994\n",
      "Iteration 93, loss = 1.02147093\n",
      "Iteration 94, loss = 1.02132468\n",
      "Iteration 95, loss = 1.02102138\n",
      "Iteration 96, loss = 1.02076621\n",
      "Iteration 97, loss = 1.02036615\n",
      "Iteration 98, loss = 1.02107439\n",
      "Iteration 99, loss = 1.02090753\n",
      "Iteration 100, loss = 1.01925600\n",
      "Iteration 1, loss = 1.65961426\n",
      "Iteration 2, loss = 1.47096377\n",
      "Iteration 3, loss = 1.35476455\n",
      "Iteration 4, loss = 1.27997979\n",
      "Iteration 5, loss = 1.23646801\n",
      "Iteration 6, loss = 1.22347141\n",
      "Iteration 7, loss = 1.22310328\n",
      "Iteration 8, loss = 1.21639136\n",
      "Iteration 9, loss = 1.20880539\n",
      "Iteration 10, loss = 1.20146913\n",
      "Iteration 11, loss = 1.19621051\n",
      "Iteration 12, loss = 1.19098286\n",
      "Iteration 13, loss = 1.18111853\n",
      "Iteration 14, loss = 1.16617252\n",
      "Iteration 15, loss = 1.15303051\n",
      "Iteration 16, loss = 1.13894374\n",
      "Iteration 17, loss = 1.12656953\n",
      "Iteration 18, loss = 1.11295735\n",
      "Iteration 19, loss = 1.10042958\n",
      "Iteration 20, loss = 1.09097473\n",
      "Iteration 21, loss = 1.08227317\n",
      "Iteration 22, loss = 1.07395177\n",
      "Iteration 23, loss = 1.06804096\n",
      "Iteration 24, loss = 1.06132599\n",
      "Iteration 25, loss = 1.05502107\n",
      "Iteration 26, loss = 1.04970412\n",
      "Iteration 27, loss = 1.04591133\n",
      "Iteration 28, loss = 1.04335184\n",
      "Iteration 29, loss = 1.03604524\n",
      "Iteration 30, loss = 1.03449913\n",
      "Iteration 31, loss = 1.03061222\n",
      "Iteration 32, loss = 1.02772237\n",
      "Iteration 33, loss = 1.02546835\n",
      "Iteration 34, loss = 1.02355527\n",
      "Iteration 35, loss = 1.02080280\n",
      "Iteration 36, loss = 1.01951698\n",
      "Iteration 37, loss = 1.01873636\n",
      "Iteration 38, loss = 1.01746875\n",
      "Iteration 39, loss = 1.01523296\n",
      "Iteration 40, loss = 1.01351350\n",
      "Iteration 41, loss = 1.01249634\n",
      "Iteration 42, loss = 1.01139691\n",
      "Iteration 43, loss = 1.01128249\n",
      "Iteration 44, loss = 1.00886026\n",
      "Iteration 45, loss = 1.01039225\n",
      "Iteration 46, loss = 1.00784224\n",
      "Iteration 47, loss = 1.00668909\n",
      "Iteration 48, loss = 1.00501998\n",
      "Iteration 49, loss = 1.00551071\n",
      "Iteration 50, loss = 1.00350897\n",
      "Iteration 51, loss = 1.00288356\n",
      "Iteration 52, loss = 1.00210627\n",
      "Iteration 53, loss = 1.00127442\n",
      "Iteration 54, loss = 1.00026620\n",
      "Iteration 55, loss = 1.00010764\n",
      "Iteration 56, loss = 1.00038770\n",
      "Iteration 57, loss = 0.99851341\n",
      "Iteration 58, loss = 0.99786993\n",
      "Iteration 59, loss = 0.99744640\n",
      "Iteration 60, loss = 0.99813147\n",
      "Iteration 61, loss = 0.99627437\n",
      "Iteration 62, loss = 0.99545546\n",
      "Iteration 63, loss = 0.99500373\n",
      "Iteration 64, loss = 0.99489773\n",
      "Iteration 65, loss = 0.99550186\n",
      "Iteration 66, loss = 0.99619357\n",
      "Iteration 67, loss = 0.99315624\n",
      "Iteration 68, loss = 0.99250290\n",
      "Iteration 69, loss = 0.99228987\n",
      "Iteration 70, loss = 0.99212309\n",
      "Iteration 71, loss = 0.99125884\n",
      "Iteration 72, loss = 0.99059674\n",
      "Iteration 73, loss = 0.99207985\n",
      "Iteration 74, loss = 0.99028253\n",
      "Iteration 75, loss = 0.99033088\n",
      "Iteration 76, loss = 0.98894226\n",
      "Iteration 77, loss = 0.98815156\n",
      "Iteration 78, loss = 0.98914484\n",
      "Iteration 79, loss = 0.98878101\n",
      "Iteration 80, loss = 0.98716339\n",
      "Iteration 81, loss = 0.98737825\n",
      "Iteration 82, loss = 0.98721405\n",
      "Iteration 83, loss = 0.98584211\n",
      "Iteration 84, loss = 0.98594456\n",
      "Iteration 85, loss = 0.98621633\n",
      "Iteration 86, loss = 0.98498701\n",
      "Iteration 87, loss = 0.98540380\n",
      "Iteration 88, loss = 0.98390693\n",
      "Iteration 89, loss = 0.98424731\n",
      "Iteration 90, loss = 0.98423375\n",
      "Iteration 91, loss = 0.98359320\n",
      "Iteration 92, loss = 0.98304941\n",
      "Iteration 93, loss = 0.98308092\n",
      "Iteration 94, loss = 0.98275255\n",
      "Iteration 95, loss = 0.98229404\n",
      "Iteration 96, loss = 0.98391621\n",
      "Iteration 97, loss = 0.98207697\n",
      "Iteration 98, loss = 0.98083078\n",
      "Iteration 99, loss = 0.98082084\n",
      "Iteration 100, loss = 0.98083766\n",
      "Iteration 1, loss = 15.17848865\n",
      "Iteration 2, loss = 5.58202203\n",
      "Iteration 3, loss = 3.15394703\n",
      "Iteration 4, loss = 3.20190440\n",
      "Iteration 5, loss = 1.81545017\n",
      "Iteration 6, loss = 1.52701278\n",
      "Iteration 7, loss = 1.44837845\n",
      "Iteration 8, loss = 1.39983490\n",
      "Iteration 9, loss = 1.30623847\n",
      "Iteration 10, loss = 1.29677200\n",
      "Iteration 11, loss = 1.26303551\n",
      "Iteration 12, loss = 1.23646831\n",
      "Iteration 13, loss = 1.21123796\n",
      "Iteration 14, loss = 1.18993952\n",
      "Iteration 15, loss = 1.17755626\n",
      "Iteration 16, loss = 1.17159521\n",
      "Iteration 17, loss = 1.17645653\n",
      "Iteration 18, loss = 1.17228879\n",
      "Iteration 19, loss = 1.16134611\n",
      "Iteration 20, loss = 1.15914792\n",
      "Iteration 21, loss = 1.15765627\n",
      "Iteration 22, loss = 1.15851448\n",
      "Iteration 23, loss = 1.16185107\n",
      "Iteration 24, loss = 1.15503708\n",
      "Iteration 25, loss = 1.14991370\n",
      "Iteration 26, loss = 1.15277883\n",
      "Iteration 27, loss = 1.16193078\n",
      "Iteration 28, loss = 1.15264502\n",
      "Iteration 29, loss = 1.15306767\n",
      "Iteration 30, loss = 1.15196469\n",
      "Iteration 31, loss = 1.15416492\n",
      "Iteration 32, loss = 1.15811362\n",
      "Iteration 33, loss = 1.16225198\n",
      "Iteration 34, loss = 1.16404902\n",
      "Iteration 35, loss = 1.15700216\n",
      "Iteration 36, loss = 1.14994499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.55653808\n",
      "Iteration 2, loss = 1.32613033\n",
      "Iteration 3, loss = 1.23645662\n",
      "Iteration 4, loss = 1.22178148\n",
      "Iteration 5, loss = 1.20289762\n",
      "Iteration 6, loss = 1.18468576\n",
      "Iteration 7, loss = 1.17410349\n",
      "Iteration 8, loss = 1.15513009\n",
      "Iteration 9, loss = 1.14458228\n",
      "Iteration 10, loss = 1.13032953\n",
      "Iteration 11, loss = 1.11730046\n",
      "Iteration 12, loss = 1.10732329\n",
      "Iteration 13, loss = 1.09692277\n",
      "Iteration 14, loss = 1.08861495\n",
      "Iteration 15, loss = 1.08076215\n",
      "Iteration 16, loss = 1.07355859\n",
      "Iteration 17, loss = 1.06701029\n",
      "Iteration 18, loss = 1.06292833\n",
      "Iteration 19, loss = 1.05687764\n",
      "Iteration 20, loss = 1.05240735\n",
      "Iteration 21, loss = 1.04818823\n",
      "Iteration 22, loss = 1.04608363\n",
      "Iteration 23, loss = 1.04254862\n",
      "Iteration 24, loss = 1.04114366\n",
      "Iteration 25, loss = 1.03697813\n",
      "Iteration 26, loss = 1.03433147\n",
      "Iteration 27, loss = 1.03283226\n",
      "Iteration 28, loss = 1.03113886\n",
      "Iteration 29, loss = 1.02844378\n",
      "Iteration 30, loss = 1.03020310\n",
      "Iteration 31, loss = 1.03299952\n",
      "Iteration 32, loss = 1.02400612\n",
      "Iteration 33, loss = 1.02650576\n",
      "Iteration 34, loss = 1.02271957\n",
      "Iteration 35, loss = 1.01963136\n",
      "Iteration 36, loss = 1.02288648\n",
      "Iteration 37, loss = 1.01761254\n",
      "Iteration 38, loss = 1.01897502\n",
      "Iteration 39, loss = 1.01480886\n",
      "Iteration 40, loss = 1.01488107\n",
      "Iteration 41, loss = 1.01352679\n",
      "Iteration 42, loss = 1.01135678\n",
      "Iteration 43, loss = 1.01227430\n",
      "Iteration 44, loss = 1.01015554\n",
      "Iteration 45, loss = 1.01197421\n",
      "Iteration 46, loss = 1.01079641\n",
      "Iteration 47, loss = 1.00800211\n",
      "Iteration 48, loss = 1.00880246\n",
      "Iteration 49, loss = 1.00665365\n",
      "Iteration 50, loss = 1.00605874\n",
      "Iteration 51, loss = 1.00443058\n",
      "Iteration 52, loss = 1.00518533\n",
      "Iteration 53, loss = 1.00588761\n",
      "Iteration 54, loss = 1.00166313\n",
      "Iteration 55, loss = 1.00339885\n",
      "Iteration 56, loss = 0.99881988\n",
      "Iteration 57, loss = 1.00449356\n",
      "Iteration 58, loss = 0.99925751\n",
      "Iteration 59, loss = 0.99819742\n",
      "Iteration 60, loss = 0.99919295\n",
      "Iteration 61, loss = 1.00128351\n",
      "Iteration 62, loss = 0.99675202\n",
      "Iteration 63, loss = 0.99722014\n",
      "Iteration 64, loss = 0.99635722\n",
      "Iteration 65, loss = 0.99680368\n",
      "Iteration 66, loss = 0.99315511\n",
      "Iteration 67, loss = 0.99498903\n",
      "Iteration 68, loss = 0.99421479\n",
      "Iteration 69, loss = 0.99130787\n",
      "Iteration 70, loss = 0.99396838\n",
      "Iteration 71, loss = 0.98756157\n",
      "Iteration 72, loss = 0.99074493\n",
      "Iteration 73, loss = 0.98852082\n",
      "Iteration 74, loss = 0.98725573\n",
      "Iteration 75, loss = 0.98791428\n",
      "Iteration 76, loss = 0.98651304\n",
      "Iteration 77, loss = 0.98673022\n",
      "Iteration 78, loss = 0.99584362\n",
      "Iteration 79, loss = 0.98291466\n",
      "Iteration 80, loss = 0.98800009\n",
      "Iteration 81, loss = 0.98326683\n",
      "Iteration 82, loss = 0.98653541\n",
      "Iteration 83, loss = 0.98455928\n",
      "Iteration 84, loss = 0.98297418\n",
      "Iteration 85, loss = 0.98533406\n",
      "Iteration 86, loss = 0.98640427\n",
      "Iteration 87, loss = 0.98063831\n",
      "Iteration 88, loss = 0.97891213\n",
      "Iteration 89, loss = 0.97966081\n",
      "Iteration 90, loss = 0.98276076\n",
      "Iteration 91, loss = 0.97622010\n",
      "Iteration 92, loss = 0.97822075\n",
      "Iteration 93, loss = 0.97682064\n",
      "Iteration 94, loss = 0.97603292\n",
      "Iteration 95, loss = 0.97596772\n",
      "Iteration 96, loss = 0.97250138\n",
      "Iteration 97, loss = 0.97646541\n",
      "Iteration 98, loss = 0.97106762\n",
      "Iteration 99, loss = 0.97177251\n",
      "Iteration 100, loss = 0.97189485\n",
      "Iteration 1, loss = 1.55485953\n",
      "Iteration 2, loss = 1.32147486\n",
      "Iteration 3, loss = 1.23894612\n",
      "Iteration 4, loss = 1.22569899\n",
      "Iteration 5, loss = 1.20700779\n",
      "Iteration 6, loss = 1.18473328\n",
      "Iteration 7, loss = 1.17625834\n",
      "Iteration 8, loss = 1.16135019\n",
      "Iteration 9, loss = 1.14721432\n",
      "Iteration 10, loss = 1.13335402\n",
      "Iteration 11, loss = 1.11934802\n",
      "Iteration 12, loss = 1.11009726\n",
      "Iteration 13, loss = 1.09624474\n",
      "Iteration 14, loss = 1.08677072\n",
      "Iteration 15, loss = 1.07683138\n",
      "Iteration 16, loss = 1.06833398\n",
      "Iteration 17, loss = 1.06186598\n",
      "Iteration 18, loss = 1.05478977\n",
      "Iteration 19, loss = 1.04964549\n",
      "Iteration 20, loss = 1.04558112\n",
      "Iteration 21, loss = 1.04150041\n",
      "Iteration 22, loss = 1.03758228\n",
      "Iteration 23, loss = 1.03656947\n",
      "Iteration 24, loss = 1.03848340\n",
      "Iteration 25, loss = 1.02963526\n",
      "Iteration 26, loss = 1.03159361\n",
      "Iteration 27, loss = 1.02898737\n",
      "Iteration 28, loss = 1.02703611\n",
      "Iteration 29, loss = 1.02637957\n",
      "Iteration 30, loss = 1.02760692\n",
      "Iteration 31, loss = 1.02502609\n",
      "Iteration 32, loss = 1.02152393\n",
      "Iteration 33, loss = 1.02000917\n",
      "Iteration 34, loss = 1.01942815\n",
      "Iteration 35, loss = 1.01884369\n",
      "Iteration 36, loss = 1.01671416\n",
      "Iteration 37, loss = 1.01485952\n",
      "Iteration 38, loss = 1.01575796\n",
      "Iteration 39, loss = 1.01352605\n",
      "Iteration 40, loss = 1.01196367\n",
      "Iteration 41, loss = 1.01449511\n",
      "Iteration 42, loss = 1.01051836\n",
      "Iteration 43, loss = 1.01150381\n",
      "Iteration 44, loss = 1.00815890\n",
      "Iteration 45, loss = 1.00866691\n",
      "Iteration 46, loss = 1.00872463\n",
      "Iteration 47, loss = 1.00651614\n",
      "Iteration 48, loss = 1.00714654\n",
      "Iteration 49, loss = 1.00389741\n",
      "Iteration 50, loss = 1.00395344\n",
      "Iteration 51, loss = 1.00377255\n",
      "Iteration 52, loss = 1.00344968\n",
      "Iteration 53, loss = 1.00242522\n",
      "Iteration 54, loss = 0.99992216\n",
      "Iteration 55, loss = 1.00177489\n",
      "Iteration 56, loss = 0.99785285\n",
      "Iteration 57, loss = 0.99965264\n",
      "Iteration 58, loss = 0.99726700\n",
      "Iteration 59, loss = 0.99679246\n",
      "Iteration 60, loss = 0.99967759\n",
      "Iteration 61, loss = 0.99818870\n",
      "Iteration 62, loss = 0.99564118\n",
      "Iteration 63, loss = 0.99568349\n",
      "Iteration 64, loss = 0.99397698\n",
      "Iteration 65, loss = 0.99462174\n",
      "Iteration 66, loss = 0.99208074\n",
      "Iteration 67, loss = 0.99055267\n",
      "Iteration 68, loss = 0.99080985\n",
      "Iteration 69, loss = 0.98865966\n",
      "Iteration 70, loss = 0.98916289\n",
      "Iteration 71, loss = 0.98702370\n",
      "Iteration 72, loss = 0.98648648\n",
      "Iteration 73, loss = 0.98583855\n",
      "Iteration 74, loss = 0.98439329\n",
      "Iteration 75, loss = 0.98365577\n",
      "Iteration 76, loss = 0.98288351\n",
      "Iteration 77, loss = 0.98400041\n",
      "Iteration 78, loss = 0.98468071\n",
      "Iteration 79, loss = 0.98403674\n",
      "Iteration 80, loss = 0.98258873\n",
      "Iteration 81, loss = 0.97855382\n",
      "Iteration 82, loss = 0.97938991\n",
      "Iteration 83, loss = 0.97953338\n",
      "Iteration 84, loss = 0.97670663\n",
      "Iteration 85, loss = 0.97938890\n",
      "Iteration 86, loss = 0.97934009\n",
      "Iteration 87, loss = 0.97473759\n",
      "Iteration 88, loss = 0.97686588\n",
      "Iteration 89, loss = 0.97247521\n",
      "Iteration 90, loss = 0.97591759\n",
      "Iteration 91, loss = 0.97172059\n",
      "Iteration 92, loss = 0.97184639\n",
      "Iteration 93, loss = 0.97202394\n",
      "Iteration 94, loss = 0.96972497\n",
      "Iteration 95, loss = 0.97066986\n",
      "Iteration 96, loss = 0.96839772\n",
      "Iteration 97, loss = 0.96926750\n",
      "Iteration 98, loss = 0.97034540\n",
      "Iteration 99, loss = 0.96848468\n",
      "Iteration 100, loss = 0.97313361\n",
      "Iteration 1, loss = 1.55258738\n",
      "Iteration 2, loss = 1.32069921\n",
      "Iteration 3, loss = 1.23256021\n",
      "Iteration 4, loss = 1.21768202\n",
      "Iteration 5, loss = 1.19707207\n",
      "Iteration 6, loss = 1.17689019\n",
      "Iteration 7, loss = 1.16551652\n",
      "Iteration 8, loss = 1.14600768\n",
      "Iteration 9, loss = 1.13359898\n",
      "Iteration 10, loss = 1.12149636\n",
      "Iteration 11, loss = 1.10852996\n",
      "Iteration 12, loss = 1.09705395\n",
      "Iteration 13, loss = 1.08676442\n",
      "Iteration 14, loss = 1.08039161\n",
      "Iteration 15, loss = 1.07313846\n",
      "Iteration 16, loss = 1.06463971\n",
      "Iteration 17, loss = 1.05937894\n",
      "Iteration 18, loss = 1.05563213\n",
      "Iteration 19, loss = 1.05079442\n",
      "Iteration 20, loss = 1.05028312\n",
      "Iteration 21, loss = 1.04619759\n",
      "Iteration 22, loss = 1.04485467\n",
      "Iteration 23, loss = 1.04009109\n",
      "Iteration 24, loss = 1.03849042\n",
      "Iteration 25, loss = 1.03557839\n",
      "Iteration 26, loss = 1.03376125\n",
      "Iteration 27, loss = 1.03280861\n",
      "Iteration 28, loss = 1.03061599\n",
      "Iteration 29, loss = 1.02907785\n",
      "Iteration 30, loss = 1.02934448\n",
      "Iteration 31, loss = 1.02564315\n",
      "Iteration 32, loss = 1.02628249\n",
      "Iteration 33, loss = 1.02398036\n",
      "Iteration 34, loss = 1.02416515\n",
      "Iteration 35, loss = 1.02247861\n",
      "Iteration 36, loss = 1.02346256\n",
      "Iteration 37, loss = 1.02150433\n",
      "Iteration 38, loss = 1.02311320\n",
      "Iteration 39, loss = 1.01990042\n",
      "Iteration 40, loss = 1.01937982\n",
      "Iteration 41, loss = 1.01853831\n",
      "Iteration 42, loss = 1.01596350\n",
      "Iteration 43, loss = 1.01693884\n",
      "Iteration 44, loss = 1.01511163\n",
      "Iteration 45, loss = 1.01636603\n",
      "Iteration 46, loss = 1.01532811\n",
      "Iteration 47, loss = 1.01211922\n",
      "Iteration 48, loss = 1.01464694\n",
      "Iteration 49, loss = 1.01247594\n",
      "Iteration 50, loss = 1.01175761\n",
      "Iteration 51, loss = 1.01411293\n",
      "Iteration 52, loss = 1.01099850\n",
      "Iteration 53, loss = 1.01013698\n",
      "Iteration 54, loss = 1.01090371\n",
      "Iteration 55, loss = 1.00863191\n",
      "Iteration 56, loss = 1.00847018\n",
      "Iteration 57, loss = 1.00666324\n",
      "Iteration 58, loss = 1.00628425\n",
      "Iteration 59, loss = 1.00553768\n",
      "Iteration 60, loss = 1.00513988\n",
      "Iteration 61, loss = 1.00479414\n",
      "Iteration 62, loss = 1.00649739\n",
      "Iteration 63, loss = 1.00588070\n",
      "Iteration 64, loss = 1.00195823\n",
      "Iteration 65, loss = 1.00409177\n",
      "Iteration 66, loss = 1.00356103\n",
      "Iteration 67, loss = 1.00405966\n",
      "Iteration 68, loss = 1.00394336\n",
      "Iteration 69, loss = 1.00128366\n",
      "Iteration 70, loss = 0.99937534\n",
      "Iteration 71, loss = 1.00064477\n",
      "Iteration 72, loss = 1.00066441\n",
      "Iteration 73, loss = 0.99810142\n",
      "Iteration 74, loss = 0.99772605\n",
      "Iteration 75, loss = 0.99676957\n",
      "Iteration 76, loss = 0.99553376\n",
      "Iteration 77, loss = 0.99510896\n",
      "Iteration 78, loss = 0.99461714\n",
      "Iteration 79, loss = 0.99658149\n",
      "Iteration 80, loss = 0.99720698\n",
      "Iteration 81, loss = 0.99181761\n",
      "Iteration 82, loss = 0.99366846\n",
      "Iteration 83, loss = 0.99293876\n",
      "Iteration 84, loss = 0.99297477\n",
      "Iteration 85, loss = 0.99047173\n",
      "Iteration 86, loss = 0.99482133\n",
      "Iteration 87, loss = 0.98736362\n",
      "Iteration 88, loss = 0.99141502\n",
      "Iteration 89, loss = 0.98940504\n",
      "Iteration 90, loss = 0.99445305\n",
      "Iteration 91, loss = 0.98596387\n",
      "Iteration 92, loss = 0.98861507\n",
      "Iteration 93, loss = 0.98650125\n",
      "Iteration 94, loss = 0.98597625\n",
      "Iteration 95, loss = 0.98372869\n",
      "Iteration 96, loss = 0.98519493\n",
      "Iteration 97, loss = 0.98491223\n",
      "Iteration 98, loss = 0.98467704\n",
      "Iteration 99, loss = 0.98735710\n",
      "Iteration 100, loss = 0.98282173\n",
      "Iteration 1, loss = 1.55642630\n",
      "Iteration 2, loss = 1.32479661\n",
      "Iteration 3, loss = 1.24229737\n",
      "Iteration 4, loss = 1.22235234\n",
      "Iteration 5, loss = 1.21071160\n",
      "Iteration 6, loss = 1.18845236\n",
      "Iteration 7, loss = 1.18188609\n",
      "Iteration 8, loss = 1.16235210\n",
      "Iteration 9, loss = 1.15266833\n",
      "Iteration 10, loss = 1.14053900\n",
      "Iteration 11, loss = 1.12769809\n",
      "Iteration 12, loss = 1.11646564\n",
      "Iteration 13, loss = 1.10851188\n",
      "Iteration 14, loss = 1.09863939\n",
      "Iteration 15, loss = 1.09180996\n",
      "Iteration 16, loss = 1.08485481\n",
      "Iteration 17, loss = 1.07915097\n",
      "Iteration 18, loss = 1.07504957\n",
      "Iteration 19, loss = 1.07025718\n",
      "Iteration 20, loss = 1.06737614\n",
      "Iteration 21, loss = 1.06425244\n",
      "Iteration 22, loss = 1.06343592\n",
      "Iteration 23, loss = 1.05942656\n",
      "Iteration 24, loss = 1.05807359\n",
      "Iteration 25, loss = 1.05621295\n",
      "Iteration 26, loss = 1.05278595\n",
      "Iteration 27, loss = 1.05316693\n",
      "Iteration 28, loss = 1.05152467\n",
      "Iteration 29, loss = 1.05168601\n",
      "Iteration 30, loss = 1.05117565\n",
      "Iteration 31, loss = 1.04591999\n",
      "Iteration 32, loss = 1.04702214\n",
      "Iteration 33, loss = 1.04553090\n",
      "Iteration 34, loss = 1.04394544\n",
      "Iteration 35, loss = 1.04253966\n",
      "Iteration 36, loss = 1.04080601\n",
      "Iteration 37, loss = 1.03810925\n",
      "Iteration 38, loss = 1.03825341\n",
      "Iteration 39, loss = 1.03917055\n",
      "Iteration 40, loss = 1.03619297\n",
      "Iteration 41, loss = 1.03880074\n",
      "Iteration 42, loss = 1.03975402\n",
      "Iteration 43, loss = 1.03434399\n",
      "Iteration 44, loss = 1.03258071\n",
      "Iteration 45, loss = 1.03262243\n",
      "Iteration 46, loss = 1.02940936\n",
      "Iteration 47, loss = 1.02953671\n",
      "Iteration 48, loss = 1.03147930\n",
      "Iteration 49, loss = 1.02979313\n",
      "Iteration 50, loss = 1.02636082\n",
      "Iteration 51, loss = 1.02635861\n",
      "Iteration 52, loss = 1.02591070\n",
      "Iteration 53, loss = 1.02484175\n",
      "Iteration 54, loss = 1.02445524\n",
      "Iteration 55, loss = 1.02427608\n",
      "Iteration 56, loss = 1.02226905\n",
      "Iteration 57, loss = 1.02240311\n",
      "Iteration 58, loss = 1.01913650\n",
      "Iteration 59, loss = 1.01970844\n",
      "Iteration 60, loss = 1.01766733\n",
      "Iteration 61, loss = 1.01984414\n",
      "Iteration 62, loss = 1.01726676\n",
      "Iteration 63, loss = 1.01757912\n",
      "Iteration 64, loss = 1.01498194\n",
      "Iteration 65, loss = 1.01407573\n",
      "Iteration 66, loss = 1.01408064\n",
      "Iteration 67, loss = 1.01142232\n",
      "Iteration 68, loss = 1.01181104\n",
      "Iteration 69, loss = 1.01054833\n",
      "Iteration 70, loss = 1.00960724\n",
      "Iteration 71, loss = 1.00876480\n",
      "Iteration 72, loss = 1.00742031\n",
      "Iteration 73, loss = 1.00735528\n",
      "Iteration 74, loss = 1.01041520\n",
      "Iteration 75, loss = 1.00450186\n",
      "Iteration 76, loss = 1.00640658\n",
      "Iteration 77, loss = 1.00647159\n",
      "Iteration 78, loss = 1.00233475\n",
      "Iteration 79, loss = 1.00576947\n",
      "Iteration 80, loss = 1.01032324\n",
      "Iteration 81, loss = 0.99887652\n",
      "Iteration 82, loss = 1.00510458\n",
      "Iteration 83, loss = 1.00097379\n",
      "Iteration 84, loss = 1.00163777\n",
      "Iteration 85, loss = 0.99877672\n",
      "Iteration 86, loss = 0.99842974\n",
      "Iteration 87, loss = 0.99701879\n",
      "Iteration 88, loss = 0.99726823\n",
      "Iteration 89, loss = 0.99619654\n",
      "Iteration 90, loss = 0.99568579\n",
      "Iteration 91, loss = 0.99187118\n",
      "Iteration 92, loss = 0.99933260\n",
      "Iteration 93, loss = 0.99354771\n",
      "Iteration 94, loss = 0.99402270\n",
      "Iteration 95, loss = 0.98997059\n",
      "Iteration 96, loss = 0.99277307\n",
      "Iteration 97, loss = 0.99134653\n",
      "Iteration 98, loss = 0.99264585\n",
      "Iteration 99, loss = 0.98773656\n",
      "Iteration 100, loss = 0.99107334\n",
      "Iteration 1, loss = 1.55210051\n",
      "Iteration 2, loss = 1.31774568\n",
      "Iteration 3, loss = 1.22856925\n",
      "Iteration 4, loss = 1.21540738\n",
      "Iteration 5, loss = 1.19812969\n",
      "Iteration 6, loss = 1.17490698\n",
      "Iteration 7, loss = 1.16115219\n",
      "Iteration 8, loss = 1.14397382\n",
      "Iteration 9, loss = 1.12279464\n",
      "Iteration 10, loss = 1.11174156\n",
      "Iteration 11, loss = 1.09987811\n",
      "Iteration 12, loss = 1.08604935\n",
      "Iteration 13, loss = 1.07712889\n",
      "Iteration 14, loss = 1.06868205\n",
      "Iteration 15, loss = 1.05905984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 1.05225378\n",
      "Iteration 17, loss = 1.04935644\n",
      "Iteration 18, loss = 1.04003263\n",
      "Iteration 19, loss = 1.03637083\n",
      "Iteration 20, loss = 1.03223050\n",
      "Iteration 21, loss = 1.02706083\n",
      "Iteration 22, loss = 1.02449068\n",
      "Iteration 23, loss = 1.02207396\n",
      "Iteration 24, loss = 1.01930929\n",
      "Iteration 25, loss = 1.01661153\n",
      "Iteration 26, loss = 1.01444492\n",
      "Iteration 27, loss = 1.01166938\n",
      "Iteration 28, loss = 1.00977384\n",
      "Iteration 29, loss = 1.01364448\n",
      "Iteration 30, loss = 1.01213663\n",
      "Iteration 31, loss = 1.00745028\n",
      "Iteration 32, loss = 1.00605036\n",
      "Iteration 33, loss = 1.00242872\n",
      "Iteration 34, loss = 1.00201963\n",
      "Iteration 35, loss = 1.00234061\n",
      "Iteration 36, loss = 1.00090206\n",
      "Iteration 37, loss = 0.99843259\n",
      "Iteration 38, loss = 0.99756988\n",
      "Iteration 39, loss = 0.99482442\n",
      "Iteration 40, loss = 0.99445301\n",
      "Iteration 41, loss = 0.99387798\n",
      "Iteration 42, loss = 0.99221732\n",
      "Iteration 43, loss = 0.99110320\n",
      "Iteration 44, loss = 0.99127056\n",
      "Iteration 45, loss = 0.99269680\n",
      "Iteration 46, loss = 0.98945351\n",
      "Iteration 47, loss = 0.98852757\n",
      "Iteration 48, loss = 0.98748921\n",
      "Iteration 49, loss = 0.98688452\n",
      "Iteration 50, loss = 0.98587761\n",
      "Iteration 51, loss = 0.98562544\n",
      "Iteration 52, loss = 0.98414201\n",
      "Iteration 53, loss = 0.98302287\n",
      "Iteration 54, loss = 0.98355203\n",
      "Iteration 55, loss = 0.98222063\n",
      "Iteration 56, loss = 0.98110706\n",
      "Iteration 57, loss = 0.98019910\n",
      "Iteration 58, loss = 0.97992503\n",
      "Iteration 59, loss = 0.97779287\n",
      "Iteration 60, loss = 0.97891788\n",
      "Iteration 61, loss = 0.97807196\n",
      "Iteration 62, loss = 0.97880922\n",
      "Iteration 63, loss = 0.97548661\n",
      "Iteration 64, loss = 0.97888099\n",
      "Iteration 65, loss = 0.97715765\n",
      "Iteration 66, loss = 0.97612217\n",
      "Iteration 67, loss = 0.97277726\n",
      "Iteration 68, loss = 0.97428862\n",
      "Iteration 69, loss = 0.97381732\n",
      "Iteration 70, loss = 0.97229109\n",
      "Iteration 71, loss = 0.97129223\n",
      "Iteration 72, loss = 0.97146970\n",
      "Iteration 73, loss = 0.97116007\n",
      "Iteration 74, loss = 0.96945731\n",
      "Iteration 75, loss = 0.96771358\n",
      "Iteration 76, loss = 0.96697807\n",
      "Iteration 77, loss = 0.96794217\n",
      "Iteration 78, loss = 0.96635888\n",
      "Iteration 79, loss = 0.96557762\n",
      "Iteration 80, loss = 0.96541759\n",
      "Iteration 81, loss = 0.96458390\n",
      "Iteration 82, loss = 0.96387242\n",
      "Iteration 83, loss = 0.96574101\n",
      "Iteration 84, loss = 0.96601450\n",
      "Iteration 85, loss = 0.96282830\n",
      "Iteration 86, loss = 0.96150531\n",
      "Iteration 87, loss = 0.96297837\n",
      "Iteration 88, loss = 0.96449142\n",
      "Iteration 89, loss = 0.96537664\n",
      "Iteration 90, loss = 0.95862047\n",
      "Iteration 91, loss = 0.96375211\n",
      "Iteration 92, loss = 0.95697069\n",
      "Iteration 93, loss = 0.96046532\n",
      "Iteration 94, loss = 0.95814496\n",
      "Iteration 95, loss = 0.95692178\n",
      "Iteration 96, loss = 0.95760791\n",
      "Iteration 97, loss = 0.96214669\n",
      "Iteration 98, loss = 0.96203447\n",
      "Iteration 99, loss = 0.95724569\n",
      "Iteration 100, loss = 0.95434933\n",
      "Iteration 1, loss = 21.67446715\n",
      "Iteration 2, loss = 6.79786864\n",
      "Iteration 3, loss = 5.12713490\n",
      "Iteration 4, loss = 4.42458505\n",
      "Iteration 5, loss = 3.21830371\n",
      "Iteration 6, loss = 2.91585589\n",
      "Iteration 7, loss = 2.76063290\n",
      "Iteration 8, loss = 2.52864578\n",
      "Iteration 9, loss = 2.25022670\n",
      "Iteration 10, loss = 2.13485805\n",
      "Iteration 11, loss = 1.95092080\n",
      "Iteration 12, loss = 1.73330097\n",
      "Iteration 13, loss = 1.44590959\n",
      "Iteration 14, loss = 1.32047686\n",
      "Iteration 15, loss = 1.30161970\n",
      "Iteration 16, loss = 1.27899352\n",
      "Iteration 17, loss = 1.25271828\n",
      "Iteration 18, loss = 1.25056711\n",
      "Iteration 19, loss = 1.23643009\n",
      "Iteration 20, loss = 1.23584567\n",
      "Iteration 21, loss = 1.20930389\n",
      "Iteration 22, loss = 1.17343832\n",
      "Iteration 23, loss = 1.20024605\n",
      "Iteration 24, loss = 1.17115016\n",
      "Iteration 25, loss = 1.15237604\n",
      "Iteration 26, loss = 1.15277329\n",
      "Iteration 27, loss = 1.14681038\n",
      "Iteration 28, loss = 1.14443122\n",
      "Iteration 29, loss = 1.12567490\n",
      "Iteration 30, loss = 1.14082036\n",
      "Iteration 31, loss = 1.13992010\n",
      "Iteration 32, loss = 1.11760888\n",
      "Iteration 33, loss = 1.11944611\n",
      "Iteration 34, loss = 1.15409398\n",
      "Iteration 35, loss = 1.17057258\n",
      "Iteration 36, loss = 1.18606147\n",
      "Iteration 37, loss = 1.16790496\n",
      "Iteration 38, loss = 1.14503505\n",
      "Iteration 39, loss = 1.12229083\n",
      "Iteration 40, loss = 1.11538399\n",
      "Iteration 41, loss = 1.11271976\n",
      "Iteration 42, loss = 1.09858134\n",
      "Iteration 43, loss = 1.10000421\n",
      "Iteration 44, loss = 1.10498356\n",
      "Iteration 45, loss = 1.14054432\n",
      "Iteration 46, loss = 1.15887815\n",
      "Iteration 47, loss = 1.18100038\n",
      "Iteration 48, loss = 1.25262299\n",
      "Iteration 49, loss = 1.20815135\n",
      "Iteration 50, loss = 1.18841870\n",
      "Iteration 51, loss = 1.19326390\n",
      "Iteration 52, loss = 1.19357686\n",
      "Iteration 53, loss = 1.17792795\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.46783380\n",
      "Iteration 2, loss = 1.22216364\n",
      "Iteration 3, loss = 1.19118828\n",
      "Iteration 4, loss = 1.17502333\n",
      "Iteration 5, loss = 1.15819224\n",
      "Iteration 6, loss = 1.12911969\n",
      "Iteration 7, loss = 1.11696893\n",
      "Iteration 8, loss = 1.10253843\n",
      "Iteration 9, loss = 1.09744789\n",
      "Iteration 10, loss = 1.08565556\n",
      "Iteration 11, loss = 1.07856706\n",
      "Iteration 12, loss = 1.07198774\n",
      "Iteration 13, loss = 1.06748524\n",
      "Iteration 14, loss = 1.06507832\n",
      "Iteration 15, loss = 1.05855632\n",
      "Iteration 16, loss = 1.05438419\n",
      "Iteration 17, loss = 1.05066557\n",
      "Iteration 18, loss = 1.04712323\n",
      "Iteration 19, loss = 1.04629813\n",
      "Iteration 20, loss = 1.04255892\n",
      "Iteration 21, loss = 1.03932569\n",
      "Iteration 22, loss = 1.03732299\n",
      "Iteration 23, loss = 1.03379482\n",
      "Iteration 24, loss = 1.03424807\n",
      "Iteration 25, loss = 1.03160981\n",
      "Iteration 26, loss = 1.02972824\n",
      "Iteration 27, loss = 1.02752975\n",
      "Iteration 28, loss = 1.02786026\n",
      "Iteration 29, loss = 1.02672288\n",
      "Iteration 30, loss = 1.02565789\n",
      "Iteration 31, loss = 1.02103234\n",
      "Iteration 32, loss = 1.02008740\n",
      "Iteration 33, loss = 1.01820857\n",
      "Iteration 34, loss = 1.01710553\n",
      "Iteration 35, loss = 1.01943493\n",
      "Iteration 36, loss = 1.01776507\n",
      "Iteration 37, loss = 1.01392445\n",
      "Iteration 38, loss = 1.01551543\n",
      "Iteration 39, loss = 1.01300548\n",
      "Iteration 40, loss = 1.01318145\n",
      "Iteration 41, loss = 1.00869568\n",
      "Iteration 42, loss = 1.01103291\n",
      "Iteration 43, loss = 1.00690016\n",
      "Iteration 44, loss = 1.00383671\n",
      "Iteration 45, loss = 1.00343236\n",
      "Iteration 46, loss = 1.00343440\n",
      "Iteration 47, loss = 1.00281940\n",
      "Iteration 48, loss = 0.99890985\n",
      "Iteration 49, loss = 1.00044421\n",
      "Iteration 50, loss = 1.00508614\n",
      "Iteration 51, loss = 0.99673383\n",
      "Iteration 52, loss = 0.99804015\n",
      "Iteration 53, loss = 1.00068565\n",
      "Iteration 54, loss = 0.99491875\n",
      "Iteration 55, loss = 0.99239244\n",
      "Iteration 56, loss = 0.99344092\n",
      "Iteration 57, loss = 0.99179923\n",
      "Iteration 58, loss = 0.99477176\n",
      "Iteration 59, loss = 0.98838427\n",
      "Iteration 60, loss = 0.98844109\n",
      "Iteration 61, loss = 0.98452455\n",
      "Iteration 62, loss = 0.98225613\n",
      "Iteration 63, loss = 0.98459615\n",
      "Iteration 64, loss = 0.98312955\n",
      "Iteration 65, loss = 0.98100281\n",
      "Iteration 66, loss = 0.98057871\n",
      "Iteration 67, loss = 0.97797589\n",
      "Iteration 68, loss = 0.98040223\n",
      "Iteration 69, loss = 0.97931174\n",
      "Iteration 70, loss = 0.97816338\n",
      "Iteration 71, loss = 0.97996854\n",
      "Iteration 72, loss = 0.97146209\n",
      "Iteration 73, loss = 0.97188382\n",
      "Iteration 74, loss = 0.97672105\n",
      "Iteration 75, loss = 0.96745769\n",
      "Iteration 76, loss = 0.96923410\n",
      "Iteration 77, loss = 0.96792894\n",
      "Iteration 78, loss = 0.96343665\n",
      "Iteration 79, loss = 0.96602377\n",
      "Iteration 80, loss = 0.96425557\n",
      "Iteration 81, loss = 0.96502878\n",
      "Iteration 82, loss = 0.96536387\n",
      "Iteration 83, loss = 0.96631423\n",
      "Iteration 84, loss = 0.96152612\n",
      "Iteration 85, loss = 0.96022944\n",
      "Iteration 86, loss = 0.96080685\n",
      "Iteration 87, loss = 0.95767448\n",
      "Iteration 88, loss = 0.95672080\n",
      "Iteration 89, loss = 0.95961936\n",
      "Iteration 90, loss = 0.96179507\n",
      "Iteration 91, loss = 0.95915111\n",
      "Iteration 92, loss = 0.95517089\n",
      "Iteration 93, loss = 0.95530653\n",
      "Iteration 94, loss = 0.95295359\n",
      "Iteration 95, loss = 0.95120434\n",
      "Iteration 96, loss = 0.95267732\n",
      "Iteration 97, loss = 0.95087972\n",
      "Iteration 98, loss = 0.94833700\n",
      "Iteration 99, loss = 0.95195798\n",
      "Iteration 100, loss = 0.94819163\n",
      "Iteration 1, loss = 1.46755854\n",
      "Iteration 2, loss = 1.22190120\n",
      "Iteration 3, loss = 1.18417775\n",
      "Iteration 4, loss = 1.17014317\n",
      "Iteration 5, loss = 1.14939762\n",
      "Iteration 6, loss = 1.11917370\n",
      "Iteration 7, loss = 1.10227071\n",
      "Iteration 8, loss = 1.08805683\n",
      "Iteration 9, loss = 1.07777431\n",
      "Iteration 10, loss = 1.06976733\n",
      "Iteration 11, loss = 1.06166011\n",
      "Iteration 12, loss = 1.05689748\n",
      "Iteration 13, loss = 1.05435153\n",
      "Iteration 14, loss = 1.05072691\n",
      "Iteration 15, loss = 1.04650518\n",
      "Iteration 16, loss = 1.04196225\n",
      "Iteration 17, loss = 1.03938068\n",
      "Iteration 18, loss = 1.03595820\n",
      "Iteration 19, loss = 1.03247562\n",
      "Iteration 20, loss = 1.03311644\n",
      "Iteration 21, loss = 1.03239555\n",
      "Iteration 22, loss = 1.02482397\n",
      "Iteration 23, loss = 1.02638430\n",
      "Iteration 24, loss = 1.02621349\n",
      "Iteration 25, loss = 1.02073612\n",
      "Iteration 26, loss = 1.02224023\n",
      "Iteration 27, loss = 1.02020883\n",
      "Iteration 28, loss = 1.01785180\n",
      "Iteration 29, loss = 1.01767977\n",
      "Iteration 30, loss = 1.01659248\n",
      "Iteration 31, loss = 1.01361946\n",
      "Iteration 32, loss = 1.01267863\n",
      "Iteration 33, loss = 1.01041174\n",
      "Iteration 34, loss = 1.00878257\n",
      "Iteration 35, loss = 1.00725044\n",
      "Iteration 36, loss = 1.00687301\n",
      "Iteration 37, loss = 1.00670263\n",
      "Iteration 38, loss = 1.00758795\n",
      "Iteration 39, loss = 1.00743021\n",
      "Iteration 40, loss = 1.00105561\n",
      "Iteration 41, loss = 1.00300516\n",
      "Iteration 42, loss = 1.00235249\n",
      "Iteration 43, loss = 0.99810188\n",
      "Iteration 44, loss = 1.00071895\n",
      "Iteration 45, loss = 1.00081177\n",
      "Iteration 46, loss = 0.99761629\n",
      "Iteration 47, loss = 0.99735657\n",
      "Iteration 48, loss = 0.99677379\n",
      "Iteration 49, loss = 0.99242078\n",
      "Iteration 50, loss = 0.99429333\n",
      "Iteration 51, loss = 0.99387478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.98771898\n",
      "Iteration 53, loss = 0.99411345\n",
      "Iteration 54, loss = 0.98696144\n",
      "Iteration 55, loss = 0.98974003\n",
      "Iteration 56, loss = 0.98891806\n",
      "Iteration 57, loss = 0.98824455\n",
      "Iteration 58, loss = 0.98912716\n",
      "Iteration 59, loss = 0.98262199\n",
      "Iteration 60, loss = 0.98562393\n",
      "Iteration 61, loss = 0.98285840\n",
      "Iteration 62, loss = 0.98008584\n",
      "Iteration 63, loss = 0.98334202\n",
      "Iteration 64, loss = 0.98271898\n",
      "Iteration 65, loss = 0.97546374\n",
      "Iteration 66, loss = 0.97774254\n",
      "Iteration 67, loss = 0.97469119\n",
      "Iteration 68, loss = 0.97558034\n",
      "Iteration 69, loss = 0.98071541\n",
      "Iteration 70, loss = 0.97858403\n",
      "Iteration 71, loss = 0.97775250\n",
      "Iteration 72, loss = 0.97035947\n",
      "Iteration 73, loss = 0.96992215\n",
      "Iteration 74, loss = 0.97555802\n",
      "Iteration 75, loss = 0.96514381\n",
      "Iteration 76, loss = 0.96995368\n",
      "Iteration 77, loss = 0.96710854\n",
      "Iteration 78, loss = 0.96360392\n",
      "Iteration 79, loss = 0.96452650\n",
      "Iteration 80, loss = 0.96534940\n",
      "Iteration 81, loss = 0.96011238\n",
      "Iteration 82, loss = 0.96897252\n",
      "Iteration 83, loss = 0.96426221\n",
      "Iteration 84, loss = 0.95634323\n",
      "Iteration 85, loss = 0.96115635\n",
      "Iteration 86, loss = 0.95721812\n",
      "Iteration 87, loss = 0.95255712\n",
      "Iteration 88, loss = 0.95577112\n",
      "Iteration 89, loss = 0.95386868\n",
      "Iteration 90, loss = 0.95173141\n",
      "Iteration 91, loss = 0.95204093\n",
      "Iteration 92, loss = 0.94843893\n",
      "Iteration 93, loss = 0.94999098\n",
      "Iteration 94, loss = 0.94815846\n",
      "Iteration 95, loss = 0.94741730\n",
      "Iteration 96, loss = 0.94749943\n",
      "Iteration 97, loss = 0.94758427\n",
      "Iteration 98, loss = 0.94631794\n",
      "Iteration 99, loss = 0.94569563\n",
      "Iteration 100, loss = 0.94531112\n",
      "Iteration 1, loss = 1.46084570\n",
      "Iteration 2, loss = 1.21682988\n",
      "Iteration 3, loss = 1.18213171\n",
      "Iteration 4, loss = 1.16596967\n",
      "Iteration 5, loss = 1.14330599\n",
      "Iteration 6, loss = 1.11750342\n",
      "Iteration 7, loss = 1.10158739\n",
      "Iteration 8, loss = 1.09129229\n",
      "Iteration 9, loss = 1.08241002\n",
      "Iteration 10, loss = 1.07558845\n",
      "Iteration 11, loss = 1.06590772\n",
      "Iteration 12, loss = 1.06174005\n",
      "Iteration 13, loss = 1.06301703\n",
      "Iteration 14, loss = 1.05226855\n",
      "Iteration 15, loss = 1.05345772\n",
      "Iteration 16, loss = 1.04592147\n",
      "Iteration 17, loss = 1.04284472\n",
      "Iteration 18, loss = 1.03909623\n",
      "Iteration 19, loss = 1.04540081\n",
      "Iteration 20, loss = 1.04039084\n",
      "Iteration 21, loss = 1.03545583\n",
      "Iteration 22, loss = 1.03237545\n",
      "Iteration 23, loss = 1.02843073\n",
      "Iteration 24, loss = 1.02639492\n",
      "Iteration 25, loss = 1.02375560\n",
      "Iteration 26, loss = 1.02209735\n",
      "Iteration 27, loss = 1.02033178\n",
      "Iteration 28, loss = 1.01966635\n",
      "Iteration 29, loss = 1.01729398\n",
      "Iteration 30, loss = 1.01587497\n",
      "Iteration 31, loss = 1.01726334\n",
      "Iteration 32, loss = 1.01498251\n",
      "Iteration 33, loss = 1.01113516\n",
      "Iteration 34, loss = 1.01102796\n",
      "Iteration 35, loss = 1.00902762\n",
      "Iteration 36, loss = 1.00852171\n",
      "Iteration 37, loss = 1.00702061\n",
      "Iteration 38, loss = 1.00814107\n",
      "Iteration 39, loss = 1.00889924\n",
      "Iteration 40, loss = 1.00461159\n",
      "Iteration 41, loss = 1.00735917\n",
      "Iteration 42, loss = 1.00251675\n",
      "Iteration 43, loss = 1.00868609\n",
      "Iteration 44, loss = 1.00035827\n",
      "Iteration 45, loss = 1.00343271\n",
      "Iteration 46, loss = 1.00222795\n",
      "Iteration 47, loss = 0.99847362\n",
      "Iteration 48, loss = 0.99758211\n",
      "Iteration 49, loss = 0.99665364\n",
      "Iteration 50, loss = 0.99487211\n",
      "Iteration 51, loss = 0.99443512\n",
      "Iteration 52, loss = 0.99260218\n",
      "Iteration 53, loss = 0.99250312\n",
      "Iteration 54, loss = 0.98989346\n",
      "Iteration 55, loss = 0.99183734\n",
      "Iteration 56, loss = 0.99045066\n",
      "Iteration 57, loss = 0.98624402\n",
      "Iteration 58, loss = 0.98891835\n",
      "Iteration 59, loss = 0.98617055\n",
      "Iteration 60, loss = 0.98550303\n",
      "Iteration 61, loss = 0.98962931\n",
      "Iteration 62, loss = 0.98778943\n",
      "Iteration 63, loss = 0.98260337\n",
      "Iteration 64, loss = 0.98013507\n",
      "Iteration 65, loss = 0.98007807\n",
      "Iteration 66, loss = 0.97757305\n",
      "Iteration 67, loss = 0.97646710\n",
      "Iteration 68, loss = 0.97575616\n",
      "Iteration 69, loss = 0.97650048\n",
      "Iteration 70, loss = 0.97432333\n",
      "Iteration 71, loss = 0.97254590\n",
      "Iteration 72, loss = 0.97277866\n",
      "Iteration 73, loss = 0.97299161\n",
      "Iteration 74, loss = 0.98444393\n",
      "Iteration 75, loss = 0.96926824\n",
      "Iteration 76, loss = 0.96899579\n",
      "Iteration 77, loss = 0.96950264\n",
      "Iteration 78, loss = 0.96369340\n",
      "Iteration 79, loss = 0.97008237\n",
      "Iteration 80, loss = 0.97336683\n",
      "Iteration 81, loss = 0.96275275\n",
      "Iteration 82, loss = 0.97103988\n",
      "Iteration 83, loss = 0.97479100\n",
      "Iteration 84, loss = 0.96370638\n",
      "Iteration 85, loss = 0.96950102\n",
      "Iteration 86, loss = 0.96086074\n",
      "Iteration 87, loss = 0.95832441\n",
      "Iteration 88, loss = 0.95817893\n",
      "Iteration 89, loss = 0.95499662\n",
      "Iteration 90, loss = 0.95990215\n",
      "Iteration 91, loss = 0.95216876\n",
      "Iteration 92, loss = 0.95464622\n",
      "Iteration 93, loss = 0.95716618\n",
      "Iteration 94, loss = 0.95585811\n",
      "Iteration 95, loss = 0.95756903\n",
      "Iteration 96, loss = 0.95071848\n",
      "Iteration 97, loss = 0.95114568\n",
      "Iteration 98, loss = 0.95097354\n",
      "Iteration 99, loss = 0.95301008\n",
      "Iteration 100, loss = 0.95062950\n",
      "Iteration 1, loss = 1.46546552\n",
      "Iteration 2, loss = 1.22363205\n",
      "Iteration 3, loss = 1.19707825\n",
      "Iteration 4, loss = 1.18020593\n",
      "Iteration 5, loss = 1.16470530\n",
      "Iteration 6, loss = 1.13928319\n",
      "Iteration 7, loss = 1.11907287\n",
      "Iteration 8, loss = 1.10757405\n",
      "Iteration 9, loss = 1.09858113\n",
      "Iteration 10, loss = 1.09157470\n",
      "Iteration 11, loss = 1.08099119\n",
      "Iteration 12, loss = 1.07873461\n",
      "Iteration 13, loss = 1.07587550\n",
      "Iteration 14, loss = 1.06927652\n",
      "Iteration 15, loss = 1.06838857\n",
      "Iteration 16, loss = 1.06770707\n",
      "Iteration 17, loss = 1.06008009\n",
      "Iteration 18, loss = 1.05760787\n",
      "Iteration 19, loss = 1.05435615\n",
      "Iteration 20, loss = 1.05054800\n",
      "Iteration 21, loss = 1.05021397\n",
      "Iteration 22, loss = 1.04554597\n",
      "Iteration 23, loss = 1.04276019\n",
      "Iteration 24, loss = 1.04144340\n",
      "Iteration 25, loss = 1.03854969\n",
      "Iteration 26, loss = 1.04138790\n",
      "Iteration 27, loss = 1.03626475\n",
      "Iteration 28, loss = 1.03551586\n",
      "Iteration 29, loss = 1.03162380\n",
      "Iteration 30, loss = 1.03376134\n",
      "Iteration 31, loss = 1.02844249\n",
      "Iteration 32, loss = 1.02844854\n",
      "Iteration 33, loss = 1.02531482\n",
      "Iteration 34, loss = 1.02286909\n",
      "Iteration 35, loss = 1.01925685\n",
      "Iteration 36, loss = 1.02039390\n",
      "Iteration 37, loss = 1.01495362\n",
      "Iteration 38, loss = 1.01639187\n",
      "Iteration 39, loss = 1.01482287\n",
      "Iteration 40, loss = 1.01085227\n",
      "Iteration 41, loss = 1.01494776\n",
      "Iteration 42, loss = 1.00869467\n",
      "Iteration 43, loss = 1.00873796\n",
      "Iteration 44, loss = 1.00458462\n",
      "Iteration 45, loss = 1.00419658\n",
      "Iteration 46, loss = 0.99886686\n",
      "Iteration 47, loss = 1.00008812\n",
      "Iteration 48, loss = 0.99914617\n",
      "Iteration 49, loss = 0.99807235\n",
      "Iteration 50, loss = 0.99404450\n",
      "Iteration 51, loss = 0.99812865\n",
      "Iteration 52, loss = 0.98781272\n",
      "Iteration 53, loss = 0.99262612\n",
      "Iteration 54, loss = 0.98772635\n",
      "Iteration 55, loss = 0.98326204\n",
      "Iteration 56, loss = 0.98649890\n",
      "Iteration 57, loss = 0.98334547\n",
      "Iteration 58, loss = 0.98387056\n",
      "Iteration 59, loss = 0.97729040\n",
      "Iteration 60, loss = 0.97666146\n",
      "Iteration 61, loss = 0.97461715\n",
      "Iteration 62, loss = 0.97467848\n",
      "Iteration 63, loss = 0.97156383\n",
      "Iteration 64, loss = 0.96912510\n",
      "Iteration 65, loss = 0.96881859\n",
      "Iteration 66, loss = 0.96732296\n",
      "Iteration 67, loss = 0.96504990\n",
      "Iteration 68, loss = 0.96567423\n",
      "Iteration 69, loss = 0.96450807\n",
      "Iteration 70, loss = 0.96300741\n",
      "Iteration 71, loss = 0.96267468\n",
      "Iteration 72, loss = 0.96295656\n",
      "Iteration 73, loss = 0.95797580\n",
      "Iteration 74, loss = 0.96169293\n",
      "Iteration 75, loss = 0.96035339\n",
      "Iteration 76, loss = 0.95730654\n",
      "Iteration 77, loss = 0.95668208\n",
      "Iteration 78, loss = 0.95934401\n",
      "Iteration 79, loss = 0.96043421\n",
      "Iteration 80, loss = 0.95318372\n",
      "Iteration 81, loss = 0.95677798\n",
      "Iteration 82, loss = 0.95600243\n",
      "Iteration 83, loss = 0.95822551\n",
      "Iteration 84, loss = 0.95469916\n",
      "Iteration 85, loss = 0.95112431\n",
      "Iteration 86, loss = 0.94935094\n",
      "Iteration 87, loss = 0.94918709\n",
      "Iteration 88, loss = 0.94796166\n",
      "Iteration 89, loss = 0.94671292\n",
      "Iteration 90, loss = 0.94558686\n",
      "Iteration 91, loss = 0.94560370\n",
      "Iteration 92, loss = 0.94584138\n",
      "Iteration 93, loss = 0.94684830\n",
      "Iteration 94, loss = 0.94529661\n",
      "Iteration 95, loss = 0.94903139\n",
      "Iteration 96, loss = 0.94154100\n",
      "Iteration 97, loss = 0.94145062\n",
      "Iteration 98, loss = 0.94283568\n",
      "Iteration 99, loss = 0.94115929\n",
      "Iteration 100, loss = 0.94066585\n",
      "Iteration 1, loss = 1.46069432\n",
      "Iteration 2, loss = 1.21518119\n",
      "Iteration 3, loss = 1.17391790\n",
      "Iteration 4, loss = 1.15974089\n",
      "Iteration 5, loss = 1.13770387\n",
      "Iteration 6, loss = 1.10557078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 1.08216815\n",
      "Iteration 8, loss = 1.07530603\n",
      "Iteration 9, loss = 1.06696226\n",
      "Iteration 10, loss = 1.05421936\n",
      "Iteration 11, loss = 1.04715806\n",
      "Iteration 12, loss = 1.04065386\n",
      "Iteration 13, loss = 1.03466406\n",
      "Iteration 14, loss = 1.02961853\n",
      "Iteration 15, loss = 1.02498887\n",
      "Iteration 16, loss = 1.02656989\n",
      "Iteration 17, loss = 1.01527164\n",
      "Iteration 18, loss = 1.01847544\n",
      "Iteration 19, loss = 1.01346869\n",
      "Iteration 20, loss = 1.00982805\n",
      "Iteration 21, loss = 1.00445084\n",
      "Iteration 22, loss = 1.00368106\n",
      "Iteration 23, loss = 1.00143630\n",
      "Iteration 24, loss = 1.00414528\n",
      "Iteration 25, loss = 1.00619869\n",
      "Iteration 26, loss = 1.00349971\n",
      "Iteration 27, loss = 0.99567947\n",
      "Iteration 28, loss = 0.99369840\n",
      "Iteration 29, loss = 0.99065056\n",
      "Iteration 30, loss = 0.99246148\n",
      "Iteration 31, loss = 0.98835323\n",
      "Iteration 32, loss = 0.98908969\n",
      "Iteration 33, loss = 0.98434595\n",
      "Iteration 34, loss = 0.98440385\n",
      "Iteration 35, loss = 0.98314261\n",
      "Iteration 36, loss = 0.98205207\n",
      "Iteration 37, loss = 0.98062802\n",
      "Iteration 38, loss = 0.98164683\n",
      "Iteration 39, loss = 0.97676513\n",
      "Iteration 40, loss = 0.98067449\n",
      "Iteration 41, loss = 0.97532733\n",
      "Iteration 42, loss = 0.97791158\n",
      "Iteration 43, loss = 0.97526805\n",
      "Iteration 44, loss = 0.97436218\n",
      "Iteration 45, loss = 0.97425895\n",
      "Iteration 46, loss = 0.97522428\n",
      "Iteration 47, loss = 0.97264852\n",
      "Iteration 48, loss = 0.97323941\n",
      "Iteration 49, loss = 0.97165588\n",
      "Iteration 50, loss = 0.97009197\n",
      "Iteration 51, loss = 0.97059711\n",
      "Iteration 52, loss = 0.96963114\n",
      "Iteration 53, loss = 0.96390137\n",
      "Iteration 54, loss = 0.96696694\n",
      "Iteration 55, loss = 0.96469314\n",
      "Iteration 56, loss = 0.96226494\n",
      "Iteration 57, loss = 0.96383261\n",
      "Iteration 58, loss = 0.96213907\n",
      "Iteration 59, loss = 0.96005820\n",
      "Iteration 60, loss = 0.95813900\n",
      "Iteration 61, loss = 0.95836233\n",
      "Iteration 62, loss = 0.95471053\n",
      "Iteration 63, loss = 0.95434251\n",
      "Iteration 64, loss = 0.95505875\n",
      "Iteration 65, loss = 0.95331106\n",
      "Iteration 66, loss = 0.95318540\n",
      "Iteration 67, loss = 0.95033307\n",
      "Iteration 68, loss = 0.95044951\n",
      "Iteration 69, loss = 0.95178653\n",
      "Iteration 70, loss = 0.95382946\n",
      "Iteration 71, loss = 0.95033697\n",
      "Iteration 72, loss = 0.94593126\n",
      "Iteration 73, loss = 0.95159332\n",
      "Iteration 74, loss = 0.94485042\n",
      "Iteration 75, loss = 0.94323647\n",
      "Iteration 76, loss = 0.94525514\n",
      "Iteration 77, loss = 0.94163201\n",
      "Iteration 78, loss = 0.93928511\n",
      "Iteration 79, loss = 0.93923632\n",
      "Iteration 80, loss = 0.94165101\n",
      "Iteration 81, loss = 0.93852716\n",
      "Iteration 82, loss = 0.94020989\n",
      "Iteration 83, loss = 0.94114185\n",
      "Iteration 84, loss = 0.93856719\n",
      "Iteration 85, loss = 0.93482001\n",
      "Iteration 86, loss = 0.93385829\n",
      "Iteration 87, loss = 0.93637884\n",
      "Iteration 88, loss = 0.93343388\n",
      "Iteration 89, loss = 0.93144422\n",
      "Iteration 90, loss = 0.93419223\n",
      "Iteration 91, loss = 0.92950551\n",
      "Iteration 92, loss = 0.92768278\n",
      "Iteration 93, loss = 0.92834165\n",
      "Iteration 94, loss = 0.92821133\n",
      "Iteration 95, loss = 0.93011438\n",
      "Iteration 96, loss = 0.92370287\n",
      "Iteration 97, loss = 0.92495529\n",
      "Iteration 98, loss = 0.92650306\n",
      "Iteration 99, loss = 0.92397456\n",
      "Iteration 100, loss = 0.92300799\n",
      "Iteration 1, loss = 8.22382320\n",
      "Iteration 2, loss = 4.39707570\n",
      "Iteration 3, loss = 2.70451629\n",
      "Iteration 4, loss = 2.18580650\n",
      "Iteration 5, loss = 1.92850482\n",
      "Iteration 6, loss = 1.67794887\n",
      "Iteration 7, loss = 1.45018729\n",
      "Iteration 8, loss = 1.37677088\n",
      "Iteration 9, loss = 1.29392899\n",
      "Iteration 10, loss = 1.23841528\n",
      "Iteration 11, loss = 1.22199363\n",
      "Iteration 12, loss = 1.28845447\n",
      "Iteration 13, loss = 1.24735593\n",
      "Iteration 14, loss = 1.36434532\n",
      "Iteration 15, loss = 1.40671541\n",
      "Iteration 16, loss = 1.30446999\n",
      "Iteration 17, loss = 1.27893595\n",
      "Iteration 18, loss = 1.20257119\n",
      "Iteration 19, loss = 1.20833951\n",
      "Iteration 20, loss = 1.15356604\n",
      "Iteration 21, loss = 1.16972045\n",
      "Iteration 22, loss = 1.16985811\n",
      "Iteration 23, loss = 1.18029274\n",
      "Iteration 24, loss = 1.13511099\n",
      "Iteration 25, loss = 1.13508545\n",
      "Iteration 26, loss = 1.12050537\n",
      "Iteration 27, loss = 1.13428593\n",
      "Iteration 28, loss = 1.15267273\n",
      "Iteration 29, loss = 1.22611409\n",
      "Iteration 30, loss = 1.30206852\n",
      "Iteration 31, loss = 1.30874048\n",
      "Iteration 32, loss = 1.18380832\n",
      "Iteration 33, loss = 1.21099274\n",
      "Iteration 34, loss = 1.17454233\n",
      "Iteration 35, loss = 1.20454052\n",
      "Iteration 36, loss = 1.20702421\n",
      "Iteration 37, loss = 1.17349155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35207746\n",
      "Iteration 2, loss = 1.20114026\n",
      "Iteration 3, loss = 1.19430311\n",
      "Iteration 4, loss = 1.17601712\n",
      "Iteration 5, loss = 1.14028711\n",
      "Iteration 6, loss = 1.12092398\n",
      "Iteration 7, loss = 1.10877618\n",
      "Iteration 8, loss = 1.09239377\n",
      "Iteration 9, loss = 1.08328871\n",
      "Iteration 10, loss = 1.07707000\n",
      "Iteration 11, loss = 1.06316849\n",
      "Iteration 12, loss = 1.06006602\n",
      "Iteration 13, loss = 1.05095738\n",
      "Iteration 14, loss = 1.05052041\n",
      "Iteration 15, loss = 1.04370316\n",
      "Iteration 16, loss = 1.04083608\n",
      "Iteration 17, loss = 1.03626173\n",
      "Iteration 18, loss = 1.03036468\n",
      "Iteration 19, loss = 1.02984548\n",
      "Iteration 20, loss = 1.02418689\n",
      "Iteration 21, loss = 1.02296479\n",
      "Iteration 22, loss = 1.02000471\n",
      "Iteration 23, loss = 1.01654628\n",
      "Iteration 24, loss = 1.01531598\n",
      "Iteration 25, loss = 1.01205348\n",
      "Iteration 26, loss = 1.01130755\n",
      "Iteration 27, loss = 1.01466938\n",
      "Iteration 28, loss = 1.01074140\n",
      "Iteration 29, loss = 1.00268170\n",
      "Iteration 30, loss = 1.00546369\n",
      "Iteration 31, loss = 1.00016727\n",
      "Iteration 32, loss = 0.99453130\n",
      "Iteration 33, loss = 0.99398237\n",
      "Iteration 34, loss = 0.99276854\n",
      "Iteration 35, loss = 0.99003444\n",
      "Iteration 36, loss = 0.98764007\n",
      "Iteration 37, loss = 0.99022477\n",
      "Iteration 38, loss = 0.98201956\n",
      "Iteration 39, loss = 0.97963855\n",
      "Iteration 40, loss = 0.98220944\n",
      "Iteration 41, loss = 0.98194452\n",
      "Iteration 42, loss = 0.98432619\n",
      "Iteration 43, loss = 0.97291118\n",
      "Iteration 44, loss = 0.97358807\n",
      "Iteration 45, loss = 0.97243180\n",
      "Iteration 46, loss = 0.96654962\n",
      "Iteration 47, loss = 0.96835569\n",
      "Iteration 48, loss = 0.96737409\n",
      "Iteration 49, loss = 0.96718571\n",
      "Iteration 50, loss = 0.96903639\n",
      "Iteration 51, loss = 0.96843788\n",
      "Iteration 52, loss = 0.96874491\n",
      "Iteration 53, loss = 0.96018811\n",
      "Iteration 54, loss = 0.95609602\n",
      "Iteration 55, loss = 0.95910557\n",
      "Iteration 56, loss = 0.95379879\n",
      "Iteration 57, loss = 0.95018985\n",
      "Iteration 58, loss = 0.94786830\n",
      "Iteration 59, loss = 0.94788372\n",
      "Iteration 60, loss = 0.94497950\n",
      "Iteration 61, loss = 0.94297540\n",
      "Iteration 62, loss = 0.94239577\n",
      "Iteration 63, loss = 0.93951311\n",
      "Iteration 64, loss = 0.93917956\n",
      "Iteration 65, loss = 0.93995241\n",
      "Iteration 66, loss = 0.93481990\n",
      "Iteration 67, loss = 0.93921150\n",
      "Iteration 68, loss = 0.93542302\n",
      "Iteration 69, loss = 0.93356180\n",
      "Iteration 70, loss = 0.93136091\n",
      "Iteration 71, loss = 0.92987513\n",
      "Iteration 72, loss = 0.93181745\n",
      "Iteration 73, loss = 0.93446607\n",
      "Iteration 74, loss = 0.93235245\n",
      "Iteration 75, loss = 0.93564186\n",
      "Iteration 76, loss = 0.92927643\n",
      "Iteration 77, loss = 0.92759774\n",
      "Iteration 78, loss = 0.92633615\n",
      "Iteration 79, loss = 0.92412533\n",
      "Iteration 80, loss = 0.92451260\n",
      "Iteration 81, loss = 0.92134469\n",
      "Iteration 82, loss = 0.91948574\n",
      "Iteration 83, loss = 0.92327852\n",
      "Iteration 84, loss = 0.92235840\n",
      "Iteration 85, loss = 0.91928488\n",
      "Iteration 86, loss = 0.91809254\n",
      "Iteration 87, loss = 0.91623708\n",
      "Iteration 88, loss = 0.92037504\n",
      "Iteration 89, loss = 0.91363474\n",
      "Iteration 90, loss = 0.91646982\n",
      "Iteration 91, loss = 0.91557863\n",
      "Iteration 92, loss = 0.91140550\n",
      "Iteration 93, loss = 0.90944691\n",
      "Iteration 94, loss = 0.90953458\n",
      "Iteration 95, loss = 0.90773884\n",
      "Iteration 96, loss = 0.90773991\n",
      "Iteration 97, loss = 0.91011321\n",
      "Iteration 98, loss = 0.91634365\n",
      "Iteration 99, loss = 0.91341610\n",
      "Iteration 100, loss = 0.90311083\n",
      "Iteration 1, loss = 1.35212483\n",
      "Iteration 2, loss = 1.20235876\n",
      "Iteration 3, loss = 1.19282643\n",
      "Iteration 4, loss = 1.16281149\n",
      "Iteration 5, loss = 1.13135987\n",
      "Iteration 6, loss = 1.10978376\n",
      "Iteration 7, loss = 1.09310208\n",
      "Iteration 8, loss = 1.07501313\n",
      "Iteration 9, loss = 1.06447565\n",
      "Iteration 10, loss = 1.05401894\n",
      "Iteration 11, loss = 1.04624623\n",
      "Iteration 12, loss = 1.04315294\n",
      "Iteration 13, loss = 1.03630339\n",
      "Iteration 14, loss = 1.03477197\n",
      "Iteration 15, loss = 1.03366373\n",
      "Iteration 16, loss = 1.03274858\n",
      "Iteration 17, loss = 1.02581615\n",
      "Iteration 18, loss = 1.02073147\n",
      "Iteration 19, loss = 1.01950315\n",
      "Iteration 20, loss = 1.01542007\n",
      "Iteration 21, loss = 1.01272852\n",
      "Iteration 22, loss = 1.01109953\n",
      "Iteration 23, loss = 1.00754484\n",
      "Iteration 24, loss = 1.00546155\n",
      "Iteration 25, loss = 1.00375072\n",
      "Iteration 26, loss = 1.00327256\n",
      "Iteration 27, loss = 1.01270810\n",
      "Iteration 28, loss = 1.00671587\n",
      "Iteration 29, loss = 0.99576961\n",
      "Iteration 30, loss = 1.00254216\n",
      "Iteration 31, loss = 0.99786210\n",
      "Iteration 32, loss = 0.99523307\n",
      "Iteration 33, loss = 0.99084706\n",
      "Iteration 34, loss = 0.98791897\n",
      "Iteration 35, loss = 0.98807156\n",
      "Iteration 36, loss = 0.98440213\n",
      "Iteration 37, loss = 0.98553030\n",
      "Iteration 38, loss = 0.98017560\n",
      "Iteration 39, loss = 0.98153883\n",
      "Iteration 40, loss = 0.97625452\n",
      "Iteration 41, loss = 0.97750596\n",
      "Iteration 42, loss = 0.97634592\n",
      "Iteration 43, loss = 0.97187939\n",
      "Iteration 44, loss = 0.97341656\n",
      "Iteration 45, loss = 0.97061740\n",
      "Iteration 46, loss = 0.96725039\n",
      "Iteration 47, loss = 0.96722803\n",
      "Iteration 48, loss = 0.96175221\n",
      "Iteration 49, loss = 0.96794426\n",
      "Iteration 50, loss = 0.96282765\n",
      "Iteration 51, loss = 0.96201456\n",
      "Iteration 52, loss = 0.96803514\n",
      "Iteration 53, loss = 0.95971894\n",
      "Iteration 54, loss = 0.95837992\n",
      "Iteration 55, loss = 0.95389601\n",
      "Iteration 56, loss = 0.96098548\n",
      "Iteration 57, loss = 0.95407136\n",
      "Iteration 58, loss = 0.95281232\n",
      "Iteration 59, loss = 0.94894422\n",
      "Iteration 60, loss = 0.95053421\n",
      "Iteration 61, loss = 0.94430849\n",
      "Iteration 62, loss = 0.94355727\n",
      "Iteration 63, loss = 0.94518831\n",
      "Iteration 64, loss = 0.94109489\n",
      "Iteration 65, loss = 0.94027445\n",
      "Iteration 66, loss = 0.93890888\n",
      "Iteration 67, loss = 0.93868536\n",
      "Iteration 68, loss = 0.93724608\n",
      "Iteration 69, loss = 0.93422343\n",
      "Iteration 70, loss = 0.93378840\n",
      "Iteration 71, loss = 0.93129875\n",
      "Iteration 72, loss = 0.93372801\n",
      "Iteration 73, loss = 0.93279722\n",
      "Iteration 74, loss = 0.92836948\n",
      "Iteration 75, loss = 0.92928461\n",
      "Iteration 76, loss = 0.93155710\n",
      "Iteration 77, loss = 0.92880859\n",
      "Iteration 78, loss = 0.92564269\n",
      "Iteration 79, loss = 0.92984379\n",
      "Iteration 80, loss = 0.92222440\n",
      "Iteration 81, loss = 0.92891302\n",
      "Iteration 82, loss = 0.92302527\n",
      "Iteration 83, loss = 0.91931136\n",
      "Iteration 84, loss = 0.92001399\n",
      "Iteration 85, loss = 0.91744096\n",
      "Iteration 86, loss = 0.91644000\n",
      "Iteration 87, loss = 0.92075472\n",
      "Iteration 88, loss = 0.91239468\n",
      "Iteration 89, loss = 0.91174463\n",
      "Iteration 90, loss = 0.91375352\n",
      "Iteration 91, loss = 0.91261209\n",
      "Iteration 92, loss = 0.91145155\n",
      "Iteration 93, loss = 0.91027768\n",
      "Iteration 94, loss = 0.90734124\n",
      "Iteration 95, loss = 0.91294071\n",
      "Iteration 96, loss = 0.90623585\n",
      "Iteration 97, loss = 0.90502250\n",
      "Iteration 98, loss = 0.90318049\n",
      "Iteration 99, loss = 0.91103298\n",
      "Iteration 100, loss = 0.90011030\n",
      "Iteration 1, loss = 1.35363575\n",
      "Iteration 2, loss = 1.20396004\n",
      "Iteration 3, loss = 1.18329236\n",
      "Iteration 4, loss = 1.16330954\n",
      "Iteration 5, loss = 1.12945806\n",
      "Iteration 6, loss = 1.10882178\n",
      "Iteration 7, loss = 1.09489350\n",
      "Iteration 8, loss = 1.07733149\n",
      "Iteration 9, loss = 1.07149938\n",
      "Iteration 10, loss = 1.06048251\n",
      "Iteration 11, loss = 1.05232045\n",
      "Iteration 12, loss = 1.05081001\n",
      "Iteration 13, loss = 1.04579321\n",
      "Iteration 14, loss = 1.03739829\n",
      "Iteration 15, loss = 1.03388467\n",
      "Iteration 16, loss = 1.03360993\n",
      "Iteration 17, loss = 1.02714111\n",
      "Iteration 18, loss = 1.02508005\n",
      "Iteration 19, loss = 1.02159185\n",
      "Iteration 20, loss = 1.02473422\n",
      "Iteration 21, loss = 1.01909490\n",
      "Iteration 22, loss = 1.01584336\n",
      "Iteration 23, loss = 1.01741605\n",
      "Iteration 24, loss = 1.02203109\n",
      "Iteration 25, loss = 1.01717853\n",
      "Iteration 26, loss = 1.01344830\n",
      "Iteration 27, loss = 1.01976435\n",
      "Iteration 28, loss = 1.00698228\n",
      "Iteration 29, loss = 1.01241702\n",
      "Iteration 30, loss = 1.00967991\n",
      "Iteration 31, loss = 1.00679880\n",
      "Iteration 32, loss = 1.00997367\n",
      "Iteration 33, loss = 1.00243578\n",
      "Iteration 34, loss = 0.99987389\n",
      "Iteration 35, loss = 0.99965687\n",
      "Iteration 36, loss = 0.99647680\n",
      "Iteration 37, loss = 0.99454777\n",
      "Iteration 38, loss = 0.99452376\n",
      "Iteration 39, loss = 0.99089582\n",
      "Iteration 40, loss = 0.98806858\n",
      "Iteration 41, loss = 0.98816012\n",
      "Iteration 42, loss = 0.99257951\n",
      "Iteration 43, loss = 0.98674116\n",
      "Iteration 44, loss = 0.98431863\n",
      "Iteration 45, loss = 0.98442950\n",
      "Iteration 46, loss = 0.98136087\n",
      "Iteration 47, loss = 0.97545409\n",
      "Iteration 48, loss = 0.97623054\n",
      "Iteration 49, loss = 0.98225763\n",
      "Iteration 50, loss = 0.97313328\n",
      "Iteration 51, loss = 0.97242479\n",
      "Iteration 52, loss = 0.97346397\n",
      "Iteration 53, loss = 0.96850604\n",
      "Iteration 54, loss = 0.96257731\n",
      "Iteration 55, loss = 0.96168415\n",
      "Iteration 56, loss = 0.96176543\n",
      "Iteration 57, loss = 0.95630442\n",
      "Iteration 58, loss = 0.95614052\n",
      "Iteration 59, loss = 0.95646471\n",
      "Iteration 60, loss = 0.95074274\n",
      "Iteration 61, loss = 0.95174677\n",
      "Iteration 62, loss = 0.94967131\n",
      "Iteration 63, loss = 0.95132555\n",
      "Iteration 64, loss = 0.95507867\n",
      "Iteration 65, loss = 0.94304620\n",
      "Iteration 66, loss = 0.94577190\n",
      "Iteration 67, loss = 0.93902861\n",
      "Iteration 68, loss = 0.94156414\n",
      "Iteration 69, loss = 0.93537478\n",
      "Iteration 70, loss = 0.93752454\n",
      "Iteration 71, loss = 0.93603485\n",
      "Iteration 72, loss = 0.93170731\n",
      "Iteration 73, loss = 0.93029931\n",
      "Iteration 74, loss = 0.92990982\n",
      "Iteration 75, loss = 0.93168723\n",
      "Iteration 76, loss = 0.92445863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77, loss = 0.92848044\n",
      "Iteration 78, loss = 0.92046801\n",
      "Iteration 79, loss = 0.92125756\n",
      "Iteration 80, loss = 0.92377050\n",
      "Iteration 81, loss = 0.91686028\n",
      "Iteration 82, loss = 0.91393988\n",
      "Iteration 83, loss = 0.91231763\n",
      "Iteration 84, loss = 0.90953129\n",
      "Iteration 85, loss = 0.90928609\n",
      "Iteration 86, loss = 0.90873682\n",
      "Iteration 87, loss = 0.91013323\n",
      "Iteration 88, loss = 0.90981322\n",
      "Iteration 89, loss = 0.90393375\n",
      "Iteration 90, loss = 0.90302547\n",
      "Iteration 91, loss = 0.89857733\n",
      "Iteration 92, loss = 0.89738118\n",
      "Iteration 93, loss = 0.89789794\n",
      "Iteration 94, loss = 0.89473625\n",
      "Iteration 95, loss = 0.89481394\n",
      "Iteration 96, loss = 0.89536617\n",
      "Iteration 97, loss = 0.89508627\n",
      "Iteration 98, loss = 0.89288367\n",
      "Iteration 99, loss = 0.89233395\n",
      "Iteration 100, loss = 0.88673631\n",
      "Iteration 1, loss = 1.34084455\n",
      "Iteration 2, loss = 1.21795571\n",
      "Iteration 3, loss = 1.19756562\n",
      "Iteration 4, loss = 1.17383152\n",
      "Iteration 5, loss = 1.14606626\n",
      "Iteration 6, loss = 1.12916564\n",
      "Iteration 7, loss = 1.11392478\n",
      "Iteration 8, loss = 1.09482346\n",
      "Iteration 9, loss = 1.08828848\n",
      "Iteration 10, loss = 1.07639785\n",
      "Iteration 11, loss = 1.06928668\n",
      "Iteration 12, loss = 1.06476768\n",
      "Iteration 13, loss = 1.06299095\n",
      "Iteration 14, loss = 1.05694269\n",
      "Iteration 15, loss = 1.05640475\n",
      "Iteration 16, loss = 1.04880324\n",
      "Iteration 17, loss = 1.04447249\n",
      "Iteration 18, loss = 1.03902138\n",
      "Iteration 19, loss = 1.03686314\n",
      "Iteration 20, loss = 1.03417591\n",
      "Iteration 21, loss = 1.02939928\n",
      "Iteration 22, loss = 1.03041639\n",
      "Iteration 23, loss = 1.02542412\n",
      "Iteration 24, loss = 1.02170988\n",
      "Iteration 25, loss = 1.01781871\n",
      "Iteration 26, loss = 1.01462686\n",
      "Iteration 27, loss = 1.01324700\n",
      "Iteration 28, loss = 1.01032788\n",
      "Iteration 29, loss = 1.00903241\n",
      "Iteration 30, loss = 1.00975885\n",
      "Iteration 31, loss = 1.00241088\n",
      "Iteration 32, loss = 1.00830014\n",
      "Iteration 33, loss = 0.99849524\n",
      "Iteration 34, loss = 0.99714587\n",
      "Iteration 35, loss = 0.99558760\n",
      "Iteration 36, loss = 0.99438673\n",
      "Iteration 37, loss = 0.99032798\n",
      "Iteration 38, loss = 0.98823723\n",
      "Iteration 39, loss = 0.98914717\n",
      "Iteration 40, loss = 0.98933565\n",
      "Iteration 41, loss = 0.98169874\n",
      "Iteration 42, loss = 0.97993094\n",
      "Iteration 43, loss = 0.98533051\n",
      "Iteration 44, loss = 0.97657448\n",
      "Iteration 45, loss = 0.97716698\n",
      "Iteration 46, loss = 0.98025364\n",
      "Iteration 47, loss = 0.97091475\n",
      "Iteration 48, loss = 0.97058374\n",
      "Iteration 49, loss = 0.97078817\n",
      "Iteration 50, loss = 0.96973286\n",
      "Iteration 51, loss = 0.96636915\n",
      "Iteration 52, loss = 0.96271934\n",
      "Iteration 53, loss = 0.96458687\n",
      "Iteration 54, loss = 0.96046790\n",
      "Iteration 55, loss = 0.96519745\n",
      "Iteration 56, loss = 0.96013347\n",
      "Iteration 57, loss = 0.95931866\n",
      "Iteration 58, loss = 0.95611044\n",
      "Iteration 59, loss = 0.96062297\n",
      "Iteration 60, loss = 0.94809008\n",
      "Iteration 61, loss = 0.95332081\n",
      "Iteration 62, loss = 0.94650540\n",
      "Iteration 63, loss = 0.94747959\n",
      "Iteration 64, loss = 0.95053275\n",
      "Iteration 65, loss = 0.93907736\n",
      "Iteration 66, loss = 0.94044472\n",
      "Iteration 67, loss = 0.93929276\n",
      "Iteration 68, loss = 0.93634357\n",
      "Iteration 69, loss = 0.93969606\n",
      "Iteration 70, loss = 0.93508633\n",
      "Iteration 71, loss = 0.93185177\n",
      "Iteration 72, loss = 0.93032411\n",
      "Iteration 73, loss = 0.92923927\n",
      "Iteration 74, loss = 0.93173634\n",
      "Iteration 75, loss = 0.92990704\n",
      "Iteration 76, loss = 0.92985724\n",
      "Iteration 77, loss = 0.92325723\n",
      "Iteration 78, loss = 0.92335122\n",
      "Iteration 79, loss = 0.92505159\n",
      "Iteration 80, loss = 0.91957834\n",
      "Iteration 81, loss = 0.91730031\n",
      "Iteration 82, loss = 0.91461893\n",
      "Iteration 83, loss = 0.91547788\n",
      "Iteration 84, loss = 0.91132011\n",
      "Iteration 85, loss = 0.91279648\n",
      "Iteration 86, loss = 0.91424782\n",
      "Iteration 87, loss = 0.91768655\n",
      "Iteration 88, loss = 0.90954267\n",
      "Iteration 89, loss = 0.91228371\n",
      "Iteration 90, loss = 0.90788252\n",
      "Iteration 91, loss = 0.91050951\n",
      "Iteration 92, loss = 0.90546459\n",
      "Iteration 93, loss = 0.90490146\n",
      "Iteration 94, loss = 0.90709966\n",
      "Iteration 95, loss = 0.89816706\n",
      "Iteration 96, loss = 0.90669548\n",
      "Iteration 97, loss = 0.89809502\n",
      "Iteration 98, loss = 0.89396083\n",
      "Iteration 99, loss = 0.89860935\n",
      "Iteration 100, loss = 0.89666825\n",
      "Iteration 1, loss = 1.34613426\n",
      "Iteration 2, loss = 1.21026609\n",
      "Iteration 3, loss = 1.18018006\n",
      "Iteration 4, loss = 1.15379941\n",
      "Iteration 5, loss = 1.12088276\n",
      "Iteration 6, loss = 1.10121358\n",
      "Iteration 7, loss = 1.08107019\n",
      "Iteration 8, loss = 1.06636017\n",
      "Iteration 9, loss = 1.05149386\n",
      "Iteration 10, loss = 1.04277989\n",
      "Iteration 11, loss = 1.03544220\n",
      "Iteration 12, loss = 1.02869726\n",
      "Iteration 13, loss = 1.01941518\n",
      "Iteration 14, loss = 1.01807461\n",
      "Iteration 15, loss = 1.01350518\n",
      "Iteration 16, loss = 1.01181808\n",
      "Iteration 17, loss = 1.00433843\n",
      "Iteration 18, loss = 0.99781887\n",
      "Iteration 19, loss = 0.99743084\n",
      "Iteration 20, loss = 0.99505105\n",
      "Iteration 21, loss = 0.99170361\n",
      "Iteration 22, loss = 0.99017007\n",
      "Iteration 23, loss = 0.98792305\n",
      "Iteration 24, loss = 0.98582776\n",
      "Iteration 25, loss = 0.98593584\n",
      "Iteration 26, loss = 0.98510214\n",
      "Iteration 27, loss = 0.97748977\n",
      "Iteration 28, loss = 0.97801406\n",
      "Iteration 29, loss = 0.97750777\n",
      "Iteration 30, loss = 0.97760500\n",
      "Iteration 31, loss = 0.97371066\n",
      "Iteration 32, loss = 0.96958327\n",
      "Iteration 33, loss = 0.97921288\n",
      "Iteration 34, loss = 0.96415351\n",
      "Iteration 35, loss = 0.96642234\n",
      "Iteration 36, loss = 0.96454386\n",
      "Iteration 37, loss = 0.96466299\n",
      "Iteration 38, loss = 0.96491283\n",
      "Iteration 39, loss = 0.96260801\n",
      "Iteration 40, loss = 0.95815022\n",
      "Iteration 41, loss = 0.95713513\n",
      "Iteration 42, loss = 0.95326790\n",
      "Iteration 43, loss = 0.95016863\n",
      "Iteration 44, loss = 0.94751129\n",
      "Iteration 45, loss = 0.94671636\n",
      "Iteration 46, loss = 0.94665450\n",
      "Iteration 47, loss = 0.94537060\n",
      "Iteration 48, loss = 0.94278799\n",
      "Iteration 49, loss = 0.94341215\n",
      "Iteration 50, loss = 0.94156041\n",
      "Iteration 51, loss = 0.93512871\n",
      "Iteration 52, loss = 0.94027446\n",
      "Iteration 53, loss = 0.93541857\n",
      "Iteration 54, loss = 0.93380385\n",
      "Iteration 55, loss = 0.93125789\n",
      "Iteration 56, loss = 0.93371485\n",
      "Iteration 57, loss = 0.93022019\n",
      "Iteration 58, loss = 0.92983773\n",
      "Iteration 59, loss = 0.92826913\n",
      "Iteration 60, loss = 0.92231809\n",
      "Iteration 61, loss = 0.93413494\n",
      "Iteration 62, loss = 0.93516988\n",
      "Iteration 63, loss = 0.91920744\n",
      "Iteration 64, loss = 0.92460072\n",
      "Iteration 65, loss = 0.91609268\n",
      "Iteration 66, loss = 0.91467631\n",
      "Iteration 67, loss = 0.91687615\n",
      "Iteration 68, loss = 0.91404115\n",
      "Iteration 69, loss = 0.90888689\n",
      "Iteration 70, loss = 0.91317194\n",
      "Iteration 71, loss = 0.91054184\n",
      "Iteration 72, loss = 0.90913481\n",
      "Iteration 73, loss = 0.90454721\n",
      "Iteration 74, loss = 0.90578166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75, loss = 0.90542214\n",
      "Iteration 76, loss = 0.90403546\n",
      "Iteration 77, loss = 0.90056419\n",
      "Iteration 78, loss = 0.89926134\n",
      "Iteration 79, loss = 0.89534166\n",
      "Iteration 80, loss = 0.89961888\n",
      "Iteration 81, loss = 0.89883572\n",
      "Iteration 82, loss = 0.89949612\n",
      "Iteration 83, loss = 0.89810410\n",
      "Iteration 84, loss = 0.89244044\n",
      "Iteration 85, loss = 0.89140278\n",
      "Iteration 86, loss = 0.88841452\n",
      "Iteration 87, loss = 0.88949120\n",
      "Iteration 88, loss = 0.88897299\n",
      "Iteration 89, loss = 0.88251057\n",
      "Iteration 90, loss = 0.88684368\n",
      "Iteration 91, loss = 0.88950239\n",
      "Iteration 92, loss = 0.89049724\n",
      "Iteration 93, loss = 0.89287199\n",
      "Iteration 94, loss = 0.88849261\n",
      "Iteration 95, loss = 0.87775470\n",
      "Iteration 96, loss = 0.88094986\n",
      "Iteration 97, loss = 0.87449043\n",
      "Iteration 98, loss = 0.87377619\n",
      "Iteration 99, loss = 0.87299893\n",
      "Iteration 100, loss = 0.86873175\n",
      "Iteration 1, loss = 7.26795660\n",
      "Iteration 2, loss = 4.98867965\n",
      "Iteration 3, loss = 3.80131411\n",
      "Iteration 4, loss = 3.56615521\n",
      "Iteration 5, loss = 3.14260714\n",
      "Iteration 6, loss = 2.69339959\n",
      "Iteration 7, loss = 1.82484091\n",
      "Iteration 8, loss = 1.62533306\n",
      "Iteration 9, loss = 1.61471421\n",
      "Iteration 10, loss = 1.46399798\n",
      "Iteration 11, loss = 1.32509780\n",
      "Iteration 12, loss = 1.36074235\n",
      "Iteration 13, loss = 1.29178754\n",
      "Iteration 14, loss = 1.23906307\n",
      "Iteration 15, loss = 1.22276527\n",
      "Iteration 16, loss = 1.24925333\n",
      "Iteration 17, loss = 1.21377302\n",
      "Iteration 18, loss = 1.22225591\n",
      "Iteration 19, loss = 1.16108291\n",
      "Iteration 20, loss = 1.19577720\n",
      "Iteration 21, loss = 1.19464268\n",
      "Iteration 22, loss = 1.15651693\n",
      "Iteration 23, loss = 1.16496359\n",
      "Iteration 24, loss = 1.18442787\n",
      "Iteration 25, loss = 1.17704331\n",
      "Iteration 26, loss = 1.18173119\n",
      "Iteration 27, loss = 1.20012727\n",
      "Iteration 28, loss = 1.17037092\n",
      "Iteration 29, loss = 1.18893823\n",
      "Iteration 30, loss = 1.16567233\n",
      "Iteration 31, loss = 1.23959940\n",
      "Iteration 32, loss = 1.22778303\n",
      "Iteration 33, loss = 1.17461589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36241227\n",
      "Iteration 2, loss = 1.23234647\n",
      "Iteration 3, loss = 1.18949601\n",
      "Iteration 4, loss = 1.14016429\n",
      "Iteration 5, loss = 1.12417377\n",
      "Iteration 6, loss = 1.10730794\n",
      "Iteration 7, loss = 1.08761942\n",
      "Iteration 8, loss = 1.07425119\n",
      "Iteration 9, loss = 1.06323792\n",
      "Iteration 10, loss = 1.05470910\n",
      "Iteration 11, loss = 1.05477519\n",
      "Iteration 12, loss = 1.05968563\n",
      "Iteration 13, loss = 1.04353949\n",
      "Iteration 14, loss = 1.03486318\n",
      "Iteration 15, loss = 1.02916056\n",
      "Iteration 16, loss = 1.02766213\n",
      "Iteration 17, loss = 1.02596897\n",
      "Iteration 18, loss = 1.03076862\n",
      "Iteration 19, loss = 1.01726307\n",
      "Iteration 20, loss = 1.01748694\n",
      "Iteration 21, loss = 1.01411909\n",
      "Iteration 22, loss = 1.01318353\n",
      "Iteration 23, loss = 1.00844454\n",
      "Iteration 24, loss = 1.00528575\n",
      "Iteration 25, loss = 1.00192403\n",
      "Iteration 26, loss = 0.99941652\n",
      "Iteration 27, loss = 0.99641822\n",
      "Iteration 28, loss = 0.99502605\n",
      "Iteration 29, loss = 0.99392971\n",
      "Iteration 30, loss = 0.98933573\n",
      "Iteration 31, loss = 0.99009145\n",
      "Iteration 32, loss = 0.98553424\n",
      "Iteration 33, loss = 0.99082911\n",
      "Iteration 34, loss = 0.99069663\n",
      "Iteration 35, loss = 0.98943727\n",
      "Iteration 36, loss = 0.98300611\n",
      "Iteration 37, loss = 0.98171481\n",
      "Iteration 38, loss = 0.97350616\n",
      "Iteration 39, loss = 0.96896341\n",
      "Iteration 40, loss = 0.96705433\n",
      "Iteration 41, loss = 0.96932192\n",
      "Iteration 42, loss = 0.96382866\n",
      "Iteration 43, loss = 0.96645433\n",
      "Iteration 44, loss = 0.96262395\n",
      "Iteration 45, loss = 0.95968811\n",
      "Iteration 46, loss = 0.95564101\n",
      "Iteration 47, loss = 0.95386537\n",
      "Iteration 48, loss = 0.95193076\n",
      "Iteration 49, loss = 0.95319536\n",
      "Iteration 50, loss = 0.94652968\n",
      "Iteration 51, loss = 0.94306968\n",
      "Iteration 52, loss = 0.94037329\n",
      "Iteration 53, loss = 0.94087635\n",
      "Iteration 54, loss = 0.93827008\n",
      "Iteration 55, loss = 0.94072727\n",
      "Iteration 56, loss = 0.93542579\n",
      "Iteration 57, loss = 0.93858046\n",
      "Iteration 58, loss = 0.94516750\n",
      "Iteration 59, loss = 0.94376304\n",
      "Iteration 60, loss = 0.93423150\n",
      "Iteration 61, loss = 0.93632480\n",
      "Iteration 62, loss = 0.93209728\n",
      "Iteration 63, loss = 0.92411477\n",
      "Iteration 64, loss = 0.92901280\n",
      "Iteration 65, loss = 0.93084599\n",
      "Iteration 66, loss = 0.93257007\n",
      "Iteration 67, loss = 0.93142926\n",
      "Iteration 68, loss = 0.93749512\n",
      "Iteration 69, loss = 0.92004969\n",
      "Iteration 70, loss = 0.93088277\n",
      "Iteration 71, loss = 0.92150013\n",
      "Iteration 72, loss = 0.91412303\n",
      "Iteration 73, loss = 0.91375235\n",
      "Iteration 74, loss = 0.90932229\n",
      "Iteration 75, loss = 0.90829458\n",
      "Iteration 76, loss = 0.90906350\n",
      "Iteration 77, loss = 0.90510821\n",
      "Iteration 78, loss = 0.90261976\n",
      "Iteration 79, loss = 0.90438694\n",
      "Iteration 80, loss = 0.90231856\n",
      "Iteration 81, loss = 0.90475653\n",
      "Iteration 82, loss = 0.90437029\n",
      "Iteration 83, loss = 0.90170393\n",
      "Iteration 84, loss = 0.89575067\n",
      "Iteration 85, loss = 0.89986814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 86, loss = 0.89647501\n",
      "Iteration 87, loss = 0.89145401\n",
      "Iteration 88, loss = 0.89371959\n",
      "Iteration 89, loss = 0.88930019\n",
      "Iteration 90, loss = 0.88843958\n",
      "Iteration 91, loss = 0.88646056\n",
      "Iteration 92, loss = 0.88973467\n",
      "Iteration 93, loss = 0.88473230\n",
      "Iteration 94, loss = 0.88156488\n",
      "Iteration 95, loss = 0.88780053\n",
      "Iteration 96, loss = 0.89038666\n",
      "Iteration 97, loss = 0.88105264\n",
      "Iteration 98, loss = 0.88770326\n",
      "Iteration 99, loss = 0.88316872\n",
      "Iteration 100, loss = 0.88296410\n",
      "Iteration 1, loss = 1.36045737\n",
      "Iteration 2, loss = 1.22864802\n",
      "Iteration 3, loss = 1.19941862\n",
      "Iteration 4, loss = 1.14569640\n",
      "Iteration 5, loss = 1.12124219\n",
      "Iteration 6, loss = 1.10118297\n",
      "Iteration 7, loss = 1.07945284\n",
      "Iteration 8, loss = 1.06354768\n",
      "Iteration 9, loss = 1.05604528\n",
      "Iteration 10, loss = 1.04602533\n",
      "Iteration 11, loss = 1.04494050\n",
      "Iteration 12, loss = 1.04679450\n",
      "Iteration 13, loss = 1.03685539\n",
      "Iteration 14, loss = 1.03230612\n",
      "Iteration 15, loss = 1.02312198\n",
      "Iteration 16, loss = 1.01966544\n",
      "Iteration 17, loss = 1.01928414\n",
      "Iteration 18, loss = 1.01751046\n",
      "Iteration 19, loss = 1.01374840\n",
      "Iteration 20, loss = 1.00958004\n",
      "Iteration 21, loss = 1.00700607\n",
      "Iteration 22, loss = 1.00552012\n",
      "Iteration 23, loss = 1.00192221\n",
      "Iteration 24, loss = 1.00114695\n",
      "Iteration 25, loss = 0.99683468\n",
      "Iteration 26, loss = 0.99528222\n",
      "Iteration 27, loss = 0.99474377\n",
      "Iteration 28, loss = 0.99189927\n",
      "Iteration 29, loss = 0.99220966\n",
      "Iteration 30, loss = 0.98735211\n",
      "Iteration 31, loss = 0.98861494\n",
      "Iteration 32, loss = 0.98330778\n",
      "Iteration 33, loss = 0.98403562\n",
      "Iteration 34, loss = 0.98603851\n",
      "Iteration 35, loss = 0.97827131\n",
      "Iteration 36, loss = 0.97533762\n",
      "Iteration 37, loss = 0.97855326\n",
      "Iteration 38, loss = 0.97262983\n",
      "Iteration 39, loss = 0.97250312\n",
      "Iteration 40, loss = 0.96776615\n",
      "Iteration 41, loss = 0.97225760\n",
      "Iteration 42, loss = 0.96251536\n",
      "Iteration 43, loss = 0.96868826\n",
      "Iteration 44, loss = 0.96741107\n",
      "Iteration 45, loss = 0.96630330\n",
      "Iteration 46, loss = 0.95685419\n",
      "Iteration 47, loss = 0.96024118\n",
      "Iteration 48, loss = 0.95898445\n",
      "Iteration 49, loss = 0.95547048\n",
      "Iteration 50, loss = 0.95592529\n",
      "Iteration 51, loss = 0.95109832\n",
      "Iteration 52, loss = 0.94799397\n",
      "Iteration 53, loss = 0.95051500\n",
      "Iteration 54, loss = 0.94184148\n",
      "Iteration 55, loss = 0.94390501\n",
      "Iteration 56, loss = 0.93944810\n",
      "Iteration 57, loss = 0.94138670\n",
      "Iteration 58, loss = 0.93836831\n",
      "Iteration 59, loss = 0.94453768\n",
      "Iteration 60, loss = 0.94749194\n",
      "Iteration 61, loss = 0.95097505\n",
      "Iteration 62, loss = 0.94758620\n",
      "Iteration 63, loss = 0.92838628\n",
      "Iteration 64, loss = 0.93262530\n",
      "Iteration 65, loss = 0.93584475\n",
      "Iteration 66, loss = 0.94399002\n",
      "Iteration 67, loss = 0.93866451\n",
      "Iteration 68, loss = 0.93117194\n",
      "Iteration 69, loss = 0.92854498\n",
      "Iteration 70, loss = 0.92017913\n",
      "Iteration 71, loss = 0.92635107\n",
      "Iteration 72, loss = 0.93984745\n",
      "Iteration 73, loss = 0.92796009\n",
      "Iteration 74, loss = 0.92026664\n",
      "Iteration 75, loss = 0.91724541\n",
      "Iteration 76, loss = 0.91769681\n",
      "Iteration 77, loss = 0.91019746\n",
      "Iteration 78, loss = 0.90977931\n",
      "Iteration 79, loss = 0.91014656\n",
      "Iteration 80, loss = 0.91248820\n",
      "Iteration 81, loss = 0.90935862\n",
      "Iteration 82, loss = 0.90436001\n",
      "Iteration 83, loss = 0.90520730\n",
      "Iteration 84, loss = 0.90356119\n",
      "Iteration 85, loss = 0.89804467\n",
      "Iteration 86, loss = 0.90085795\n",
      "Iteration 87, loss = 0.90069778\n",
      "Iteration 88, loss = 0.90168874\n",
      "Iteration 89, loss = 0.90009849\n",
      "Iteration 90, loss = 0.89636299\n",
      "Iteration 91, loss = 0.89338889\n",
      "Iteration 92, loss = 0.88682552\n",
      "Iteration 93, loss = 0.88772595\n",
      "Iteration 94, loss = 0.88579518\n",
      "Iteration 95, loss = 0.88753305\n",
      "Iteration 96, loss = 0.88475676\n",
      "Iteration 97, loss = 0.88430307\n",
      "Iteration 98, loss = 0.89367913\n",
      "Iteration 99, loss = 0.89621405\n",
      "Iteration 100, loss = 0.88929666\n",
      "Iteration 1, loss = 1.35998929\n",
      "Iteration 2, loss = 1.20887565\n",
      "Iteration 3, loss = 1.19384270\n",
      "Iteration 4, loss = 1.13203062\n",
      "Iteration 5, loss = 1.11674405\n",
      "Iteration 6, loss = 1.10236766\n",
      "Iteration 7, loss = 1.08600929\n",
      "Iteration 8, loss = 1.06899889\n",
      "Iteration 9, loss = 1.06706641\n",
      "Iteration 10, loss = 1.05603745\n",
      "Iteration 11, loss = 1.05565586\n",
      "Iteration 12, loss = 1.05612706\n",
      "Iteration 13, loss = 1.03697914\n",
      "Iteration 14, loss = 1.04070445\n",
      "Iteration 15, loss = 1.03816115\n",
      "Iteration 16, loss = 1.03035054\n",
      "Iteration 17, loss = 1.02749882\n",
      "Iteration 18, loss = 1.02684627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 1.02258456\n",
      "Iteration 20, loss = 1.01979927\n",
      "Iteration 21, loss = 1.01810280\n",
      "Iteration 22, loss = 1.01362908\n",
      "Iteration 23, loss = 1.01479522\n",
      "Iteration 24, loss = 1.01069971\n",
      "Iteration 25, loss = 1.00956053\n",
      "Iteration 26, loss = 1.01100502\n",
      "Iteration 27, loss = 1.01093662\n",
      "Iteration 28, loss = 1.00435378\n",
      "Iteration 29, loss = 1.00327272\n",
      "Iteration 30, loss = 1.00676797\n",
      "Iteration 31, loss = 1.00467327\n",
      "Iteration 32, loss = 1.00322246\n",
      "Iteration 33, loss = 1.00090554\n",
      "Iteration 34, loss = 0.99896939\n",
      "Iteration 35, loss = 0.99589476\n",
      "Iteration 36, loss = 0.99126838\n",
      "Iteration 37, loss = 0.99749487\n",
      "Iteration 38, loss = 0.99387113\n",
      "Iteration 39, loss = 0.98777655\n",
      "Iteration 40, loss = 0.99302223\n",
      "Iteration 41, loss = 0.98869737\n",
      "Iteration 42, loss = 0.98218257\n",
      "Iteration 43, loss = 0.98579629\n",
      "Iteration 44, loss = 0.98275164\n",
      "Iteration 45, loss = 0.97887875\n",
      "Iteration 46, loss = 0.97196116\n",
      "Iteration 47, loss = 0.97331732\n",
      "Iteration 48, loss = 0.97163563\n",
      "Iteration 49, loss = 0.96678648\n",
      "Iteration 50, loss = 0.96803653\n",
      "Iteration 51, loss = 0.96353550\n",
      "Iteration 52, loss = 0.95781770\n",
      "Iteration 53, loss = 0.96010983\n",
      "Iteration 54, loss = 0.95414233\n",
      "Iteration 55, loss = 0.95305200\n",
      "Iteration 56, loss = 0.95366731\n",
      "Iteration 57, loss = 0.95575474\n",
      "Iteration 58, loss = 0.95240956\n",
      "Iteration 59, loss = 0.95110624\n",
      "Iteration 60, loss = 0.94711281\n",
      "Iteration 61, loss = 0.94351984\n",
      "Iteration 62, loss = 0.94153745\n",
      "Iteration 63, loss = 0.93695240\n",
      "Iteration 64, loss = 0.94035217\n",
      "Iteration 65, loss = 0.94562827\n",
      "Iteration 66, loss = 0.93352722\n",
      "Iteration 67, loss = 0.93297663\n",
      "Iteration 68, loss = 0.93350345\n",
      "Iteration 69, loss = 0.92900917\n",
      "Iteration 70, loss = 0.92420152\n",
      "Iteration 71, loss = 0.92792928\n",
      "Iteration 72, loss = 0.93986669\n",
      "Iteration 73, loss = 0.92642881\n",
      "Iteration 74, loss = 0.92677108\n",
      "Iteration 75, loss = 0.91644868\n",
      "Iteration 76, loss = 0.91495607\n",
      "Iteration 77, loss = 0.91450083\n",
      "Iteration 78, loss = 0.90927669\n",
      "Iteration 79, loss = 0.90921127\n",
      "Iteration 80, loss = 0.90754525\n",
      "Iteration 81, loss = 0.90349871\n",
      "Iteration 82, loss = 0.90297672\n",
      "Iteration 83, loss = 0.90762786\n",
      "Iteration 84, loss = 0.90482562\n",
      "Iteration 85, loss = 0.89865621\n",
      "Iteration 86, loss = 0.90119118\n",
      "Iteration 87, loss = 0.90037673\n",
      "Iteration 88, loss = 0.90713141\n",
      "Iteration 89, loss = 0.90151246\n",
      "Iteration 90, loss = 0.90913751\n",
      "Iteration 91, loss = 0.90920969\n",
      "Iteration 92, loss = 0.90477042\n",
      "Iteration 93, loss = 0.91606926\n",
      "Iteration 94, loss = 0.89526129\n",
      "Iteration 95, loss = 0.89571462\n",
      "Iteration 96, loss = 0.89266799\n",
      "Iteration 97, loss = 0.89150406\n",
      "Iteration 98, loss = 0.89141182\n",
      "Iteration 99, loss = 0.88725875\n",
      "Iteration 100, loss = 0.87738193\n",
      "Iteration 1, loss = 1.36451595\n",
      "Iteration 2, loss = 1.21607131\n",
      "Iteration 3, loss = 1.19944308\n",
      "Iteration 4, loss = 1.15729133\n",
      "Iteration 5, loss = 1.13866830\n",
      "Iteration 6, loss = 1.12276483\n",
      "Iteration 7, loss = 1.10069308\n",
      "Iteration 8, loss = 1.09302816\n",
      "Iteration 9, loss = 1.08205385\n",
      "Iteration 10, loss = 1.07218597\n",
      "Iteration 11, loss = 1.07042556\n",
      "Iteration 12, loss = 1.06919822\n",
      "Iteration 13, loss = 1.05400876\n",
      "Iteration 14, loss = 1.05105229\n",
      "Iteration 15, loss = 1.05309290\n",
      "Iteration 16, loss = 1.04811037\n",
      "Iteration 17, loss = 1.04250034\n",
      "Iteration 18, loss = 1.04304393\n",
      "Iteration 19, loss = 1.03354332\n",
      "Iteration 20, loss = 1.03163159\n",
      "Iteration 21, loss = 1.03279037\n",
      "Iteration 22, loss = 1.03777311\n",
      "Iteration 23, loss = 1.03644658\n",
      "Iteration 24, loss = 1.02589192\n",
      "Iteration 25, loss = 1.02146913\n",
      "Iteration 26, loss = 1.02205432\n",
      "Iteration 27, loss = 1.02203239\n",
      "Iteration 28, loss = 1.01943832\n",
      "Iteration 29, loss = 1.02431807\n",
      "Iteration 30, loss = 1.01644255\n",
      "Iteration 31, loss = 1.01040059\n",
      "Iteration 32, loss = 1.00975368\n",
      "Iteration 33, loss = 1.00226910\n",
      "Iteration 34, loss = 1.00542265\n",
      "Iteration 35, loss = 1.00236687\n",
      "Iteration 36, loss = 0.99813066\n",
      "Iteration 37, loss = 0.99698928\n",
      "Iteration 38, loss = 0.98951391\n",
      "Iteration 39, loss = 0.98906141\n",
      "Iteration 40, loss = 0.98811269\n",
      "Iteration 41, loss = 0.98695686\n",
      "Iteration 42, loss = 0.98038920\n",
      "Iteration 43, loss = 0.98255146\n",
      "Iteration 44, loss = 0.97959147\n",
      "Iteration 45, loss = 0.97776714\n",
      "Iteration 46, loss = 0.97498652\n",
      "Iteration 47, loss = 0.97649992\n",
      "Iteration 48, loss = 0.97092613\n",
      "Iteration 49, loss = 0.96952848\n",
      "Iteration 50, loss = 0.96695785\n",
      "Iteration 51, loss = 0.97039912\n",
      "Iteration 52, loss = 0.97026517\n",
      "Iteration 53, loss = 0.97487623\n",
      "Iteration 54, loss = 0.96456695\n",
      "Iteration 55, loss = 0.96258152\n",
      "Iteration 56, loss = 0.95778477\n",
      "Iteration 57, loss = 0.96559754\n",
      "Iteration 58, loss = 0.95584172\n",
      "Iteration 59, loss = 0.95768737\n",
      "Iteration 60, loss = 0.97165210\n",
      "Iteration 61, loss = 0.96088492\n",
      "Iteration 62, loss = 0.95462234\n",
      "Iteration 63, loss = 0.95069293\n",
      "Iteration 64, loss = 0.95062815\n",
      "Iteration 65, loss = 0.95059637\n",
      "Iteration 66, loss = 0.94888384\n",
      "Iteration 67, loss = 0.93979080\n",
      "Iteration 68, loss = 0.94122309\n",
      "Iteration 69, loss = 0.94311003\n",
      "Iteration 70, loss = 0.94227926\n",
      "Iteration 71, loss = 0.93460222\n",
      "Iteration 72, loss = 0.93913067\n",
      "Iteration 73, loss = 0.93683982\n",
      "Iteration 74, loss = 0.93490031\n",
      "Iteration 75, loss = 0.93015624\n",
      "Iteration 76, loss = 0.92820876\n",
      "Iteration 77, loss = 0.93336357\n",
      "Iteration 78, loss = 0.92874866\n",
      "Iteration 79, loss = 0.92646019\n",
      "Iteration 80, loss = 0.92223192\n",
      "Iteration 81, loss = 0.92344464\n",
      "Iteration 82, loss = 0.92330683\n",
      "Iteration 83, loss = 0.92327022\n",
      "Iteration 84, loss = 0.92391323\n",
      "Iteration 85, loss = 0.92032408\n",
      "Iteration 86, loss = 0.92547318\n",
      "Iteration 87, loss = 0.92468356\n",
      "Iteration 88, loss = 0.92521346\n",
      "Iteration 89, loss = 0.92514575\n",
      "Iteration 90, loss = 0.93505568\n",
      "Iteration 91, loss = 0.91368599\n",
      "Iteration 92, loss = 0.91929471\n",
      "Iteration 93, loss = 0.91418166\n",
      "Iteration 94, loss = 0.91154507\n",
      "Iteration 95, loss = 0.90615282\n",
      "Iteration 96, loss = 0.90185586\n",
      "Iteration 97, loss = 0.90233451\n",
      "Iteration 98, loss = 0.90604241\n",
      "Iteration 99, loss = 0.91055893\n",
      "Iteration 100, loss = 0.90258184\n",
      "Iteration 1, loss = 1.37110364\n",
      "Iteration 2, loss = 1.19411514\n",
      "Iteration 3, loss = 1.17522355\n",
      "Iteration 4, loss = 1.13071215\n",
      "Iteration 5, loss = 1.09416935\n",
      "Iteration 6, loss = 1.07739223\n",
      "Iteration 7, loss = 1.05812368\n",
      "Iteration 8, loss = 1.04713637\n",
      "Iteration 9, loss = 1.03295243\n",
      "Iteration 10, loss = 1.02890796\n",
      "Iteration 11, loss = 1.02009938\n",
      "Iteration 12, loss = 1.01758547\n",
      "Iteration 13, loss = 1.01977881\n",
      "Iteration 14, loss = 1.00380066\n",
      "Iteration 15, loss = 1.00918814\n",
      "Iteration 16, loss = 1.00113681\n",
      "Iteration 17, loss = 1.00070239\n",
      "Iteration 18, loss = 1.00032023\n",
      "Iteration 19, loss = 0.99321569\n",
      "Iteration 20, loss = 0.99699552\n",
      "Iteration 21, loss = 1.00380457\n",
      "Iteration 22, loss = 0.99681281\n",
      "Iteration 23, loss = 0.98999520\n",
      "Iteration 24, loss = 0.99335473\n",
      "Iteration 25, loss = 0.98048617\n",
      "Iteration 26, loss = 0.98426578\n",
      "Iteration 27, loss = 0.97598051\n",
      "Iteration 28, loss = 0.97706328\n",
      "Iteration 29, loss = 0.97407169\n",
      "Iteration 30, loss = 0.97347757\n",
      "Iteration 31, loss = 0.97488354\n",
      "Iteration 32, loss = 0.96741309\n",
      "Iteration 33, loss = 0.96341160\n",
      "Iteration 34, loss = 0.95921682\n",
      "Iteration 35, loss = 0.96407699\n",
      "Iteration 36, loss = 0.96183590\n",
      "Iteration 37, loss = 0.95481610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.95241809\n",
      "Iteration 39, loss = 0.95237182\n",
      "Iteration 40, loss = 0.94916491\n",
      "Iteration 41, loss = 0.94864120\n",
      "Iteration 42, loss = 0.94586163\n",
      "Iteration 43, loss = 0.94908723\n",
      "Iteration 44, loss = 0.93869323\n",
      "Iteration 45, loss = 0.94155055\n",
      "Iteration 46, loss = 0.93901989\n",
      "Iteration 47, loss = 0.93421771\n",
      "Iteration 48, loss = 0.93094489\n",
      "Iteration 49, loss = 0.92765183\n",
      "Iteration 50, loss = 0.93257553\n",
      "Iteration 51, loss = 0.92809955\n",
      "Iteration 52, loss = 0.92371304\n",
      "Iteration 53, loss = 0.92082933\n",
      "Iteration 54, loss = 0.91927832\n",
      "Iteration 55, loss = 0.92334234\n",
      "Iteration 56, loss = 0.92799787\n",
      "Iteration 57, loss = 0.92132637\n",
      "Iteration 58, loss = 0.92424957\n",
      "Iteration 59, loss = 0.91194657\n",
      "Iteration 60, loss = 0.90841061\n",
      "Iteration 61, loss = 0.90827764\n",
      "Iteration 62, loss = 0.90596195\n",
      "Iteration 63, loss = 0.90366459\n",
      "Iteration 64, loss = 0.90237842\n",
      "Iteration 65, loss = 0.90959856\n",
      "Iteration 66, loss = 0.90188755\n",
      "Iteration 67, loss = 0.90079665\n",
      "Iteration 68, loss = 0.90087684\n",
      "Iteration 69, loss = 0.89776520\n",
      "Iteration 70, loss = 0.89697823\n",
      "Iteration 71, loss = 0.90452934\n",
      "Iteration 72, loss = 0.90689876\n",
      "Iteration 73, loss = 0.90370719\n",
      "Iteration 74, loss = 0.88792130\n",
      "Iteration 75, loss = 0.89622154\n",
      "Iteration 76, loss = 0.88395559\n",
      "Iteration 77, loss = 0.88676909\n",
      "Iteration 78, loss = 0.88331986\n",
      "Iteration 79, loss = 0.88594578\n",
      "Iteration 80, loss = 0.88652172\n",
      "Iteration 81, loss = 0.88543265\n",
      "Iteration 82, loss = 0.88182284\n",
      "Iteration 83, loss = 0.87523150\n",
      "Iteration 84, loss = 0.87427580\n",
      "Iteration 85, loss = 0.87436741\n",
      "Iteration 86, loss = 0.87349963\n",
      "Iteration 87, loss = 0.87348369\n",
      "Iteration 88, loss = 0.87269081\n",
      "Iteration 89, loss = 0.87233831\n",
      "Iteration 90, loss = 0.86442386\n",
      "Iteration 91, loss = 0.86436446\n",
      "Iteration 92, loss = 0.86512520\n",
      "Iteration 93, loss = 0.86794487\n",
      "Iteration 94, loss = 0.85979211\n",
      "Iteration 95, loss = 0.85950457\n",
      "Iteration 96, loss = 0.86482587\n",
      "Iteration 97, loss = 0.86216682\n",
      "Iteration 98, loss = 0.86342081\n",
      "Iteration 99, loss = 0.85468135\n",
      "Iteration 100, loss = 0.85861463\n",
      "Iteration 1, loss = 13.43255776\n",
      "Iteration 2, loss = 11.50059086\n",
      "Iteration 3, loss = 6.33953315\n",
      "Iteration 4, loss = 4.46575465\n",
      "Iteration 5, loss = 4.33029220\n",
      "Iteration 6, loss = 3.47132068\n",
      "Iteration 7, loss = 2.89165162\n",
      "Iteration 8, loss = 2.47563378\n",
      "Iteration 9, loss = 1.81278398\n",
      "Iteration 10, loss = 1.57173774\n",
      "Iteration 11, loss = 1.57300569\n",
      "Iteration 12, loss = 1.31343307\n",
      "Iteration 13, loss = 1.28489107\n",
      "Iteration 14, loss = 1.22695487\n",
      "Iteration 15, loss = 1.18563459\n",
      "Iteration 16, loss = 1.15958535\n",
      "Iteration 17, loss = 1.15150999\n",
      "Iteration 18, loss = 1.15568472\n",
      "Iteration 19, loss = 1.14430658\n",
      "Iteration 20, loss = 1.15296117\n",
      "Iteration 21, loss = 1.17564192\n",
      "Iteration 22, loss = 1.14999003\n",
      "Iteration 23, loss = 1.12956569\n",
      "Iteration 24, loss = 1.13525162\n",
      "Iteration 25, loss = 1.17122270\n",
      "Iteration 26, loss = 1.27530146\n",
      "Iteration 27, loss = 1.32442182\n",
      "Iteration 28, loss = 1.20252239\n",
      "Iteration 29, loss = 1.13305170\n",
      "Iteration 30, loss = 1.11643330\n",
      "Iteration 31, loss = 1.16984261\n",
      "Iteration 32, loss = 1.22142453\n",
      "Iteration 33, loss = 1.28320955\n",
      "Iteration 34, loss = 1.25009721\n",
      "Iteration 35, loss = 1.18456087\n",
      "Iteration 36, loss = 1.16360094\n",
      "Iteration 37, loss = 1.11143982\n",
      "Iteration 38, loss = 1.12619166\n",
      "Iteration 39, loss = 1.12004369\n",
      "Iteration 40, loss = 1.13406313\n",
      "Iteration 41, loss = 1.18314388\n",
      "Iteration 42, loss = 1.19525395\n",
      "Iteration 43, loss = 1.12148754\n",
      "Iteration 44, loss = 1.11102667\n",
      "Iteration 45, loss = 1.16479104\n",
      "Iteration 46, loss = 1.15521917\n",
      "Iteration 47, loss = 1.12310410\n",
      "Iteration 48, loss = 1.08502491\n",
      "Iteration 49, loss = 1.08943695\n",
      "Iteration 50, loss = 1.13371518\n",
      "Iteration 51, loss = 1.06339012\n",
      "Iteration 52, loss = 1.06008733\n",
      "Iteration 53, loss = 1.05187682\n",
      "Iteration 54, loss = 1.06560678\n",
      "Iteration 55, loss = 1.04915315\n",
      "Iteration 56, loss = 1.04725673\n",
      "Iteration 57, loss = 1.08313732\n",
      "Iteration 58, loss = 1.08460862\n",
      "Iteration 59, loss = 1.08724842\n",
      "Iteration 60, loss = 1.09948041\n",
      "Iteration 61, loss = 1.09432118\n",
      "Iteration 62, loss = 1.10049830\n",
      "Iteration 63, loss = 1.10481448\n",
      "Iteration 64, loss = 1.15556409\n",
      "Iteration 65, loss = 1.11565619\n",
      "Iteration 66, loss = 1.09794008\n",
      "Iteration 67, loss = 1.09834560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35094045\n",
      "Iteration 2, loss = 1.18760478\n",
      "Iteration 3, loss = 1.16420548\n",
      "Iteration 4, loss = 1.12282909\n",
      "Iteration 5, loss = 1.10376890\n",
      "Iteration 6, loss = 1.08672737\n",
      "Iteration 7, loss = 1.07501186\n",
      "Iteration 8, loss = 1.06377993\n",
      "Iteration 9, loss = 1.05205883\n",
      "Iteration 10, loss = 1.05504689\n",
      "Iteration 11, loss = 1.04387056\n",
      "Iteration 12, loss = 1.03590547\n",
      "Iteration 13, loss = 1.03205304\n",
      "Iteration 14, loss = 1.03209464\n",
      "Iteration 15, loss = 1.03727479\n",
      "Iteration 16, loss = 1.03026750\n",
      "Iteration 17, loss = 1.02667199\n",
      "Iteration 18, loss = 1.02688144\n",
      "Iteration 19, loss = 1.02129020\n",
      "Iteration 20, loss = 1.01665637\n",
      "Iteration 21, loss = 1.01762166\n",
      "Iteration 22, loss = 1.01814065\n",
      "Iteration 23, loss = 1.01334983\n",
      "Iteration 24, loss = 1.00667582\n",
      "Iteration 25, loss = 1.00430412\n",
      "Iteration 26, loss = 1.00387601\n",
      "Iteration 27, loss = 1.00421735\n",
      "Iteration 28, loss = 0.99706176\n",
      "Iteration 29, loss = 0.99837947\n",
      "Iteration 30, loss = 1.00585496\n",
      "Iteration 31, loss = 0.99636509\n",
      "Iteration 32, loss = 0.98631025\n",
      "Iteration 33, loss = 0.98880451\n",
      "Iteration 34, loss = 0.99082178\n",
      "Iteration 35, loss = 0.97711761\n",
      "Iteration 36, loss = 0.97659613\n",
      "Iteration 37, loss = 0.97440088\n",
      "Iteration 38, loss = 0.97743178\n",
      "Iteration 39, loss = 0.97410453\n",
      "Iteration 40, loss = 0.97099888\n",
      "Iteration 41, loss = 0.96396284\n",
      "Iteration 42, loss = 0.97021096\n",
      "Iteration 43, loss = 0.96787579\n",
      "Iteration 44, loss = 0.96166988\n",
      "Iteration 45, loss = 0.96375853\n",
      "Iteration 46, loss = 0.95600462\n",
      "Iteration 47, loss = 0.95221329\n",
      "Iteration 48, loss = 0.95123834\n",
      "Iteration 49, loss = 0.94346907\n",
      "Iteration 50, loss = 0.93938433\n",
      "Iteration 51, loss = 0.94834833\n",
      "Iteration 52, loss = 0.93893600\n",
      "Iteration 53, loss = 0.93822040\n",
      "Iteration 54, loss = 0.94285435\n",
      "Iteration 55, loss = 0.94859659\n",
      "Iteration 56, loss = 0.95223071\n",
      "Iteration 57, loss = 0.94032723\n",
      "Iteration 58, loss = 0.93525148\n",
      "Iteration 59, loss = 0.92413236\n",
      "Iteration 60, loss = 0.93025595\n",
      "Iteration 61, loss = 0.92306489\n",
      "Iteration 62, loss = 0.92360918\n",
      "Iteration 63, loss = 0.92741930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64, loss = 0.92100085\n",
      "Iteration 65, loss = 0.91907261\n",
      "Iteration 66, loss = 0.92221642\n",
      "Iteration 67, loss = 0.93748855\n",
      "Iteration 68, loss = 0.91919805\n",
      "Iteration 69, loss = 0.92161241\n",
      "Iteration 70, loss = 0.92079379\n",
      "Iteration 71, loss = 0.91853171\n",
      "Iteration 72, loss = 0.90851849\n",
      "Iteration 73, loss = 0.90262056\n",
      "Iteration 74, loss = 0.89692271\n",
      "Iteration 75, loss = 0.89411085\n",
      "Iteration 76, loss = 0.90186612\n",
      "Iteration 77, loss = 0.89970616\n",
      "Iteration 78, loss = 0.89524772\n",
      "Iteration 79, loss = 0.89506680\n",
      "Iteration 80, loss = 0.88401657\n",
      "Iteration 81, loss = 0.88414903\n",
      "Iteration 82, loss = 0.87763960\n",
      "Iteration 83, loss = 0.88107341\n",
      "Iteration 84, loss = 0.87988346\n",
      "Iteration 85, loss = 0.88566838\n",
      "Iteration 86, loss = 0.88190686\n",
      "Iteration 87, loss = 0.87913592\n",
      "Iteration 88, loss = 0.87081232\n",
      "Iteration 89, loss = 0.87125541\n",
      "Iteration 90, loss = 0.86949553\n",
      "Iteration 91, loss = 0.86619382\n",
      "Iteration 92, loss = 0.86562429\n",
      "Iteration 93, loss = 0.86521000\n",
      "Iteration 94, loss = 0.86838061\n",
      "Iteration 95, loss = 0.86520711\n",
      "Iteration 96, loss = 0.85883198\n",
      "Iteration 97, loss = 0.86102525\n",
      "Iteration 98, loss = 0.86278220\n",
      "Iteration 99, loss = 0.86556541\n",
      "Iteration 100, loss = 0.85730408\n",
      "Iteration 1, loss = 1.33861864\n",
      "Iteration 2, loss = 1.18725847\n",
      "Iteration 3, loss = 1.16385952\n",
      "Iteration 4, loss = 1.11812313\n",
      "Iteration 5, loss = 1.10637778\n",
      "Iteration 6, loss = 1.08046416\n",
      "Iteration 7, loss = 1.06168056\n",
      "Iteration 8, loss = 1.05623497\n",
      "Iteration 9, loss = 1.04503232\n",
      "Iteration 10, loss = 1.04004314\n",
      "Iteration 11, loss = 1.03307519\n",
      "Iteration 12, loss = 1.03065273\n",
      "Iteration 13, loss = 1.03600312\n",
      "Iteration 14, loss = 1.01964215\n",
      "Iteration 15, loss = 1.01854740\n",
      "Iteration 16, loss = 1.01827979\n",
      "Iteration 17, loss = 1.01164725\n",
      "Iteration 18, loss = 1.01140601\n",
      "Iteration 19, loss = 1.01100925\n",
      "Iteration 20, loss = 1.01221918\n",
      "Iteration 21, loss = 1.00486311\n",
      "Iteration 22, loss = 1.00105813\n",
      "Iteration 23, loss = 1.00076837\n",
      "Iteration 24, loss = 0.99541747\n",
      "Iteration 25, loss = 0.99544183\n",
      "Iteration 26, loss = 1.00438196\n",
      "Iteration 27, loss = 1.00043015\n",
      "Iteration 28, loss = 0.99105827\n",
      "Iteration 29, loss = 0.99358114\n",
      "Iteration 30, loss = 0.99361961\n",
      "Iteration 31, loss = 0.98658518\n",
      "Iteration 32, loss = 0.97823013\n",
      "Iteration 33, loss = 0.98225510\n",
      "Iteration 34, loss = 0.98505615\n",
      "Iteration 35, loss = 0.97468573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.96764729\n",
      "Iteration 37, loss = 0.96866537\n",
      "Iteration 38, loss = 0.96310362\n",
      "Iteration 39, loss = 0.96074537\n",
      "Iteration 40, loss = 0.96101785\n",
      "Iteration 41, loss = 0.95254145\n",
      "Iteration 42, loss = 0.96378520\n",
      "Iteration 43, loss = 0.95613702\n",
      "Iteration 44, loss = 0.95045503\n",
      "Iteration 45, loss = 0.95048026\n",
      "Iteration 46, loss = 0.93940093\n",
      "Iteration 47, loss = 0.94278585\n",
      "Iteration 48, loss = 0.93855876\n",
      "Iteration 49, loss = 0.93301223\n",
      "Iteration 50, loss = 0.92899631\n",
      "Iteration 51, loss = 0.93162098\n",
      "Iteration 52, loss = 0.92804992\n",
      "Iteration 53, loss = 0.92620143\n",
      "Iteration 54, loss = 0.92725335\n",
      "Iteration 55, loss = 0.92233956\n",
      "Iteration 56, loss = 0.92653409\n",
      "Iteration 57, loss = 0.92120095\n",
      "Iteration 58, loss = 0.91524397\n",
      "Iteration 59, loss = 0.91478728\n",
      "Iteration 60, loss = 0.91192489\n",
      "Iteration 61, loss = 0.90852940\n",
      "Iteration 62, loss = 0.90978242\n",
      "Iteration 63, loss = 0.90569782\n",
      "Iteration 64, loss = 0.90163227\n",
      "Iteration 65, loss = 0.90067527\n",
      "Iteration 66, loss = 0.90586555\n",
      "Iteration 67, loss = 0.90492171\n",
      "Iteration 68, loss = 0.89541230\n",
      "Iteration 69, loss = 0.89575380\n",
      "Iteration 70, loss = 0.90181052\n",
      "Iteration 71, loss = 0.89593116\n",
      "Iteration 72, loss = 0.89050707\n",
      "Iteration 73, loss = 0.88930082\n",
      "Iteration 74, loss = 0.88931782\n",
      "Iteration 75, loss = 0.88743891\n",
      "Iteration 76, loss = 0.88338512\n",
      "Iteration 77, loss = 0.87967814\n",
      "Iteration 78, loss = 0.87866048\n",
      "Iteration 79, loss = 0.88201562\n",
      "Iteration 80, loss = 0.86862937\n",
      "Iteration 81, loss = 0.87665796\n",
      "Iteration 82, loss = 0.87526217\n",
      "Iteration 83, loss = 0.88060310\n",
      "Iteration 84, loss = 0.87929207\n",
      "Iteration 85, loss = 0.86347381\n",
      "Iteration 86, loss = 0.86511060\n",
      "Iteration 87, loss = 0.86333427\n",
      "Iteration 88, loss = 0.86587919\n",
      "Iteration 89, loss = 0.86716646\n",
      "Iteration 90, loss = 0.86744282\n",
      "Iteration 91, loss = 0.86933242\n",
      "Iteration 92, loss = 0.85235178\n",
      "Iteration 93, loss = 0.86069640\n",
      "Iteration 94, loss = 0.85842182\n",
      "Iteration 95, loss = 0.85998058\n",
      "Iteration 96, loss = 0.85974170\n",
      "Iteration 97, loss = 0.86248338\n",
      "Iteration 98, loss = 0.88739677\n",
      "Iteration 99, loss = 0.86847258\n",
      "Iteration 100, loss = 0.85901829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.33987623\n",
      "Iteration 2, loss = 1.18836152\n",
      "Iteration 3, loss = 1.14246080\n",
      "Iteration 4, loss = 1.11988462\n",
      "Iteration 5, loss = 1.10275979\n",
      "Iteration 6, loss = 1.07460897\n",
      "Iteration 7, loss = 1.06815558\n",
      "Iteration 8, loss = 1.05751091\n",
      "Iteration 9, loss = 1.05899009\n",
      "Iteration 10, loss = 1.04951101\n",
      "Iteration 11, loss = 1.04352121\n",
      "Iteration 12, loss = 1.03799442\n",
      "Iteration 13, loss = 1.02870506\n",
      "Iteration 14, loss = 1.02421099\n",
      "Iteration 15, loss = 1.02758452\n",
      "Iteration 16, loss = 1.02116680\n",
      "Iteration 17, loss = 1.01710339\n",
      "Iteration 18, loss = 1.01710081\n",
      "Iteration 19, loss = 1.01164937\n",
      "Iteration 20, loss = 1.00894689\n",
      "Iteration 21, loss = 1.01274351\n",
      "Iteration 22, loss = 1.01085071\n",
      "Iteration 23, loss = 1.00636666\n",
      "Iteration 24, loss = 1.00309030\n",
      "Iteration 25, loss = 0.99966660\n",
      "Iteration 26, loss = 1.00181353\n",
      "Iteration 27, loss = 0.99932975\n",
      "Iteration 28, loss = 0.99227301\n",
      "Iteration 29, loss = 0.98966750\n",
      "Iteration 30, loss = 0.98647152\n",
      "Iteration 31, loss = 0.98243341\n",
      "Iteration 32, loss = 0.98494139\n",
      "Iteration 33, loss = 0.99102718\n",
      "Iteration 34, loss = 0.99157675\n",
      "Iteration 35, loss = 0.97687363\n",
      "Iteration 36, loss = 0.96920989\n",
      "Iteration 37, loss = 0.97665387\n",
      "Iteration 38, loss = 0.96171846\n",
      "Iteration 39, loss = 0.96245090\n",
      "Iteration 40, loss = 0.96209670\n",
      "Iteration 41, loss = 0.95582084\n",
      "Iteration 42, loss = 0.95523663\n",
      "Iteration 43, loss = 0.95394421\n",
      "Iteration 44, loss = 0.95500405\n",
      "Iteration 45, loss = 0.94466612\n",
      "Iteration 46, loss = 0.94029040\n",
      "Iteration 47, loss = 0.94010280\n",
      "Iteration 48, loss = 0.93620501\n",
      "Iteration 49, loss = 0.93843306\n",
      "Iteration 50, loss = 0.93013021\n",
      "Iteration 51, loss = 0.92719197\n",
      "Iteration 52, loss = 0.92852234\n",
      "Iteration 53, loss = 0.92428904\n",
      "Iteration 54, loss = 0.92211292\n",
      "Iteration 55, loss = 0.92013969\n",
      "Iteration 56, loss = 0.91620594\n",
      "Iteration 57, loss = 0.91594839\n",
      "Iteration 58, loss = 0.90761765\n",
      "Iteration 59, loss = 0.90722638\n",
      "Iteration 60, loss = 0.90968703\n",
      "Iteration 61, loss = 0.90649411\n",
      "Iteration 62, loss = 0.90662883\n",
      "Iteration 63, loss = 0.90464497\n",
      "Iteration 64, loss = 0.90242768\n",
      "Iteration 65, loss = 0.90352292\n",
      "Iteration 66, loss = 0.89719993\n",
      "Iteration 67, loss = 0.89874661\n",
      "Iteration 68, loss = 0.89169282\n",
      "Iteration 69, loss = 0.89780475\n",
      "Iteration 70, loss = 0.90917919\n",
      "Iteration 71, loss = 0.91985683\n",
      "Iteration 72, loss = 0.89892486\n",
      "Iteration 73, loss = 0.89338720\n",
      "Iteration 74, loss = 0.88553555\n",
      "Iteration 75, loss = 0.87449433\n",
      "Iteration 76, loss = 0.87497900\n",
      "Iteration 77, loss = 0.87849183\n",
      "Iteration 78, loss = 0.87408123\n",
      "Iteration 79, loss = 0.87539182\n",
      "Iteration 80, loss = 0.86802726\n",
      "Iteration 81, loss = 0.86726403\n",
      "Iteration 82, loss = 0.86760518\n",
      "Iteration 83, loss = 0.86954945\n",
      "Iteration 84, loss = 0.86377536\n",
      "Iteration 85, loss = 0.85804394\n",
      "Iteration 86, loss = 0.85579228\n",
      "Iteration 87, loss = 0.85145492\n",
      "Iteration 88, loss = 0.85390114\n",
      "Iteration 89, loss = 0.85340118\n",
      "Iteration 90, loss = 0.85564271\n",
      "Iteration 91, loss = 0.85605882\n",
      "Iteration 92, loss = 0.86417410\n",
      "Iteration 93, loss = 0.87354592\n",
      "Iteration 94, loss = 0.85643883\n",
      "Iteration 95, loss = 0.85427993\n",
      "Iteration 96, loss = 0.83884110\n",
      "Iteration 97, loss = 0.84123669\n",
      "Iteration 98, loss = 0.84404608\n",
      "Iteration 99, loss = 0.83771303\n",
      "Iteration 100, loss = 0.83909323\n",
      "Iteration 1, loss = 1.32764726\n",
      "Iteration 2, loss = 1.19892077\n",
      "Iteration 3, loss = 1.16186457\n",
      "Iteration 4, loss = 1.13406214\n",
      "Iteration 5, loss = 1.12328141\n",
      "Iteration 6, loss = 1.09613632\n",
      "Iteration 7, loss = 1.08333957\n",
      "Iteration 8, loss = 1.07099354\n",
      "Iteration 9, loss = 1.06338388\n",
      "Iteration 10, loss = 1.05742026\n",
      "Iteration 11, loss = 1.05361766\n",
      "Iteration 12, loss = 1.05110975\n",
      "Iteration 13, loss = 1.04181424\n",
      "Iteration 14, loss = 1.03797648\n",
      "Iteration 15, loss = 1.03438465\n",
      "Iteration 16, loss = 1.03259873\n",
      "Iteration 17, loss = 1.03399912\n",
      "Iteration 18, loss = 1.03001923\n",
      "Iteration 19, loss = 1.01914273\n",
      "Iteration 20, loss = 1.01399506\n",
      "Iteration 21, loss = 1.01287579\n",
      "Iteration 22, loss = 1.00800617\n",
      "Iteration 23, loss = 1.00516181\n",
      "Iteration 24, loss = 1.00614176\n",
      "Iteration 25, loss = 0.99967582\n",
      "Iteration 26, loss = 1.00374261\n",
      "Iteration 27, loss = 0.99577392\n",
      "Iteration 28, loss = 0.99165480\n",
      "Iteration 29, loss = 0.99037556\n",
      "Iteration 30, loss = 0.99083268\n",
      "Iteration 31, loss = 0.98187408\n",
      "Iteration 32, loss = 0.97811418\n",
      "Iteration 33, loss = 0.97476471\n",
      "Iteration 34, loss = 0.97595300\n",
      "Iteration 35, loss = 0.97334398\n",
      "Iteration 36, loss = 0.97037141\n",
      "Iteration 37, loss = 0.98299319\n",
      "Iteration 38, loss = 0.97356337\n",
      "Iteration 39, loss = 0.96178966\n",
      "Iteration 40, loss = 0.95433006\n",
      "Iteration 41, loss = 0.96372891\n",
      "Iteration 42, loss = 0.95370683\n",
      "Iteration 43, loss = 0.94453284\n",
      "Iteration 44, loss = 0.94619485\n",
      "Iteration 45, loss = 0.94168795\n",
      "Iteration 46, loss = 0.93872150\n",
      "Iteration 47, loss = 0.93974127\n",
      "Iteration 48, loss = 0.93331539\n",
      "Iteration 49, loss = 0.92597917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.92765138\n",
      "Iteration 51, loss = 0.92182038\n",
      "Iteration 52, loss = 0.92414425\n",
      "Iteration 53, loss = 0.92271777\n",
      "Iteration 54, loss = 0.92101411\n",
      "Iteration 55, loss = 0.92061784\n",
      "Iteration 56, loss = 0.91576580\n",
      "Iteration 57, loss = 0.90977537\n",
      "Iteration 58, loss = 0.90969567\n",
      "Iteration 59, loss = 0.90460753\n",
      "Iteration 60, loss = 0.90070166\n",
      "Iteration 61, loss = 0.90016717\n",
      "Iteration 62, loss = 0.90270873\n",
      "Iteration 63, loss = 0.90225175\n",
      "Iteration 64, loss = 0.88884046\n",
      "Iteration 65, loss = 0.89518622\n",
      "Iteration 66, loss = 0.89425814\n",
      "Iteration 67, loss = 0.89043116\n",
      "Iteration 68, loss = 0.89182455\n",
      "Iteration 69, loss = 0.88570419\n",
      "Iteration 70, loss = 0.88898287\n",
      "Iteration 71, loss = 0.88348113\n",
      "Iteration 72, loss = 0.88462590\n",
      "Iteration 73, loss = 0.87606762\n",
      "Iteration 74, loss = 0.87599459\n",
      "Iteration 75, loss = 0.87194561\n",
      "Iteration 76, loss = 0.87594231\n",
      "Iteration 77, loss = 0.87810006\n",
      "Iteration 78, loss = 0.86732179\n",
      "Iteration 79, loss = 0.86809887\n",
      "Iteration 80, loss = 0.86765748\n",
      "Iteration 81, loss = 0.86542460\n",
      "Iteration 82, loss = 0.87733133\n",
      "Iteration 83, loss = 0.87754987\n",
      "Iteration 84, loss = 0.87808198\n",
      "Iteration 85, loss = 0.86128389\n",
      "Iteration 86, loss = 0.85621645\n",
      "Iteration 87, loss = 0.85451347\n",
      "Iteration 88, loss = 0.85522902\n",
      "Iteration 89, loss = 0.85132802\n",
      "Iteration 90, loss = 0.84940975\n",
      "Iteration 91, loss = 0.84101403\n",
      "Iteration 92, loss = 0.85314425\n",
      "Iteration 93, loss = 0.84921886\n",
      "Iteration 94, loss = 0.85071944\n",
      "Iteration 95, loss = 0.85169077\n",
      "Iteration 96, loss = 0.83835149\n",
      "Iteration 97, loss = 0.83810529\n",
      "Iteration 98, loss = 0.83751823\n",
      "Iteration 99, loss = 0.83263418\n",
      "Iteration 100, loss = 0.83368562\n",
      "Iteration 1, loss = 1.32003058\n",
      "Iteration 2, loss = 1.18048125\n",
      "Iteration 3, loss = 1.13817246\n",
      "Iteration 4, loss = 1.08497924\n",
      "Iteration 5, loss = 1.07400778\n",
      "Iteration 6, loss = 1.05288974\n",
      "Iteration 7, loss = 1.04749552\n",
      "Iteration 8, loss = 1.02753254\n",
      "Iteration 9, loss = 1.02128742\n",
      "Iteration 10, loss = 1.01711381\n",
      "Iteration 11, loss = 1.01542935\n",
      "Iteration 12, loss = 1.01282109\n",
      "Iteration 13, loss = 1.01018410\n",
      "Iteration 14, loss = 1.00299565\n",
      "Iteration 15, loss = 0.99466646\n",
      "Iteration 16, loss = 0.99560871\n",
      "Iteration 17, loss = 0.99390323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.99177753\n",
      "Iteration 19, loss = 0.99332025\n",
      "Iteration 20, loss = 0.98679775\n",
      "Iteration 21, loss = 0.98655009\n",
      "Iteration 22, loss = 0.97711249\n",
      "Iteration 23, loss = 0.99015746\n",
      "Iteration 24, loss = 0.99163555\n",
      "Iteration 25, loss = 0.97687213\n",
      "Iteration 26, loss = 0.97196302\n",
      "Iteration 27, loss = 0.96449032\n",
      "Iteration 28, loss = 0.96725833\n",
      "Iteration 29, loss = 0.96563554\n",
      "Iteration 30, loss = 0.96032372\n",
      "Iteration 31, loss = 0.95479794\n",
      "Iteration 32, loss = 0.95037912\n",
      "Iteration 33, loss = 0.95161497\n",
      "Iteration 34, loss = 0.94612340\n",
      "Iteration 35, loss = 0.94464467\n",
      "Iteration 36, loss = 0.93846023\n",
      "Iteration 37, loss = 0.94403913\n",
      "Iteration 38, loss = 0.94448335\n",
      "Iteration 39, loss = 0.94616679\n",
      "Iteration 40, loss = 0.94112516\n",
      "Iteration 41, loss = 0.94210860\n",
      "Iteration 42, loss = 0.93531163\n",
      "Iteration 43, loss = 0.92746872\n",
      "Iteration 44, loss = 0.91996520\n",
      "Iteration 45, loss = 0.92057566\n",
      "Iteration 46, loss = 0.91776926\n",
      "Iteration 47, loss = 0.92225406\n",
      "Iteration 48, loss = 0.92242858\n",
      "Iteration 49, loss = 0.92180723\n",
      "Iteration 50, loss = 0.91613100\n",
      "Iteration 51, loss = 0.90536468\n",
      "Iteration 52, loss = 0.90300234\n",
      "Iteration 53, loss = 0.90295075\n",
      "Iteration 54, loss = 0.90715786\n",
      "Iteration 55, loss = 0.90226539\n",
      "Iteration 56, loss = 0.89621192\n",
      "Iteration 57, loss = 0.89195397\n",
      "Iteration 58, loss = 0.89252391\n",
      "Iteration 59, loss = 0.89968335\n",
      "Iteration 60, loss = 0.88504566\n",
      "Iteration 61, loss = 0.88748557\n",
      "Iteration 62, loss = 0.88054378\n",
      "Iteration 63, loss = 0.88033349\n",
      "Iteration 64, loss = 0.88124192\n",
      "Iteration 65, loss = 0.87713534\n",
      "Iteration 66, loss = 0.87444174\n",
      "Iteration 67, loss = 0.87219464\n",
      "Iteration 68, loss = 0.88747593\n",
      "Iteration 69, loss = 0.88516053\n",
      "Iteration 70, loss = 0.88675919\n",
      "Iteration 71, loss = 0.87744349\n",
      "Iteration 72, loss = 0.87364333\n",
      "Iteration 73, loss = 0.86738161\n",
      "Iteration 74, loss = 0.87023889\n",
      "Iteration 75, loss = 0.86162769\n",
      "Iteration 76, loss = 0.86414025\n",
      "Iteration 77, loss = 0.86395999\n",
      "Iteration 78, loss = 0.87558817\n",
      "Iteration 79, loss = 0.87593000\n",
      "Iteration 80, loss = 0.86522305\n",
      "Iteration 81, loss = 0.86452956\n",
      "Iteration 82, loss = 0.85733961\n",
      "Iteration 83, loss = 0.85161234\n",
      "Iteration 84, loss = 0.84709444\n",
      "Iteration 85, loss = 0.84194801\n",
      "Iteration 86, loss = 0.84702335\n",
      "Iteration 87, loss = 0.83775282\n",
      "Iteration 88, loss = 0.83868536\n",
      "Iteration 89, loss = 0.83568553\n",
      "Iteration 90, loss = 0.83442112\n",
      "Iteration 91, loss = 0.84873444\n",
      "Iteration 92, loss = 0.84726096\n",
      "Iteration 93, loss = 0.84230757\n",
      "Iteration 94, loss = 0.83684178\n",
      "Iteration 95, loss = 0.83736836\n",
      "Iteration 96, loss = 0.84226723\n",
      "Iteration 97, loss = 0.83799720\n",
      "Iteration 98, loss = 0.84497122\n",
      "Iteration 99, loss = 0.84006161\n",
      "Iteration 100, loss = 0.82199415\n",
      "Iteration 1, loss = 13.45934272\n",
      "Iteration 2, loss = 17.03954720\n",
      "Iteration 3, loss = 12.04310407\n",
      "Iteration 4, loss = 8.54506858\n",
      "Iteration 5, loss = 6.24777361\n",
      "Iteration 6, loss = 4.86619197\n",
      "Iteration 7, loss = 2.83428136\n",
      "Iteration 8, loss = 2.20628398\n",
      "Iteration 9, loss = 1.90561886\n",
      "Iteration 10, loss = 1.69491002\n",
      "Iteration 11, loss = 1.55525275\n",
      "Iteration 12, loss = 1.36658382\n",
      "Iteration 13, loss = 1.31235181\n",
      "Iteration 14, loss = 1.22769700\n",
      "Iteration 15, loss = 1.19028952\n",
      "Iteration 16, loss = 1.20718901\n",
      "Iteration 17, loss = 1.20559319\n",
      "Iteration 18, loss = 1.20489075\n",
      "Iteration 19, loss = 1.19084968\n",
      "Iteration 20, loss = 1.16453507\n",
      "Iteration 21, loss = 1.17273413\n",
      "Iteration 22, loss = 1.16395618\n",
      "Iteration 23, loss = 1.16568937\n",
      "Iteration 24, loss = 1.13629311\n",
      "Iteration 25, loss = 1.14911006\n",
      "Iteration 26, loss = 1.13202388\n",
      "Iteration 27, loss = 1.13005260\n",
      "Iteration 28, loss = 1.12860081\n",
      "Iteration 29, loss = 1.13556252\n",
      "Iteration 30, loss = 1.14879548\n",
      "Iteration 31, loss = 1.15118754\n",
      "Iteration 32, loss = 1.16282746\n",
      "Iteration 33, loss = 1.17406578\n",
      "Iteration 34, loss = 1.15961681\n",
      "Iteration 35, loss = 1.13828233\n",
      "Iteration 36, loss = 1.14860643\n",
      "Iteration 37, loss = 1.14172057\n",
      "Iteration 38, loss = 1.13662488\n",
      "Iteration 39, loss = 1.10768768\n",
      "Iteration 40, loss = 1.12076249\n",
      "Iteration 41, loss = 1.11618836\n",
      "Iteration 42, loss = 1.11847363\n",
      "Iteration 43, loss = 1.12498165\n",
      "Iteration 44, loss = 1.12814186\n",
      "Iteration 45, loss = 1.11040659\n",
      "Iteration 46, loss = 1.13412941\n",
      "Iteration 47, loss = 1.11734652\n",
      "Iteration 48, loss = 1.10062137\n",
      "Iteration 49, loss = 1.08936179\n",
      "Iteration 50, loss = 1.09526270\n",
      "Iteration 51, loss = 1.09300634\n",
      "Iteration 52, loss = 1.11051523\n",
      "Iteration 53, loss = 1.09491052\n",
      "Iteration 54, loss = 1.12328072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55, loss = 1.10388636\n",
      "Iteration 56, loss = 1.10456217\n",
      "Iteration 57, loss = 1.08017973\n",
      "Iteration 58, loss = 1.10753147\n",
      "Iteration 59, loss = 1.09273732\n",
      "Iteration 60, loss = 1.12789048\n",
      "Iteration 61, loss = 1.13149594\n",
      "Iteration 62, loss = 1.08674475\n",
      "Iteration 63, loss = 1.07350239\n",
      "Iteration 64, loss = 1.08344461\n",
      "Iteration 65, loss = 1.12034505\n",
      "Iteration 66, loss = 1.07606151\n",
      "Iteration 67, loss = 1.15423660\n",
      "Iteration 68, loss = 1.13002471\n",
      "Iteration 69, loss = 1.05278406\n",
      "Iteration 70, loss = 1.04466103\n",
      "Iteration 71, loss = 1.05269726\n",
      "Iteration 72, loss = 1.08505378\n",
      "Iteration 73, loss = 1.13604339\n",
      "Iteration 74, loss = 1.12557987\n",
      "Iteration 75, loss = 1.11582115\n",
      "Iteration 76, loss = 1.05843934\n",
      "Iteration 77, loss = 1.06782844\n",
      "Iteration 78, loss = 1.06317013\n",
      "Iteration 79, loss = 1.09616939\n",
      "Iteration 80, loss = 1.06782864\n",
      "Iteration 81, loss = 1.04191997\n",
      "Iteration 82, loss = 1.07292195\n",
      "Iteration 83, loss = 1.10385147\n",
      "Iteration 84, loss = 1.14472497\n",
      "Iteration 85, loss = 1.08116218\n",
      "Iteration 86, loss = 1.09457823\n",
      "Iteration 87, loss = 1.07265420\n",
      "Iteration 88, loss = 1.06667963\n",
      "Iteration 89, loss = 1.05598918\n",
      "Iteration 90, loss = 1.03179665\n",
      "Iteration 91, loss = 1.02873317\n",
      "Iteration 92, loss = 1.03118824\n",
      "Iteration 93, loss = 1.07921365\n",
      "Iteration 94, loss = 1.10852380\n",
      "Iteration 95, loss = 1.14633445\n",
      "Iteration 96, loss = 1.13688682\n",
      "Iteration 97, loss = 1.03174597\n",
      "Iteration 98, loss = 1.02555677\n",
      "Iteration 99, loss = 1.02558216\n",
      "Iteration 100, loss = 1.02984941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Designing the classifier neural network\n",
    "neurons = [2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "\n",
    "#setting variables to test performance\n",
    "cv_scores_mean = []\n",
    "cv_scores_std = []\n",
    "test_scores = []\n",
    "\n",
    " #looping over number of neurons\n",
    "for num_neurons in neurons:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(num_neurons,), # hidden layer with num_neurons number of neurons\n",
    "                        activation = 'relu',  # ReLU is the default option\n",
    "                        # solver='sgd',  # default is Adam\n",
    "                        alpha=0.01,  # regularization parameter, default is 0.0001 (increase up to 1.0 for stronger regularization)\n",
    "                        learning_rate_init=.01 ,  # initial step-size for updating the weights, default is 0.001\n",
    "                        max_iter=100,  # number of epochs, default=200\n",
    "                        random_state=seed,\n",
    "                        verbose=1, \n",
    "                        )\n",
    "    scores = cross_val_score(mlp, X_train_scaled, y_train, cv=5, scoring=scoring_method)\n",
    "    cv_scores_mean.append(scores.mean())\n",
    "    cv_scores_std.append(scores.std())\n",
    "    \n",
    "\n",
    "\n",
    "    #train the classifier and tesing it\n",
    "    mlp.fit(X_train, y_train)\n",
    "    test_acc = accuracy_score(y_test, mlp.predict(X_test_scaled))\n",
    "    test_scores.append(test_acc)\n",
    "\n",
    "#print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "#print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b124fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.473306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons:   2 | CV Accuracy: 0.473 | Test Accuracy: 0.445\n",
      "Neurons:   4 | CV Accuracy: 0.530 | Test Accuracy: 0.445\n",
      "Neurons:   8 | CV Accuracy: 0.537 | Test Accuracy: 0.113\n",
      "Neurons:  16 | CV Accuracy: 0.543 | Test Accuracy: 0.297\n",
      "Neurons:  32 | CV Accuracy: 0.551 | Test Accuracy: 0.129\n",
      "Neurons:  64 | CV Accuracy: 0.544 | Test Accuracy: 0.222\n",
      "Neurons: 128 | CV Accuracy: 0.569 | Test Accuracy: 0.461\n",
      "Neurons: 256 | CV Accuracy: 0.561 | Test Accuracy: 0.302\n",
      "Neurons: 512 | CV Accuracy: 0.558 | Test Accuracy: 0.453\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: %f\" % mlp.score(X_train_scaled, y_train))\n",
    "\n",
    "# ---- Plot Results ----\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.errorbar(neurons, cv_scores_mean, yerr=cv_scores_std, fmt='-o', label='Cross-validation Accuracy')\n",
    "plt.plot(neurons, test_scores, '-s', label='Test Accuracy')\n",
    "plt.xlabel('Number of Neurons in Hidden Layer', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('MLP Performance vs. Hidden Layer Size')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ---- Print Results ----\n",
    "for n, cv, test in zip(neurons, cv_scores_mean, test_scores):\n",
    "    print(f\"Neurons: {n:3d} | CV Accuracy: {cv:.3f} | Test Accuracy: {test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741a43f",
   "metadata": {},
   "source": [
    "# Discussion on model performance changes across the range of hyperparameter values (i.e., number of neurons)\n",
    "The accracy of model increases with an increase in number fo neurons for training dataset wheeras the accuracy is very low (< 0.30) for test data for number of neurons upto 100 and it performs better for number of neurons > 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b7d612",
   "metadata": {},
   "source": [
    "# 3. Pick the best performing model (i.e., neurons=128) you found in part 2. Use cross-validation on the same training set to see if using a different activation function would significantly alter the modelâ€™s performance. Report and discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18d1dea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.35207746\n",
      "Iteration 2, loss = 1.20114026\n",
      "Iteration 3, loss = 1.19430311\n",
      "Iteration 4, loss = 1.17601712\n",
      "Iteration 5, loss = 1.14028711\n",
      "Iteration 6, loss = 1.12092398\n",
      "Iteration 7, loss = 1.10877618\n",
      "Iteration 8, loss = 1.09239377\n",
      "Iteration 9, loss = 1.08328871\n",
      "Iteration 10, loss = 1.07707000\n",
      "Iteration 11, loss = 1.06316849\n",
      "Iteration 12, loss = 1.06006602\n",
      "Iteration 13, loss = 1.05095738\n",
      "Iteration 14, loss = 1.05052041\n",
      "Iteration 15, loss = 1.04370316\n",
      "Iteration 16, loss = 1.04083608\n",
      "Iteration 17, loss = 1.03626173\n",
      "Iteration 18, loss = 1.03036468\n",
      "Iteration 19, loss = 1.02984548\n",
      "Iteration 20, loss = 1.02418689\n",
      "Iteration 21, loss = 1.02296479\n",
      "Iteration 22, loss = 1.02000471\n",
      "Iteration 23, loss = 1.01654628\n",
      "Iteration 24, loss = 1.01531598\n",
      "Iteration 25, loss = 1.01205348\n",
      "Iteration 26, loss = 1.01130755\n",
      "Iteration 27, loss = 1.01466938\n",
      "Iteration 28, loss = 1.01074140\n",
      "Iteration 29, loss = 1.00268170\n",
      "Iteration 30, loss = 1.00546369\n",
      "Iteration 31, loss = 1.00016727\n",
      "Iteration 32, loss = 0.99453130\n",
      "Iteration 33, loss = 0.99398237\n",
      "Iteration 34, loss = 0.99276854\n",
      "Iteration 35, loss = 0.99003444\n",
      "Iteration 36, loss = 0.98764007\n",
      "Iteration 37, loss = 0.99022477\n",
      "Iteration 38, loss = 0.98201956\n",
      "Iteration 39, loss = 0.97963855\n",
      "Iteration 40, loss = 0.98220944\n",
      "Iteration 41, loss = 0.98194452\n",
      "Iteration 42, loss = 0.98432619\n",
      "Iteration 43, loss = 0.97291118\n",
      "Iteration 44, loss = 0.97358807\n",
      "Iteration 45, loss = 0.97243180\n",
      "Iteration 46, loss = 0.96654962\n",
      "Iteration 47, loss = 0.96835569\n",
      "Iteration 48, loss = 0.96737409\n",
      "Iteration 49, loss = 0.96718571\n",
      "Iteration 50, loss = 0.96903639\n",
      "Iteration 51, loss = 0.96843788\n",
      "Iteration 52, loss = 0.96874491\n",
      "Iteration 53, loss = 0.96018811\n",
      "Iteration 54, loss = 0.95609602\n",
      "Iteration 55, loss = 0.95910557\n",
      "Iteration 56, loss = 0.95379879\n",
      "Iteration 57, loss = 0.95018985\n",
      "Iteration 58, loss = 0.94786830\n",
      "Iteration 59, loss = 0.94788372\n",
      "Iteration 60, loss = 0.94497950\n",
      "Iteration 61, loss = 0.94297540\n",
      "Iteration 62, loss = 0.94239577\n",
      "Iteration 63, loss = 0.93951311\n",
      "Iteration 64, loss = 0.93917956\n",
      "Iteration 65, loss = 0.93995241\n",
      "Iteration 66, loss = 0.93481990\n",
      "Iteration 67, loss = 0.93921150\n",
      "Iteration 68, loss = 0.93542302\n",
      "Iteration 69, loss = 0.93356180\n",
      "Iteration 70, loss = 0.93136091\n",
      "Iteration 71, loss = 0.92987513\n",
      "Iteration 72, loss = 0.93181745\n",
      "Iteration 73, loss = 0.93446607\n",
      "Iteration 74, loss = 0.93235245\n",
      "Iteration 75, loss = 0.93564186\n",
      "Iteration 76, loss = 0.92927643\n",
      "Iteration 77, loss = 0.92759774\n",
      "Iteration 78, loss = 0.92633615\n",
      "Iteration 79, loss = 0.92412533\n",
      "Iteration 80, loss = 0.92451260\n",
      "Iteration 81, loss = 0.92134469\n",
      "Iteration 82, loss = 0.91948574\n",
      "Iteration 83, loss = 0.92327852\n",
      "Iteration 84, loss = 0.92235840\n",
      "Iteration 85, loss = 0.91928488\n",
      "Iteration 86, loss = 0.91809254\n",
      "Iteration 87, loss = 0.91623708\n",
      "Iteration 88, loss = 0.92037504\n",
      "Iteration 89, loss = 0.91363474\n",
      "Iteration 90, loss = 0.91646982\n",
      "Iteration 91, loss = 0.91557863\n",
      "Iteration 92, loss = 0.91140550\n",
      "Iteration 93, loss = 0.90944691\n",
      "Iteration 94, loss = 0.90953458\n",
      "Iteration 95, loss = 0.90773884\n",
      "Iteration 96, loss = 0.90773991\n",
      "Iteration 97, loss = 0.91011321\n",
      "Iteration 98, loss = 0.91634365\n",
      "Iteration 99, loss = 0.91341610\n",
      "Iteration 100, loss = 0.90311083\n",
      "Iteration 1, loss = 1.35212483\n",
      "Iteration 2, loss = 1.20235876\n",
      "Iteration 3, loss = 1.19282643\n",
      "Iteration 4, loss = 1.16281149\n",
      "Iteration 5, loss = 1.13135987\n",
      "Iteration 6, loss = 1.10978376\n",
      "Iteration 7, loss = 1.09310208\n",
      "Iteration 8, loss = 1.07501313\n",
      "Iteration 9, loss = 1.06447565\n",
      "Iteration 10, loss = 1.05401894\n",
      "Iteration 11, loss = 1.04624623\n",
      "Iteration 12, loss = 1.04315294\n",
      "Iteration 13, loss = 1.03630339\n",
      "Iteration 14, loss = 1.03477197\n",
      "Iteration 15, loss = 1.03366373\n",
      "Iteration 16, loss = 1.03274858\n",
      "Iteration 17, loss = 1.02581615\n",
      "Iteration 18, loss = 1.02073147\n",
      "Iteration 19, loss = 1.01950315\n",
      "Iteration 20, loss = 1.01542007\n",
      "Iteration 21, loss = 1.01272852\n",
      "Iteration 22, loss = 1.01109953\n",
      "Iteration 23, loss = 1.00754484\n",
      "Iteration 24, loss = 1.00546155\n",
      "Iteration 25, loss = 1.00375072\n",
      "Iteration 26, loss = 1.00327256\n",
      "Iteration 27, loss = 1.01270810\n",
      "Iteration 28, loss = 1.00671587\n",
      "Iteration 29, loss = 0.99576961\n",
      "Iteration 30, loss = 1.00254216\n",
      "Iteration 31, loss = 0.99786210\n",
      "Iteration 32, loss = 0.99523307\n",
      "Iteration 33, loss = 0.99084706\n",
      "Iteration 34, loss = 0.98791897\n",
      "Iteration 35, loss = 0.98807156\n",
      "Iteration 36, loss = 0.98440213\n",
      "Iteration 37, loss = 0.98553030\n",
      "Iteration 38, loss = 0.98017560\n",
      "Iteration 39, loss = 0.98153883\n",
      "Iteration 40, loss = 0.97625452\n",
      "Iteration 41, loss = 0.97750596\n",
      "Iteration 42, loss = 0.97634592\n",
      "Iteration 43, loss = 0.97187939\n",
      "Iteration 44, loss = 0.97341656\n",
      "Iteration 45, loss = 0.97061740\n",
      "Iteration 46, loss = 0.96725039\n",
      "Iteration 47, loss = 0.96722803\n",
      "Iteration 48, loss = 0.96175221\n",
      "Iteration 49, loss = 0.96794426\n",
      "Iteration 50, loss = 0.96282765\n",
      "Iteration 51, loss = 0.96201456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.96803514\n",
      "Iteration 53, loss = 0.95971894\n",
      "Iteration 54, loss = 0.95837992\n",
      "Iteration 55, loss = 0.95389601\n",
      "Iteration 56, loss = 0.96098548\n",
      "Iteration 57, loss = 0.95407136\n",
      "Iteration 58, loss = 0.95281232\n",
      "Iteration 59, loss = 0.94894422\n",
      "Iteration 60, loss = 0.95053421\n",
      "Iteration 61, loss = 0.94430849\n",
      "Iteration 62, loss = 0.94355727\n",
      "Iteration 63, loss = 0.94518831\n",
      "Iteration 64, loss = 0.94109489\n",
      "Iteration 65, loss = 0.94027445\n",
      "Iteration 66, loss = 0.93890888\n",
      "Iteration 67, loss = 0.93868536\n",
      "Iteration 68, loss = 0.93724608\n",
      "Iteration 69, loss = 0.93422343\n",
      "Iteration 70, loss = 0.93378840\n",
      "Iteration 71, loss = 0.93129875\n",
      "Iteration 72, loss = 0.93372801\n",
      "Iteration 73, loss = 0.93279722\n",
      "Iteration 74, loss = 0.92836948\n",
      "Iteration 75, loss = 0.92928461\n",
      "Iteration 76, loss = 0.93155710\n",
      "Iteration 77, loss = 0.92880859\n",
      "Iteration 78, loss = 0.92564269\n",
      "Iteration 79, loss = 0.92984379\n",
      "Iteration 80, loss = 0.92222440\n",
      "Iteration 81, loss = 0.92891302\n",
      "Iteration 82, loss = 0.92302527\n",
      "Iteration 83, loss = 0.91931136\n",
      "Iteration 84, loss = 0.92001399\n",
      "Iteration 85, loss = 0.91744096\n",
      "Iteration 86, loss = 0.91644000\n",
      "Iteration 87, loss = 0.92075472\n",
      "Iteration 88, loss = 0.91239468\n",
      "Iteration 89, loss = 0.91174463\n",
      "Iteration 90, loss = 0.91375352\n",
      "Iteration 91, loss = 0.91261209\n",
      "Iteration 92, loss = 0.91145155\n",
      "Iteration 93, loss = 0.91027768\n",
      "Iteration 94, loss = 0.90734124\n",
      "Iteration 95, loss = 0.91294071\n",
      "Iteration 96, loss = 0.90623585\n",
      "Iteration 97, loss = 0.90502250\n",
      "Iteration 98, loss = 0.90318049\n",
      "Iteration 99, loss = 0.91103298\n",
      "Iteration 100, loss = 0.90011030\n",
      "Iteration 1, loss = 1.35363575\n",
      "Iteration 2, loss = 1.20396004\n",
      "Iteration 3, loss = 1.18329236\n",
      "Iteration 4, loss = 1.16330954\n",
      "Iteration 5, loss = 1.12945806\n",
      "Iteration 6, loss = 1.10882178\n",
      "Iteration 7, loss = 1.09489350\n",
      "Iteration 8, loss = 1.07733149\n",
      "Iteration 9, loss = 1.07149938\n",
      "Iteration 10, loss = 1.06048251\n",
      "Iteration 11, loss = 1.05232045\n",
      "Iteration 12, loss = 1.05081001\n",
      "Iteration 13, loss = 1.04579321\n",
      "Iteration 14, loss = 1.03739829\n",
      "Iteration 15, loss = 1.03388467\n",
      "Iteration 16, loss = 1.03360993\n",
      "Iteration 17, loss = 1.02714111\n",
      "Iteration 18, loss = 1.02508005\n",
      "Iteration 19, loss = 1.02159185\n",
      "Iteration 20, loss = 1.02473422\n",
      "Iteration 21, loss = 1.01909490\n",
      "Iteration 22, loss = 1.01584336\n",
      "Iteration 23, loss = 1.01741605\n",
      "Iteration 24, loss = 1.02203109\n",
      "Iteration 25, loss = 1.01717853\n",
      "Iteration 26, loss = 1.01344830\n",
      "Iteration 27, loss = 1.01976435\n",
      "Iteration 28, loss = 1.00698228\n",
      "Iteration 29, loss = 1.01241702\n",
      "Iteration 30, loss = 1.00967991\n",
      "Iteration 31, loss = 1.00679880\n",
      "Iteration 32, loss = 1.00997367\n",
      "Iteration 33, loss = 1.00243578\n",
      "Iteration 34, loss = 0.99987389\n",
      "Iteration 35, loss = 0.99965687\n",
      "Iteration 36, loss = 0.99647680\n",
      "Iteration 37, loss = 0.99454777\n",
      "Iteration 38, loss = 0.99452376\n",
      "Iteration 39, loss = 0.99089582\n",
      "Iteration 40, loss = 0.98806858\n",
      "Iteration 41, loss = 0.98816012\n",
      "Iteration 42, loss = 0.99257951\n",
      "Iteration 43, loss = 0.98674116\n",
      "Iteration 44, loss = 0.98431863\n",
      "Iteration 45, loss = 0.98442950\n",
      "Iteration 46, loss = 0.98136087\n",
      "Iteration 47, loss = 0.97545409\n",
      "Iteration 48, loss = 0.97623054\n",
      "Iteration 49, loss = 0.98225763\n",
      "Iteration 50, loss = 0.97313328\n",
      "Iteration 51, loss = 0.97242479\n",
      "Iteration 52, loss = 0.97346397\n",
      "Iteration 53, loss = 0.96850604\n",
      "Iteration 54, loss = 0.96257731\n",
      "Iteration 55, loss = 0.96168415\n",
      "Iteration 56, loss = 0.96176543\n",
      "Iteration 57, loss = 0.95630442\n",
      "Iteration 58, loss = 0.95614052\n",
      "Iteration 59, loss = 0.95646471\n",
      "Iteration 60, loss = 0.95074274\n",
      "Iteration 61, loss = 0.95174677\n",
      "Iteration 62, loss = 0.94967131\n",
      "Iteration 63, loss = 0.95132555\n",
      "Iteration 64, loss = 0.95507867\n",
      "Iteration 65, loss = 0.94304620\n",
      "Iteration 66, loss = 0.94577190\n",
      "Iteration 67, loss = 0.93902861\n",
      "Iteration 68, loss = 0.94156414\n",
      "Iteration 69, loss = 0.93537478\n",
      "Iteration 70, loss = 0.93752454\n",
      "Iteration 71, loss = 0.93603485\n",
      "Iteration 72, loss = 0.93170731\n",
      "Iteration 73, loss = 0.93029931\n",
      "Iteration 74, loss = 0.92990982\n",
      "Iteration 75, loss = 0.93168723\n",
      "Iteration 76, loss = 0.92445863\n",
      "Iteration 77, loss = 0.92848044\n",
      "Iteration 78, loss = 0.92046801\n",
      "Iteration 79, loss = 0.92125756\n",
      "Iteration 80, loss = 0.92377050\n",
      "Iteration 81, loss = 0.91686028\n",
      "Iteration 82, loss = 0.91393988\n",
      "Iteration 83, loss = 0.91231763\n",
      "Iteration 84, loss = 0.90953129\n",
      "Iteration 85, loss = 0.90928609\n",
      "Iteration 86, loss = 0.90873682\n",
      "Iteration 87, loss = 0.91013323\n",
      "Iteration 88, loss = 0.90981322\n",
      "Iteration 89, loss = 0.90393375\n",
      "Iteration 90, loss = 0.90302547\n",
      "Iteration 91, loss = 0.89857733\n",
      "Iteration 92, loss = 0.89738118\n",
      "Iteration 93, loss = 0.89789794\n",
      "Iteration 94, loss = 0.89473625\n",
      "Iteration 95, loss = 0.89481394\n",
      "Iteration 96, loss = 0.89536617\n",
      "Iteration 97, loss = 0.89508627\n",
      "Iteration 98, loss = 0.89288367\n",
      "Iteration 99, loss = 0.89233395\n",
      "Iteration 100, loss = 0.88673631\n",
      "Iteration 1, loss = 1.34084455\n",
      "Iteration 2, loss = 1.21795571\n",
      "Iteration 3, loss = 1.19756562\n",
      "Iteration 4, loss = 1.17383152\n",
      "Iteration 5, loss = 1.14606626\n",
      "Iteration 6, loss = 1.12916564\n",
      "Iteration 7, loss = 1.11392478\n",
      "Iteration 8, loss = 1.09482346\n",
      "Iteration 9, loss = 1.08828848\n",
      "Iteration 10, loss = 1.07639785\n",
      "Iteration 11, loss = 1.06928668\n",
      "Iteration 12, loss = 1.06476768\n",
      "Iteration 13, loss = 1.06299095\n",
      "Iteration 14, loss = 1.05694269\n",
      "Iteration 15, loss = 1.05640475\n",
      "Iteration 16, loss = 1.04880324\n",
      "Iteration 17, loss = 1.04447249\n",
      "Iteration 18, loss = 1.03902138\n",
      "Iteration 19, loss = 1.03686314\n",
      "Iteration 20, loss = 1.03417591\n",
      "Iteration 21, loss = 1.02939928\n",
      "Iteration 22, loss = 1.03041639\n",
      "Iteration 23, loss = 1.02542412\n",
      "Iteration 24, loss = 1.02170988\n",
      "Iteration 25, loss = 1.01781871\n",
      "Iteration 26, loss = 1.01462686\n",
      "Iteration 27, loss = 1.01324700\n",
      "Iteration 28, loss = 1.01032788\n",
      "Iteration 29, loss = 1.00903241\n",
      "Iteration 30, loss = 1.00975885\n",
      "Iteration 31, loss = 1.00241088\n",
      "Iteration 32, loss = 1.00830014\n",
      "Iteration 33, loss = 0.99849524\n",
      "Iteration 34, loss = 0.99714587\n",
      "Iteration 35, loss = 0.99558760\n",
      "Iteration 36, loss = 0.99438673\n",
      "Iteration 37, loss = 0.99032798\n",
      "Iteration 38, loss = 0.98823723\n",
      "Iteration 39, loss = 0.98914717\n",
      "Iteration 40, loss = 0.98933565\n",
      "Iteration 41, loss = 0.98169874\n",
      "Iteration 42, loss = 0.97993094\n",
      "Iteration 43, loss = 0.98533051\n",
      "Iteration 44, loss = 0.97657448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.97716698\n",
      "Iteration 46, loss = 0.98025364\n",
      "Iteration 47, loss = 0.97091475\n",
      "Iteration 48, loss = 0.97058374\n",
      "Iteration 49, loss = 0.97078817\n",
      "Iteration 50, loss = 0.96973286\n",
      "Iteration 51, loss = 0.96636915\n",
      "Iteration 52, loss = 0.96271934\n",
      "Iteration 53, loss = 0.96458687\n",
      "Iteration 54, loss = 0.96046790\n",
      "Iteration 55, loss = 0.96519745\n",
      "Iteration 56, loss = 0.96013347\n",
      "Iteration 57, loss = 0.95931866\n",
      "Iteration 58, loss = 0.95611044\n",
      "Iteration 59, loss = 0.96062297\n",
      "Iteration 60, loss = 0.94809008\n",
      "Iteration 61, loss = 0.95332081\n",
      "Iteration 62, loss = 0.94650540\n",
      "Iteration 63, loss = 0.94747959\n",
      "Iteration 64, loss = 0.95053275\n",
      "Iteration 65, loss = 0.93907736\n",
      "Iteration 66, loss = 0.94044472\n",
      "Iteration 67, loss = 0.93929276\n",
      "Iteration 68, loss = 0.93634357\n",
      "Iteration 69, loss = 0.93969606\n",
      "Iteration 70, loss = 0.93508633\n",
      "Iteration 71, loss = 0.93185177\n",
      "Iteration 72, loss = 0.93032411\n",
      "Iteration 73, loss = 0.92923927\n",
      "Iteration 74, loss = 0.93173634\n",
      "Iteration 75, loss = 0.92990704\n",
      "Iteration 76, loss = 0.92985724\n",
      "Iteration 77, loss = 0.92325723\n",
      "Iteration 78, loss = 0.92335122\n",
      "Iteration 79, loss = 0.92505159\n",
      "Iteration 80, loss = 0.91957834\n",
      "Iteration 81, loss = 0.91730031\n",
      "Iteration 82, loss = 0.91461893\n",
      "Iteration 83, loss = 0.91547788\n",
      "Iteration 84, loss = 0.91132011\n",
      "Iteration 85, loss = 0.91279648\n",
      "Iteration 86, loss = 0.91424782\n",
      "Iteration 87, loss = 0.91768655\n",
      "Iteration 88, loss = 0.90954267\n",
      "Iteration 89, loss = 0.91228371\n",
      "Iteration 90, loss = 0.90788252\n",
      "Iteration 91, loss = 0.91050951\n",
      "Iteration 92, loss = 0.90546459\n",
      "Iteration 93, loss = 0.90490146\n",
      "Iteration 94, loss = 0.90709966\n",
      "Iteration 95, loss = 0.89816706\n",
      "Iteration 96, loss = 0.90669548\n",
      "Iteration 97, loss = 0.89809502\n",
      "Iteration 98, loss = 0.89396083\n",
      "Iteration 99, loss = 0.89860935\n",
      "Iteration 100, loss = 0.89666825\n",
      "Iteration 1, loss = 1.34613426\n",
      "Iteration 2, loss = 1.21026609\n",
      "Iteration 3, loss = 1.18018006\n",
      "Iteration 4, loss = 1.15379941\n",
      "Iteration 5, loss = 1.12088276\n",
      "Iteration 6, loss = 1.10121358\n",
      "Iteration 7, loss = 1.08107019\n",
      "Iteration 8, loss = 1.06636017\n",
      "Iteration 9, loss = 1.05149386\n",
      "Iteration 10, loss = 1.04277989\n",
      "Iteration 11, loss = 1.03544220\n",
      "Iteration 12, loss = 1.02869726\n",
      "Iteration 13, loss = 1.01941518\n",
      "Iteration 14, loss = 1.01807461\n",
      "Iteration 15, loss = 1.01350518\n",
      "Iteration 16, loss = 1.01181808\n",
      "Iteration 17, loss = 1.00433843\n",
      "Iteration 18, loss = 0.99781887\n",
      "Iteration 19, loss = 0.99743084\n",
      "Iteration 20, loss = 0.99505105\n",
      "Iteration 21, loss = 0.99170361\n",
      "Iteration 22, loss = 0.99017007\n",
      "Iteration 23, loss = 0.98792305\n",
      "Iteration 24, loss = 0.98582776\n",
      "Iteration 25, loss = 0.98593584\n",
      "Iteration 26, loss = 0.98510214\n",
      "Iteration 27, loss = 0.97748977\n",
      "Iteration 28, loss = 0.97801406\n",
      "Iteration 29, loss = 0.97750777\n",
      "Iteration 30, loss = 0.97760500\n",
      "Iteration 31, loss = 0.97371066\n",
      "Iteration 32, loss = 0.96958327\n",
      "Iteration 33, loss = 0.97921288\n",
      "Iteration 34, loss = 0.96415351\n",
      "Iteration 35, loss = 0.96642234\n",
      "Iteration 36, loss = 0.96454386\n",
      "Iteration 37, loss = 0.96466299\n",
      "Iteration 38, loss = 0.96491283\n",
      "Iteration 39, loss = 0.96260801\n",
      "Iteration 40, loss = 0.95815022\n",
      "Iteration 41, loss = 0.95713513\n",
      "Iteration 42, loss = 0.95326790\n",
      "Iteration 43, loss = 0.95016863\n",
      "Iteration 44, loss = 0.94751129\n",
      "Iteration 45, loss = 0.94671636\n",
      "Iteration 46, loss = 0.94665450\n",
      "Iteration 47, loss = 0.94537060\n",
      "Iteration 48, loss = 0.94278799\n",
      "Iteration 49, loss = 0.94341215\n",
      "Iteration 50, loss = 0.94156041\n",
      "Iteration 51, loss = 0.93512871\n",
      "Iteration 52, loss = 0.94027446\n",
      "Iteration 53, loss = 0.93541857\n",
      "Iteration 54, loss = 0.93380385\n",
      "Iteration 55, loss = 0.93125789\n",
      "Iteration 56, loss = 0.93371485\n",
      "Iteration 57, loss = 0.93022019\n",
      "Iteration 58, loss = 0.92983773\n",
      "Iteration 59, loss = 0.92826913\n",
      "Iteration 60, loss = 0.92231809\n",
      "Iteration 61, loss = 0.93413494\n",
      "Iteration 62, loss = 0.93516988\n",
      "Iteration 63, loss = 0.91920744\n",
      "Iteration 64, loss = 0.92460072\n",
      "Iteration 65, loss = 0.91609268\n",
      "Iteration 66, loss = 0.91467631\n",
      "Iteration 67, loss = 0.91687615\n",
      "Iteration 68, loss = 0.91404115\n",
      "Iteration 69, loss = 0.90888689\n",
      "Iteration 70, loss = 0.91317194\n",
      "Iteration 71, loss = 0.91054184\n",
      "Iteration 72, loss = 0.90913481\n",
      "Iteration 73, loss = 0.90454721\n",
      "Iteration 74, loss = 0.90578166\n",
      "Iteration 75, loss = 0.90542214\n",
      "Iteration 76, loss = 0.90403546\n",
      "Iteration 77, loss = 0.90056419\n",
      "Iteration 78, loss = 0.89926134\n",
      "Iteration 79, loss = 0.89534166\n",
      "Iteration 80, loss = 0.89961888\n",
      "Iteration 81, loss = 0.89883572\n",
      "Iteration 82, loss = 0.89949612\n",
      "Iteration 83, loss = 0.89810410\n",
      "Iteration 84, loss = 0.89244044\n",
      "Iteration 85, loss = 0.89140278\n",
      "Iteration 86, loss = 0.88841452\n",
      "Iteration 87, loss = 0.88949120\n",
      "Iteration 88, loss = 0.88897299\n",
      "Iteration 89, loss = 0.88251057\n",
      "Iteration 90, loss = 0.88684368\n",
      "Iteration 91, loss = 0.88950239\n",
      "Iteration 92, loss = 0.89049724\n",
      "Iteration 93, loss = 0.89287199\n",
      "Iteration 94, loss = 0.88849261\n",
      "Iteration 95, loss = 0.87775470\n",
      "Iteration 96, loss = 0.88094986\n",
      "Iteration 97, loss = 0.87449043\n",
      "Iteration 98, loss = 0.87377619\n",
      "Iteration 99, loss = 0.87299893\n",
      "Iteration 100, loss = 0.86873175\n",
      "Iteration 1, loss = 7.26795660\n",
      "Iteration 2, loss = 4.98867965\n",
      "Iteration 3, loss = 3.80131411\n",
      "Iteration 4, loss = 3.56615521\n",
      "Iteration 5, loss = 3.14260714\n",
      "Iteration 6, loss = 2.69339959\n",
      "Iteration 7, loss = 1.82484091\n",
      "Iteration 8, loss = 1.62533306\n",
      "Iteration 9, loss = 1.61471421\n",
      "Iteration 10, loss = 1.46399798\n",
      "Iteration 11, loss = 1.32509780\n",
      "Iteration 12, loss = 1.36074235\n",
      "Iteration 13, loss = 1.29178754\n",
      "Iteration 14, loss = 1.23906307\n",
      "Iteration 15, loss = 1.22276527\n",
      "Iteration 16, loss = 1.24925333\n",
      "Iteration 17, loss = 1.21377302\n",
      "Iteration 18, loss = 1.22225591\n",
      "Iteration 19, loss = 1.16108291\n",
      "Iteration 20, loss = 1.19577720\n",
      "Iteration 21, loss = 1.19464268\n",
      "Iteration 22, loss = 1.15651693\n",
      "Iteration 23, loss = 1.16496359\n",
      "Iteration 24, loss = 1.18442787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 1.17704331\n",
      "Iteration 26, loss = 1.18173119\n",
      "Iteration 27, loss = 1.20012727\n",
      "Iteration 28, loss = 1.17037092\n",
      "Iteration 29, loss = 1.18893823\n",
      "Iteration 30, loss = 1.16567233\n",
      "Iteration 31, loss = 1.23959940\n",
      "Iteration 32, loss = 1.22778303\n",
      "Iteration 33, loss = 1.17461589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32140264\n",
      "Iteration 2, loss = 1.19418181\n",
      "Iteration 3, loss = 1.18164091\n",
      "Iteration 4, loss = 1.14305026\n",
      "Iteration 5, loss = 1.11563786\n",
      "Iteration 6, loss = 1.10087035\n",
      "Iteration 7, loss = 1.08612296\n",
      "Iteration 8, loss = 1.07755043\n",
      "Iteration 9, loss = 1.06578562\n",
      "Iteration 10, loss = 1.06351014\n",
      "Iteration 11, loss = 1.04765731\n",
      "Iteration 12, loss = 1.05220387\n",
      "Iteration 13, loss = 1.04564634\n",
      "Iteration 14, loss = 1.04426677\n",
      "Iteration 15, loss = 1.04722048\n",
      "Iteration 16, loss = 1.04157208\n",
      "Iteration 17, loss = 1.03827394\n",
      "Iteration 18, loss = 1.03501249\n",
      "Iteration 19, loss = 1.03354436\n",
      "Iteration 20, loss = 1.02822919\n",
      "Iteration 21, loss = 1.03298533\n",
      "Iteration 22, loss = 1.02889267\n",
      "Iteration 23, loss = 1.02843288\n",
      "Iteration 24, loss = 1.02989713\n",
      "Iteration 25, loss = 1.02553723\n",
      "Iteration 26, loss = 1.02518879\n",
      "Iteration 27, loss = 1.03064615\n",
      "Iteration 28, loss = 1.02999637\n",
      "Iteration 29, loss = 1.02148179\n",
      "Iteration 30, loss = 1.02795238\n",
      "Iteration 31, loss = 1.02478193\n",
      "Iteration 32, loss = 1.02052623\n",
      "Iteration 33, loss = 1.02104057\n",
      "Iteration 34, loss = 1.02136211\n",
      "Iteration 35, loss = 1.02253081\n",
      "Iteration 36, loss = 1.02018965\n",
      "Iteration 37, loss = 1.02641367\n",
      "Iteration 38, loss = 1.01842623\n",
      "Iteration 39, loss = 1.01872474\n",
      "Iteration 40, loss = 1.02255455\n",
      "Iteration 41, loss = 1.02551775\n",
      "Iteration 42, loss = 1.02704988\n",
      "Iteration 43, loss = 1.01914871\n",
      "Iteration 44, loss = 1.02110240\n",
      "Iteration 45, loss = 1.02107968\n",
      "Iteration 46, loss = 1.01792531\n",
      "Iteration 47, loss = 1.02142443\n",
      "Iteration 48, loss = 1.02013462\n",
      "Iteration 49, loss = 1.01980853\n",
      "Iteration 50, loss = 1.02250886\n",
      "Iteration 51, loss = 1.02685609\n",
      "Iteration 52, loss = 1.02790572\n",
      "Iteration 53, loss = 1.01974982\n",
      "Iteration 54, loss = 1.01871387\n",
      "Iteration 55, loss = 1.02201674\n",
      "Iteration 56, loss = 1.02092625\n",
      "Iteration 57, loss = 1.01874443\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.31993184\n",
      "Iteration 2, loss = 1.19127101\n",
      "Iteration 3, loss = 1.17489885\n",
      "Iteration 4, loss = 1.12608057\n",
      "Iteration 5, loss = 1.09785024\n",
      "Iteration 6, loss = 1.08435638\n",
      "Iteration 7, loss = 1.06812891\n",
      "Iteration 8, loss = 1.06014805\n",
      "Iteration 9, loss = 1.05562674\n",
      "Iteration 10, loss = 1.04531883\n",
      "Iteration 11, loss = 1.04007243\n",
      "Iteration 12, loss = 1.04195423\n",
      "Iteration 13, loss = 1.03626348\n",
      "Iteration 14, loss = 1.02787954\n",
      "Iteration 15, loss = 1.03197242\n",
      "Iteration 16, loss = 1.02917992\n",
      "Iteration 17, loss = 1.02814544\n",
      "Iteration 18, loss = 1.02461579\n",
      "Iteration 19, loss = 1.01668714\n",
      "Iteration 20, loss = 1.01916330\n",
      "Iteration 21, loss = 1.01606181\n",
      "Iteration 22, loss = 1.01633520\n",
      "Iteration 23, loss = 1.01420674\n",
      "Iteration 24, loss = 1.01740591\n",
      "Iteration 25, loss = 1.01415526\n",
      "Iteration 26, loss = 1.01329369\n",
      "Iteration 27, loss = 1.02239703\n",
      "Iteration 28, loss = 1.02682493\n",
      "Iteration 29, loss = 1.01221751\n",
      "Iteration 30, loss = 1.01386391\n",
      "Iteration 31, loss = 1.01608405\n",
      "Iteration 32, loss = 1.01433586\n",
      "Iteration 33, loss = 1.01376843\n",
      "Iteration 34, loss = 1.01063834\n",
      "Iteration 35, loss = 1.00955423\n",
      "Iteration 36, loss = 1.01138058\n",
      "Iteration 37, loss = 1.01374095\n",
      "Iteration 38, loss = 1.00999385\n",
      "Iteration 39, loss = 1.01274213\n",
      "Iteration 40, loss = 1.00998730\n",
      "Iteration 41, loss = 1.01464671\n",
      "Iteration 42, loss = 1.01283053\n",
      "Iteration 43, loss = 1.01084172\n",
      "Iteration 44, loss = 1.01352045\n",
      "Iteration 45, loss = 1.01289008\n",
      "Iteration 46, loss = 1.00985869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.31834479\n",
      "Iteration 2, loss = 1.19432979\n",
      "Iteration 3, loss = 1.17579510\n",
      "Iteration 4, loss = 1.12649221\n",
      "Iteration 5, loss = 1.09906260\n",
      "Iteration 6, loss = 1.08859342\n",
      "Iteration 7, loss = 1.07632431\n",
      "Iteration 8, loss = 1.06226797\n",
      "Iteration 9, loss = 1.05765250\n",
      "Iteration 10, loss = 1.04822585\n",
      "Iteration 11, loss = 1.04460851\n",
      "Iteration 12, loss = 1.04538637\n",
      "Iteration 13, loss = 1.04054018\n",
      "Iteration 14, loss = 1.03155479\n",
      "Iteration 15, loss = 1.03009523\n",
      "Iteration 16, loss = 1.03099290\n",
      "Iteration 17, loss = 1.02577854\n",
      "Iteration 18, loss = 1.02656188\n",
      "Iteration 19, loss = 1.02277941\n",
      "Iteration 20, loss = 1.02866704\n",
      "Iteration 21, loss = 1.02652717\n",
      "Iteration 22, loss = 1.02104470\n",
      "Iteration 23, loss = 1.02025420\n",
      "Iteration 24, loss = 1.02864435\n",
      "Iteration 25, loss = 1.02593659\n",
      "Iteration 26, loss = 1.02425313\n",
      "Iteration 27, loss = 1.03665278\n",
      "Iteration 28, loss = 1.01659135\n",
      "Iteration 29, loss = 1.02746728\n",
      "Iteration 30, loss = 1.02782241\n",
      "Iteration 31, loss = 1.02552883\n",
      "Iteration 32, loss = 1.03035847\n",
      "Iteration 33, loss = 1.02369390\n",
      "Iteration 34, loss = 1.01954649\n",
      "Iteration 35, loss = 1.02226774\n",
      "Iteration 36, loss = 1.01815617\n",
      "Iteration 37, loss = 1.01702491\n",
      "Iteration 38, loss = 1.01729770\n",
      "Iteration 39, loss = 1.01625281\n",
      "Iteration 40, loss = 1.01527814\n",
      "Iteration 41, loss = 1.01731190\n",
      "Iteration 42, loss = 1.02178024\n",
      "Iteration 43, loss = 1.01768713\n",
      "Iteration 44, loss = 1.01754774\n",
      "Iteration 45, loss = 1.01853387\n",
      "Iteration 46, loss = 1.01703438\n",
      "Iteration 47, loss = 1.01611293\n",
      "Iteration 48, loss = 1.01573489\n",
      "Iteration 49, loss = 1.02257722\n",
      "Iteration 50, loss = 1.01720199\n",
      "Iteration 51, loss = 1.01860979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.30635009\n",
      "Iteration 2, loss = 1.20792457\n",
      "Iteration 3, loss = 1.18831099\n",
      "Iteration 4, loss = 1.13011768\n",
      "Iteration 5, loss = 1.11035239\n",
      "Iteration 6, loss = 1.10139130\n",
      "Iteration 7, loss = 1.09057628\n",
      "Iteration 8, loss = 1.07677010\n",
      "Iteration 9, loss = 1.07116088\n",
      "Iteration 10, loss = 1.06422360\n",
      "Iteration 11, loss = 1.06116679\n",
      "Iteration 12, loss = 1.06249864\n",
      "Iteration 13, loss = 1.06045820\n",
      "Iteration 14, loss = 1.05981056\n",
      "Iteration 15, loss = 1.06204551\n",
      "Iteration 16, loss = 1.05144692\n",
      "Iteration 17, loss = 1.05218751\n",
      "Iteration 18, loss = 1.04774461\n",
      "Iteration 19, loss = 1.04937979\n",
      "Iteration 20, loss = 1.04438588\n",
      "Iteration 21, loss = 1.04202927\n",
      "Iteration 22, loss = 1.04841727\n",
      "Iteration 23, loss = 1.04556105\n",
      "Iteration 24, loss = 1.03935365\n",
      "Iteration 25, loss = 1.03709514\n",
      "Iteration 26, loss = 1.03332148\n",
      "Iteration 27, loss = 1.03567948\n",
      "Iteration 28, loss = 1.03350057\n",
      "Iteration 29, loss = 1.03348531\n",
      "Iteration 30, loss = 1.03608516\n",
      "Iteration 31, loss = 1.03318854\n",
      "Iteration 32, loss = 1.04206793\n",
      "Iteration 33, loss = 1.03224962\n",
      "Iteration 34, loss = 1.03411641\n",
      "Iteration 35, loss = 1.03276651\n",
      "Iteration 36, loss = 1.03676850\n",
      "Iteration 37, loss = 1.03090893\n",
      "Iteration 38, loss = 1.03227147\n",
      "Iteration 39, loss = 1.03465941\n",
      "Iteration 40, loss = 1.03848997\n",
      "Iteration 41, loss = 1.03090484\n",
      "Iteration 42, loss = 1.03140448\n",
      "Iteration 43, loss = 1.03830142\n",
      "Iteration 44, loss = 1.03112379\n",
      "Iteration 45, loss = 1.03481706\n",
      "Iteration 46, loss = 1.03839006\n",
      "Iteration 47, loss = 1.02935866\n",
      "Iteration 48, loss = 1.03144128\n",
      "Iteration 49, loss = 1.03637653\n",
      "Iteration 50, loss = 1.03255389\n",
      "Iteration 51, loss = 1.03355390\n",
      "Iteration 52, loss = 1.03384727\n",
      "Iteration 53, loss = 1.03230347\n",
      "Iteration 54, loss = 1.03150456\n",
      "Iteration 55, loss = 1.03581906\n",
      "Iteration 56, loss = 1.03205403\n",
      "Iteration 57, loss = 1.03110124\n",
      "Iteration 58, loss = 1.02930372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.31257488\n",
      "Iteration 2, loss = 1.18900276\n",
      "Iteration 3, loss = 1.16870866\n",
      "Iteration 4, loss = 1.11723640\n",
      "Iteration 5, loss = 1.08750447\n",
      "Iteration 6, loss = 1.08508023\n",
      "Iteration 7, loss = 1.06761525\n",
      "Iteration 8, loss = 1.06110811\n",
      "Iteration 9, loss = 1.04634327\n",
      "Iteration 10, loss = 1.02778411\n",
      "Iteration 11, loss = 1.03171636\n",
      "Iteration 12, loss = 1.02605657\n",
      "Iteration 13, loss = 1.01687607\n",
      "Iteration 14, loss = 1.01818637\n",
      "Iteration 15, loss = 1.01606479\n",
      "Iteration 16, loss = 1.01475516\n",
      "Iteration 17, loss = 1.00904476\n",
      "Iteration 18, loss = 0.99998879\n",
      "Iteration 19, loss = 1.00095877\n",
      "Iteration 20, loss = 1.00066149\n",
      "Iteration 21, loss = 0.99860924\n",
      "Iteration 22, loss = 0.99863999\n",
      "Iteration 23, loss = 0.99810155\n",
      "Iteration 24, loss = 0.99780243\n",
      "Iteration 25, loss = 0.99885533\n",
      "Iteration 26, loss = 1.00057778\n",
      "Iteration 27, loss = 0.99283810\n",
      "Iteration 28, loss = 0.99442478\n",
      "Iteration 29, loss = 0.99618197\n",
      "Iteration 30, loss = 0.99849168\n",
      "Iteration 31, loss = 0.99437814\n",
      "Iteration 32, loss = 0.99167562\n",
      "Iteration 33, loss = 1.00487597\n",
      "Iteration 34, loss = 0.98895826\n",
      "Iteration 35, loss = 0.99315701\n",
      "Iteration 36, loss = 0.99203597\n",
      "Iteration 37, loss = 0.99388308\n",
      "Iteration 38, loss = 0.99570562\n",
      "Iteration 39, loss = 0.99441562\n",
      "Iteration 40, loss = 0.99214841\n",
      "Iteration 41, loss = 0.99276618\n",
      "Iteration 42, loss = 0.99023841\n",
      "Iteration 43, loss = 0.98852467\n",
      "Iteration 44, loss = 0.98799482\n",
      "Iteration 45, loss = 0.98934967\n",
      "Iteration 46, loss = 0.99072727\n",
      "Iteration 47, loss = 0.99108347\n",
      "Iteration 48, loss = 0.98949966\n",
      "Iteration 49, loss = 0.99085770\n",
      "Iteration 50, loss = 0.99155195\n",
      "Iteration 51, loss = 0.98803829\n",
      "Iteration 52, loss = 0.99465638\n",
      "Iteration 53, loss = 0.99191871\n",
      "Iteration 54, loss = 0.99168111\n",
      "Iteration 55, loss = 0.99111707\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47166479\n",
      "Iteration 2, loss = 1.25874092\n",
      "Iteration 3, loss = 1.22109083\n",
      "Iteration 4, loss = 1.19432083\n",
      "Iteration 5, loss = 1.20224832\n",
      "Iteration 6, loss = 1.19464029\n",
      "Iteration 7, loss = 1.16632696\n",
      "Iteration 8, loss = 1.16956319\n",
      "Iteration 9, loss = 1.17626819\n",
      "Iteration 10, loss = 1.16959396\n",
      "Iteration 11, loss = 1.14047256\n",
      "Iteration 12, loss = 1.14973604\n",
      "Iteration 13, loss = 1.12633497\n",
      "Iteration 14, loss = 1.12815954\n",
      "Iteration 15, loss = 1.11316608\n",
      "Iteration 16, loss = 1.12320451\n",
      "Iteration 17, loss = 1.13163920\n",
      "Iteration 18, loss = 1.11779444\n",
      "Iteration 19, loss = 1.10744963\n",
      "Iteration 20, loss = 1.11604715\n",
      "Iteration 21, loss = 1.11268297\n",
      "Iteration 22, loss = 1.12315250\n",
      "Iteration 23, loss = 1.12350604\n",
      "Iteration 24, loss = 1.14979220\n",
      "Iteration 25, loss = 1.13001978\n",
      "Iteration 26, loss = 1.14710947\n",
      "Iteration 27, loss = 1.19108423\n",
      "Iteration 28, loss = 1.15512596\n",
      "Iteration 29, loss = 1.12469176\n",
      "Iteration 30, loss = 1.10382165\n",
      "Iteration 31, loss = 1.10017624\n",
      "Iteration 32, loss = 1.08572319\n",
      "Iteration 33, loss = 1.09024007\n",
      "Iteration 34, loss = 1.08498250\n",
      "Iteration 35, loss = 1.08236313\n",
      "Iteration 36, loss = 1.08676824\n",
      "Iteration 37, loss = 1.09230690\n",
      "Iteration 38, loss = 1.08797282\n",
      "Iteration 39, loss = 1.07942351\n",
      "Iteration 40, loss = 1.08459321\n",
      "Iteration 41, loss = 1.07234069\n",
      "Iteration 42, loss = 1.07468995\n",
      "Iteration 43, loss = 1.07006161\n",
      "Iteration 44, loss = 1.07152100\n",
      "Iteration 45, loss = 1.06533815\n",
      "Iteration 46, loss = 1.06528055\n",
      "Iteration 47, loss = 1.07731697\n",
      "Iteration 48, loss = 1.05854230\n",
      "Iteration 49, loss = 1.07877156\n",
      "Iteration 50, loss = 1.07843100\n",
      "Iteration 51, loss = 1.08413341\n",
      "Iteration 52, loss = 1.06365405\n",
      "Iteration 53, loss = 1.11743016\n",
      "Iteration 54, loss = 1.05890351\n",
      "Iteration 55, loss = 1.05194137\n",
      "Iteration 56, loss = 1.05328989\n",
      "Iteration 57, loss = 1.07881836\n",
      "Iteration 58, loss = 1.04213648\n",
      "Iteration 59, loss = 1.04820502\n",
      "Iteration 60, loss = 1.03043347\n",
      "Iteration 61, loss = 1.03766830\n",
      "Iteration 62, loss = 1.03405793\n",
      "Iteration 63, loss = 1.05894543\n",
      "Iteration 64, loss = 1.08397662\n",
      "Iteration 65, loss = 1.05893938\n",
      "Iteration 66, loss = 1.07097844\n",
      "Iteration 67, loss = 1.11035513\n",
      "Iteration 68, loss = 1.07222786\n",
      "Iteration 69, loss = 1.08214098\n",
      "Iteration 70, loss = 1.05238278\n",
      "Iteration 71, loss = 1.05804861\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36755632\n",
      "Iteration 2, loss = 1.27557858\n",
      "Iteration 3, loss = 1.28430298\n",
      "Iteration 4, loss = 1.24551681\n",
      "Iteration 5, loss = 1.23724015\n",
      "Iteration 6, loss = 1.21455158\n",
      "Iteration 7, loss = 1.22317130\n",
      "Iteration 8, loss = 1.20100034\n",
      "Iteration 9, loss = 1.20138048\n",
      "Iteration 10, loss = 1.19201264\n",
      "Iteration 11, loss = 1.18600569\n",
      "Iteration 12, loss = 1.18082443\n",
      "Iteration 13, loss = 1.17338097\n",
      "Iteration 14, loss = 1.16709705\n",
      "Iteration 15, loss = 1.16026695\n",
      "Iteration 16, loss = 1.15611661\n",
      "Iteration 17, loss = 1.15036768\n",
      "Iteration 18, loss = 1.14217560\n",
      "Iteration 19, loss = 1.13189114\n",
      "Iteration 20, loss = 1.12976211\n",
      "Iteration 21, loss = 1.12135288\n",
      "Iteration 22, loss = 1.11929819\n",
      "Iteration 23, loss = 1.11039493\n",
      "Iteration 24, loss = 1.10870306\n",
      "Iteration 25, loss = 1.10472351\n",
      "Iteration 26, loss = 1.10234818\n",
      "Iteration 27, loss = 1.10520100\n",
      "Iteration 28, loss = 1.08882844\n",
      "Iteration 29, loss = 1.08628817\n",
      "Iteration 30, loss = 1.08366047\n",
      "Iteration 31, loss = 1.08492940\n",
      "Iteration 32, loss = 1.07831050\n",
      "Iteration 33, loss = 1.07100290\n",
      "Iteration 34, loss = 1.07080188\n",
      "Iteration 35, loss = 1.06715576\n",
      "Iteration 36, loss = 1.06331617\n",
      "Iteration 37, loss = 1.06407105\n",
      "Iteration 38, loss = 1.06035285\n",
      "Iteration 39, loss = 1.05977527\n",
      "Iteration 40, loss = 1.05914706\n",
      "Iteration 41, loss = 1.05927888\n",
      "Iteration 42, loss = 1.06794338\n",
      "Iteration 43, loss = 1.05805697\n",
      "Iteration 44, loss = 1.04869520\n",
      "Iteration 45, loss = 1.05388583\n",
      "Iteration 46, loss = 1.05017882\n",
      "Iteration 47, loss = 1.04830838\n",
      "Iteration 48, loss = 1.04625423\n",
      "Iteration 49, loss = 1.04895781\n",
      "Iteration 50, loss = 1.05228039\n",
      "Iteration 51, loss = 1.05022427\n",
      "Iteration 52, loss = 1.05393070\n",
      "Iteration 53, loss = 1.05649269\n",
      "Iteration 54, loss = 1.05071884\n",
      "Iteration 55, loss = 1.04257864\n",
      "Iteration 56, loss = 1.04864327\n",
      "Iteration 57, loss = 1.04731705\n",
      "Iteration 58, loss = 1.04627792\n",
      "Iteration 59, loss = 1.04270827\n",
      "Iteration 60, loss = 1.03785268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61, loss = 1.04154884\n",
      "Iteration 62, loss = 1.03909204\n",
      "Iteration 63, loss = 1.03540692\n",
      "Iteration 64, loss = 1.03977337\n",
      "Iteration 65, loss = 1.03885630\n",
      "Iteration 66, loss = 1.03539324\n",
      "Iteration 67, loss = 1.04036045\n",
      "Iteration 68, loss = 1.04222833\n",
      "Iteration 69, loss = 1.03635103\n",
      "Iteration 70, loss = 1.03407955\n",
      "Iteration 71, loss = 1.03618171\n",
      "Iteration 72, loss = 1.04045062\n",
      "Iteration 73, loss = 1.04010269\n",
      "Iteration 74, loss = 1.03685039\n",
      "Iteration 75, loss = 1.04281853\n",
      "Iteration 76, loss = 1.04562037\n",
      "Iteration 77, loss = 1.04154702\n",
      "Iteration 78, loss = 1.03290320\n",
      "Iteration 79, loss = 1.03506766\n",
      "Iteration 80, loss = 1.03789953\n",
      "Iteration 81, loss = 1.03871354\n",
      "Iteration 82, loss = 1.03452970\n",
      "Iteration 83, loss = 1.03451357\n",
      "Iteration 84, loss = 1.03564151\n",
      "Iteration 85, loss = 1.03262830\n",
      "Iteration 86, loss = 1.03371061\n",
      "Iteration 87, loss = 1.03451273\n",
      "Iteration 88, loss = 1.03535810\n",
      "Iteration 89, loss = 1.03453050\n",
      "Iteration 90, loss = 1.03117952\n",
      "Iteration 91, loss = 1.03304270\n",
      "Iteration 92, loss = 1.03616336\n",
      "Iteration 93, loss = 1.03907365\n",
      "Iteration 94, loss = 1.03791116\n",
      "Iteration 95, loss = 1.03480428\n",
      "Iteration 96, loss = 1.03367095\n",
      "Iteration 97, loss = 1.03335264\n",
      "Iteration 98, loss = 1.04215512\n",
      "Iteration 99, loss = 1.05190727\n",
      "Iteration 100, loss = 1.04036292\n",
      "Iteration 1, loss = 1.36765017\n",
      "Iteration 2, loss = 1.28953000\n",
      "Iteration 3, loss = 1.28084360\n",
      "Iteration 4, loss = 1.24076028\n",
      "Iteration 5, loss = 1.23337949\n",
      "Iteration 6, loss = 1.21687598\n",
      "Iteration 7, loss = 1.21752818\n",
      "Iteration 8, loss = 1.19854806\n",
      "Iteration 9, loss = 1.19999481\n",
      "Iteration 10, loss = 1.18872795\n",
      "Iteration 11, loss = 1.18296313\n",
      "Iteration 12, loss = 1.17738018\n",
      "Iteration 13, loss = 1.16953877\n",
      "Iteration 14, loss = 1.16097228\n",
      "Iteration 15, loss = 1.15459383\n",
      "Iteration 16, loss = 1.14804470\n",
      "Iteration 17, loss = 1.14325516\n",
      "Iteration 18, loss = 1.13606385\n",
      "Iteration 19, loss = 1.12064902\n",
      "Iteration 20, loss = 1.11988264\n",
      "Iteration 21, loss = 1.11092973\n",
      "Iteration 22, loss = 1.10327291\n",
      "Iteration 23, loss = 1.09751003\n",
      "Iteration 24, loss = 1.09446196\n",
      "Iteration 25, loss = 1.09013359\n",
      "Iteration 26, loss = 1.08199982\n",
      "Iteration 27, loss = 1.08589845\n",
      "Iteration 28, loss = 1.07955859\n",
      "Iteration 29, loss = 1.07219199\n",
      "Iteration 30, loss = 1.06227619\n",
      "Iteration 31, loss = 1.06619902\n",
      "Iteration 32, loss = 1.06393234\n",
      "Iteration 33, loss = 1.06098023\n",
      "Iteration 34, loss = 1.05985706\n",
      "Iteration 35, loss = 1.05368668\n",
      "Iteration 36, loss = 1.05036383\n",
      "Iteration 37, loss = 1.05140392\n",
      "Iteration 38, loss = 1.04754502\n",
      "Iteration 39, loss = 1.04921979\n",
      "Iteration 40, loss = 1.04434703\n",
      "Iteration 41, loss = 1.04554324\n",
      "Iteration 42, loss = 1.04573942\n",
      "Iteration 43, loss = 1.04313117\n",
      "Iteration 44, loss = 1.03731140\n",
      "Iteration 45, loss = 1.04560286\n",
      "Iteration 46, loss = 1.04071331\n",
      "Iteration 47, loss = 1.04091652\n",
      "Iteration 48, loss = 1.03775448\n",
      "Iteration 49, loss = 1.04542162\n",
      "Iteration 50, loss = 1.04101016\n",
      "Iteration 51, loss = 1.03430823\n",
      "Iteration 52, loss = 1.04398869\n",
      "Iteration 53, loss = 1.04692939\n",
      "Iteration 54, loss = 1.04673710\n",
      "Iteration 55, loss = 1.03341439\n",
      "Iteration 56, loss = 1.03376102\n",
      "Iteration 57, loss = 1.03470045\n",
      "Iteration 58, loss = 1.03387791\n",
      "Iteration 59, loss = 1.03420307\n",
      "Iteration 60, loss = 1.03107029\n",
      "Iteration 61, loss = 1.03692891\n",
      "Iteration 62, loss = 1.02977527\n",
      "Iteration 63, loss = 1.03031184\n",
      "Iteration 64, loss = 1.03109358\n",
      "Iteration 65, loss = 1.03282216\n",
      "Iteration 66, loss = 1.03238334\n",
      "Iteration 67, loss = 1.03429121\n",
      "Iteration 68, loss = 1.03738165\n",
      "Iteration 69, loss = 1.03471978\n",
      "Iteration 70, loss = 1.03145646\n",
      "Iteration 71, loss = 1.03025090\n",
      "Iteration 72, loss = 1.03301118\n",
      "Iteration 73, loss = 1.02980284\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36185703\n",
      "Iteration 2, loss = 1.29085656\n",
      "Iteration 3, loss = 1.26134329\n",
      "Iteration 4, loss = 1.24132146\n",
      "Iteration 5, loss = 1.22435984\n",
      "Iteration 6, loss = 1.21485424\n",
      "Iteration 7, loss = 1.20589559\n",
      "Iteration 8, loss = 1.19778447\n",
      "Iteration 9, loss = 1.19349750\n",
      "Iteration 10, loss = 1.17869933\n",
      "Iteration 11, loss = 1.17210622\n",
      "Iteration 12, loss = 1.17041642\n",
      "Iteration 13, loss = 1.16354813\n",
      "Iteration 14, loss = 1.14859041\n",
      "Iteration 15, loss = 1.14303054\n",
      "Iteration 16, loss = 1.13740797\n",
      "Iteration 17, loss = 1.12766775\n",
      "Iteration 18, loss = 1.12095793\n",
      "Iteration 19, loss = 1.11320509\n",
      "Iteration 20, loss = 1.11016235\n",
      "Iteration 21, loss = 1.10352445\n",
      "Iteration 22, loss = 1.09766073\n",
      "Iteration 23, loss = 1.09443480\n",
      "Iteration 24, loss = 1.09272740\n",
      "Iteration 25, loss = 1.08530133\n",
      "Iteration 26, loss = 1.08207123\n",
      "Iteration 27, loss = 1.08797066\n",
      "Iteration 28, loss = 1.07562682\n",
      "Iteration 29, loss = 1.07188089\n",
      "Iteration 30, loss = 1.06637924\n",
      "Iteration 31, loss = 1.06823019\n",
      "Iteration 32, loss = 1.07144469\n",
      "Iteration 33, loss = 1.06799655\n",
      "Iteration 34, loss = 1.06253720\n",
      "Iteration 35, loss = 1.05665923\n",
      "Iteration 36, loss = 1.05859195\n",
      "Iteration 37, loss = 1.05794570\n",
      "Iteration 38, loss = 1.05470105\n",
      "Iteration 39, loss = 1.05004691\n",
      "Iteration 40, loss = 1.04714879\n",
      "Iteration 41, loss = 1.04760151\n",
      "Iteration 42, loss = 1.05238953\n",
      "Iteration 43, loss = 1.04871189\n",
      "Iteration 44, loss = 1.04679604\n",
      "Iteration 45, loss = 1.04773411\n",
      "Iteration 46, loss = 1.04596696\n",
      "Iteration 47, loss = 1.04139684\n",
      "Iteration 48, loss = 1.04139189\n",
      "Iteration 49, loss = 1.05100372\n",
      "Iteration 50, loss = 1.04707485\n",
      "Iteration 51, loss = 1.04228645\n",
      "Iteration 52, loss = 1.04497501\n",
      "Iteration 53, loss = 1.04167701\n",
      "Iteration 54, loss = 1.03434283\n",
      "Iteration 55, loss = 1.03606914\n",
      "Iteration 56, loss = 1.03601725\n",
      "Iteration 57, loss = 1.03489776\n",
      "Iteration 58, loss = 1.03671654\n",
      "Iteration 59, loss = 1.03644364\n",
      "Iteration 60, loss = 1.03942728\n",
      "Iteration 61, loss = 1.04230063\n",
      "Iteration 62, loss = 1.04246793\n",
      "Iteration 63, loss = 1.04364949\n",
      "Iteration 64, loss = 1.05260843\n",
      "Iteration 65, loss = 1.03813400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.34178337\n",
      "Iteration 2, loss = 1.29984577\n",
      "Iteration 3, loss = 1.27649271\n",
      "Iteration 4, loss = 1.24369856\n",
      "Iteration 5, loss = 1.22898439\n",
      "Iteration 6, loss = 1.21059552\n",
      "Iteration 7, loss = 1.21861748\n",
      "Iteration 8, loss = 1.20032597\n",
      "Iteration 9, loss = 1.19668916\n",
      "Iteration 10, loss = 1.19293053\n",
      "Iteration 11, loss = 1.18550048\n",
      "Iteration 12, loss = 1.18299280\n",
      "Iteration 13, loss = 1.17988362\n",
      "Iteration 14, loss = 1.16879304\n",
      "Iteration 15, loss = 1.16414598\n",
      "Iteration 16, loss = 1.15525166\n",
      "Iteration 17, loss = 1.14840795\n",
      "Iteration 18, loss = 1.14080723\n",
      "Iteration 19, loss = 1.13595265\n",
      "Iteration 20, loss = 1.12931528\n",
      "Iteration 21, loss = 1.12170923\n",
      "Iteration 22, loss = 1.11950938\n",
      "Iteration 23, loss = 1.12164762\n",
      "Iteration 24, loss = 1.11895548\n",
      "Iteration 25, loss = 1.10705751\n",
      "Iteration 26, loss = 1.10031632\n",
      "Iteration 27, loss = 1.09537758\n",
      "Iteration 28, loss = 1.09501532\n",
      "Iteration 29, loss = 1.08773835\n",
      "Iteration 30, loss = 1.08795596\n",
      "Iteration 31, loss = 1.08093021\n",
      "Iteration 32, loss = 1.08800119\n",
      "Iteration 33, loss = 1.07961150\n",
      "Iteration 34, loss = 1.07739464\n",
      "Iteration 35, loss = 1.07429073\n",
      "Iteration 36, loss = 1.07592375\n",
      "Iteration 37, loss = 1.07422220\n",
      "Iteration 38, loss = 1.07046925\n",
      "Iteration 39, loss = 1.07209874\n",
      "Iteration 40, loss = 1.07139827\n",
      "Iteration 41, loss = 1.06372390\n",
      "Iteration 42, loss = 1.06788674\n",
      "Iteration 43, loss = 1.06825069\n",
      "Iteration 44, loss = 1.06081454\n",
      "Iteration 45, loss = 1.06520226\n",
      "Iteration 46, loss = 1.07217273\n",
      "Iteration 47, loss = 1.06327146\n",
      "Iteration 48, loss = 1.05827908\n",
      "Iteration 49, loss = 1.05840101\n",
      "Iteration 50, loss = 1.06105688\n",
      "Iteration 51, loss = 1.05781417\n",
      "Iteration 52, loss = 1.05771397\n",
      "Iteration 53, loss = 1.05524723\n",
      "Iteration 54, loss = 1.05506083\n",
      "Iteration 55, loss = 1.05731268\n",
      "Iteration 56, loss = 1.05257737\n",
      "Iteration 57, loss = 1.05195032\n",
      "Iteration 58, loss = 1.05372234\n",
      "Iteration 59, loss = 1.06050261\n",
      "Iteration 60, loss = 1.05638861\n",
      "Iteration 61, loss = 1.06172032\n",
      "Iteration 62, loss = 1.05105711\n",
      "Iteration 63, loss = 1.06202257\n",
      "Iteration 64, loss = 1.05934993\n",
      "Iteration 65, loss = 1.04735106\n",
      "Iteration 66, loss = 1.05205118\n",
      "Iteration 67, loss = 1.04899794\n",
      "Iteration 68, loss = 1.04793789\n",
      "Iteration 69, loss = 1.05761783\n",
      "Iteration 70, loss = 1.05341640\n",
      "Iteration 71, loss = 1.04837163\n",
      "Iteration 72, loss = 1.04806240\n",
      "Iteration 73, loss = 1.04619810\n",
      "Iteration 74, loss = 1.04868631\n",
      "Iteration 75, loss = 1.05130167\n",
      "Iteration 76, loss = 1.05057807\n",
      "Iteration 77, loss = 1.05004778\n",
      "Iteration 78, loss = 1.04332220\n",
      "Iteration 79, loss = 1.05148328\n",
      "Iteration 80, loss = 1.04479619\n",
      "Iteration 81, loss = 1.04505628\n",
      "Iteration 82, loss = 1.04678036\n",
      "Iteration 83, loss = 1.04419212\n",
      "Iteration 84, loss = 1.04483617\n",
      "Iteration 85, loss = 1.04450840\n",
      "Iteration 86, loss = 1.04900667\n",
      "Iteration 87, loss = 1.04561621\n",
      "Iteration 88, loss = 1.04543492\n",
      "Iteration 89, loss = 1.04630039\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35889464\n",
      "Iteration 2, loss = 1.29215455\n",
      "Iteration 3, loss = 1.27741477\n",
      "Iteration 4, loss = 1.24414137\n",
      "Iteration 5, loss = 1.23629048\n",
      "Iteration 6, loss = 1.22569319\n",
      "Iteration 7, loss = 1.21957744\n",
      "Iteration 8, loss = 1.19689075\n",
      "Iteration 9, loss = 1.19506039\n",
      "Iteration 10, loss = 1.19059777\n",
      "Iteration 11, loss = 1.18038848\n",
      "Iteration 12, loss = 1.17532092\n",
      "Iteration 13, loss = 1.16232835\n",
      "Iteration 14, loss = 1.16131330\n",
      "Iteration 15, loss = 1.15952659\n",
      "Iteration 16, loss = 1.15441652\n",
      "Iteration 17, loss = 1.13904263\n",
      "Iteration 18, loss = 1.12816584\n",
      "Iteration 19, loss = 1.11908048\n",
      "Iteration 20, loss = 1.10725564\n",
      "Iteration 21, loss = 1.10161057\n",
      "Iteration 22, loss = 1.09577539\n",
      "Iteration 23, loss = 1.08927194\n",
      "Iteration 24, loss = 1.08564334\n",
      "Iteration 25, loss = 1.08239366\n",
      "Iteration 26, loss = 1.07566742\n",
      "Iteration 27, loss = 1.06915391\n",
      "Iteration 28, loss = 1.06726728\n",
      "Iteration 29, loss = 1.06276093\n",
      "Iteration 30, loss = 1.06236914\n",
      "Iteration 31, loss = 1.05728821\n",
      "Iteration 32, loss = 1.04506209\n",
      "Iteration 33, loss = 1.05905300\n",
      "Iteration 34, loss = 1.04018958\n",
      "Iteration 35, loss = 1.04353230\n",
      "Iteration 36, loss = 1.03879468\n",
      "Iteration 37, loss = 1.03479437\n",
      "Iteration 38, loss = 1.03550190\n",
      "Iteration 39, loss = 1.03661613\n",
      "Iteration 40, loss = 1.03222847\n",
      "Iteration 41, loss = 1.02782546\n",
      "Iteration 42, loss = 1.02853064\n",
      "Iteration 43, loss = 1.02514170\n",
      "Iteration 44, loss = 1.02011728\n",
      "Iteration 45, loss = 1.02395006\n",
      "Iteration 46, loss = 1.02469497\n",
      "Iteration 47, loss = 1.02819664\n",
      "Iteration 48, loss = 1.01771782\n",
      "Iteration 49, loss = 1.01830344\n",
      "Iteration 50, loss = 1.01763767\n",
      "Iteration 51, loss = 1.01616516\n",
      "Iteration 52, loss = 1.01655419\n",
      "Iteration 53, loss = 1.01553336\n",
      "Iteration 54, loss = 1.01481923\n",
      "Iteration 55, loss = 1.01366036\n",
      "Iteration 56, loss = 1.01572868\n",
      "Iteration 57, loss = 1.01306523\n",
      "Iteration 58, loss = 1.00996346\n",
      "Iteration 59, loss = 1.01659881\n",
      "Iteration 60, loss = 1.01639748\n",
      "Iteration 61, loss = 1.02442536\n",
      "Iteration 62, loss = 1.01731800\n",
      "Iteration 63, loss = 1.01105363\n",
      "Iteration 64, loss = 1.01055440\n",
      "Iteration 65, loss = 1.00807948\n",
      "Iteration 66, loss = 1.00695770\n",
      "Iteration 67, loss = 1.00970954\n",
      "Iteration 68, loss = 1.00585355\n",
      "Iteration 69, loss = 1.00549991\n",
      "Iteration 70, loss = 1.01066923\n",
      "Iteration 71, loss = 1.00916232\n",
      "Iteration 72, loss = 1.00582465\n",
      "Iteration 73, loss = 1.00855860\n",
      "Iteration 74, loss = 1.00765999\n",
      "Iteration 75, loss = 1.00317218\n",
      "Iteration 76, loss = 1.00544307\n",
      "Iteration 77, loss = 1.00745440\n",
      "Iteration 78, loss = 1.00318003\n",
      "Iteration 79, loss = 1.00219485\n",
      "Iteration 80, loss = 1.00686374\n",
      "Iteration 81, loss = 1.00561683\n",
      "Iteration 82, loss = 1.00357738\n",
      "Iteration 83, loss = 1.00436008\n",
      "Iteration 84, loss = 1.00750341\n",
      "Iteration 85, loss = 1.00692741\n",
      "Iteration 86, loss = 1.00709973\n",
      "Iteration 87, loss = 1.01032221\n",
      "Iteration 88, loss = 1.00924972\n",
      "Iteration 89, loss = 1.00015971\n",
      "Iteration 90, loss = 1.00173017\n",
      "Iteration 91, loss = 1.00701508\n",
      "Iteration 92, loss = 1.00994146\n",
      "Iteration 93, loss = 1.01917194\n",
      "Iteration 94, loss = 1.01387510\n",
      "Iteration 95, loss = 1.00275096\n",
      "Iteration 96, loss = 1.01233704\n",
      "Iteration 97, loss = 1.00347195\n",
      "Iteration 98, loss = 1.00559229\n",
      "Iteration 99, loss = 1.00513650\n",
      "Iteration 100, loss = 1.00389859\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35472222\n",
      "Iteration 2, loss = 1.24224679\n",
      "Iteration 3, loss = 1.22463499\n",
      "Iteration 4, loss = 1.21755960\n",
      "Iteration 5, loss = 1.20023739\n",
      "Iteration 6, loss = 1.19288762\n",
      "Iteration 7, loss = 1.17317853\n",
      "Iteration 8, loss = 1.18081133\n",
      "Iteration 9, loss = 1.16118921\n",
      "Iteration 10, loss = 1.15232540\n",
      "Iteration 11, loss = 1.13999091\n",
      "Iteration 12, loss = 1.14087035\n",
      "Iteration 13, loss = 1.12210817\n",
      "Iteration 14, loss = 1.12076696\n",
      "Iteration 15, loss = 1.11700803\n",
      "Iteration 16, loss = 1.11380739\n",
      "Iteration 17, loss = 1.12770493\n",
      "Iteration 18, loss = 1.10843738\n",
      "Iteration 19, loss = 1.09772465\n",
      "Iteration 20, loss = 1.10327487\n",
      "Iteration 21, loss = 1.09552412\n",
      "Iteration 22, loss = 1.09091733\n",
      "Iteration 23, loss = 1.09811706\n",
      "Iteration 24, loss = 1.09883468\n",
      "Iteration 25, loss = 1.08863479\n",
      "Iteration 26, loss = 1.10216165\n",
      "Iteration 27, loss = 1.10687242\n",
      "Iteration 28, loss = 1.10387571\n",
      "Iteration 29, loss = 1.08591815\n",
      "Iteration 30, loss = 1.08054031\n",
      "Iteration 31, loss = 1.08547109\n",
      "Iteration 32, loss = 1.07344551\n",
      "Iteration 33, loss = 1.06184442\n",
      "Iteration 34, loss = 1.06771341\n",
      "Iteration 35, loss = 1.07419279\n",
      "Iteration 36, loss = 1.07872543\n",
      "Iteration 37, loss = 1.07411125\n",
      "Iteration 38, loss = 1.06868892\n",
      "Iteration 39, loss = 1.05513845\n",
      "Iteration 40, loss = 1.05412419\n",
      "Iteration 41, loss = 1.04716003\n",
      "Iteration 42, loss = 1.04914961\n",
      "Iteration 43, loss = 1.05607310\n",
      "Iteration 44, loss = 1.05322570\n",
      "Iteration 45, loss = 1.04004998\n",
      "Iteration 46, loss = 1.03713394\n",
      "Iteration 47, loss = 1.04225313\n",
      "Iteration 48, loss = 1.03204420\n",
      "Iteration 49, loss = 1.03158377\n",
      "Iteration 50, loss = 1.03536166\n",
      "Iteration 51, loss = 1.05051648\n",
      "Iteration 52, loss = 1.03972287\n",
      "Iteration 53, loss = 1.05186742\n",
      "Iteration 54, loss = 1.03368904\n",
      "Iteration 55, loss = 1.02962778\n",
      "Iteration 56, loss = 1.03696656\n",
      "Iteration 57, loss = 1.05304176\n",
      "Iteration 58, loss = 1.03790138\n",
      "Iteration 59, loss = 1.03677274\n",
      "Iteration 60, loss = 1.01784214\n",
      "Iteration 61, loss = 1.03042872\n",
      "Iteration 62, loss = 1.01799717\n",
      "Iteration 63, loss = 1.03027534\n",
      "Iteration 64, loss = 1.05802237\n",
      "Iteration 65, loss = 1.03916881\n",
      "Iteration 66, loss = 1.04207004\n",
      "Iteration 67, loss = 1.03447555\n",
      "Iteration 68, loss = 1.02125640\n",
      "Iteration 69, loss = 1.02586329\n",
      "Iteration 70, loss = 1.01379093\n",
      "Iteration 71, loss = 1.03495279\n",
      "Iteration 72, loss = 1.03247735\n",
      "Iteration 73, loss = 1.00947059\n",
      "Iteration 74, loss = 1.02424349\n",
      "Iteration 75, loss = 1.02726555\n",
      "Iteration 76, loss = 1.03267373\n",
      "Iteration 77, loss = 1.00779812\n",
      "Iteration 78, loss = 1.00134394\n",
      "Iteration 79, loss = 1.01137254\n",
      "Iteration 80, loss = 1.01614566\n",
      "Iteration 81, loss = 1.01691546\n",
      "Iteration 82, loss = 1.03594367\n",
      "Iteration 83, loss = 1.00021672\n",
      "Iteration 84, loss = 1.01686663\n",
      "Iteration 85, loss = 1.00699567\n",
      "Iteration 86, loss = 1.00905821\n",
      "Iteration 87, loss = 1.00676111\n",
      "Iteration 88, loss = 1.01031834\n",
      "Iteration 89, loss = 1.02063380\n",
      "Iteration 90, loss = 1.01933954\n",
      "Iteration 91, loss = 1.03184712\n",
      "Iteration 92, loss = 1.01737660\n",
      "Iteration 93, loss = 1.02618711\n",
      "Iteration 94, loss = 1.00504503\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32046409\n",
      "Iteration 2, loss = 1.19601043\n",
      "Iteration 3, loss = 1.18075948\n",
      "Iteration 4, loss = 1.14115382\n",
      "Iteration 5, loss = 1.11527467\n",
      "Iteration 6, loss = 1.10046674\n",
      "Iteration 7, loss = 1.08501704\n",
      "Iteration 8, loss = 1.07672667\n",
      "Iteration 9, loss = 1.06553382\n",
      "Iteration 10, loss = 1.06306807\n",
      "Iteration 11, loss = 1.04715553\n",
      "Iteration 12, loss = 1.05259482\n",
      "Iteration 13, loss = 1.04595038\n",
      "Iteration 14, loss = 1.04413236\n",
      "Iteration 15, loss = 1.04809667\n",
      "Iteration 16, loss = 1.04224028\n",
      "Iteration 17, loss = 1.03887273\n",
      "Iteration 18, loss = 1.03571209\n",
      "Iteration 19, loss = 1.03399294\n",
      "Iteration 20, loss = 1.02862319\n",
      "Iteration 21, loss = 1.03352949\n",
      "Iteration 22, loss = 1.02931275\n",
      "Iteration 23, loss = 1.02896539\n",
      "Iteration 24, loss = 1.03053524\n",
      "Iteration 25, loss = 1.02608012\n",
      "Iteration 26, loss = 1.02561977\n",
      "Iteration 27, loss = 1.03094808\n",
      "Iteration 28, loss = 1.03041848\n",
      "Iteration 29, loss = 1.02181460\n",
      "Iteration 30, loss = 1.02826378\n",
      "Iteration 31, loss = 1.02515847\n",
      "Iteration 32, loss = 1.02090199\n",
      "Iteration 33, loss = 1.02150413\n",
      "Iteration 34, loss = 1.02183071\n",
      "Iteration 35, loss = 1.02304050\n",
      "Iteration 36, loss = 1.02067575\n",
      "Iteration 37, loss = 1.02675108\n",
      "Iteration 38, loss = 1.01893294\n",
      "Iteration 39, loss = 1.01923786\n",
      "Iteration 40, loss = 1.02306934\n",
      "Iteration 41, loss = 1.02593454\n",
      "Iteration 42, loss = 1.02727747\n",
      "Iteration 43, loss = 1.01960730\n",
      "Iteration 44, loss = 1.02163918\n",
      "Iteration 45, loss = 1.02143044\n",
      "Iteration 46, loss = 1.01863757\n",
      "Iteration 47, loss = 1.02181681\n",
      "Iteration 48, loss = 1.02060072\n",
      "Iteration 49, loss = 1.02031348\n",
      "Iteration 50, loss = 1.02299749\n",
      "Iteration 51, loss = 1.02743456\n",
      "Iteration 52, loss = 1.02803101\n",
      "Iteration 53, loss = 1.02007867\n",
      "Iteration 54, loss = 1.01933990\n",
      "Iteration 55, loss = 1.02255931\n",
      "Iteration 56, loss = 1.02165002\n",
      "Iteration 57, loss = 1.01922692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.31889946\n",
      "Iteration 2, loss = 1.19224011\n",
      "Iteration 3, loss = 1.17367104\n",
      "Iteration 4, loss = 1.12346525\n",
      "Iteration 5, loss = 1.09705254\n",
      "Iteration 6, loss = 1.08347220\n",
      "Iteration 7, loss = 1.06685545\n",
      "Iteration 8, loss = 1.05942241\n",
      "Iteration 9, loss = 1.05519931\n",
      "Iteration 10, loss = 1.04445046\n",
      "Iteration 11, loss = 1.03979708\n",
      "Iteration 12, loss = 1.04229200\n",
      "Iteration 13, loss = 1.03695865\n",
      "Iteration 14, loss = 1.02831247\n",
      "Iteration 15, loss = 1.03234876\n",
      "Iteration 16, loss = 1.02917046\n",
      "Iteration 17, loss = 1.02859472\n",
      "Iteration 18, loss = 1.02519443\n",
      "Iteration 19, loss = 1.01697857\n",
      "Iteration 20, loss = 1.01929239\n",
      "Iteration 21, loss = 1.01596496\n",
      "Iteration 22, loss = 1.01653475\n",
      "Iteration 23, loss = 1.01421698\n",
      "Iteration 24, loss = 1.01770693\n",
      "Iteration 25, loss = 1.01452849\n",
      "Iteration 26, loss = 1.01336322\n",
      "Iteration 27, loss = 1.02182592\n",
      "Iteration 28, loss = 1.02709346\n",
      "Iteration 29, loss = 1.01269406\n",
      "Iteration 30, loss = 1.01370510\n",
      "Iteration 31, loss = 1.01595398\n",
      "Iteration 32, loss = 1.01421754\n",
      "Iteration 33, loss = 1.01392603\n",
      "Iteration 34, loss = 1.01108671\n",
      "Iteration 35, loss = 1.00981990\n",
      "Iteration 36, loss = 1.01139666\n",
      "Iteration 37, loss = 1.01379960\n",
      "Iteration 38, loss = 1.01032872\n",
      "Iteration 39, loss = 1.01288104\n",
      "Iteration 40, loss = 1.01028050\n",
      "Iteration 41, loss = 1.01542935\n",
      "Iteration 42, loss = 1.01341958\n",
      "Iteration 43, loss = 1.01148182\n",
      "Iteration 44, loss = 1.01399352\n",
      "Iteration 45, loss = 1.01347226\n",
      "Iteration 46, loss = 1.01039756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.31720747\n",
      "Iteration 2, loss = 1.19526906\n",
      "Iteration 3, loss = 1.17553211\n",
      "Iteration 4, loss = 1.12449458\n",
      "Iteration 5, loss = 1.09854892\n",
      "Iteration 6, loss = 1.08836818\n",
      "Iteration 7, loss = 1.07578853\n",
      "Iteration 8, loss = 1.06173268\n",
      "Iteration 9, loss = 1.05703480\n",
      "Iteration 10, loss = 1.04786963\n",
      "Iteration 11, loss = 1.04456255\n",
      "Iteration 12, loss = 1.04554079\n",
      "Iteration 13, loss = 1.04021833\n",
      "Iteration 14, loss = 1.03150133\n",
      "Iteration 15, loss = 1.03013683\n",
      "Iteration 16, loss = 1.03117089\n",
      "Iteration 17, loss = 1.02596631\n",
      "Iteration 18, loss = 1.02688587\n",
      "Iteration 19, loss = 1.02295867\n",
      "Iteration 20, loss = 1.02888841\n",
      "Iteration 21, loss = 1.02719938\n",
      "Iteration 22, loss = 1.02146844\n",
      "Iteration 23, loss = 1.02040080\n",
      "Iteration 24, loss = 1.02856635\n",
      "Iteration 25, loss = 1.02623331\n",
      "Iteration 26, loss = 1.02461900\n",
      "Iteration 27, loss = 1.03724020\n",
      "Iteration 28, loss = 1.01688665\n",
      "Iteration 29, loss = 1.02759933\n",
      "Iteration 30, loss = 1.02811185\n",
      "Iteration 31, loss = 1.02612372\n",
      "Iteration 32, loss = 1.03096169\n",
      "Iteration 33, loss = 1.02448506\n",
      "Iteration 34, loss = 1.02003583\n",
      "Iteration 35, loss = 1.02293568\n",
      "Iteration 36, loss = 1.01886772\n",
      "Iteration 37, loss = 1.01760096\n",
      "Iteration 38, loss = 1.01775222\n",
      "Iteration 39, loss = 1.01688374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.30520875\n",
      "Iteration 2, loss = 1.20943841\n",
      "Iteration 3, loss = 1.18797665\n",
      "Iteration 4, loss = 1.12767546\n",
      "Iteration 5, loss = 1.10994080\n",
      "Iteration 6, loss = 1.10085226\n",
      "Iteration 7, loss = 1.09031099\n",
      "Iteration 8, loss = 1.07662538\n",
      "Iteration 9, loss = 1.07079987\n",
      "Iteration 10, loss = 1.06408041\n",
      "Iteration 11, loss = 1.06123029\n",
      "Iteration 12, loss = 1.06277858\n",
      "Iteration 13, loss = 1.06052582\n",
      "Iteration 14, loss = 1.06029243\n",
      "Iteration 15, loss = 1.06301793\n",
      "Iteration 16, loss = 1.05196442\n",
      "Iteration 17, loss = 1.05293732\n",
      "Iteration 18, loss = 1.04836772\n",
      "Iteration 19, loss = 1.05011561\n",
      "Iteration 20, loss = 1.04504061\n",
      "Iteration 21, loss = 1.04240648\n",
      "Iteration 22, loss = 1.04894086\n",
      "Iteration 23, loss = 1.04652212\n",
      "Iteration 24, loss = 1.03991870\n",
      "Iteration 25, loss = 1.03768675\n",
      "Iteration 26, loss = 1.03368603\n",
      "Iteration 27, loss = 1.03613938\n",
      "Iteration 28, loss = 1.03395013\n",
      "Iteration 29, loss = 1.03387338\n",
      "Iteration 30, loss = 1.03648827\n",
      "Iteration 31, loss = 1.03358592\n",
      "Iteration 32, loss = 1.04237947\n",
      "Iteration 33, loss = 1.03277522\n",
      "Iteration 34, loss = 1.03469123\n",
      "Iteration 35, loss = 1.03327721\n",
      "Iteration 36, loss = 1.03728074\n",
      "Iteration 37, loss = 1.03146583\n",
      "Iteration 38, loss = 1.03291134\n",
      "Iteration 39, loss = 1.03522984\n",
      "Iteration 40, loss = 1.03904644\n",
      "Iteration 41, loss = 1.03150511\n",
      "Iteration 42, loss = 1.03226175\n",
      "Iteration 43, loss = 1.03873820\n",
      "Iteration 44, loss = 1.03187452\n",
      "Iteration 45, loss = 1.03540292\n",
      "Iteration 46, loss = 1.03854699\n",
      "Iteration 47, loss = 1.03008585\n",
      "Iteration 48, loss = 1.03210318\n",
      "Iteration 49, loss = 1.03688063\n",
      "Iteration 50, loss = 1.03312365\n",
      "Iteration 51, loss = 1.03410198\n",
      "Iteration 52, loss = 1.03420676\n",
      "Iteration 53, loss = 1.03290211\n",
      "Iteration 54, loss = 1.03206249\n",
      "Iteration 55, loss = 1.03614141\n",
      "Iteration 56, loss = 1.03285052\n",
      "Iteration 57, loss = 1.03179166\n",
      "Iteration 58, loss = 1.03024107\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.31130160\n",
      "Iteration 2, loss = 1.18902933\n",
      "Iteration 3, loss = 1.16725871\n",
      "Iteration 4, loss = 1.11520108\n",
      "Iteration 5, loss = 1.08659607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 1.08376205\n",
      "Iteration 7, loss = 1.06635677\n",
      "Iteration 8, loss = 1.06063306\n",
      "Iteration 9, loss = 1.04607456\n",
      "Iteration 10, loss = 1.02681159\n",
      "Iteration 11, loss = 1.03065087\n",
      "Iteration 12, loss = 1.02570560\n",
      "Iteration 13, loss = 1.01675376\n",
      "Iteration 14, loss = 1.01767592\n",
      "Iteration 15, loss = 1.01613135\n",
      "Iteration 16, loss = 1.01457451\n",
      "Iteration 17, loss = 1.00999254\n",
      "Iteration 18, loss = 1.00049927\n",
      "Iteration 19, loss = 1.00052582\n",
      "Iteration 20, loss = 1.00108360\n",
      "Iteration 21, loss = 0.99840802\n",
      "Iteration 22, loss = 0.99937094\n",
      "Iteration 23, loss = 0.99840138\n",
      "Iteration 24, loss = 0.99857878\n",
      "Iteration 25, loss = 0.99879562\n",
      "Iteration 26, loss = 1.00094445\n",
      "Iteration 27, loss = 0.99339789\n",
      "Iteration 28, loss = 0.99455999\n",
      "Iteration 29, loss = 0.99610253\n",
      "Iteration 30, loss = 0.99890406\n",
      "Iteration 31, loss = 0.99517268\n",
      "Iteration 32, loss = 0.99177154\n",
      "Iteration 33, loss = 1.00605258\n",
      "Iteration 34, loss = 0.98976806\n",
      "Iteration 35, loss = 0.99335717\n",
      "Iteration 36, loss = 0.99253932\n",
      "Iteration 37, loss = 0.99435951\n",
      "Iteration 38, loss = 0.99627725\n",
      "Iteration 39, loss = 0.99521854\n",
      "Iteration 40, loss = 0.99278195\n",
      "Iteration 41, loss = 0.99330239\n",
      "Iteration 42, loss = 0.99112005\n",
      "Iteration 43, loss = 0.98913624\n",
      "Iteration 44, loss = 0.98852553\n",
      "Iteration 45, loss = 0.99002762\n",
      "Iteration 46, loss = 0.99140240\n",
      "Iteration 47, loss = 0.99178364\n",
      "Iteration 48, loss = 0.99005048\n",
      "Iteration 49, loss = 0.99139867\n",
      "Iteration 50, loss = 0.99209035\n",
      "Iteration 51, loss = 0.98862068\n",
      "Iteration 52, loss = 0.99508010\n",
      "Iteration 53, loss = 0.99251888\n",
      "Iteration 54, loss = 0.99218296\n",
      "Iteration 55, loss = 0.99171657\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.78510933\n",
      "Iteration 2, loss = 11.62037128\n",
      "Iteration 3, loss = 10.63758943\n",
      "Iteration 4, loss = 8.06002554\n",
      "Iteration 5, loss = 5.54235889\n",
      "Iteration 6, loss = 5.13903269\n",
      "Iteration 7, loss = 3.78411244\n",
      "Iteration 8, loss = 2.81118983\n",
      "Iteration 9, loss = 2.16087593\n",
      "Iteration 10, loss = 2.29191393\n",
      "Iteration 11, loss = 2.29808423\n",
      "Iteration 12, loss = 1.67283601\n",
      "Iteration 13, loss = 1.87387803\n",
      "Iteration 14, loss = 1.75752200\n",
      "Iteration 15, loss = 1.46052351\n",
      "Iteration 16, loss = 1.48891684\n",
      "Iteration 17, loss = 1.40177376\n",
      "Iteration 18, loss = 1.64505675\n",
      "Iteration 19, loss = 1.47465544\n",
      "Iteration 20, loss = 1.46441326\n",
      "Iteration 21, loss = 1.50364954\n",
      "Iteration 22, loss = 1.54274595\n",
      "Iteration 23, loss = 1.47817706\n",
      "Iteration 24, loss = 1.39625316\n",
      "Iteration 25, loss = 1.30108159\n",
      "Iteration 26, loss = 1.28821623\n",
      "Iteration 27, loss = 1.33657598\n",
      "Iteration 28, loss = 1.30112583\n",
      "Iteration 29, loss = 1.36496799\n",
      "Iteration 30, loss = 1.40903319\n",
      "Iteration 31, loss = 1.39231874\n",
      "Iteration 32, loss = 1.57426457\n",
      "Iteration 33, loss = 1.74845866\n",
      "Iteration 34, loss = 2.59405811\n",
      "Iteration 35, loss = 3.22615112\n",
      "Iteration 36, loss = 3.01145925\n",
      "Iteration 37, loss = 3.32457050\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#looping over different type of activator with num_mneurons = 128\n",
    "\n",
    "num_neurons = 128 \n",
    "activators = ['relu', 'tanh', 'logistic', 'identity']\n",
    "\n",
    "#variables to comapare results\n",
    "cv_mean_act = []\n",
    "cv_std_act = []\n",
    "test_scores_act = []\n",
    "\n",
    "for act_type in activators:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(num_neurons,), # hidden layer with num_neurons number of neurons\n",
    "                        activation = act_type,  # taking different activator types as input\n",
    "                        # solver='sgd',  # default is Adam\n",
    "                        alpha=0.01,  # regularization parameter, default is 0.0001 (increase up to 1.0 for stronger regularization)\n",
    "                        learning_rate_init=.01 ,  # initial step-size for updating the weights, default is 0.001\n",
    "                        max_iter=100,  # number of epochs, default=200\n",
    "                        random_state=seed,\n",
    "                        verbose=1, \n",
    "                        )\n",
    "    scores = cross_val_score(mlp, X_train_scaled, y_train, cv=5, scoring=scoring_method)\n",
    "    cv_mean_act.append(scores.mean())\n",
    "    cv_std_act.append(scores.std())\n",
    "    \n",
    "\n",
    "\n",
    "    #train the classifier and tesing it\n",
    "    mlp.fit(X_train, y_train)\n",
    "    test_acc = accuracy_score(y_test, mlp.predict(X_test_scaled))\n",
    "    test_scores_act.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56f56a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best activation function: 'relu' with accuracy = 0.569\n"
     ]
    }
   ],
   "source": [
    "# -------- PLOT RESULTS --------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(activators, cv_mean_act, yerr=cv_std_act, color=['C0', 'C1', 'C2', 'C3'], capsize=5)\n",
    "plt.xlabel(\"Activation Function\", fontsize=16)\n",
    "plt.ylabel(\"Cross-Validation Accuracy\", fontsize=16)\n",
    "#plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best activation\n",
    "best_idx = np.argmax(cv_mean_act)\n",
    "print(f\"\\n Best activation function: '{activators[best_idx]}' with accuracy = {cv_mean_act[best_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88caeeae",
   "metadata": {},
   "source": [
    "# 4. Pick the best performing model you found in part 3. Use cross-validation on the same training set to study the impact on the modelâ€™s performance when varying two different hyperparameters for the optimizer (e.g., epoch, batch size, learning rate). Report and discuss whether they make a significant difference for your particular model choice, and how (e.g., speed, score).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4e0bcc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.51795934\n",
      "Iteration 2, loss = 1.51705042\n",
      "Iteration 3, loss = 1.51563190\n",
      "Iteration 4, loss = 1.51395390\n",
      "Iteration 5, loss = 1.51204054\n",
      "Iteration 6, loss = 1.51006090\n",
      "Iteration 7, loss = 1.50791726\n",
      "Iteration 8, loss = 1.50585063\n",
      "Iteration 9, loss = 1.50365860\n",
      "Iteration 10, loss = 1.50155956\n",
      "Iteration 11, loss = 1.49938411\n",
      "Iteration 12, loss = 1.49722701\n",
      "Iteration 13, loss = 1.49505249\n",
      "Iteration 14, loss = 1.49291987\n",
      "Iteration 15, loss = 1.49084982\n",
      "Iteration 16, loss = 1.48873433\n",
      "Iteration 17, loss = 1.48666366\n",
      "Iteration 18, loss = 1.48461941\n",
      "Iteration 19, loss = 1.48255723\n",
      "Iteration 20, loss = 1.48051503\n",
      "Iteration 21, loss = 1.47854494\n",
      "Iteration 22, loss = 1.47651625\n",
      "Iteration 23, loss = 1.47458308\n",
      "Iteration 24, loss = 1.47256681\n",
      "Iteration 25, loss = 1.47068022\n",
      "Iteration 26, loss = 1.46873689\n",
      "Iteration 27, loss = 1.46685642\n",
      "Iteration 28, loss = 1.46494032\n",
      "Iteration 29, loss = 1.46305436\n",
      "Iteration 30, loss = 1.46121989\n",
      "Iteration 31, loss = 1.45932861\n",
      "Iteration 32, loss = 1.45751655\n",
      "Iteration 33, loss = 1.45571420\n",
      "Iteration 34, loss = 1.45390281\n",
      "Iteration 35, loss = 1.45212043\n",
      "Iteration 36, loss = 1.45036721\n",
      "Iteration 37, loss = 1.44859514\n",
      "Iteration 38, loss = 1.44686068\n",
      "Iteration 39, loss = 1.44514744\n",
      "Iteration 40, loss = 1.44340978\n",
      "Iteration 41, loss = 1.44175367\n",
      "Iteration 42, loss = 1.44001564\n",
      "Iteration 43, loss = 1.43838127\n",
      "Iteration 44, loss = 1.43675793\n",
      "Iteration 45, loss = 1.43508843\n",
      "Iteration 46, loss = 1.43345425\n",
      "Iteration 47, loss = 1.43183976\n",
      "Iteration 48, loss = 1.43024126\n",
      "Iteration 49, loss = 1.42866703\n",
      "Iteration 50, loss = 1.42708343\n",
      "Iteration 51, loss = 1.42553796\n",
      "Iteration 52, loss = 1.42401669\n",
      "Iteration 53, loss = 1.42245557\n",
      "Iteration 54, loss = 1.42094127\n",
      "Iteration 55, loss = 1.41943302\n",
      "Iteration 56, loss = 1.41793309\n",
      "Iteration 57, loss = 1.41646394\n",
      "Iteration 58, loss = 1.41499838\n",
      "Iteration 59, loss = 1.41356350\n",
      "Iteration 60, loss = 1.41209482\n",
      "Iteration 61, loss = 1.41062669\n",
      "Iteration 62, loss = 1.40922488\n",
      "Iteration 63, loss = 1.40783932\n",
      "Iteration 64, loss = 1.40643139\n",
      "Iteration 65, loss = 1.40503486\n",
      "Iteration 66, loss = 1.40370262\n",
      "Iteration 67, loss = 1.40234845\n",
      "Iteration 68, loss = 1.40099742\n",
      "Iteration 69, loss = 1.39963462\n",
      "Iteration 70, loss = 1.39829548\n",
      "Iteration 71, loss = 1.39699229\n",
      "Iteration 72, loss = 1.39573202\n",
      "Iteration 73, loss = 1.39439779\n",
      "Iteration 74, loss = 1.39310628\n",
      "Iteration 75, loss = 1.39185441\n",
      "Iteration 76, loss = 1.39058302\n",
      "Iteration 77, loss = 1.38932133\n",
      "Iteration 78, loss = 1.38810664\n",
      "Iteration 79, loss = 1.38686143\n",
      "Iteration 80, loss = 1.38563443\n",
      "Iteration 81, loss = 1.38444965\n",
      "Iteration 82, loss = 1.38323147\n",
      "Iteration 83, loss = 1.38201235\n",
      "Iteration 84, loss = 1.38086811\n",
      "Iteration 85, loss = 1.37970259\n",
      "Iteration 86, loss = 1.37851855\n",
      "Iteration 87, loss = 1.37736990\n",
      "Iteration 88, loss = 1.37624318\n",
      "Iteration 89, loss = 1.37510405\n",
      "Iteration 90, loss = 1.37399168\n",
      "Iteration 91, loss = 1.37289476\n",
      "Iteration 92, loss = 1.37176033\n",
      "Iteration 93, loss = 1.37067069\n",
      "Iteration 94, loss = 1.36961570\n",
      "Iteration 95, loss = 1.36852283\n",
      "Iteration 96, loss = 1.36743471\n",
      "Iteration 97, loss = 1.36636144\n",
      "Iteration 98, loss = 1.36539367\n",
      "Iteration 99, loss = 1.36430354\n",
      "Iteration 100, loss = 1.36325407\n",
      "Iteration 1, loss = 1.51819130\n",
      "Iteration 2, loss = 1.51727358\n",
      "Iteration 3, loss = 1.51587303\n",
      "Iteration 4, loss = 1.51420044\n",
      "Iteration 5, loss = 1.51230362\n",
      "Iteration 6, loss = 1.51030071\n",
      "Iteration 7, loss = 1.50819191\n",
      "Iteration 8, loss = 1.50609928\n",
      "Iteration 9, loss = 1.50394512\n",
      "Iteration 10, loss = 1.50182610\n",
      "Iteration 11, loss = 1.49965659\n",
      "Iteration 12, loss = 1.49752529\n",
      "Iteration 13, loss = 1.49534804\n",
      "Iteration 14, loss = 1.49327593\n",
      "Iteration 15, loss = 1.49114649\n",
      "Iteration 16, loss = 1.48905235\n",
      "Iteration 17, loss = 1.48700325\n",
      "Iteration 18, loss = 1.48493274\n",
      "Iteration 19, loss = 1.48291566\n",
      "Iteration 20, loss = 1.48087592\n",
      "Iteration 21, loss = 1.47889326\n",
      "Iteration 22, loss = 1.47688986\n",
      "Iteration 23, loss = 1.47493448\n",
      "Iteration 24, loss = 1.47295349\n",
      "Iteration 25, loss = 1.47102541\n",
      "Iteration 26, loss = 1.46911937\n",
      "Iteration 27, loss = 1.46725943\n",
      "Iteration 28, loss = 1.46530053\n",
      "Iteration 29, loss = 1.46344730\n",
      "Iteration 30, loss = 1.46161328\n",
      "Iteration 31, loss = 1.45973573\n",
      "Iteration 32, loss = 1.45789718\n",
      "Iteration 33, loss = 1.45609640\n",
      "Iteration 34, loss = 1.45434371\n",
      "Iteration 35, loss = 1.45253964\n",
      "Iteration 36, loss = 1.45077171\n",
      "Iteration 37, loss = 1.44900855\n",
      "Iteration 38, loss = 1.44730896\n",
      "Iteration 39, loss = 1.44556116\n",
      "Iteration 40, loss = 1.44385118\n",
      "Iteration 41, loss = 1.44218454\n",
      "Iteration 42, loss = 1.44046367\n",
      "Iteration 43, loss = 1.43879942\n",
      "Iteration 44, loss = 1.43719807\n",
      "Iteration 45, loss = 1.43554439\n",
      "Iteration 46, loss = 1.43390499\n",
      "Iteration 47, loss = 1.43228597\n",
      "Iteration 48, loss = 1.43070776\n",
      "Iteration 49, loss = 1.42914018\n",
      "Iteration 50, loss = 1.42752924\n",
      "Iteration 51, loss = 1.42598308\n",
      "Iteration 52, loss = 1.42446093\n",
      "Iteration 53, loss = 1.42291117\n",
      "Iteration 54, loss = 1.42140855\n",
      "Iteration 55, loss = 1.41989870\n",
      "Iteration 56, loss = 1.41839660\n",
      "Iteration 57, loss = 1.41693592\n",
      "Iteration 58, loss = 1.41546219\n",
      "Iteration 59, loss = 1.41401256\n",
      "Iteration 60, loss = 1.41256988\n",
      "Iteration 61, loss = 1.41109985\n",
      "Iteration 62, loss = 1.40968512\n",
      "Iteration 63, loss = 1.40830564\n",
      "Iteration 64, loss = 1.40690038\n",
      "Iteration 65, loss = 1.40548069\n",
      "Iteration 66, loss = 1.40415226\n",
      "Iteration 67, loss = 1.40279734\n",
      "Iteration 68, loss = 1.40145779\n",
      "Iteration 69, loss = 1.40006433\n",
      "Iteration 70, loss = 1.39876576\n",
      "Iteration 71, loss = 1.39745390\n",
      "Iteration 72, loss = 1.39615932\n",
      "Iteration 73, loss = 1.39485511\n",
      "Iteration 74, loss = 1.39355865\n",
      "Iteration 75, loss = 1.39229259\n",
      "Iteration 76, loss = 1.39102155\n",
      "Iteration 77, loss = 1.38979339\n",
      "Iteration 78, loss = 1.38857708\n",
      "Iteration 79, loss = 1.38730621\n",
      "Iteration 80, loss = 1.38609531\n",
      "Iteration 81, loss = 1.38489059\n",
      "Iteration 82, loss = 1.38369281\n",
      "Iteration 83, loss = 1.38247403\n",
      "Iteration 84, loss = 1.38132876\n",
      "Iteration 85, loss = 1.38015832\n",
      "Iteration 86, loss = 1.37899404\n",
      "Iteration 87, loss = 1.37784180\n",
      "Iteration 88, loss = 1.37670816\n",
      "Iteration 89, loss = 1.37556786\n",
      "Iteration 90, loss = 1.37444069\n",
      "Iteration 91, loss = 1.37335825\n",
      "Iteration 92, loss = 1.37224059\n",
      "Iteration 93, loss = 1.37115582\n",
      "Iteration 94, loss = 1.37006517\n",
      "Iteration 95, loss = 1.36901861\n",
      "Iteration 96, loss = 1.36790523\n",
      "Iteration 97, loss = 1.36686077\n",
      "Iteration 98, loss = 1.36584592\n",
      "Iteration 99, loss = 1.36479123\n",
      "Iteration 100, loss = 1.36373726\n",
      "Iteration 1, loss = 1.51839966\n",
      "Iteration 2, loss = 1.51747517\n",
      "Iteration 3, loss = 1.51608259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 1.51442186\n",
      "Iteration 5, loss = 1.51247442\n",
      "Iteration 6, loss = 1.51046279\n",
      "Iteration 7, loss = 1.50839139\n",
      "Iteration 8, loss = 1.50626096\n",
      "Iteration 9, loss = 1.50409802\n",
      "Iteration 10, loss = 1.50194993\n",
      "Iteration 11, loss = 1.49979671\n",
      "Iteration 12, loss = 1.49766518\n",
      "Iteration 13, loss = 1.49549599\n",
      "Iteration 14, loss = 1.49341029\n",
      "Iteration 15, loss = 1.49124916\n",
      "Iteration 16, loss = 1.48917105\n",
      "Iteration 17, loss = 1.48710057\n",
      "Iteration 18, loss = 1.48503989\n",
      "Iteration 19, loss = 1.48301407\n",
      "Iteration 20, loss = 1.48095911\n",
      "Iteration 21, loss = 1.47897518\n",
      "Iteration 22, loss = 1.47695910\n",
      "Iteration 23, loss = 1.47498361\n",
      "Iteration 24, loss = 1.47303393\n",
      "Iteration 25, loss = 1.47113600\n",
      "Iteration 26, loss = 1.46918206\n",
      "Iteration 27, loss = 1.46729072\n",
      "Iteration 28, loss = 1.46534281\n",
      "Iteration 29, loss = 1.46350391\n",
      "Iteration 30, loss = 1.46164176\n",
      "Iteration 31, loss = 1.45978543\n",
      "Iteration 32, loss = 1.45793439\n",
      "Iteration 33, loss = 1.45613312\n",
      "Iteration 34, loss = 1.45433746\n",
      "Iteration 35, loss = 1.45257448\n",
      "Iteration 36, loss = 1.45079761\n",
      "Iteration 37, loss = 1.44901622\n",
      "Iteration 38, loss = 1.44730687\n",
      "Iteration 39, loss = 1.44555391\n",
      "Iteration 40, loss = 1.44385195\n",
      "Iteration 41, loss = 1.44216914\n",
      "Iteration 42, loss = 1.44043353\n",
      "Iteration 43, loss = 1.43881153\n",
      "Iteration 44, loss = 1.43715580\n",
      "Iteration 45, loss = 1.43549423\n",
      "Iteration 46, loss = 1.43387165\n",
      "Iteration 47, loss = 1.43227372\n",
      "Iteration 48, loss = 1.43067602\n",
      "Iteration 49, loss = 1.42910244\n",
      "Iteration 50, loss = 1.42746879\n",
      "Iteration 51, loss = 1.42595244\n",
      "Iteration 52, loss = 1.42441580\n",
      "Iteration 53, loss = 1.42286224\n",
      "Iteration 54, loss = 1.42132043\n",
      "Iteration 55, loss = 1.41982726\n",
      "Iteration 56, loss = 1.41835834\n",
      "Iteration 57, loss = 1.41688052\n",
      "Iteration 58, loss = 1.41538244\n",
      "Iteration 59, loss = 1.41395460\n",
      "Iteration 60, loss = 1.41251118\n",
      "Iteration 61, loss = 1.41103963\n",
      "Iteration 62, loss = 1.40963081\n",
      "Iteration 63, loss = 1.40824102\n",
      "Iteration 64, loss = 1.40683220\n",
      "Iteration 65, loss = 1.40541026\n",
      "Iteration 66, loss = 1.40406461\n",
      "Iteration 67, loss = 1.40270575\n",
      "Iteration 68, loss = 1.40137085\n",
      "Iteration 69, loss = 1.39999044\n",
      "Iteration 70, loss = 1.39866642\n",
      "Iteration 71, loss = 1.39737614\n",
      "Iteration 72, loss = 1.39604320\n",
      "Iteration 73, loss = 1.39474016\n",
      "Iteration 74, loss = 1.39346441\n",
      "Iteration 75, loss = 1.39214891\n",
      "Iteration 76, loss = 1.39090465\n",
      "Iteration 77, loss = 1.38965859\n",
      "Iteration 78, loss = 1.38842628\n",
      "Iteration 79, loss = 1.38718478\n",
      "Iteration 80, loss = 1.38595845\n",
      "Iteration 81, loss = 1.38472681\n",
      "Iteration 82, loss = 1.38354390\n",
      "Iteration 83, loss = 1.38232017\n",
      "Iteration 84, loss = 1.38114498\n",
      "Iteration 85, loss = 1.37997364\n",
      "Iteration 86, loss = 1.37881361\n",
      "Iteration 87, loss = 1.37767671\n",
      "Iteration 88, loss = 1.37652284\n",
      "Iteration 89, loss = 1.37536751\n",
      "Iteration 90, loss = 1.37425283\n",
      "Iteration 91, loss = 1.37312607\n",
      "Iteration 92, loss = 1.37202463\n",
      "Iteration 93, loss = 1.37094988\n",
      "Iteration 94, loss = 1.36983766\n",
      "Iteration 95, loss = 1.36878059\n",
      "Iteration 96, loss = 1.36770451\n",
      "Iteration 97, loss = 1.36661669\n",
      "Iteration 98, loss = 1.36559708\n",
      "Iteration 99, loss = 1.36454379\n",
      "Iteration 100, loss = 1.36349695\n",
      "Iteration 1, loss = 1.51851606\n",
      "Iteration 2, loss = 1.51760970\n",
      "Iteration 3, loss = 1.51621518\n",
      "Iteration 4, loss = 1.51454296\n",
      "Iteration 5, loss = 1.51262134\n",
      "Iteration 6, loss = 1.51063123\n",
      "Iteration 7, loss = 1.50857347\n",
      "Iteration 8, loss = 1.50643161\n",
      "Iteration 9, loss = 1.50432164\n",
      "Iteration 10, loss = 1.50215518\n",
      "Iteration 11, loss = 1.50001243\n",
      "Iteration 12, loss = 1.49789324\n",
      "Iteration 13, loss = 1.49575912\n",
      "Iteration 14, loss = 1.49369717\n",
      "Iteration 15, loss = 1.49152397\n",
      "Iteration 16, loss = 1.48947489\n",
      "Iteration 17, loss = 1.48742341\n",
      "Iteration 18, loss = 1.48534480\n",
      "Iteration 19, loss = 1.48337952\n",
      "Iteration 20, loss = 1.48130122\n",
      "Iteration 21, loss = 1.47933459\n",
      "Iteration 22, loss = 1.47738676\n",
      "Iteration 23, loss = 1.47536600\n",
      "Iteration 24, loss = 1.47345225\n",
      "Iteration 25, loss = 1.47152835\n",
      "Iteration 26, loss = 1.46961255\n",
      "Iteration 27, loss = 1.46773043\n",
      "Iteration 28, loss = 1.46583604\n",
      "Iteration 29, loss = 1.46397202\n",
      "Iteration 30, loss = 1.46213209\n",
      "Iteration 31, loss = 1.46028861\n",
      "Iteration 32, loss = 1.45845547\n",
      "Iteration 33, loss = 1.45664042\n",
      "Iteration 34, loss = 1.45490675\n",
      "Iteration 35, loss = 1.45311841\n",
      "Iteration 36, loss = 1.45133199\n",
      "Iteration 37, loss = 1.44959622\n",
      "Iteration 38, loss = 1.44790936\n",
      "Iteration 39, loss = 1.44618575\n",
      "Iteration 40, loss = 1.44449637\n",
      "Iteration 41, loss = 1.44276309\n",
      "Iteration 42, loss = 1.44109989\n",
      "Iteration 43, loss = 1.43946330\n",
      "Iteration 44, loss = 1.43782113\n",
      "Iteration 45, loss = 1.43618150\n",
      "Iteration 46, loss = 1.43458049\n",
      "Iteration 47, loss = 1.43298964\n",
      "Iteration 48, loss = 1.43139755\n",
      "Iteration 49, loss = 1.42984176\n",
      "Iteration 50, loss = 1.42822992\n",
      "Iteration 51, loss = 1.42669801\n",
      "Iteration 52, loss = 1.42518525\n",
      "Iteration 53, loss = 1.42363456\n",
      "Iteration 54, loss = 1.42213618\n",
      "Iteration 55, loss = 1.42063849\n",
      "Iteration 56, loss = 1.41916475\n",
      "Iteration 57, loss = 1.41771340\n",
      "Iteration 58, loss = 1.41622090\n",
      "Iteration 59, loss = 1.41479487\n",
      "Iteration 60, loss = 1.41337000\n",
      "Iteration 61, loss = 1.41190514\n",
      "Iteration 62, loss = 1.41049945\n",
      "Iteration 63, loss = 1.40913149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64, loss = 1.40772255\n",
      "Iteration 65, loss = 1.40634034\n",
      "Iteration 66, loss = 1.40498200\n",
      "Iteration 67, loss = 1.40361958\n",
      "Iteration 68, loss = 1.40229981\n",
      "Iteration 69, loss = 1.40092397\n",
      "Iteration 70, loss = 1.39961955\n",
      "Iteration 71, loss = 1.39835256\n",
      "Iteration 72, loss = 1.39702361\n",
      "Iteration 73, loss = 1.39573759\n",
      "Iteration 74, loss = 1.39444929\n",
      "Iteration 75, loss = 1.39318555\n",
      "Iteration 76, loss = 1.39193293\n",
      "Iteration 77, loss = 1.39070166\n",
      "Iteration 78, loss = 1.38948582\n",
      "Iteration 79, loss = 1.38825005\n",
      "Iteration 80, loss = 1.38704569\n",
      "Iteration 81, loss = 1.38580917\n",
      "Iteration 82, loss = 1.38462906\n",
      "Iteration 83, loss = 1.38343646\n",
      "Iteration 84, loss = 1.38227104\n",
      "Iteration 85, loss = 1.38111167\n",
      "Iteration 86, loss = 1.37997868\n",
      "Iteration 87, loss = 1.37883314\n",
      "Iteration 88, loss = 1.37767462\n",
      "Iteration 89, loss = 1.37652335\n",
      "Iteration 90, loss = 1.37544729\n",
      "Iteration 91, loss = 1.37431851\n",
      "Iteration 92, loss = 1.37324322\n",
      "Iteration 93, loss = 1.37215330\n",
      "Iteration 94, loss = 1.37107560\n",
      "Iteration 95, loss = 1.37002239\n",
      "Iteration 96, loss = 1.36895514\n",
      "Iteration 97, loss = 1.36787428\n",
      "Iteration 98, loss = 1.36685027\n",
      "Iteration 99, loss = 1.36582762\n",
      "Iteration 100, loss = 1.36479679\n",
      "Iteration 1, loss = 1.51733147\n",
      "Iteration 2, loss = 1.51640786\n",
      "Iteration 3, loss = 1.51500472\n",
      "Iteration 4, loss = 1.51329856\n",
      "Iteration 5, loss = 1.51138214\n",
      "Iteration 6, loss = 1.50932341\n",
      "Iteration 7, loss = 1.50724672\n",
      "Iteration 8, loss = 1.50510173\n",
      "Iteration 9, loss = 1.50291713\n",
      "Iteration 10, loss = 1.50072855\n",
      "Iteration 11, loss = 1.49863191\n",
      "Iteration 12, loss = 1.49641030\n",
      "Iteration 13, loss = 1.49427528\n",
      "Iteration 14, loss = 1.49217983\n",
      "Iteration 15, loss = 1.48996964\n",
      "Iteration 16, loss = 1.48789065\n",
      "Iteration 17, loss = 1.48577255\n",
      "Iteration 18, loss = 1.48372250\n",
      "Iteration 19, loss = 1.48164400\n",
      "Iteration 20, loss = 1.47961764\n",
      "Iteration 21, loss = 1.47758380\n",
      "Iteration 22, loss = 1.47558994\n",
      "Iteration 23, loss = 1.47361342\n",
      "Iteration 24, loss = 1.47159766\n",
      "Iteration 25, loss = 1.46965390\n",
      "Iteration 26, loss = 1.46771276\n",
      "Iteration 27, loss = 1.46579489\n",
      "Iteration 28, loss = 1.46385164\n",
      "Iteration 29, loss = 1.46200313\n",
      "Iteration 30, loss = 1.46009750\n",
      "Iteration 31, loss = 1.45827837\n",
      "Iteration 32, loss = 1.45642748\n",
      "Iteration 33, loss = 1.45454835\n",
      "Iteration 34, loss = 1.45270548\n",
      "Iteration 35, loss = 1.45097123\n",
      "Iteration 36, loss = 1.44918581\n",
      "Iteration 37, loss = 1.44739472\n",
      "Iteration 38, loss = 1.44566381\n",
      "Iteration 39, loss = 1.44387130\n",
      "Iteration 40, loss = 1.44218331\n",
      "Iteration 41, loss = 1.44044449\n",
      "Iteration 42, loss = 1.43875995\n",
      "Iteration 43, loss = 1.43707609\n",
      "Iteration 44, loss = 1.43539608\n",
      "Iteration 45, loss = 1.43373958\n",
      "Iteration 46, loss = 1.43208876\n",
      "Iteration 47, loss = 1.43046999\n",
      "Iteration 48, loss = 1.42889030\n",
      "Iteration 49, loss = 1.42724527\n",
      "Iteration 50, loss = 1.42569023\n",
      "Iteration 51, loss = 1.42407889\n",
      "Iteration 52, loss = 1.42251842\n",
      "Iteration 53, loss = 1.42096546\n",
      "Iteration 54, loss = 1.41941319\n",
      "Iteration 55, loss = 1.41791355\n",
      "Iteration 56, loss = 1.41640140\n",
      "Iteration 57, loss = 1.41493414\n",
      "Iteration 58, loss = 1.41342042\n",
      "Iteration 59, loss = 1.41192901\n",
      "Iteration 60, loss = 1.41049686\n",
      "Iteration 61, loss = 1.40901003\n",
      "Iteration 62, loss = 1.40762310\n",
      "Iteration 63, loss = 1.40617118\n",
      "Iteration 64, loss = 1.40477739\n",
      "Iteration 65, loss = 1.40334572\n",
      "Iteration 66, loss = 1.40194946\n",
      "Iteration 67, loss = 1.40061320\n",
      "Iteration 68, loss = 1.39922579\n",
      "Iteration 69, loss = 1.39782814\n",
      "Iteration 70, loss = 1.39655156\n",
      "Iteration 71, loss = 1.39518645\n",
      "Iteration 72, loss = 1.39387312\n",
      "Iteration 73, loss = 1.39256649\n",
      "Iteration 74, loss = 1.39124839\n",
      "Iteration 75, loss = 1.38998437\n",
      "Iteration 76, loss = 1.38871178\n",
      "Iteration 77, loss = 1.38743053\n",
      "Iteration 78, loss = 1.38617293\n",
      "Iteration 79, loss = 1.38492831\n",
      "Iteration 80, loss = 1.38370508\n",
      "Iteration 81, loss = 1.38246804\n",
      "Iteration 82, loss = 1.38128021\n",
      "Iteration 83, loss = 1.38006253\n",
      "Iteration 84, loss = 1.37885558\n",
      "Iteration 85, loss = 1.37765348\n",
      "Iteration 86, loss = 1.37652790\n",
      "Iteration 87, loss = 1.37535891\n",
      "Iteration 88, loss = 1.37417813\n",
      "Iteration 89, loss = 1.37303938\n",
      "Iteration 90, loss = 1.37188406\n",
      "Iteration 91, loss = 1.37080022\n",
      "Iteration 92, loss = 1.36963786\n",
      "Iteration 93, loss = 1.36857212\n",
      "Iteration 94, loss = 1.36744267\n",
      "Iteration 95, loss = 1.36636305\n",
      "Iteration 96, loss = 1.36528622\n",
      "Iteration 97, loss = 1.36422691\n",
      "Iteration 98, loss = 1.36314118\n",
      "Iteration 99, loss = 1.36210794\n",
      "Iteration 100, loss = 1.36105258\n",
      "Iteration 1, loss = 4.43345300\n",
      "Iteration 2, loss = 3.07029574\n",
      "Iteration 3, loss = 2.50808665\n",
      "Iteration 4, loss = 2.03708777\n",
      "Iteration 5, loss = 1.68507293\n",
      "Iteration 6, loss = 1.57597924\n",
      "Iteration 7, loss = 1.51822548\n",
      "Iteration 8, loss = 1.44598983\n",
      "Iteration 9, loss = 1.38503399\n",
      "Iteration 10, loss = 1.35231126\n",
      "Iteration 11, loss = 1.37573043\n",
      "Iteration 12, loss = 1.30694759\n",
      "Iteration 13, loss = 1.35396206\n",
      "Iteration 14, loss = 1.31161156\n",
      "Iteration 15, loss = 1.27676910\n",
      "Iteration 16, loss = 1.29359380\n",
      "Iteration 17, loss = 1.25726497\n",
      "Iteration 18, loss = 1.28365329\n",
      "Iteration 19, loss = 1.25803401\n",
      "Iteration 20, loss = 1.25036002\n",
      "Iteration 21, loss = 1.29774782\n",
      "Iteration 22, loss = 1.25911064\n",
      "Iteration 23, loss = 1.26921091\n",
      "Iteration 24, loss = 1.26514135\n",
      "Iteration 25, loss = 1.25450716\n",
      "Iteration 26, loss = 1.26916925\n",
      "Iteration 27, loss = 1.34516135\n",
      "Iteration 28, loss = 1.25843289\n",
      "Iteration 29, loss = 1.27344830\n",
      "Iteration 30, loss = 1.24343643\n",
      "Iteration 31, loss = 1.26699230\n",
      "Iteration 32, loss = 1.25267699\n",
      "Iteration 33, loss = 1.25526647\n",
      "Iteration 34, loss = 1.22883235\n",
      "Iteration 35, loss = 1.21913500\n",
      "Iteration 36, loss = 1.21472488\n",
      "Iteration 37, loss = 1.22706315\n",
      "Iteration 38, loss = 1.26404473\n",
      "Iteration 39, loss = 1.31419483\n",
      "Iteration 40, loss = 1.22620133\n",
      "Iteration 41, loss = 1.24723230\n",
      "Iteration 42, loss = 1.20274176\n",
      "Iteration 43, loss = 1.20699742\n",
      "Iteration 44, loss = 1.20035913\n",
      "Iteration 45, loss = 1.21347143\n",
      "Iteration 46, loss = 1.20566820\n",
      "Iteration 47, loss = 1.21848343\n",
      "Iteration 48, loss = 1.19731686\n",
      "Iteration 49, loss = 1.28100707\n",
      "Iteration 50, loss = 1.27519888\n",
      "Iteration 51, loss = 1.26266473\n",
      "Iteration 52, loss = 1.27835891\n",
      "Iteration 53, loss = 1.22881630\n",
      "Iteration 54, loss = 1.26607181\n",
      "Iteration 55, loss = 1.19440010\n",
      "Iteration 56, loss = 1.27278405\n",
      "Iteration 57, loss = 1.19212362\n",
      "Iteration 58, loss = 1.20113652\n",
      "Iteration 59, loss = 1.18287510\n",
      "Iteration 60, loss = 1.18706274\n",
      "Iteration 61, loss = 1.19357985\n",
      "Iteration 62, loss = 1.22880394\n",
      "Iteration 63, loss = 1.23260703\n",
      "Iteration 64, loss = 1.22011158\n",
      "Iteration 65, loss = 1.18406217\n",
      "Iteration 66, loss = 1.21498076\n",
      "Iteration 67, loss = 1.19838271\n",
      "Iteration 68, loss = 1.20448478\n",
      "Iteration 69, loss = 1.18936638\n",
      "Iteration 70, loss = 1.18997175\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.51795934\n",
      "Iteration 2, loss = 1.51705042\n",
      "Iteration 3, loss = 1.51563190\n",
      "Iteration 4, loss = 1.51395390\n",
      "Iteration 5, loss = 1.51204054\n",
      "Iteration 6, loss = 1.51006090\n",
      "Iteration 7, loss = 1.50791726\n",
      "Iteration 8, loss = 1.50585063\n",
      "Iteration 9, loss = 1.50365860\n",
      "Iteration 10, loss = 1.50155956\n",
      "Iteration 11, loss = 1.49938411\n",
      "Iteration 12, loss = 1.49722701\n",
      "Iteration 13, loss = 1.49505249\n",
      "Iteration 14, loss = 1.49291987\n",
      "Iteration 15, loss = 1.49084982\n",
      "Iteration 16, loss = 1.48873433\n",
      "Iteration 17, loss = 1.48666366\n",
      "Iteration 18, loss = 1.48461941\n",
      "Iteration 19, loss = 1.48255723\n",
      "Iteration 20, loss = 1.48051503\n",
      "Iteration 21, loss = 1.47854494\n",
      "Iteration 22, loss = 1.47651625\n",
      "Iteration 23, loss = 1.47458308\n",
      "Iteration 24, loss = 1.47256681\n",
      "Iteration 25, loss = 1.47068022\n",
      "Iteration 26, loss = 1.46873689\n",
      "Iteration 27, loss = 1.46685642\n",
      "Iteration 28, loss = 1.46494032\n",
      "Iteration 29, loss = 1.46305436\n",
      "Iteration 30, loss = 1.46121989\n",
      "Iteration 31, loss = 1.45932861\n",
      "Iteration 32, loss = 1.45751655\n",
      "Iteration 33, loss = 1.45571420\n",
      "Iteration 34, loss = 1.45390281\n",
      "Iteration 35, loss = 1.45212043\n",
      "Iteration 36, loss = 1.45036721\n",
      "Iteration 37, loss = 1.44859514\n",
      "Iteration 38, loss = 1.44686068\n",
      "Iteration 39, loss = 1.44514744\n",
      "Iteration 40, loss = 1.44340978\n",
      "Iteration 41, loss = 1.44175367\n",
      "Iteration 42, loss = 1.44001564\n",
      "Iteration 43, loss = 1.43838127\n",
      "Iteration 44, loss = 1.43675793\n",
      "Iteration 45, loss = 1.43508843\n",
      "Iteration 46, loss = 1.43345425\n",
      "Iteration 47, loss = 1.43183976\n",
      "Iteration 48, loss = 1.43024126\n",
      "Iteration 49, loss = 1.42866703\n",
      "Iteration 50, loss = 1.42708343\n",
      "Iteration 51, loss = 1.42553796\n",
      "Iteration 52, loss = 1.42401669\n",
      "Iteration 53, loss = 1.42245557\n",
      "Iteration 54, loss = 1.42094127\n",
      "Iteration 55, loss = 1.41943302\n",
      "Iteration 56, loss = 1.41793309\n",
      "Iteration 57, loss = 1.41646394\n",
      "Iteration 58, loss = 1.41499838\n",
      "Iteration 59, loss = 1.41356350\n",
      "Iteration 60, loss = 1.41209482\n",
      "Iteration 61, loss = 1.41062669\n",
      "Iteration 62, loss = 1.40922488\n",
      "Iteration 63, loss = 1.40783932\n",
      "Iteration 64, loss = 1.40643139\n",
      "Iteration 65, loss = 1.40503486\n",
      "Iteration 66, loss = 1.40370262\n",
      "Iteration 67, loss = 1.40234845\n",
      "Iteration 68, loss = 1.40099742\n",
      "Iteration 69, loss = 1.39963462\n",
      "Iteration 70, loss = 1.39829548\n",
      "Iteration 71, loss = 1.39699229\n",
      "Iteration 72, loss = 1.39573202\n",
      "Iteration 73, loss = 1.39439779\n",
      "Iteration 74, loss = 1.39310628\n",
      "Iteration 75, loss = 1.39185441\n",
      "Iteration 76, loss = 1.39058302\n",
      "Iteration 77, loss = 1.38932133\n",
      "Iteration 78, loss = 1.38810664\n",
      "Iteration 79, loss = 1.38686143\n",
      "Iteration 80, loss = 1.38563443\n",
      "Iteration 81, loss = 1.38444965\n",
      "Iteration 82, loss = 1.38323147\n",
      "Iteration 83, loss = 1.38201235\n",
      "Iteration 84, loss = 1.38086811\n",
      "Iteration 85, loss = 1.37970259\n",
      "Iteration 86, loss = 1.37851855\n",
      "Iteration 87, loss = 1.37736990\n",
      "Iteration 88, loss = 1.37624318\n",
      "Iteration 89, loss = 1.37510405\n",
      "Iteration 90, loss = 1.37399168\n",
      "Iteration 91, loss = 1.37289476\n",
      "Iteration 92, loss = 1.37176033\n",
      "Iteration 93, loss = 1.37067069\n",
      "Iteration 94, loss = 1.36961570\n",
      "Iteration 95, loss = 1.36852283\n",
      "Iteration 96, loss = 1.36743471\n",
      "Iteration 97, loss = 1.36636144\n",
      "Iteration 98, loss = 1.36539367\n",
      "Iteration 99, loss = 1.36430354\n",
      "Iteration 100, loss = 1.36325407\n",
      "Iteration 101, loss = 1.36225524\n",
      "Iteration 102, loss = 1.36122921\n",
      "Iteration 103, loss = 1.36023317\n",
      "Iteration 104, loss = 1.35923078\n",
      "Iteration 105, loss = 1.35824037\n",
      "Iteration 106, loss = 1.35725978\n",
      "Iteration 107, loss = 1.35629092\n",
      "Iteration 108, loss = 1.35531511\n",
      "Iteration 109, loss = 1.35435635\n",
      "Iteration 110, loss = 1.35341828\n",
      "Iteration 111, loss = 1.35247842\n",
      "Iteration 112, loss = 1.35153781\n",
      "Iteration 113, loss = 1.35060690\n",
      "Iteration 114, loss = 1.34967896\n",
      "Iteration 115, loss = 1.34879743\n",
      "Iteration 116, loss = 1.34786441\n",
      "Iteration 117, loss = 1.34699296\n",
      "Iteration 118, loss = 1.34608291\n",
      "Iteration 119, loss = 1.34520933\n",
      "Iteration 120, loss = 1.34435394\n",
      "Iteration 121, loss = 1.34347946\n",
      "Iteration 122, loss = 1.34260326\n",
      "Iteration 123, loss = 1.34174676\n",
      "Iteration 124, loss = 1.34090900\n",
      "Iteration 125, loss = 1.34008132\n",
      "Iteration 126, loss = 1.33922535\n",
      "Iteration 127, loss = 1.33842835\n",
      "Iteration 128, loss = 1.33758486\n",
      "Iteration 129, loss = 1.33679398\n",
      "Iteration 130, loss = 1.33598152\n",
      "Iteration 131, loss = 1.33518500\n",
      "Iteration 132, loss = 1.33438190\n",
      "Iteration 133, loss = 1.33359456\n",
      "Iteration 134, loss = 1.33280305\n",
      "Iteration 135, loss = 1.33205001\n",
      "Iteration 136, loss = 1.33126398\n",
      "Iteration 137, loss = 1.33052051\n",
      "Iteration 138, loss = 1.32976569\n",
      "Iteration 139, loss = 1.32899222\n",
      "Iteration 140, loss = 1.32827389\n",
      "Iteration 141, loss = 1.32752373\n",
      "Iteration 142, loss = 1.32682236\n",
      "Iteration 143, loss = 1.32607031\n",
      "Iteration 144, loss = 1.32534502\n",
      "Iteration 145, loss = 1.32463216\n",
      "Iteration 146, loss = 1.32392316\n",
      "Iteration 147, loss = 1.32322157\n",
      "Iteration 148, loss = 1.32252577\n",
      "Iteration 149, loss = 1.32181129\n",
      "Iteration 150, loss = 1.32115087\n",
      "Iteration 151, loss = 1.32045282\n",
      "Iteration 152, loss = 1.31977969\n",
      "Iteration 153, loss = 1.31910888\n",
      "Iteration 154, loss = 1.31844137\n",
      "Iteration 155, loss = 1.31777454\n",
      "Iteration 156, loss = 1.31712262\n",
      "Iteration 157, loss = 1.31647851\n",
      "Iteration 158, loss = 1.31580415\n",
      "Iteration 159, loss = 1.31518512\n",
      "Iteration 160, loss = 1.31453266\n",
      "Iteration 161, loss = 1.31391649\n",
      "Iteration 162, loss = 1.31330480\n",
      "Iteration 163, loss = 1.31266278\n",
      "Iteration 164, loss = 1.31204052\n",
      "Iteration 165, loss = 1.31142879\n",
      "Iteration 166, loss = 1.31080930\n",
      "Iteration 167, loss = 1.31021408\n",
      "Iteration 168, loss = 1.30961801\n",
      "Iteration 169, loss = 1.30901784\n",
      "Iteration 170, loss = 1.30843917\n",
      "Iteration 171, loss = 1.30784042\n",
      "Iteration 172, loss = 1.30726533\n",
      "Iteration 173, loss = 1.30669133\n",
      "Iteration 174, loss = 1.30611918\n",
      "Iteration 175, loss = 1.30553898\n",
      "Iteration 176, loss = 1.30498591\n",
      "Iteration 177, loss = 1.30442216\n",
      "Iteration 178, loss = 1.30387311\n",
      "Iteration 179, loss = 1.30330146\n",
      "Iteration 180, loss = 1.30277156\n",
      "Iteration 181, loss = 1.30222342\n",
      "Iteration 182, loss = 1.30168383\n",
      "Iteration 183, loss = 1.30114940\n",
      "Iteration 184, loss = 1.30061033\n",
      "Iteration 185, loss = 1.30008050\n",
      "Iteration 186, loss = 1.29959121\n",
      "Iteration 187, loss = 1.29903341\n",
      "Iteration 188, loss = 1.29852517\n",
      "Iteration 189, loss = 1.29801814\n",
      "Iteration 190, loss = 1.29750158\n",
      "Iteration 191, loss = 1.29700331\n",
      "Iteration 192, loss = 1.29650253\n",
      "Iteration 193, loss = 1.29599038\n",
      "Iteration 194, loss = 1.29551111\n",
      "Iteration 195, loss = 1.29502021\n",
      "Iteration 196, loss = 1.29453040\n",
      "Iteration 197, loss = 1.29404232\n",
      "Iteration 198, loss = 1.29355613\n",
      "Iteration 199, loss = 1.29310356\n",
      "Iteration 200, loss = 1.29262336\n",
      "Iteration 1, loss = 1.51819130\n",
      "Iteration 2, loss = 1.51727358\n",
      "Iteration 3, loss = 1.51587303\n",
      "Iteration 4, loss = 1.51420044\n",
      "Iteration 5, loss = 1.51230362\n",
      "Iteration 6, loss = 1.51030071\n",
      "Iteration 7, loss = 1.50819191\n",
      "Iteration 8, loss = 1.50609928\n",
      "Iteration 9, loss = 1.50394512\n",
      "Iteration 10, loss = 1.50182610\n",
      "Iteration 11, loss = 1.49965659\n",
      "Iteration 12, loss = 1.49752529\n",
      "Iteration 13, loss = 1.49534804\n",
      "Iteration 14, loss = 1.49327593\n",
      "Iteration 15, loss = 1.49114649\n",
      "Iteration 16, loss = 1.48905235\n",
      "Iteration 17, loss = 1.48700325\n",
      "Iteration 18, loss = 1.48493274\n",
      "Iteration 19, loss = 1.48291566\n",
      "Iteration 20, loss = 1.48087592\n",
      "Iteration 21, loss = 1.47889326\n",
      "Iteration 22, loss = 1.47688986\n",
      "Iteration 23, loss = 1.47493448\n",
      "Iteration 24, loss = 1.47295349\n",
      "Iteration 25, loss = 1.47102541\n",
      "Iteration 26, loss = 1.46911937\n",
      "Iteration 27, loss = 1.46725943\n",
      "Iteration 28, loss = 1.46530053\n",
      "Iteration 29, loss = 1.46344730\n",
      "Iteration 30, loss = 1.46161328\n",
      "Iteration 31, loss = 1.45973573\n",
      "Iteration 32, loss = 1.45789718\n",
      "Iteration 33, loss = 1.45609640\n",
      "Iteration 34, loss = 1.45434371\n",
      "Iteration 35, loss = 1.45253964\n",
      "Iteration 36, loss = 1.45077171\n",
      "Iteration 37, loss = 1.44900855\n",
      "Iteration 38, loss = 1.44730896\n",
      "Iteration 39, loss = 1.44556116\n",
      "Iteration 40, loss = 1.44385118\n",
      "Iteration 41, loss = 1.44218454\n",
      "Iteration 42, loss = 1.44046367\n",
      "Iteration 43, loss = 1.43879942\n",
      "Iteration 44, loss = 1.43719807\n",
      "Iteration 45, loss = 1.43554439\n",
      "Iteration 46, loss = 1.43390499\n",
      "Iteration 47, loss = 1.43228597\n",
      "Iteration 48, loss = 1.43070776\n",
      "Iteration 49, loss = 1.42914018\n",
      "Iteration 50, loss = 1.42752924\n",
      "Iteration 51, loss = 1.42598308\n",
      "Iteration 52, loss = 1.42446093\n",
      "Iteration 53, loss = 1.42291117\n",
      "Iteration 54, loss = 1.42140855\n",
      "Iteration 55, loss = 1.41989870\n",
      "Iteration 56, loss = 1.41839660\n",
      "Iteration 57, loss = 1.41693592\n",
      "Iteration 58, loss = 1.41546219\n",
      "Iteration 59, loss = 1.41401256\n",
      "Iteration 60, loss = 1.41256988\n",
      "Iteration 61, loss = 1.41109985\n",
      "Iteration 62, loss = 1.40968512\n",
      "Iteration 63, loss = 1.40830564\n",
      "Iteration 64, loss = 1.40690038\n",
      "Iteration 65, loss = 1.40548069\n",
      "Iteration 66, loss = 1.40415226\n",
      "Iteration 67, loss = 1.40279734\n",
      "Iteration 68, loss = 1.40145779\n",
      "Iteration 69, loss = 1.40006433\n",
      "Iteration 70, loss = 1.39876576\n",
      "Iteration 71, loss = 1.39745390\n",
      "Iteration 72, loss = 1.39615932\n",
      "Iteration 73, loss = 1.39485511\n",
      "Iteration 74, loss = 1.39355865\n",
      "Iteration 75, loss = 1.39229259\n",
      "Iteration 76, loss = 1.39102155\n",
      "Iteration 77, loss = 1.38979339\n",
      "Iteration 78, loss = 1.38857708\n",
      "Iteration 79, loss = 1.38730621\n",
      "Iteration 80, loss = 1.38609531\n",
      "Iteration 81, loss = 1.38489059\n",
      "Iteration 82, loss = 1.38369281\n",
      "Iteration 83, loss = 1.38247403\n",
      "Iteration 84, loss = 1.38132876\n",
      "Iteration 85, loss = 1.38015832\n",
      "Iteration 86, loss = 1.37899404\n",
      "Iteration 87, loss = 1.37784180\n",
      "Iteration 88, loss = 1.37670816\n",
      "Iteration 89, loss = 1.37556786\n",
      "Iteration 90, loss = 1.37444069\n",
      "Iteration 91, loss = 1.37335825\n",
      "Iteration 92, loss = 1.37224059\n",
      "Iteration 93, loss = 1.37115582\n",
      "Iteration 94, loss = 1.37006517\n",
      "Iteration 95, loss = 1.36901861\n",
      "Iteration 96, loss = 1.36790523\n",
      "Iteration 97, loss = 1.36686077\n",
      "Iteration 98, loss = 1.36584592\n",
      "Iteration 99, loss = 1.36479123\n",
      "Iteration 100, loss = 1.36373726\n",
      "Iteration 101, loss = 1.36273747\n",
      "Iteration 102, loss = 1.36171948\n",
      "Iteration 103, loss = 1.36070169\n",
      "Iteration 104, loss = 1.35971738\n",
      "Iteration 105, loss = 1.35874340\n",
      "Iteration 106, loss = 1.35774883\n",
      "Iteration 107, loss = 1.35677280\n",
      "Iteration 108, loss = 1.35581797\n",
      "Iteration 109, loss = 1.35486922\n",
      "Iteration 110, loss = 1.35393579\n",
      "Iteration 111, loss = 1.35295482\n",
      "Iteration 112, loss = 1.35202602\n",
      "Iteration 113, loss = 1.35111299\n",
      "Iteration 114, loss = 1.35017973\n",
      "Iteration 115, loss = 1.34929679\n",
      "Iteration 116, loss = 1.34836229\n",
      "Iteration 117, loss = 1.34749179\n",
      "Iteration 118, loss = 1.34658287\n",
      "Iteration 119, loss = 1.34570628\n",
      "Iteration 120, loss = 1.34484392\n",
      "Iteration 121, loss = 1.34398397\n",
      "Iteration 122, loss = 1.34310832\n",
      "Iteration 123, loss = 1.34223322\n",
      "Iteration 124, loss = 1.34141580\n",
      "Iteration 125, loss = 1.34057366\n",
      "Iteration 126, loss = 1.33973060\n",
      "Iteration 127, loss = 1.33892575\n",
      "Iteration 128, loss = 1.33809081\n",
      "Iteration 129, loss = 1.33728939\n",
      "Iteration 130, loss = 1.33646709\n",
      "Iteration 131, loss = 1.33568700\n",
      "Iteration 132, loss = 1.33487903\n",
      "Iteration 133, loss = 1.33408951\n",
      "Iteration 134, loss = 1.33330347\n",
      "Iteration 135, loss = 1.33252566\n",
      "Iteration 136, loss = 1.33176459\n",
      "Iteration 137, loss = 1.33100142\n",
      "Iteration 138, loss = 1.33026045\n",
      "Iteration 139, loss = 1.32948828\n",
      "Iteration 140, loss = 1.32877348\n",
      "Iteration 141, loss = 1.32799759\n",
      "Iteration 142, loss = 1.32730527\n",
      "Iteration 143, loss = 1.32654594\n",
      "Iteration 144, loss = 1.32583998\n",
      "Iteration 145, loss = 1.32511387\n",
      "Iteration 146, loss = 1.32441035\n",
      "Iteration 147, loss = 1.32370161\n",
      "Iteration 148, loss = 1.32299598\n",
      "Iteration 149, loss = 1.32229641\n",
      "Iteration 150, loss = 1.32162594\n",
      "Iteration 151, loss = 1.32093640\n",
      "Iteration 152, loss = 1.32026743\n",
      "Iteration 153, loss = 1.31958753\n",
      "Iteration 154, loss = 1.31892017\n",
      "Iteration 155, loss = 1.31826902\n",
      "Iteration 156, loss = 1.31760618\n",
      "Iteration 157, loss = 1.31694886\n",
      "Iteration 158, loss = 1.31629202\n",
      "Iteration 159, loss = 1.31564468\n",
      "Iteration 160, loss = 1.31501760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 161, loss = 1.31439741\n",
      "Iteration 162, loss = 1.31376581\n",
      "Iteration 163, loss = 1.31313866\n",
      "Iteration 164, loss = 1.31251368\n",
      "Iteration 165, loss = 1.31191112\n",
      "Iteration 166, loss = 1.31127406\n",
      "Iteration 167, loss = 1.31068174\n",
      "Iteration 168, loss = 1.31009836\n",
      "Iteration 169, loss = 1.30948524\n",
      "Iteration 170, loss = 1.30890608\n",
      "Iteration 171, loss = 1.30831364\n",
      "Iteration 172, loss = 1.30773626\n",
      "Iteration 173, loss = 1.30715298\n",
      "Iteration 174, loss = 1.30658668\n",
      "Iteration 175, loss = 1.30599542\n",
      "Iteration 176, loss = 1.30544603\n",
      "Iteration 177, loss = 1.30488475\n",
      "Iteration 178, loss = 1.30431699\n",
      "Iteration 179, loss = 1.30377691\n",
      "Iteration 180, loss = 1.30322139\n",
      "Iteration 181, loss = 1.30268608\n",
      "Iteration 182, loss = 1.30212880\n",
      "Iteration 183, loss = 1.30160210\n",
      "Iteration 184, loss = 1.30106408\n",
      "Iteration 185, loss = 1.30053443\n",
      "Iteration 186, loss = 1.30003049\n",
      "Iteration 187, loss = 1.29947265\n",
      "Iteration 188, loss = 1.29897014\n",
      "Iteration 189, loss = 1.29846527\n",
      "Iteration 190, loss = 1.29794708\n",
      "Iteration 191, loss = 1.29745344\n",
      "Iteration 192, loss = 1.29693784\n",
      "Iteration 193, loss = 1.29643938\n",
      "Iteration 194, loss = 1.29594164\n",
      "Iteration 195, loss = 1.29546028\n",
      "Iteration 196, loss = 1.29497332\n",
      "Iteration 197, loss = 1.29447380\n",
      "Iteration 198, loss = 1.29398767\n",
      "Iteration 199, loss = 1.29353702\n",
      "Iteration 200, loss = 1.29304013\n",
      "Iteration 1, loss = 1.51839966\n",
      "Iteration 2, loss = 1.51747517\n",
      "Iteration 3, loss = 1.51608259\n",
      "Iteration 4, loss = 1.51442186\n",
      "Iteration 5, loss = 1.51247442\n",
      "Iteration 6, loss = 1.51046279\n",
      "Iteration 7, loss = 1.50839139\n",
      "Iteration 8, loss = 1.50626096\n",
      "Iteration 9, loss = 1.50409802\n",
      "Iteration 10, loss = 1.50194993\n",
      "Iteration 11, loss = 1.49979671\n",
      "Iteration 12, loss = 1.49766518\n",
      "Iteration 13, loss = 1.49549599\n",
      "Iteration 14, loss = 1.49341029\n",
      "Iteration 15, loss = 1.49124916\n",
      "Iteration 16, loss = 1.48917105\n",
      "Iteration 17, loss = 1.48710057\n",
      "Iteration 18, loss = 1.48503989\n",
      "Iteration 19, loss = 1.48301407\n",
      "Iteration 20, loss = 1.48095911\n",
      "Iteration 21, loss = 1.47897518\n",
      "Iteration 22, loss = 1.47695910\n",
      "Iteration 23, loss = 1.47498361\n",
      "Iteration 24, loss = 1.47303393\n",
      "Iteration 25, loss = 1.47113600\n",
      "Iteration 26, loss = 1.46918206\n",
      "Iteration 27, loss = 1.46729072\n",
      "Iteration 28, loss = 1.46534281\n",
      "Iteration 29, loss = 1.46350391\n",
      "Iteration 30, loss = 1.46164176\n",
      "Iteration 31, loss = 1.45978543\n",
      "Iteration 32, loss = 1.45793439\n",
      "Iteration 33, loss = 1.45613312\n",
      "Iteration 34, loss = 1.45433746\n",
      "Iteration 35, loss = 1.45257448\n",
      "Iteration 36, loss = 1.45079761\n",
      "Iteration 37, loss = 1.44901622\n",
      "Iteration 38, loss = 1.44730687\n",
      "Iteration 39, loss = 1.44555391\n",
      "Iteration 40, loss = 1.44385195\n",
      "Iteration 41, loss = 1.44216914\n",
      "Iteration 42, loss = 1.44043353\n",
      "Iteration 43, loss = 1.43881153\n",
      "Iteration 44, loss = 1.43715580\n",
      "Iteration 45, loss = 1.43549423\n",
      "Iteration 46, loss = 1.43387165\n",
      "Iteration 47, loss = 1.43227372\n",
      "Iteration 48, loss = 1.43067602\n",
      "Iteration 49, loss = 1.42910244\n",
      "Iteration 50, loss = 1.42746879\n",
      "Iteration 51, loss = 1.42595244\n",
      "Iteration 52, loss = 1.42441580\n",
      "Iteration 53, loss = 1.42286224\n",
      "Iteration 54, loss = 1.42132043\n",
      "Iteration 55, loss = 1.41982726\n",
      "Iteration 56, loss = 1.41835834\n",
      "Iteration 57, loss = 1.41688052\n",
      "Iteration 58, loss = 1.41538244\n",
      "Iteration 59, loss = 1.41395460\n",
      "Iteration 60, loss = 1.41251118\n",
      "Iteration 61, loss = 1.41103963\n",
      "Iteration 62, loss = 1.40963081\n",
      "Iteration 63, loss = 1.40824102\n",
      "Iteration 64, loss = 1.40683220\n",
      "Iteration 65, loss = 1.40541026\n",
      "Iteration 66, loss = 1.40406461\n",
      "Iteration 67, loss = 1.40270575\n",
      "Iteration 68, loss = 1.40137085\n",
      "Iteration 69, loss = 1.39999044\n",
      "Iteration 70, loss = 1.39866642\n",
      "Iteration 71, loss = 1.39737614\n",
      "Iteration 72, loss = 1.39604320\n",
      "Iteration 73, loss = 1.39474016\n",
      "Iteration 74, loss = 1.39346441\n",
      "Iteration 75, loss = 1.39214891\n",
      "Iteration 76, loss = 1.39090465\n",
      "Iteration 77, loss = 1.38965859\n",
      "Iteration 78, loss = 1.38842628\n",
      "Iteration 79, loss = 1.38718478\n",
      "Iteration 80, loss = 1.38595845\n",
      "Iteration 81, loss = 1.38472681\n",
      "Iteration 82, loss = 1.38354390\n",
      "Iteration 83, loss = 1.38232017\n",
      "Iteration 84, loss = 1.38114498\n",
      "Iteration 85, loss = 1.37997364\n",
      "Iteration 86, loss = 1.37881361\n",
      "Iteration 87, loss = 1.37767671\n",
      "Iteration 88, loss = 1.37652284\n",
      "Iteration 89, loss = 1.37536751\n",
      "Iteration 90, loss = 1.37425283\n",
      "Iteration 91, loss = 1.37312607\n",
      "Iteration 92, loss = 1.37202463\n",
      "Iteration 93, loss = 1.37094988\n",
      "Iteration 94, loss = 1.36983766\n",
      "Iteration 95, loss = 1.36878059\n",
      "Iteration 96, loss = 1.36770451\n",
      "Iteration 97, loss = 1.36661669\n",
      "Iteration 98, loss = 1.36559708\n",
      "Iteration 99, loss = 1.36454379\n",
      "Iteration 100, loss = 1.36349695\n",
      "Iteration 101, loss = 1.36250014\n",
      "Iteration 102, loss = 1.36147002\n",
      "Iteration 103, loss = 1.36042860\n",
      "Iteration 104, loss = 1.35944287\n",
      "Iteration 105, loss = 1.35846620\n",
      "Iteration 106, loss = 1.35748029\n",
      "Iteration 107, loss = 1.35649023\n",
      "Iteration 108, loss = 1.35553235\n",
      "Iteration 109, loss = 1.35458145\n",
      "Iteration 110, loss = 1.35361147\n",
      "Iteration 111, loss = 1.35263286\n",
      "Iteration 112, loss = 1.35170413\n",
      "Iteration 113, loss = 1.35077143\n",
      "Iteration 114, loss = 1.34984976\n",
      "Iteration 115, loss = 1.34896545\n",
      "Iteration 116, loss = 1.34802728\n",
      "Iteration 117, loss = 1.34712680\n",
      "Iteration 118, loss = 1.34622642\n",
      "Iteration 119, loss = 1.34535320\n",
      "Iteration 120, loss = 1.34447466\n",
      "Iteration 121, loss = 1.34360810\n",
      "Iteration 122, loss = 1.34272822\n",
      "Iteration 123, loss = 1.34186075\n",
      "Iteration 124, loss = 1.34103223\n",
      "Iteration 125, loss = 1.34016644\n",
      "Iteration 126, loss = 1.33933763\n",
      "Iteration 127, loss = 1.33852562\n",
      "Iteration 128, loss = 1.33766862\n",
      "Iteration 129, loss = 1.33687385\n",
      "Iteration 130, loss = 1.33607001\n",
      "Iteration 131, loss = 1.33524016\n",
      "Iteration 132, loss = 1.33444438\n",
      "Iteration 133, loss = 1.33365735\n",
      "Iteration 134, loss = 1.33286551\n",
      "Iteration 135, loss = 1.33208937\n",
      "Iteration 136, loss = 1.33130768\n",
      "Iteration 137, loss = 1.33053248\n",
      "Iteration 138, loss = 1.32980453\n",
      "Iteration 139, loss = 1.32902412\n",
      "Iteration 140, loss = 1.32829496\n",
      "Iteration 141, loss = 1.32751461\n",
      "Iteration 142, loss = 1.32681222\n",
      "Iteration 143, loss = 1.32606644\n",
      "Iteration 144, loss = 1.32533985\n",
      "Iteration 145, loss = 1.32460673\n",
      "Iteration 146, loss = 1.32388581\n",
      "Iteration 147, loss = 1.32320605\n",
      "Iteration 148, loss = 1.32248416\n",
      "Iteration 149, loss = 1.32176710\n",
      "Iteration 150, loss = 1.32108876\n",
      "Iteration 151, loss = 1.32040208\n",
      "Iteration 152, loss = 1.31970962\n",
      "Iteration 153, loss = 1.31903811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 154, loss = 1.31836552\n",
      "Iteration 155, loss = 1.31769328\n",
      "Iteration 156, loss = 1.31702621\n",
      "Iteration 157, loss = 1.31637886\n",
      "Iteration 158, loss = 1.31570097\n",
      "Iteration 159, loss = 1.31506265\n",
      "Iteration 160, loss = 1.31442575\n",
      "Iteration 161, loss = 1.31378081\n",
      "Iteration 162, loss = 1.31314691\n",
      "Iteration 163, loss = 1.31251975\n",
      "Iteration 164, loss = 1.31187911\n",
      "Iteration 165, loss = 1.31128049\n",
      "Iteration 166, loss = 1.31064055\n",
      "Iteration 167, loss = 1.31004197\n",
      "Iteration 168, loss = 1.30943683\n",
      "Iteration 169, loss = 1.30884291\n",
      "Iteration 170, loss = 1.30823595\n",
      "Iteration 171, loss = 1.30763632\n",
      "Iteration 172, loss = 1.30706306\n",
      "Iteration 173, loss = 1.30647060\n",
      "Iteration 174, loss = 1.30589022\n",
      "Iteration 175, loss = 1.30531152\n",
      "Iteration 176, loss = 1.30475113\n",
      "Iteration 177, loss = 1.30417916\n",
      "Iteration 178, loss = 1.30361122\n",
      "Iteration 179, loss = 1.30305611\n",
      "Iteration 180, loss = 1.30248935\n",
      "Iteration 181, loss = 1.30196056\n",
      "Iteration 182, loss = 1.30139227\n",
      "Iteration 183, loss = 1.30085437\n",
      "Iteration 184, loss = 1.30032315\n",
      "Iteration 185, loss = 1.29977983\n",
      "Iteration 186, loss = 1.29927242\n",
      "Iteration 187, loss = 1.29871753\n",
      "Iteration 188, loss = 1.29820328\n",
      "Iteration 189, loss = 1.29769750\n",
      "Iteration 190, loss = 1.29717058\n",
      "Iteration 191, loss = 1.29666406\n",
      "Iteration 192, loss = 1.29615742\n",
      "Iteration 193, loss = 1.29565571\n",
      "Iteration 194, loss = 1.29514701\n",
      "Iteration 195, loss = 1.29467313\n",
      "Iteration 196, loss = 1.29417216\n",
      "Iteration 197, loss = 1.29368194\n",
      "Iteration 198, loss = 1.29318102\n",
      "Iteration 199, loss = 1.29271281\n",
      "Iteration 200, loss = 1.29221654\n",
      "Iteration 1, loss = 1.51851606\n",
      "Iteration 2, loss = 1.51760970\n",
      "Iteration 3, loss = 1.51621518\n",
      "Iteration 4, loss = 1.51454296\n",
      "Iteration 5, loss = 1.51262134\n",
      "Iteration 6, loss = 1.51063123\n",
      "Iteration 7, loss = 1.50857347\n",
      "Iteration 8, loss = 1.50643161\n",
      "Iteration 9, loss = 1.50432164\n",
      "Iteration 10, loss = 1.50215518\n",
      "Iteration 11, loss = 1.50001243\n",
      "Iteration 12, loss = 1.49789324\n",
      "Iteration 13, loss = 1.49575912\n",
      "Iteration 14, loss = 1.49369717\n",
      "Iteration 15, loss = 1.49152397\n",
      "Iteration 16, loss = 1.48947489\n",
      "Iteration 17, loss = 1.48742341\n",
      "Iteration 18, loss = 1.48534480\n",
      "Iteration 19, loss = 1.48337952\n",
      "Iteration 20, loss = 1.48130122\n",
      "Iteration 21, loss = 1.47933459\n",
      "Iteration 22, loss = 1.47738676\n",
      "Iteration 23, loss = 1.47536600\n",
      "Iteration 24, loss = 1.47345225\n",
      "Iteration 25, loss = 1.47152835\n",
      "Iteration 26, loss = 1.46961255\n",
      "Iteration 27, loss = 1.46773043\n",
      "Iteration 28, loss = 1.46583604\n",
      "Iteration 29, loss = 1.46397202\n",
      "Iteration 30, loss = 1.46213209\n",
      "Iteration 31, loss = 1.46028861\n",
      "Iteration 32, loss = 1.45845547\n",
      "Iteration 33, loss = 1.45664042\n",
      "Iteration 34, loss = 1.45490675\n",
      "Iteration 35, loss = 1.45311841\n",
      "Iteration 36, loss = 1.45133199\n",
      "Iteration 37, loss = 1.44959622\n",
      "Iteration 38, loss = 1.44790936\n",
      "Iteration 39, loss = 1.44618575\n",
      "Iteration 40, loss = 1.44449637\n",
      "Iteration 41, loss = 1.44276309\n",
      "Iteration 42, loss = 1.44109989\n",
      "Iteration 43, loss = 1.43946330\n",
      "Iteration 44, loss = 1.43782113\n",
      "Iteration 45, loss = 1.43618150\n",
      "Iteration 46, loss = 1.43458049\n",
      "Iteration 47, loss = 1.43298964\n",
      "Iteration 48, loss = 1.43139755\n",
      "Iteration 49, loss = 1.42984176\n",
      "Iteration 50, loss = 1.42822992\n",
      "Iteration 51, loss = 1.42669801\n",
      "Iteration 52, loss = 1.42518525\n",
      "Iteration 53, loss = 1.42363456\n",
      "Iteration 54, loss = 1.42213618\n",
      "Iteration 55, loss = 1.42063849\n",
      "Iteration 56, loss = 1.41916475\n",
      "Iteration 57, loss = 1.41771340\n",
      "Iteration 58, loss = 1.41622090\n",
      "Iteration 59, loss = 1.41479487\n",
      "Iteration 60, loss = 1.41337000\n",
      "Iteration 61, loss = 1.41190514\n",
      "Iteration 62, loss = 1.41049945\n",
      "Iteration 63, loss = 1.40913149\n",
      "Iteration 64, loss = 1.40772255\n",
      "Iteration 65, loss = 1.40634034\n",
      "Iteration 66, loss = 1.40498200\n",
      "Iteration 67, loss = 1.40361958\n",
      "Iteration 68, loss = 1.40229981\n",
      "Iteration 69, loss = 1.40092397\n",
      "Iteration 70, loss = 1.39961955\n",
      "Iteration 71, loss = 1.39835256\n",
      "Iteration 72, loss = 1.39702361\n",
      "Iteration 73, loss = 1.39573759\n",
      "Iteration 74, loss = 1.39444929\n",
      "Iteration 75, loss = 1.39318555\n",
      "Iteration 76, loss = 1.39193293\n",
      "Iteration 77, loss = 1.39070166\n",
      "Iteration 78, loss = 1.38948582\n",
      "Iteration 79, loss = 1.38825005\n",
      "Iteration 80, loss = 1.38704569\n",
      "Iteration 81, loss = 1.38580917\n",
      "Iteration 82, loss = 1.38462906\n",
      "Iteration 83, loss = 1.38343646\n",
      "Iteration 84, loss = 1.38227104\n",
      "Iteration 85, loss = 1.38111167\n",
      "Iteration 86, loss = 1.37997868\n",
      "Iteration 87, loss = 1.37883314\n",
      "Iteration 88, loss = 1.37767462\n",
      "Iteration 89, loss = 1.37652335\n",
      "Iteration 90, loss = 1.37544729\n",
      "Iteration 91, loss = 1.37431851\n",
      "Iteration 92, loss = 1.37324322\n",
      "Iteration 93, loss = 1.37215330\n",
      "Iteration 94, loss = 1.37107560\n",
      "Iteration 95, loss = 1.37002239\n",
      "Iteration 96, loss = 1.36895514\n",
      "Iteration 97, loss = 1.36787428\n",
      "Iteration 98, loss = 1.36685027\n",
      "Iteration 99, loss = 1.36582762\n",
      "Iteration 100, loss = 1.36479679\n",
      "Iteration 101, loss = 1.36378595\n",
      "Iteration 102, loss = 1.36278382\n",
      "Iteration 103, loss = 1.36176052\n",
      "Iteration 104, loss = 1.36076427\n",
      "Iteration 105, loss = 1.35979922\n",
      "Iteration 106, loss = 1.35882389\n",
      "Iteration 107, loss = 1.35784058\n",
      "Iteration 108, loss = 1.35691839\n",
      "Iteration 109, loss = 1.35596784\n",
      "Iteration 110, loss = 1.35500765\n",
      "Iteration 111, loss = 1.35405291\n",
      "Iteration 112, loss = 1.35312559\n",
      "Iteration 113, loss = 1.35221283\n",
      "Iteration 114, loss = 1.35130714\n",
      "Iteration 115, loss = 1.35042183\n",
      "Iteration 116, loss = 1.34950488\n",
      "Iteration 117, loss = 1.34862177\n",
      "Iteration 118, loss = 1.34774279\n",
      "Iteration 119, loss = 1.34684831\n",
      "Iteration 120, loss = 1.34599596\n",
      "Iteration 121, loss = 1.34512744\n",
      "Iteration 122, loss = 1.34426745\n",
      "Iteration 123, loss = 1.34342368\n",
      "Iteration 124, loss = 1.34258250\n",
      "Iteration 125, loss = 1.34176046\n",
      "Iteration 126, loss = 1.34091415\n",
      "Iteration 127, loss = 1.34011095\n",
      "Iteration 128, loss = 1.33928353\n",
      "Iteration 129, loss = 1.33848860\n",
      "Iteration 130, loss = 1.33769100\n",
      "Iteration 131, loss = 1.33687090\n",
      "Iteration 132, loss = 1.33608255\n",
      "Iteration 133, loss = 1.33530718\n",
      "Iteration 134, loss = 1.33453813\n",
      "Iteration 135, loss = 1.33376737\n",
      "Iteration 136, loss = 1.33298594\n",
      "Iteration 137, loss = 1.33223779\n",
      "Iteration 138, loss = 1.33149249\n",
      "Iteration 139, loss = 1.33073744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 140, loss = 1.33002734\n",
      "Iteration 141, loss = 1.32925843\n",
      "Iteration 142, loss = 1.32854662\n",
      "Iteration 143, loss = 1.32782172\n",
      "Iteration 144, loss = 1.32711199\n",
      "Iteration 145, loss = 1.32637946\n",
      "Iteration 146, loss = 1.32567331\n",
      "Iteration 147, loss = 1.32499000\n",
      "Iteration 148, loss = 1.32429325\n",
      "Iteration 149, loss = 1.32359555\n",
      "Iteration 150, loss = 1.32292608\n",
      "Iteration 151, loss = 1.32224231\n",
      "Iteration 152, loss = 1.32157131\n",
      "Iteration 153, loss = 1.32090595\n",
      "Iteration 154, loss = 1.32025239\n",
      "Iteration 155, loss = 1.31956593\n",
      "Iteration 156, loss = 1.31892429\n",
      "Iteration 157, loss = 1.31828621\n",
      "Iteration 158, loss = 1.31763651\n",
      "Iteration 159, loss = 1.31699816\n",
      "Iteration 160, loss = 1.31638081\n",
      "Iteration 161, loss = 1.31573297\n",
      "Iteration 162, loss = 1.31513027\n",
      "Iteration 163, loss = 1.31451179\n",
      "Iteration 164, loss = 1.31388313\n",
      "Iteration 165, loss = 1.31329590\n",
      "Iteration 166, loss = 1.31265555\n",
      "Iteration 167, loss = 1.31206431\n",
      "Iteration 168, loss = 1.31147527\n",
      "Iteration 169, loss = 1.31088737\n",
      "Iteration 170, loss = 1.31030630\n",
      "Iteration 171, loss = 1.30971658\n",
      "Iteration 172, loss = 1.30914717\n",
      "Iteration 173, loss = 1.30856238\n",
      "Iteration 174, loss = 1.30799567\n",
      "Iteration 175, loss = 1.30742141\n",
      "Iteration 176, loss = 1.30686922\n",
      "Iteration 177, loss = 1.30631021\n",
      "Iteration 178, loss = 1.30575801\n",
      "Iteration 179, loss = 1.30521932\n",
      "Iteration 180, loss = 1.30466464\n",
      "Iteration 181, loss = 1.30413045\n",
      "Iteration 182, loss = 1.30359171\n",
      "Iteration 183, loss = 1.30304912\n",
      "Iteration 184, loss = 1.30253777\n",
      "Iteration 185, loss = 1.30200135\n",
      "Iteration 186, loss = 1.30150297\n",
      "Iteration 187, loss = 1.30096358\n",
      "Iteration 188, loss = 1.30045327\n",
      "Iteration 189, loss = 1.29995639\n",
      "Iteration 190, loss = 1.29943758\n",
      "Iteration 191, loss = 1.29894781\n",
      "Iteration 192, loss = 1.29844315\n",
      "Iteration 193, loss = 1.29795911\n",
      "Iteration 194, loss = 1.29746132\n",
      "Iteration 195, loss = 1.29697962\n",
      "Iteration 196, loss = 1.29651006\n",
      "Iteration 197, loss = 1.29600811\n",
      "Iteration 198, loss = 1.29554009\n",
      "Iteration 199, loss = 1.29505848\n",
      "Iteration 200, loss = 1.29458171\n",
      "Iteration 1, loss = 1.51733147\n",
      "Iteration 2, loss = 1.51640786\n",
      "Iteration 3, loss = 1.51500472\n",
      "Iteration 4, loss = 1.51329856\n",
      "Iteration 5, loss = 1.51138214\n",
      "Iteration 6, loss = 1.50932341\n",
      "Iteration 7, loss = 1.50724672\n",
      "Iteration 8, loss = 1.50510173\n",
      "Iteration 9, loss = 1.50291713\n",
      "Iteration 10, loss = 1.50072855\n",
      "Iteration 11, loss = 1.49863191\n",
      "Iteration 12, loss = 1.49641030\n",
      "Iteration 13, loss = 1.49427528\n",
      "Iteration 14, loss = 1.49217983\n",
      "Iteration 15, loss = 1.48996964\n",
      "Iteration 16, loss = 1.48789065\n",
      "Iteration 17, loss = 1.48577255\n",
      "Iteration 18, loss = 1.48372250\n",
      "Iteration 19, loss = 1.48164400\n",
      "Iteration 20, loss = 1.47961764\n",
      "Iteration 21, loss = 1.47758380\n",
      "Iteration 22, loss = 1.47558994\n",
      "Iteration 23, loss = 1.47361342\n",
      "Iteration 24, loss = 1.47159766\n",
      "Iteration 25, loss = 1.46965390\n",
      "Iteration 26, loss = 1.46771276\n",
      "Iteration 27, loss = 1.46579489\n",
      "Iteration 28, loss = 1.46385164\n",
      "Iteration 29, loss = 1.46200313\n",
      "Iteration 30, loss = 1.46009750\n",
      "Iteration 31, loss = 1.45827837\n",
      "Iteration 32, loss = 1.45642748\n",
      "Iteration 33, loss = 1.45454835\n",
      "Iteration 34, loss = 1.45270548\n",
      "Iteration 35, loss = 1.45097123\n",
      "Iteration 36, loss = 1.44918581\n",
      "Iteration 37, loss = 1.44739472\n",
      "Iteration 38, loss = 1.44566381\n",
      "Iteration 39, loss = 1.44387130\n",
      "Iteration 40, loss = 1.44218331\n",
      "Iteration 41, loss = 1.44044449\n",
      "Iteration 42, loss = 1.43875995\n",
      "Iteration 43, loss = 1.43707609\n",
      "Iteration 44, loss = 1.43539608\n",
      "Iteration 45, loss = 1.43373958\n",
      "Iteration 46, loss = 1.43208876\n",
      "Iteration 47, loss = 1.43046999\n",
      "Iteration 48, loss = 1.42889030\n",
      "Iteration 49, loss = 1.42724527\n",
      "Iteration 50, loss = 1.42569023\n",
      "Iteration 51, loss = 1.42407889\n",
      "Iteration 52, loss = 1.42251842\n",
      "Iteration 53, loss = 1.42096546\n",
      "Iteration 54, loss = 1.41941319\n",
      "Iteration 55, loss = 1.41791355\n",
      "Iteration 56, loss = 1.41640140\n",
      "Iteration 57, loss = 1.41493414\n",
      "Iteration 58, loss = 1.41342042\n",
      "Iteration 59, loss = 1.41192901\n",
      "Iteration 60, loss = 1.41049686\n",
      "Iteration 61, loss = 1.40901003\n",
      "Iteration 62, loss = 1.40762310\n",
      "Iteration 63, loss = 1.40617118\n",
      "Iteration 64, loss = 1.40477739\n",
      "Iteration 65, loss = 1.40334572\n",
      "Iteration 66, loss = 1.40194946\n",
      "Iteration 67, loss = 1.40061320\n",
      "Iteration 68, loss = 1.39922579\n",
      "Iteration 69, loss = 1.39782814\n",
      "Iteration 70, loss = 1.39655156\n",
      "Iteration 71, loss = 1.39518645\n",
      "Iteration 72, loss = 1.39387312\n",
      "Iteration 73, loss = 1.39256649\n",
      "Iteration 74, loss = 1.39124839\n",
      "Iteration 75, loss = 1.38998437\n",
      "Iteration 76, loss = 1.38871178\n",
      "Iteration 77, loss = 1.38743053\n",
      "Iteration 78, loss = 1.38617293\n",
      "Iteration 79, loss = 1.38492831\n",
      "Iteration 80, loss = 1.38370508\n",
      "Iteration 81, loss = 1.38246804\n",
      "Iteration 82, loss = 1.38128021\n",
      "Iteration 83, loss = 1.38006253\n",
      "Iteration 84, loss = 1.37885558\n",
      "Iteration 85, loss = 1.37765348\n",
      "Iteration 86, loss = 1.37652790\n",
      "Iteration 87, loss = 1.37535891\n",
      "Iteration 88, loss = 1.37417813\n",
      "Iteration 89, loss = 1.37303938\n",
      "Iteration 90, loss = 1.37188406\n",
      "Iteration 91, loss = 1.37080022\n",
      "Iteration 92, loss = 1.36963786\n",
      "Iteration 93, loss = 1.36857212\n",
      "Iteration 94, loss = 1.36744267\n",
      "Iteration 95, loss = 1.36636305\n",
      "Iteration 96, loss = 1.36528622\n",
      "Iteration 97, loss = 1.36422691\n",
      "Iteration 98, loss = 1.36314118\n",
      "Iteration 99, loss = 1.36210794\n",
      "Iteration 100, loss = 1.36105258\n",
      "Iteration 101, loss = 1.36002358\n",
      "Iteration 102, loss = 1.35897624\n",
      "Iteration 103, loss = 1.35797196\n",
      "Iteration 104, loss = 1.35694935\n",
      "Iteration 105, loss = 1.35596378\n",
      "Iteration 106, loss = 1.35497172\n",
      "Iteration 107, loss = 1.35397425\n",
      "Iteration 108, loss = 1.35299273\n",
      "Iteration 109, loss = 1.35201727\n",
      "Iteration 110, loss = 1.35104159\n",
      "Iteration 111, loss = 1.35011704\n",
      "Iteration 112, loss = 1.34916402\n",
      "Iteration 113, loss = 1.34823494\n",
      "Iteration 114, loss = 1.34728943\n",
      "Iteration 115, loss = 1.34637504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 116, loss = 1.34545044\n",
      "Iteration 117, loss = 1.34455765\n",
      "Iteration 118, loss = 1.34364520\n",
      "Iteration 119, loss = 1.34276185\n",
      "Iteration 120, loss = 1.34187204\n",
      "Iteration 121, loss = 1.34099072\n",
      "Iteration 122, loss = 1.34011049\n",
      "Iteration 123, loss = 1.33923953\n",
      "Iteration 124, loss = 1.33840014\n",
      "Iteration 125, loss = 1.33755511\n",
      "Iteration 126, loss = 1.33670388\n",
      "Iteration 127, loss = 1.33586101\n",
      "Iteration 128, loss = 1.33504385\n",
      "Iteration 129, loss = 1.33421122\n",
      "Iteration 130, loss = 1.33339270\n",
      "Iteration 131, loss = 1.33257340\n",
      "Iteration 132, loss = 1.33178491\n",
      "Iteration 133, loss = 1.33097610\n",
      "Iteration 134, loss = 1.33020387\n",
      "Iteration 135, loss = 1.32939024\n",
      "Iteration 136, loss = 1.32861921\n",
      "Iteration 137, loss = 1.32785514\n",
      "Iteration 138, loss = 1.32709108\n",
      "Iteration 139, loss = 1.32631297\n",
      "Iteration 140, loss = 1.32556433\n",
      "Iteration 141, loss = 1.32482429\n",
      "Iteration 142, loss = 1.32406579\n",
      "Iteration 143, loss = 1.32334367\n",
      "Iteration 144, loss = 1.32260388\n",
      "Iteration 145, loss = 1.32189580\n",
      "Iteration 146, loss = 1.32116244\n",
      "Iteration 147, loss = 1.32044326\n",
      "Iteration 148, loss = 1.31972543\n",
      "Iteration 149, loss = 1.31903348\n",
      "Iteration 150, loss = 1.31833923\n",
      "Iteration 151, loss = 1.31764206\n",
      "Iteration 152, loss = 1.31697205\n",
      "Iteration 153, loss = 1.31628119\n",
      "Iteration 154, loss = 1.31559900\n",
      "Iteration 155, loss = 1.31492740\n",
      "Iteration 156, loss = 1.31423381\n",
      "Iteration 157, loss = 1.31358685\n",
      "Iteration 158, loss = 1.31293524\n",
      "Iteration 159, loss = 1.31229414\n",
      "Iteration 160, loss = 1.31165010\n",
      "Iteration 161, loss = 1.31100779\n",
      "Iteration 162, loss = 1.31035635\n",
      "Iteration 163, loss = 1.30973096\n",
      "Iteration 164, loss = 1.30908366\n",
      "Iteration 165, loss = 1.30847409\n",
      "Iteration 166, loss = 1.30786540\n",
      "Iteration 167, loss = 1.30724721\n",
      "Iteration 168, loss = 1.30663504\n",
      "Iteration 169, loss = 1.30602335\n",
      "Iteration 170, loss = 1.30542735\n",
      "Iteration 171, loss = 1.30483236\n",
      "Iteration 172, loss = 1.30423828\n",
      "Iteration 173, loss = 1.30365022\n",
      "Iteration 174, loss = 1.30307724\n",
      "Iteration 175, loss = 1.30249339\n",
      "Iteration 176, loss = 1.30192486\n",
      "Iteration 177, loss = 1.30134092\n",
      "Iteration 178, loss = 1.30078273\n",
      "Iteration 179, loss = 1.30021310\n",
      "Iteration 180, loss = 1.29966779\n",
      "Iteration 181, loss = 1.29909644\n",
      "Iteration 182, loss = 1.29855906\n",
      "Iteration 183, loss = 1.29802927\n",
      "Iteration 184, loss = 1.29747983\n",
      "Iteration 185, loss = 1.29692923\n",
      "Iteration 186, loss = 1.29641700\n",
      "Iteration 187, loss = 1.29588891\n",
      "Iteration 188, loss = 1.29534628\n",
      "Iteration 189, loss = 1.29484345\n",
      "Iteration 190, loss = 1.29430844\n",
      "Iteration 191, loss = 1.29379616\n",
      "Iteration 192, loss = 1.29328622\n",
      "Iteration 193, loss = 1.29278890\n",
      "Iteration 194, loss = 1.29228520\n",
      "Iteration 195, loss = 1.29179682\n",
      "Iteration 196, loss = 1.29128546\n",
      "Iteration 197, loss = 1.29078230\n",
      "Iteration 198, loss = 1.29032850\n",
      "Iteration 199, loss = 1.28982560\n",
      "Iteration 200, loss = 1.28934767\n",
      "Iteration 1, loss = 4.43345300\n",
      "Iteration 2, loss = 3.07029574\n",
      "Iteration 3, loss = 2.50808665\n",
      "Iteration 4, loss = 2.03708777\n",
      "Iteration 5, loss = 1.68507293\n",
      "Iteration 6, loss = 1.57597924\n",
      "Iteration 7, loss = 1.51822548\n",
      "Iteration 8, loss = 1.44598983\n",
      "Iteration 9, loss = 1.38503399\n",
      "Iteration 10, loss = 1.35231126\n",
      "Iteration 11, loss = 1.37573043\n",
      "Iteration 12, loss = 1.30694759\n",
      "Iteration 13, loss = 1.35396206\n",
      "Iteration 14, loss = 1.31161156\n",
      "Iteration 15, loss = 1.27676910\n",
      "Iteration 16, loss = 1.29359380\n",
      "Iteration 17, loss = 1.25726497\n",
      "Iteration 18, loss = 1.28365329\n",
      "Iteration 19, loss = 1.25803401\n",
      "Iteration 20, loss = 1.25036002\n",
      "Iteration 21, loss = 1.29774782\n",
      "Iteration 22, loss = 1.25911064\n",
      "Iteration 23, loss = 1.26921091\n",
      "Iteration 24, loss = 1.26514135\n",
      "Iteration 25, loss = 1.25450716\n",
      "Iteration 26, loss = 1.26916925\n",
      "Iteration 27, loss = 1.34516135\n",
      "Iteration 28, loss = 1.25843289\n",
      "Iteration 29, loss = 1.27344830\n",
      "Iteration 30, loss = 1.24343643\n",
      "Iteration 31, loss = 1.26699230\n",
      "Iteration 32, loss = 1.25267699\n",
      "Iteration 33, loss = 1.25526647\n",
      "Iteration 34, loss = 1.22883235\n",
      "Iteration 35, loss = 1.21913500\n",
      "Iteration 36, loss = 1.21472488\n",
      "Iteration 37, loss = 1.22706315\n",
      "Iteration 38, loss = 1.26404473\n",
      "Iteration 39, loss = 1.31419483\n",
      "Iteration 40, loss = 1.22620133\n",
      "Iteration 41, loss = 1.24723230\n",
      "Iteration 42, loss = 1.20274176\n",
      "Iteration 43, loss = 1.20699742\n",
      "Iteration 44, loss = 1.20035913\n",
      "Iteration 45, loss = 1.21347143\n",
      "Iteration 46, loss = 1.20566820\n",
      "Iteration 47, loss = 1.21848343\n",
      "Iteration 48, loss = 1.19731686\n",
      "Iteration 49, loss = 1.28100707\n",
      "Iteration 50, loss = 1.27519888\n",
      "Iteration 51, loss = 1.26266473\n",
      "Iteration 52, loss = 1.27835891\n",
      "Iteration 53, loss = 1.22881630\n",
      "Iteration 54, loss = 1.26607181\n",
      "Iteration 55, loss = 1.19440010\n",
      "Iteration 56, loss = 1.27278405\n",
      "Iteration 57, loss = 1.19212362\n",
      "Iteration 58, loss = 1.20113652\n",
      "Iteration 59, loss = 1.18287510\n",
      "Iteration 60, loss = 1.18706274\n",
      "Iteration 61, loss = 1.19357985\n",
      "Iteration 62, loss = 1.22880394\n",
      "Iteration 63, loss = 1.23260703\n",
      "Iteration 64, loss = 1.22011158\n",
      "Iteration 65, loss = 1.18406217\n",
      "Iteration 66, loss = 1.21498076\n",
      "Iteration 67, loss = 1.19838271\n",
      "Iteration 68, loss = 1.20448478\n",
      "Iteration 69, loss = 1.18936638\n",
      "Iteration 70, loss = 1.18997175\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.51795934\n",
      "Iteration 2, loss = 1.51705042\n",
      "Iteration 3, loss = 1.51563190\n",
      "Iteration 4, loss = 1.51395390\n",
      "Iteration 5, loss = 1.51204054\n",
      "Iteration 6, loss = 1.51006090\n",
      "Iteration 7, loss = 1.50791726\n",
      "Iteration 8, loss = 1.50585063\n",
      "Iteration 9, loss = 1.50365860\n",
      "Iteration 10, loss = 1.50155956\n",
      "Iteration 11, loss = 1.49938411\n",
      "Iteration 12, loss = 1.49722701\n",
      "Iteration 13, loss = 1.49505249\n",
      "Iteration 14, loss = 1.49291987\n",
      "Iteration 15, loss = 1.49084982\n",
      "Iteration 16, loss = 1.48873433\n",
      "Iteration 17, loss = 1.48666366\n",
      "Iteration 18, loss = 1.48461941\n",
      "Iteration 19, loss = 1.48255723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 1.48051503\n",
      "Iteration 21, loss = 1.47854494\n",
      "Iteration 22, loss = 1.47651625\n",
      "Iteration 23, loss = 1.47458308\n",
      "Iteration 24, loss = 1.47256681\n",
      "Iteration 25, loss = 1.47068022\n",
      "Iteration 26, loss = 1.46873689\n",
      "Iteration 27, loss = 1.46685642\n",
      "Iteration 28, loss = 1.46494032\n",
      "Iteration 29, loss = 1.46305436\n",
      "Iteration 30, loss = 1.46121989\n",
      "Iteration 31, loss = 1.45932861\n",
      "Iteration 32, loss = 1.45751655\n",
      "Iteration 33, loss = 1.45571420\n",
      "Iteration 34, loss = 1.45390281\n",
      "Iteration 35, loss = 1.45212043\n",
      "Iteration 36, loss = 1.45036721\n",
      "Iteration 37, loss = 1.44859514\n",
      "Iteration 38, loss = 1.44686068\n",
      "Iteration 39, loss = 1.44514744\n",
      "Iteration 40, loss = 1.44340978\n",
      "Iteration 41, loss = 1.44175367\n",
      "Iteration 42, loss = 1.44001564\n",
      "Iteration 43, loss = 1.43838127\n",
      "Iteration 44, loss = 1.43675793\n",
      "Iteration 45, loss = 1.43508843\n",
      "Iteration 46, loss = 1.43345425\n",
      "Iteration 47, loss = 1.43183976\n",
      "Iteration 48, loss = 1.43024126\n",
      "Iteration 49, loss = 1.42866703\n",
      "Iteration 50, loss = 1.42708343\n",
      "Iteration 51, loss = 1.42553796\n",
      "Iteration 52, loss = 1.42401669\n",
      "Iteration 53, loss = 1.42245557\n",
      "Iteration 54, loss = 1.42094127\n",
      "Iteration 55, loss = 1.41943302\n",
      "Iteration 56, loss = 1.41793309\n",
      "Iteration 57, loss = 1.41646394\n",
      "Iteration 58, loss = 1.41499838\n",
      "Iteration 59, loss = 1.41356350\n",
      "Iteration 60, loss = 1.41209482\n",
      "Iteration 61, loss = 1.41062669\n",
      "Iteration 62, loss = 1.40922488\n",
      "Iteration 63, loss = 1.40783932\n",
      "Iteration 64, loss = 1.40643139\n",
      "Iteration 65, loss = 1.40503486\n",
      "Iteration 66, loss = 1.40370262\n",
      "Iteration 67, loss = 1.40234845\n",
      "Iteration 68, loss = 1.40099742\n",
      "Iteration 69, loss = 1.39963462\n",
      "Iteration 70, loss = 1.39829548\n",
      "Iteration 71, loss = 1.39699229\n",
      "Iteration 72, loss = 1.39573202\n",
      "Iteration 73, loss = 1.39439779\n",
      "Iteration 74, loss = 1.39310628\n",
      "Iteration 75, loss = 1.39185441\n",
      "Iteration 76, loss = 1.39058302\n",
      "Iteration 77, loss = 1.38932133\n",
      "Iteration 78, loss = 1.38810664\n",
      "Iteration 79, loss = 1.38686143\n",
      "Iteration 80, loss = 1.38563443\n",
      "Iteration 81, loss = 1.38444965\n",
      "Iteration 82, loss = 1.38323147\n",
      "Iteration 83, loss = 1.38201235\n",
      "Iteration 84, loss = 1.38086811\n",
      "Iteration 85, loss = 1.37970259\n",
      "Iteration 86, loss = 1.37851855\n",
      "Iteration 87, loss = 1.37736990\n",
      "Iteration 88, loss = 1.37624318\n",
      "Iteration 89, loss = 1.37510405\n",
      "Iteration 90, loss = 1.37399168\n",
      "Iteration 91, loss = 1.37289476\n",
      "Iteration 92, loss = 1.37176033\n",
      "Iteration 93, loss = 1.37067069\n",
      "Iteration 94, loss = 1.36961570\n",
      "Iteration 95, loss = 1.36852283\n",
      "Iteration 96, loss = 1.36743471\n",
      "Iteration 97, loss = 1.36636144\n",
      "Iteration 98, loss = 1.36539367\n",
      "Iteration 99, loss = 1.36430354\n",
      "Iteration 100, loss = 1.36325407\n",
      "Iteration 101, loss = 1.36225524\n",
      "Iteration 102, loss = 1.36122921\n",
      "Iteration 103, loss = 1.36023317\n",
      "Iteration 104, loss = 1.35923078\n",
      "Iteration 105, loss = 1.35824037\n",
      "Iteration 106, loss = 1.35725978\n",
      "Iteration 107, loss = 1.35629092\n",
      "Iteration 108, loss = 1.35531511\n",
      "Iteration 109, loss = 1.35435635\n",
      "Iteration 110, loss = 1.35341828\n",
      "Iteration 111, loss = 1.35247842\n",
      "Iteration 112, loss = 1.35153781\n",
      "Iteration 113, loss = 1.35060690\n",
      "Iteration 114, loss = 1.34967896\n",
      "Iteration 115, loss = 1.34879743\n",
      "Iteration 116, loss = 1.34786441\n",
      "Iteration 117, loss = 1.34699296\n",
      "Iteration 118, loss = 1.34608291\n",
      "Iteration 119, loss = 1.34520933\n",
      "Iteration 120, loss = 1.34435394\n",
      "Iteration 121, loss = 1.34347946\n",
      "Iteration 122, loss = 1.34260326\n",
      "Iteration 123, loss = 1.34174676\n",
      "Iteration 124, loss = 1.34090900\n",
      "Iteration 125, loss = 1.34008132\n",
      "Iteration 126, loss = 1.33922535\n",
      "Iteration 127, loss = 1.33842835\n",
      "Iteration 128, loss = 1.33758486\n",
      "Iteration 129, loss = 1.33679398\n",
      "Iteration 130, loss = 1.33598152\n",
      "Iteration 131, loss = 1.33518500\n",
      "Iteration 132, loss = 1.33438190\n",
      "Iteration 133, loss = 1.33359456\n",
      "Iteration 134, loss = 1.33280305\n",
      "Iteration 135, loss = 1.33205001\n",
      "Iteration 136, loss = 1.33126398\n",
      "Iteration 137, loss = 1.33052051\n",
      "Iteration 138, loss = 1.32976569\n",
      "Iteration 139, loss = 1.32899222\n",
      "Iteration 140, loss = 1.32827389\n",
      "Iteration 141, loss = 1.32752373\n",
      "Iteration 142, loss = 1.32682236\n",
      "Iteration 143, loss = 1.32607031\n",
      "Iteration 144, loss = 1.32534502\n",
      "Iteration 145, loss = 1.32463216\n",
      "Iteration 146, loss = 1.32392316\n",
      "Iteration 147, loss = 1.32322157\n",
      "Iteration 148, loss = 1.32252577\n",
      "Iteration 149, loss = 1.32181129\n",
      "Iteration 150, loss = 1.32115087\n",
      "Iteration 151, loss = 1.32045282\n",
      "Iteration 152, loss = 1.31977969\n",
      "Iteration 153, loss = 1.31910888\n",
      "Iteration 154, loss = 1.31844137\n",
      "Iteration 155, loss = 1.31777454\n",
      "Iteration 156, loss = 1.31712262\n",
      "Iteration 157, loss = 1.31647851\n",
      "Iteration 158, loss = 1.31580415\n",
      "Iteration 159, loss = 1.31518512\n",
      "Iteration 160, loss = 1.31453266\n",
      "Iteration 161, loss = 1.31391649\n",
      "Iteration 162, loss = 1.31330480\n",
      "Iteration 163, loss = 1.31266278\n",
      "Iteration 164, loss = 1.31204052\n",
      "Iteration 165, loss = 1.31142879\n",
      "Iteration 166, loss = 1.31080930\n",
      "Iteration 167, loss = 1.31021408\n",
      "Iteration 168, loss = 1.30961801\n",
      "Iteration 169, loss = 1.30901784\n",
      "Iteration 170, loss = 1.30843917\n",
      "Iteration 171, loss = 1.30784042\n",
      "Iteration 172, loss = 1.30726533\n",
      "Iteration 173, loss = 1.30669133\n",
      "Iteration 174, loss = 1.30611918\n",
      "Iteration 175, loss = 1.30553898\n",
      "Iteration 176, loss = 1.30498591\n",
      "Iteration 177, loss = 1.30442216\n",
      "Iteration 178, loss = 1.30387311\n",
      "Iteration 179, loss = 1.30330146\n",
      "Iteration 180, loss = 1.30277156\n",
      "Iteration 181, loss = 1.30222342\n",
      "Iteration 182, loss = 1.30168383\n",
      "Iteration 183, loss = 1.30114940\n",
      "Iteration 184, loss = 1.30061033\n",
      "Iteration 185, loss = 1.30008050\n",
      "Iteration 186, loss = 1.29959121\n",
      "Iteration 187, loss = 1.29903341\n",
      "Iteration 188, loss = 1.29852517\n",
      "Iteration 189, loss = 1.29801814\n",
      "Iteration 190, loss = 1.29750158\n",
      "Iteration 191, loss = 1.29700331\n",
      "Iteration 192, loss = 1.29650253\n",
      "Iteration 193, loss = 1.29599038\n",
      "Iteration 194, loss = 1.29551111\n",
      "Iteration 195, loss = 1.29502021\n",
      "Iteration 196, loss = 1.29453040\n",
      "Iteration 197, loss = 1.29404232\n",
      "Iteration 198, loss = 1.29355613\n",
      "Iteration 199, loss = 1.29310356\n",
      "Iteration 200, loss = 1.29262336\n",
      "Iteration 201, loss = 1.29214064\n",
      "Iteration 202, loss = 1.29166962\n",
      "Iteration 203, loss = 1.29122574\n",
      "Iteration 204, loss = 1.29075125\n",
      "Iteration 205, loss = 1.29030862\n",
      "Iteration 206, loss = 1.28984257\n",
      "Iteration 207, loss = 1.28940175\n",
      "Iteration 208, loss = 1.28894336\n",
      "Iteration 209, loss = 1.28850831\n",
      "Iteration 210, loss = 1.28806670\n",
      "Iteration 211, loss = 1.28762073\n",
      "Iteration 212, loss = 1.28718214\n",
      "Iteration 213, loss = 1.28676331\n",
      "Iteration 214, loss = 1.28632571\n",
      "Iteration 215, loss = 1.28590593\n",
      "Iteration 216, loss = 1.28547818\n",
      "Iteration 217, loss = 1.28505501\n",
      "Iteration 218, loss = 1.28463667\n",
      "Iteration 219, loss = 1.28422349\n",
      "Iteration 220, loss = 1.28381423\n",
      "Iteration 221, loss = 1.28340798\n",
      "Iteration 222, loss = 1.28300146\n",
      "Iteration 223, loss = 1.28260754\n",
      "Iteration 224, loss = 1.28219605\n",
      "Iteration 225, loss = 1.28178935\n",
      "Iteration 226, loss = 1.28140297\n",
      "Iteration 227, loss = 1.28101076\n",
      "Iteration 228, loss = 1.28061865\n",
      "Iteration 229, loss = 1.28022224\n",
      "Iteration 230, loss = 1.27984890\n",
      "Iteration 231, loss = 1.27947260\n",
      "Iteration 232, loss = 1.27909136\n",
      "Iteration 233, loss = 1.27871344\n",
      "Iteration 234, loss = 1.27833807\n",
      "Iteration 235, loss = 1.27796301\n",
      "Iteration 236, loss = 1.27760006\n",
      "Iteration 237, loss = 1.27723002\n",
      "Iteration 238, loss = 1.27685680\n",
      "Iteration 239, loss = 1.27651052\n",
      "Iteration 240, loss = 1.27615667\n",
      "Iteration 241, loss = 1.27578676\n",
      "Iteration 242, loss = 1.27542823\n",
      "Iteration 243, loss = 1.27507299\n",
      "Iteration 244, loss = 1.27473186\n",
      "Iteration 245, loss = 1.27438109\n",
      "Iteration 246, loss = 1.27404569\n",
      "Iteration 247, loss = 1.27370265\n",
      "Iteration 248, loss = 1.27334629\n",
      "Iteration 249, loss = 1.27302004\n",
      "Iteration 250, loss = 1.27267387\n",
      "Iteration 251, loss = 1.27234888\n",
      "Iteration 252, loss = 1.27202466\n",
      "Iteration 253, loss = 1.27169023\n",
      "Iteration 254, loss = 1.27135332\n",
      "Iteration 255, loss = 1.27103428\n",
      "Iteration 256, loss = 1.27071111\n",
      "Iteration 257, loss = 1.27038769\n",
      "Iteration 258, loss = 1.27006734\n",
      "Iteration 259, loss = 1.26974912\n",
      "Iteration 260, loss = 1.26943033\n",
      "Iteration 261, loss = 1.26911946\n",
      "Iteration 262, loss = 1.26880444\n",
      "Iteration 263, loss = 1.26850310\n",
      "Iteration 264, loss = 1.26820522\n",
      "Iteration 265, loss = 1.26788747\n",
      "Iteration 266, loss = 1.26757698\n",
      "Iteration 267, loss = 1.26728108\n",
      "Iteration 268, loss = 1.26697511\n",
      "Iteration 269, loss = 1.26668877\n",
      "Iteration 270, loss = 1.26638543\n",
      "Iteration 271, loss = 1.26608049\n",
      "Iteration 272, loss = 1.26579687\n",
      "Iteration 273, loss = 1.26550520\n",
      "Iteration 274, loss = 1.26521969\n",
      "Iteration 275, loss = 1.26493033\n",
      "Iteration 276, loss = 1.26465009\n",
      "Iteration 277, loss = 1.26436548\n",
      "Iteration 278, loss = 1.26407172\n",
      "Iteration 279, loss = 1.26379451\n",
      "Iteration 280, loss = 1.26351992\n",
      "Iteration 281, loss = 1.26323351\n",
      "Iteration 282, loss = 1.26296400\n",
      "Iteration 283, loss = 1.26268465\n",
      "Iteration 284, loss = 1.26240851\n",
      "Iteration 285, loss = 1.26214639\n",
      "Iteration 286, loss = 1.26186826\n",
      "Iteration 287, loss = 1.26160554\n",
      "Iteration 288, loss = 1.26134791\n",
      "Iteration 289, loss = 1.26107528\n",
      "Iteration 290, loss = 1.26082387\n",
      "Iteration 291, loss = 1.26054115\n",
      "Iteration 292, loss = 1.26030330\n",
      "Iteration 293, loss = 1.26003250\n",
      "Iteration 294, loss = 1.25976475\n",
      "Iteration 295, loss = 1.25952140\n",
      "Iteration 296, loss = 1.25926350\n",
      "Iteration 297, loss = 1.25900695\n",
      "Iteration 298, loss = 1.25877434\n",
      "Iteration 299, loss = 1.25851976\n",
      "Iteration 300, loss = 1.25825282\n",
      "Iteration 1, loss = 1.51819130\n",
      "Iteration 2, loss = 1.51727358\n",
      "Iteration 3, loss = 1.51587303\n",
      "Iteration 4, loss = 1.51420044\n",
      "Iteration 5, loss = 1.51230362\n",
      "Iteration 6, loss = 1.51030071\n",
      "Iteration 7, loss = 1.50819191\n",
      "Iteration 8, loss = 1.50609928\n",
      "Iteration 9, loss = 1.50394512\n",
      "Iteration 10, loss = 1.50182610\n",
      "Iteration 11, loss = 1.49965659\n",
      "Iteration 12, loss = 1.49752529\n",
      "Iteration 13, loss = 1.49534804\n",
      "Iteration 14, loss = 1.49327593\n",
      "Iteration 15, loss = 1.49114649\n",
      "Iteration 16, loss = 1.48905235\n",
      "Iteration 17, loss = 1.48700325\n",
      "Iteration 18, loss = 1.48493274\n",
      "Iteration 19, loss = 1.48291566\n",
      "Iteration 20, loss = 1.48087592\n",
      "Iteration 21, loss = 1.47889326\n",
      "Iteration 22, loss = 1.47688986\n",
      "Iteration 23, loss = 1.47493448\n",
      "Iteration 24, loss = 1.47295349\n",
      "Iteration 25, loss = 1.47102541\n",
      "Iteration 26, loss = 1.46911937\n",
      "Iteration 27, loss = 1.46725943\n",
      "Iteration 28, loss = 1.46530053\n",
      "Iteration 29, loss = 1.46344730\n",
      "Iteration 30, loss = 1.46161328\n",
      "Iteration 31, loss = 1.45973573\n",
      "Iteration 32, loss = 1.45789718\n",
      "Iteration 33, loss = 1.45609640\n",
      "Iteration 34, loss = 1.45434371\n",
      "Iteration 35, loss = 1.45253964\n",
      "Iteration 36, loss = 1.45077171\n",
      "Iteration 37, loss = 1.44900855\n",
      "Iteration 38, loss = 1.44730896\n",
      "Iteration 39, loss = 1.44556116\n",
      "Iteration 40, loss = 1.44385118\n",
      "Iteration 41, loss = 1.44218454\n",
      "Iteration 42, loss = 1.44046367\n",
      "Iteration 43, loss = 1.43879942\n",
      "Iteration 44, loss = 1.43719807\n",
      "Iteration 45, loss = 1.43554439\n",
      "Iteration 46, loss = 1.43390499\n",
      "Iteration 47, loss = 1.43228597\n",
      "Iteration 48, loss = 1.43070776\n",
      "Iteration 49, loss = 1.42914018\n",
      "Iteration 50, loss = 1.42752924\n",
      "Iteration 51, loss = 1.42598308\n",
      "Iteration 52, loss = 1.42446093\n",
      "Iteration 53, loss = 1.42291117\n",
      "Iteration 54, loss = 1.42140855\n",
      "Iteration 55, loss = 1.41989870\n",
      "Iteration 56, loss = 1.41839660\n",
      "Iteration 57, loss = 1.41693592\n",
      "Iteration 58, loss = 1.41546219\n",
      "Iteration 59, loss = 1.41401256\n",
      "Iteration 60, loss = 1.41256988\n",
      "Iteration 61, loss = 1.41109985\n",
      "Iteration 62, loss = 1.40968512\n",
      "Iteration 63, loss = 1.40830564\n",
      "Iteration 64, loss = 1.40690038\n",
      "Iteration 65, loss = 1.40548069\n",
      "Iteration 66, loss = 1.40415226\n",
      "Iteration 67, loss = 1.40279734\n",
      "Iteration 68, loss = 1.40145779\n",
      "Iteration 69, loss = 1.40006433\n",
      "Iteration 70, loss = 1.39876576\n",
      "Iteration 71, loss = 1.39745390\n",
      "Iteration 72, loss = 1.39615932\n",
      "Iteration 73, loss = 1.39485511\n",
      "Iteration 74, loss = 1.39355865\n",
      "Iteration 75, loss = 1.39229259\n",
      "Iteration 76, loss = 1.39102155\n",
      "Iteration 77, loss = 1.38979339\n",
      "Iteration 78, loss = 1.38857708\n",
      "Iteration 79, loss = 1.38730621\n",
      "Iteration 80, loss = 1.38609531\n",
      "Iteration 81, loss = 1.38489059\n",
      "Iteration 82, loss = 1.38369281\n",
      "Iteration 83, loss = 1.38247403\n",
      "Iteration 84, loss = 1.38132876\n",
      "Iteration 85, loss = 1.38015832\n",
      "Iteration 86, loss = 1.37899404\n",
      "Iteration 87, loss = 1.37784180\n",
      "Iteration 88, loss = 1.37670816\n",
      "Iteration 89, loss = 1.37556786\n",
      "Iteration 90, loss = 1.37444069\n",
      "Iteration 91, loss = 1.37335825\n",
      "Iteration 92, loss = 1.37224059\n",
      "Iteration 93, loss = 1.37115582\n",
      "Iteration 94, loss = 1.37006517\n",
      "Iteration 95, loss = 1.36901861\n",
      "Iteration 96, loss = 1.36790523\n",
      "Iteration 97, loss = 1.36686077\n",
      "Iteration 98, loss = 1.36584592\n",
      "Iteration 99, loss = 1.36479123\n",
      "Iteration 100, loss = 1.36373726\n",
      "Iteration 101, loss = 1.36273747\n",
      "Iteration 102, loss = 1.36171948\n",
      "Iteration 103, loss = 1.36070169\n",
      "Iteration 104, loss = 1.35971738\n",
      "Iteration 105, loss = 1.35874340\n",
      "Iteration 106, loss = 1.35774883\n",
      "Iteration 107, loss = 1.35677280\n",
      "Iteration 108, loss = 1.35581797\n",
      "Iteration 109, loss = 1.35486922\n",
      "Iteration 110, loss = 1.35393579\n",
      "Iteration 111, loss = 1.35295482\n",
      "Iteration 112, loss = 1.35202602\n",
      "Iteration 113, loss = 1.35111299\n",
      "Iteration 114, loss = 1.35017973\n",
      "Iteration 115, loss = 1.34929679\n",
      "Iteration 116, loss = 1.34836229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 117, loss = 1.34749179\n",
      "Iteration 118, loss = 1.34658287\n",
      "Iteration 119, loss = 1.34570628\n",
      "Iteration 120, loss = 1.34484392\n",
      "Iteration 121, loss = 1.34398397\n",
      "Iteration 122, loss = 1.34310832\n",
      "Iteration 123, loss = 1.34223322\n",
      "Iteration 124, loss = 1.34141580\n",
      "Iteration 125, loss = 1.34057366\n",
      "Iteration 126, loss = 1.33973060\n",
      "Iteration 127, loss = 1.33892575\n",
      "Iteration 128, loss = 1.33809081\n",
      "Iteration 129, loss = 1.33728939\n",
      "Iteration 130, loss = 1.33646709\n",
      "Iteration 131, loss = 1.33568700\n",
      "Iteration 132, loss = 1.33487903\n",
      "Iteration 133, loss = 1.33408951\n",
      "Iteration 134, loss = 1.33330347\n",
      "Iteration 135, loss = 1.33252566\n",
      "Iteration 136, loss = 1.33176459\n",
      "Iteration 137, loss = 1.33100142\n",
      "Iteration 138, loss = 1.33026045\n",
      "Iteration 139, loss = 1.32948828\n",
      "Iteration 140, loss = 1.32877348\n",
      "Iteration 141, loss = 1.32799759\n",
      "Iteration 142, loss = 1.32730527\n",
      "Iteration 143, loss = 1.32654594\n",
      "Iteration 144, loss = 1.32583998\n",
      "Iteration 145, loss = 1.32511387\n",
      "Iteration 146, loss = 1.32441035\n",
      "Iteration 147, loss = 1.32370161\n",
      "Iteration 148, loss = 1.32299598\n",
      "Iteration 149, loss = 1.32229641\n",
      "Iteration 150, loss = 1.32162594\n",
      "Iteration 151, loss = 1.32093640\n",
      "Iteration 152, loss = 1.32026743\n",
      "Iteration 153, loss = 1.31958753\n",
      "Iteration 154, loss = 1.31892017\n",
      "Iteration 155, loss = 1.31826902\n",
      "Iteration 156, loss = 1.31760618\n",
      "Iteration 157, loss = 1.31694886\n",
      "Iteration 158, loss = 1.31629202\n",
      "Iteration 159, loss = 1.31564468\n",
      "Iteration 160, loss = 1.31501760\n",
      "Iteration 161, loss = 1.31439741\n",
      "Iteration 162, loss = 1.31376581\n",
      "Iteration 163, loss = 1.31313866\n",
      "Iteration 164, loss = 1.31251368\n",
      "Iteration 165, loss = 1.31191112\n",
      "Iteration 166, loss = 1.31127406\n",
      "Iteration 167, loss = 1.31068174\n",
      "Iteration 168, loss = 1.31009836\n",
      "Iteration 169, loss = 1.30948524\n",
      "Iteration 170, loss = 1.30890608\n",
      "Iteration 171, loss = 1.30831364\n",
      "Iteration 172, loss = 1.30773626\n",
      "Iteration 173, loss = 1.30715298\n",
      "Iteration 174, loss = 1.30658668\n",
      "Iteration 175, loss = 1.30599542\n",
      "Iteration 176, loss = 1.30544603\n",
      "Iteration 177, loss = 1.30488475\n",
      "Iteration 178, loss = 1.30431699\n",
      "Iteration 179, loss = 1.30377691\n",
      "Iteration 180, loss = 1.30322139\n",
      "Iteration 181, loss = 1.30268608\n",
      "Iteration 182, loss = 1.30212880\n",
      "Iteration 183, loss = 1.30160210\n",
      "Iteration 184, loss = 1.30106408\n",
      "Iteration 185, loss = 1.30053443\n",
      "Iteration 186, loss = 1.30003049\n",
      "Iteration 187, loss = 1.29947265\n",
      "Iteration 188, loss = 1.29897014\n",
      "Iteration 189, loss = 1.29846527\n",
      "Iteration 190, loss = 1.29794708\n",
      "Iteration 191, loss = 1.29745344\n",
      "Iteration 192, loss = 1.29693784\n",
      "Iteration 193, loss = 1.29643938\n",
      "Iteration 194, loss = 1.29594164\n",
      "Iteration 195, loss = 1.29546028\n",
      "Iteration 196, loss = 1.29497332\n",
      "Iteration 197, loss = 1.29447380\n",
      "Iteration 198, loss = 1.29398767\n",
      "Iteration 199, loss = 1.29353702\n",
      "Iteration 200, loss = 1.29304013\n",
      "Iteration 201, loss = 1.29256798\n",
      "Iteration 202, loss = 1.29210731\n",
      "Iteration 203, loss = 1.29164308\n",
      "Iteration 204, loss = 1.29117278\n",
      "Iteration 205, loss = 1.29072407\n",
      "Iteration 206, loss = 1.29026072\n",
      "Iteration 207, loss = 1.28982385\n",
      "Iteration 208, loss = 1.28936143\n",
      "Iteration 209, loss = 1.28891284\n",
      "Iteration 210, loss = 1.28849311\n",
      "Iteration 211, loss = 1.28803746\n",
      "Iteration 212, loss = 1.28758943\n",
      "Iteration 213, loss = 1.28717110\n",
      "Iteration 214, loss = 1.28672751\n",
      "Iteration 215, loss = 1.28631440\n",
      "Iteration 216, loss = 1.28589289\n",
      "Iteration 217, loss = 1.28546807\n",
      "Iteration 218, loss = 1.28505308\n",
      "Iteration 219, loss = 1.28462348\n",
      "Iteration 220, loss = 1.28423686\n",
      "Iteration 221, loss = 1.28381450\n",
      "Iteration 222, loss = 1.28341800\n",
      "Iteration 223, loss = 1.28299650\n",
      "Iteration 224, loss = 1.28260808\n",
      "Iteration 225, loss = 1.28219327\n",
      "Iteration 226, loss = 1.28181562\n",
      "Iteration 227, loss = 1.28141630\n",
      "Iteration 228, loss = 1.28102586\n",
      "Iteration 229, loss = 1.28064072\n",
      "Iteration 230, loss = 1.28023769\n",
      "Iteration 231, loss = 1.27987901\n",
      "Iteration 232, loss = 1.27948594\n",
      "Iteration 233, loss = 1.27911558\n",
      "Iteration 234, loss = 1.27873965\n",
      "Iteration 235, loss = 1.27835947\n",
      "Iteration 236, loss = 1.27799979\n",
      "Iteration 237, loss = 1.27762769\n",
      "Iteration 238, loss = 1.27725703\n",
      "Iteration 239, loss = 1.27689840\n",
      "Iteration 240, loss = 1.27654532\n",
      "Iteration 241, loss = 1.27617573\n",
      "Iteration 242, loss = 1.27582390\n",
      "Iteration 243, loss = 1.27546694\n",
      "Iteration 244, loss = 1.27512559\n",
      "Iteration 245, loss = 1.27476836\n",
      "Iteration 246, loss = 1.27443498\n",
      "Iteration 247, loss = 1.27409778\n",
      "Iteration 248, loss = 1.27374043\n",
      "Iteration 249, loss = 1.27340957\n",
      "Iteration 250, loss = 1.27305848\n",
      "Iteration 251, loss = 1.27273717\n",
      "Iteration 252, loss = 1.27240067\n",
      "Iteration 253, loss = 1.27208497\n",
      "Iteration 254, loss = 1.27173811\n",
      "Iteration 255, loss = 1.27142100\n",
      "Iteration 256, loss = 1.27109477\n",
      "Iteration 257, loss = 1.27076888\n",
      "Iteration 258, loss = 1.27044968\n",
      "Iteration 259, loss = 1.27012745\n",
      "Iteration 260, loss = 1.26981340\n",
      "Iteration 261, loss = 1.26949594\n",
      "Iteration 262, loss = 1.26918569\n",
      "Iteration 263, loss = 1.26888806\n",
      "Iteration 264, loss = 1.26858470\n",
      "Iteration 265, loss = 1.26826299\n",
      "Iteration 266, loss = 1.26794966\n",
      "Iteration 267, loss = 1.26765115\n",
      "Iteration 268, loss = 1.26734540\n",
      "Iteration 269, loss = 1.26705599\n",
      "Iteration 270, loss = 1.26675673\n",
      "Iteration 271, loss = 1.26645702\n",
      "Iteration 272, loss = 1.26617233\n",
      "Iteration 273, loss = 1.26587483\n",
      "Iteration 274, loss = 1.26559044\n",
      "Iteration 275, loss = 1.26529461\n",
      "Iteration 276, loss = 1.26501072\n",
      "Iteration 277, loss = 1.26473849\n",
      "Iteration 278, loss = 1.26443964\n",
      "Iteration 279, loss = 1.26415145\n",
      "Iteration 280, loss = 1.26388501\n",
      "Iteration 281, loss = 1.26360053\n",
      "Iteration 282, loss = 1.26333247\n",
      "Iteration 283, loss = 1.26305373\n",
      "Iteration 284, loss = 1.26277705\n",
      "Iteration 285, loss = 1.26250689\n",
      "Iteration 286, loss = 1.26222844\n",
      "Iteration 287, loss = 1.26196999\n",
      "Iteration 288, loss = 1.26170524\n",
      "Iteration 289, loss = 1.26144268\n",
      "Iteration 290, loss = 1.26117957\n",
      "Iteration 291, loss = 1.26089752\n",
      "Iteration 292, loss = 1.26065459\n",
      "Iteration 293, loss = 1.26039439\n",
      "Iteration 294, loss = 1.26012440\n",
      "Iteration 295, loss = 1.25987626\n",
      "Iteration 296, loss = 1.25961629\n",
      "Iteration 297, loss = 1.25936286\n",
      "Iteration 298, loss = 1.25911960\n",
      "Iteration 299, loss = 1.25886801\n",
      "Iteration 300, loss = 1.25860854\n",
      "Iteration 1, loss = 1.51839966\n",
      "Iteration 2, loss = 1.51747517\n",
      "Iteration 3, loss = 1.51608259\n",
      "Iteration 4, loss = 1.51442186\n",
      "Iteration 5, loss = 1.51247442\n",
      "Iteration 6, loss = 1.51046279\n",
      "Iteration 7, loss = 1.50839139\n",
      "Iteration 8, loss = 1.50626096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 1.50409802\n",
      "Iteration 10, loss = 1.50194993\n",
      "Iteration 11, loss = 1.49979671\n",
      "Iteration 12, loss = 1.49766518\n",
      "Iteration 13, loss = 1.49549599\n",
      "Iteration 14, loss = 1.49341029\n",
      "Iteration 15, loss = 1.49124916\n",
      "Iteration 16, loss = 1.48917105\n",
      "Iteration 17, loss = 1.48710057\n",
      "Iteration 18, loss = 1.48503989\n",
      "Iteration 19, loss = 1.48301407\n",
      "Iteration 20, loss = 1.48095911\n",
      "Iteration 21, loss = 1.47897518\n",
      "Iteration 22, loss = 1.47695910\n",
      "Iteration 23, loss = 1.47498361\n",
      "Iteration 24, loss = 1.47303393\n",
      "Iteration 25, loss = 1.47113600\n",
      "Iteration 26, loss = 1.46918206\n",
      "Iteration 27, loss = 1.46729072\n",
      "Iteration 28, loss = 1.46534281\n",
      "Iteration 29, loss = 1.46350391\n",
      "Iteration 30, loss = 1.46164176\n",
      "Iteration 31, loss = 1.45978543\n",
      "Iteration 32, loss = 1.45793439\n",
      "Iteration 33, loss = 1.45613312\n",
      "Iteration 34, loss = 1.45433746\n",
      "Iteration 35, loss = 1.45257448\n",
      "Iteration 36, loss = 1.45079761\n",
      "Iteration 37, loss = 1.44901622\n",
      "Iteration 38, loss = 1.44730687\n",
      "Iteration 39, loss = 1.44555391\n",
      "Iteration 40, loss = 1.44385195\n",
      "Iteration 41, loss = 1.44216914\n",
      "Iteration 42, loss = 1.44043353\n",
      "Iteration 43, loss = 1.43881153\n",
      "Iteration 44, loss = 1.43715580\n",
      "Iteration 45, loss = 1.43549423\n",
      "Iteration 46, loss = 1.43387165\n",
      "Iteration 47, loss = 1.43227372\n",
      "Iteration 48, loss = 1.43067602\n",
      "Iteration 49, loss = 1.42910244\n",
      "Iteration 50, loss = 1.42746879\n",
      "Iteration 51, loss = 1.42595244\n",
      "Iteration 52, loss = 1.42441580\n",
      "Iteration 53, loss = 1.42286224\n",
      "Iteration 54, loss = 1.42132043\n",
      "Iteration 55, loss = 1.41982726\n",
      "Iteration 56, loss = 1.41835834\n",
      "Iteration 57, loss = 1.41688052\n",
      "Iteration 58, loss = 1.41538244\n",
      "Iteration 59, loss = 1.41395460\n",
      "Iteration 60, loss = 1.41251118\n",
      "Iteration 61, loss = 1.41103963\n",
      "Iteration 62, loss = 1.40963081\n",
      "Iteration 63, loss = 1.40824102\n",
      "Iteration 64, loss = 1.40683220\n",
      "Iteration 65, loss = 1.40541026\n",
      "Iteration 66, loss = 1.40406461\n",
      "Iteration 67, loss = 1.40270575\n",
      "Iteration 68, loss = 1.40137085\n",
      "Iteration 69, loss = 1.39999044\n",
      "Iteration 70, loss = 1.39866642\n",
      "Iteration 71, loss = 1.39737614\n",
      "Iteration 72, loss = 1.39604320\n",
      "Iteration 73, loss = 1.39474016\n",
      "Iteration 74, loss = 1.39346441\n",
      "Iteration 75, loss = 1.39214891\n",
      "Iteration 76, loss = 1.39090465\n",
      "Iteration 77, loss = 1.38965859\n",
      "Iteration 78, loss = 1.38842628\n",
      "Iteration 79, loss = 1.38718478\n",
      "Iteration 80, loss = 1.38595845\n",
      "Iteration 81, loss = 1.38472681\n",
      "Iteration 82, loss = 1.38354390\n",
      "Iteration 83, loss = 1.38232017\n",
      "Iteration 84, loss = 1.38114498\n",
      "Iteration 85, loss = 1.37997364\n",
      "Iteration 86, loss = 1.37881361\n",
      "Iteration 87, loss = 1.37767671\n",
      "Iteration 88, loss = 1.37652284\n",
      "Iteration 89, loss = 1.37536751\n",
      "Iteration 90, loss = 1.37425283\n",
      "Iteration 91, loss = 1.37312607\n",
      "Iteration 92, loss = 1.37202463\n",
      "Iteration 93, loss = 1.37094988\n",
      "Iteration 94, loss = 1.36983766\n",
      "Iteration 95, loss = 1.36878059\n",
      "Iteration 96, loss = 1.36770451\n",
      "Iteration 97, loss = 1.36661669\n",
      "Iteration 98, loss = 1.36559708\n",
      "Iteration 99, loss = 1.36454379\n",
      "Iteration 100, loss = 1.36349695\n",
      "Iteration 101, loss = 1.36250014\n",
      "Iteration 102, loss = 1.36147002\n",
      "Iteration 103, loss = 1.36042860\n",
      "Iteration 104, loss = 1.35944287\n",
      "Iteration 105, loss = 1.35846620\n",
      "Iteration 106, loss = 1.35748029\n",
      "Iteration 107, loss = 1.35649023\n",
      "Iteration 108, loss = 1.35553235\n",
      "Iteration 109, loss = 1.35458145\n",
      "Iteration 110, loss = 1.35361147\n",
      "Iteration 111, loss = 1.35263286\n",
      "Iteration 112, loss = 1.35170413\n",
      "Iteration 113, loss = 1.35077143\n",
      "Iteration 114, loss = 1.34984976\n",
      "Iteration 115, loss = 1.34896545\n",
      "Iteration 116, loss = 1.34802728\n",
      "Iteration 117, loss = 1.34712680\n",
      "Iteration 118, loss = 1.34622642\n",
      "Iteration 119, loss = 1.34535320\n",
      "Iteration 120, loss = 1.34447466\n",
      "Iteration 121, loss = 1.34360810\n",
      "Iteration 122, loss = 1.34272822\n",
      "Iteration 123, loss = 1.34186075\n",
      "Iteration 124, loss = 1.34103223\n",
      "Iteration 125, loss = 1.34016644\n",
      "Iteration 126, loss = 1.33933763\n",
      "Iteration 127, loss = 1.33852562\n",
      "Iteration 128, loss = 1.33766862\n",
      "Iteration 129, loss = 1.33687385\n",
      "Iteration 130, loss = 1.33607001\n",
      "Iteration 131, loss = 1.33524016\n",
      "Iteration 132, loss = 1.33444438\n",
      "Iteration 133, loss = 1.33365735\n",
      "Iteration 134, loss = 1.33286551\n",
      "Iteration 135, loss = 1.33208937\n",
      "Iteration 136, loss = 1.33130768\n",
      "Iteration 137, loss = 1.33053248\n",
      "Iteration 138, loss = 1.32980453\n",
      "Iteration 139, loss = 1.32902412\n",
      "Iteration 140, loss = 1.32829496\n",
      "Iteration 141, loss = 1.32751461\n",
      "Iteration 142, loss = 1.32681222\n",
      "Iteration 143, loss = 1.32606644\n",
      "Iteration 144, loss = 1.32533985\n",
      "Iteration 145, loss = 1.32460673\n",
      "Iteration 146, loss = 1.32388581\n",
      "Iteration 147, loss = 1.32320605\n",
      "Iteration 148, loss = 1.32248416\n",
      "Iteration 149, loss = 1.32176710\n",
      "Iteration 150, loss = 1.32108876\n",
      "Iteration 151, loss = 1.32040208\n",
      "Iteration 152, loss = 1.31970962\n",
      "Iteration 153, loss = 1.31903811\n",
      "Iteration 154, loss = 1.31836552\n",
      "Iteration 155, loss = 1.31769328\n",
      "Iteration 156, loss = 1.31702621\n",
      "Iteration 157, loss = 1.31637886\n",
      "Iteration 158, loss = 1.31570097\n",
      "Iteration 159, loss = 1.31506265\n",
      "Iteration 160, loss = 1.31442575\n",
      "Iteration 161, loss = 1.31378081\n",
      "Iteration 162, loss = 1.31314691\n",
      "Iteration 163, loss = 1.31251975\n",
      "Iteration 164, loss = 1.31187911\n",
      "Iteration 165, loss = 1.31128049\n",
      "Iteration 166, loss = 1.31064055\n",
      "Iteration 167, loss = 1.31004197\n",
      "Iteration 168, loss = 1.30943683\n",
      "Iteration 169, loss = 1.30884291\n",
      "Iteration 170, loss = 1.30823595\n",
      "Iteration 171, loss = 1.30763632\n",
      "Iteration 172, loss = 1.30706306\n",
      "Iteration 173, loss = 1.30647060\n",
      "Iteration 174, loss = 1.30589022\n",
      "Iteration 175, loss = 1.30531152\n",
      "Iteration 176, loss = 1.30475113\n",
      "Iteration 177, loss = 1.30417916\n",
      "Iteration 178, loss = 1.30361122\n",
      "Iteration 179, loss = 1.30305611\n",
      "Iteration 180, loss = 1.30248935\n",
      "Iteration 181, loss = 1.30196056\n",
      "Iteration 182, loss = 1.30139227\n",
      "Iteration 183, loss = 1.30085437\n",
      "Iteration 184, loss = 1.30032315\n",
      "Iteration 185, loss = 1.29977983\n",
      "Iteration 186, loss = 1.29927242\n",
      "Iteration 187, loss = 1.29871753\n",
      "Iteration 188, loss = 1.29820328\n",
      "Iteration 189, loss = 1.29769750\n",
      "Iteration 190, loss = 1.29717058\n",
      "Iteration 191, loss = 1.29666406\n",
      "Iteration 192, loss = 1.29615742\n",
      "Iteration 193, loss = 1.29565571\n",
      "Iteration 194, loss = 1.29514701\n",
      "Iteration 195, loss = 1.29467313\n",
      "Iteration 196, loss = 1.29417216\n",
      "Iteration 197, loss = 1.29368194\n",
      "Iteration 198, loss = 1.29318102\n",
      "Iteration 199, loss = 1.29271281\n",
      "Iteration 200, loss = 1.29221654\n",
      "Iteration 201, loss = 1.29176424\n",
      "Iteration 202, loss = 1.29129064\n",
      "Iteration 203, loss = 1.29081916\n",
      "Iteration 204, loss = 1.29034339\n",
      "Iteration 205, loss = 1.28989511\n",
      "Iteration 206, loss = 1.28941708\n",
      "Iteration 207, loss = 1.28897595\n",
      "Iteration 208, loss = 1.28852122\n",
      "Iteration 209, loss = 1.28805625\n",
      "Iteration 210, loss = 1.28762007\n",
      "Iteration 211, loss = 1.28718548\n",
      "Iteration 212, loss = 1.28672593\n",
      "Iteration 213, loss = 1.28631300\n",
      "Iteration 214, loss = 1.28585147\n",
      "Iteration 215, loss = 1.28543090\n",
      "Iteration 216, loss = 1.28500877\n",
      "Iteration 217, loss = 1.28457786\n",
      "Iteration 218, loss = 1.28414614\n",
      "Iteration 219, loss = 1.28373622\n",
      "Iteration 220, loss = 1.28332664\n",
      "Iteration 221, loss = 1.28290113\n",
      "Iteration 222, loss = 1.28250177\n",
      "Iteration 223, loss = 1.28207790\n",
      "Iteration 224, loss = 1.28167707\n",
      "Iteration 225, loss = 1.28126330\n",
      "Iteration 226, loss = 1.28088258\n",
      "Iteration 227, loss = 1.28048241\n",
      "Iteration 228, loss = 1.28008195\n",
      "Iteration 229, loss = 1.27968875\n",
      "Iteration 230, loss = 1.27929794\n",
      "Iteration 231, loss = 1.27891553\n",
      "Iteration 232, loss = 1.27851884\n",
      "Iteration 233, loss = 1.27814531\n",
      "Iteration 234, loss = 1.27777269\n",
      "Iteration 235, loss = 1.27738922\n",
      "Iteration 236, loss = 1.27702490\n",
      "Iteration 237, loss = 1.27664807\n",
      "Iteration 238, loss = 1.27627188\n",
      "Iteration 239, loss = 1.27589683\n",
      "Iteration 240, loss = 1.27554597\n",
      "Iteration 241, loss = 1.27517604\n",
      "Iteration 242, loss = 1.27482029\n",
      "Iteration 243, loss = 1.27446364\n",
      "Iteration 244, loss = 1.27411492\n",
      "Iteration 245, loss = 1.27375028\n",
      "Iteration 246, loss = 1.27341146\n",
      "Iteration 247, loss = 1.27305139\n",
      "Iteration 248, loss = 1.27271675\n",
      "Iteration 249, loss = 1.27237304\n",
      "Iteration 250, loss = 1.27202057\n",
      "Iteration 251, loss = 1.27168909\n",
      "Iteration 252, loss = 1.27135101\n",
      "Iteration 253, loss = 1.27102276\n",
      "Iteration 254, loss = 1.27067944\n",
      "Iteration 255, loss = 1.27035659\n",
      "Iteration 256, loss = 1.27003119\n",
      "Iteration 257, loss = 1.26969999\n",
      "Iteration 258, loss = 1.26938272\n",
      "Iteration 259, loss = 1.26903830\n",
      "Iteration 260, loss = 1.26872820\n",
      "Iteration 261, loss = 1.26841452\n",
      "Iteration 262, loss = 1.26809271\n",
      "Iteration 263, loss = 1.26777627\n",
      "Iteration 264, loss = 1.26747722\n",
      "Iteration 265, loss = 1.26715630\n",
      "Iteration 266, loss = 1.26684326\n",
      "Iteration 267, loss = 1.26653767\n",
      "Iteration 268, loss = 1.26622835\n",
      "Iteration 269, loss = 1.26593935\n",
      "Iteration 270, loss = 1.26562982\n",
      "Iteration 271, loss = 1.26533696\n",
      "Iteration 272, loss = 1.26503459\n",
      "Iteration 273, loss = 1.26473412\n",
      "Iteration 274, loss = 1.26444366\n",
      "Iteration 275, loss = 1.26415134\n",
      "Iteration 276, loss = 1.26385589\n",
      "Iteration 277, loss = 1.26357789\n",
      "Iteration 278, loss = 1.26327748\n",
      "Iteration 279, loss = 1.26299150\n",
      "Iteration 280, loss = 1.26271466\n",
      "Iteration 281, loss = 1.26242459\n",
      "Iteration 282, loss = 1.26215545\n",
      "Iteration 283, loss = 1.26187131\n",
      "Iteration 284, loss = 1.26158655\n",
      "Iteration 285, loss = 1.26132288\n",
      "Iteration 286, loss = 1.26103303\n",
      "Iteration 287, loss = 1.26077508\n",
      "Iteration 288, loss = 1.26049812\n",
      "Iteration 289, loss = 1.26022695\n",
      "Iteration 290, loss = 1.25995689\n",
      "Iteration 291, loss = 1.25968529\n",
      "Iteration 292, loss = 1.25943832\n",
      "Iteration 293, loss = 1.25916364\n",
      "Iteration 294, loss = 1.25889687\n",
      "Iteration 295, loss = 1.25864780\n",
      "Iteration 296, loss = 1.25837525\n",
      "Iteration 297, loss = 1.25812303\n",
      "Iteration 298, loss = 1.25787920\n",
      "Iteration 299, loss = 1.25760723\n",
      "Iteration 300, loss = 1.25736003\n",
      "Iteration 1, loss = 1.51851606\n",
      "Iteration 2, loss = 1.51760970\n",
      "Iteration 3, loss = 1.51621518\n",
      "Iteration 4, loss = 1.51454296\n",
      "Iteration 5, loss = 1.51262134\n",
      "Iteration 6, loss = 1.51063123\n",
      "Iteration 7, loss = 1.50857347\n",
      "Iteration 8, loss = 1.50643161\n",
      "Iteration 9, loss = 1.50432164\n",
      "Iteration 10, loss = 1.50215518\n",
      "Iteration 11, loss = 1.50001243\n",
      "Iteration 12, loss = 1.49789324\n",
      "Iteration 13, loss = 1.49575912\n",
      "Iteration 14, loss = 1.49369717\n",
      "Iteration 15, loss = 1.49152397\n",
      "Iteration 16, loss = 1.48947489\n",
      "Iteration 17, loss = 1.48742341\n",
      "Iteration 18, loss = 1.48534480\n",
      "Iteration 19, loss = 1.48337952\n",
      "Iteration 20, loss = 1.48130122\n",
      "Iteration 21, loss = 1.47933459\n",
      "Iteration 22, loss = 1.47738676\n",
      "Iteration 23, loss = 1.47536600\n",
      "Iteration 24, loss = 1.47345225\n",
      "Iteration 25, loss = 1.47152835\n",
      "Iteration 26, loss = 1.46961255\n",
      "Iteration 27, loss = 1.46773043\n",
      "Iteration 28, loss = 1.46583604\n",
      "Iteration 29, loss = 1.46397202\n",
      "Iteration 30, loss = 1.46213209\n",
      "Iteration 31, loss = 1.46028861\n",
      "Iteration 32, loss = 1.45845547\n",
      "Iteration 33, loss = 1.45664042\n",
      "Iteration 34, loss = 1.45490675\n",
      "Iteration 35, loss = 1.45311841\n",
      "Iteration 36, loss = 1.45133199\n",
      "Iteration 37, loss = 1.44959622\n",
      "Iteration 38, loss = 1.44790936\n",
      "Iteration 39, loss = 1.44618575\n",
      "Iteration 40, loss = 1.44449637\n",
      "Iteration 41, loss = 1.44276309\n",
      "Iteration 42, loss = 1.44109989\n",
      "Iteration 43, loss = 1.43946330\n",
      "Iteration 44, loss = 1.43782113\n",
      "Iteration 45, loss = 1.43618150\n",
      "Iteration 46, loss = 1.43458049\n",
      "Iteration 47, loss = 1.43298964\n",
      "Iteration 48, loss = 1.43139755\n",
      "Iteration 49, loss = 1.42984176\n",
      "Iteration 50, loss = 1.42822992\n",
      "Iteration 51, loss = 1.42669801\n",
      "Iteration 52, loss = 1.42518525\n",
      "Iteration 53, loss = 1.42363456\n",
      "Iteration 54, loss = 1.42213618\n",
      "Iteration 55, loss = 1.42063849\n",
      "Iteration 56, loss = 1.41916475\n",
      "Iteration 57, loss = 1.41771340\n",
      "Iteration 58, loss = 1.41622090\n",
      "Iteration 59, loss = 1.41479487\n",
      "Iteration 60, loss = 1.41337000\n",
      "Iteration 61, loss = 1.41190514\n",
      "Iteration 62, loss = 1.41049945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 63, loss = 1.40913149\n",
      "Iteration 64, loss = 1.40772255\n",
      "Iteration 65, loss = 1.40634034\n",
      "Iteration 66, loss = 1.40498200\n",
      "Iteration 67, loss = 1.40361958\n",
      "Iteration 68, loss = 1.40229981\n",
      "Iteration 69, loss = 1.40092397\n",
      "Iteration 70, loss = 1.39961955\n",
      "Iteration 71, loss = 1.39835256\n",
      "Iteration 72, loss = 1.39702361\n",
      "Iteration 73, loss = 1.39573759\n",
      "Iteration 74, loss = 1.39444929\n",
      "Iteration 75, loss = 1.39318555\n",
      "Iteration 76, loss = 1.39193293\n",
      "Iteration 77, loss = 1.39070166\n",
      "Iteration 78, loss = 1.38948582\n",
      "Iteration 79, loss = 1.38825005\n",
      "Iteration 80, loss = 1.38704569\n",
      "Iteration 81, loss = 1.38580917\n",
      "Iteration 82, loss = 1.38462906\n",
      "Iteration 83, loss = 1.38343646\n",
      "Iteration 84, loss = 1.38227104\n",
      "Iteration 85, loss = 1.38111167\n",
      "Iteration 86, loss = 1.37997868\n",
      "Iteration 87, loss = 1.37883314\n",
      "Iteration 88, loss = 1.37767462\n",
      "Iteration 89, loss = 1.37652335\n",
      "Iteration 90, loss = 1.37544729\n",
      "Iteration 91, loss = 1.37431851\n",
      "Iteration 92, loss = 1.37324322\n",
      "Iteration 93, loss = 1.37215330\n",
      "Iteration 94, loss = 1.37107560\n",
      "Iteration 95, loss = 1.37002239\n",
      "Iteration 96, loss = 1.36895514\n",
      "Iteration 97, loss = 1.36787428\n",
      "Iteration 98, loss = 1.36685027\n",
      "Iteration 99, loss = 1.36582762\n",
      "Iteration 100, loss = 1.36479679\n",
      "Iteration 101, loss = 1.36378595\n",
      "Iteration 102, loss = 1.36278382\n",
      "Iteration 103, loss = 1.36176052\n",
      "Iteration 104, loss = 1.36076427\n",
      "Iteration 105, loss = 1.35979922\n",
      "Iteration 106, loss = 1.35882389\n",
      "Iteration 107, loss = 1.35784058\n",
      "Iteration 108, loss = 1.35691839\n",
      "Iteration 109, loss = 1.35596784\n",
      "Iteration 110, loss = 1.35500765\n",
      "Iteration 111, loss = 1.35405291\n",
      "Iteration 112, loss = 1.35312559\n",
      "Iteration 113, loss = 1.35221283\n",
      "Iteration 114, loss = 1.35130714\n",
      "Iteration 115, loss = 1.35042183\n",
      "Iteration 116, loss = 1.34950488\n",
      "Iteration 117, loss = 1.34862177\n",
      "Iteration 118, loss = 1.34774279\n",
      "Iteration 119, loss = 1.34684831\n",
      "Iteration 120, loss = 1.34599596\n",
      "Iteration 121, loss = 1.34512744\n",
      "Iteration 122, loss = 1.34426745\n",
      "Iteration 123, loss = 1.34342368\n",
      "Iteration 124, loss = 1.34258250\n",
      "Iteration 125, loss = 1.34176046\n",
      "Iteration 126, loss = 1.34091415\n",
      "Iteration 127, loss = 1.34011095\n",
      "Iteration 128, loss = 1.33928353\n",
      "Iteration 129, loss = 1.33848860\n",
      "Iteration 130, loss = 1.33769100\n",
      "Iteration 131, loss = 1.33687090\n",
      "Iteration 132, loss = 1.33608255\n",
      "Iteration 133, loss = 1.33530718\n",
      "Iteration 134, loss = 1.33453813\n",
      "Iteration 135, loss = 1.33376737\n",
      "Iteration 136, loss = 1.33298594\n",
      "Iteration 137, loss = 1.33223779\n",
      "Iteration 138, loss = 1.33149249\n",
      "Iteration 139, loss = 1.33073744\n",
      "Iteration 140, loss = 1.33002734\n",
      "Iteration 141, loss = 1.32925843\n",
      "Iteration 142, loss = 1.32854662\n",
      "Iteration 143, loss = 1.32782172\n",
      "Iteration 144, loss = 1.32711199\n",
      "Iteration 145, loss = 1.32637946\n",
      "Iteration 146, loss = 1.32567331\n",
      "Iteration 147, loss = 1.32499000\n",
      "Iteration 148, loss = 1.32429325\n",
      "Iteration 149, loss = 1.32359555\n",
      "Iteration 150, loss = 1.32292608\n",
      "Iteration 151, loss = 1.32224231\n",
      "Iteration 152, loss = 1.32157131\n",
      "Iteration 153, loss = 1.32090595\n",
      "Iteration 154, loss = 1.32025239\n",
      "Iteration 155, loss = 1.31956593\n",
      "Iteration 156, loss = 1.31892429\n",
      "Iteration 157, loss = 1.31828621\n",
      "Iteration 158, loss = 1.31763651\n",
      "Iteration 159, loss = 1.31699816\n",
      "Iteration 160, loss = 1.31638081\n",
      "Iteration 161, loss = 1.31573297\n",
      "Iteration 162, loss = 1.31513027\n",
      "Iteration 163, loss = 1.31451179\n",
      "Iteration 164, loss = 1.31388313\n",
      "Iteration 165, loss = 1.31329590\n",
      "Iteration 166, loss = 1.31265555\n",
      "Iteration 167, loss = 1.31206431\n",
      "Iteration 168, loss = 1.31147527\n",
      "Iteration 169, loss = 1.31088737\n",
      "Iteration 170, loss = 1.31030630\n",
      "Iteration 171, loss = 1.30971658\n",
      "Iteration 172, loss = 1.30914717\n",
      "Iteration 173, loss = 1.30856238\n",
      "Iteration 174, loss = 1.30799567\n",
      "Iteration 175, loss = 1.30742141\n",
      "Iteration 176, loss = 1.30686922\n",
      "Iteration 177, loss = 1.30631021\n",
      "Iteration 178, loss = 1.30575801\n",
      "Iteration 179, loss = 1.30521932\n",
      "Iteration 180, loss = 1.30466464\n",
      "Iteration 181, loss = 1.30413045\n",
      "Iteration 182, loss = 1.30359171\n",
      "Iteration 183, loss = 1.30304912\n",
      "Iteration 184, loss = 1.30253777\n",
      "Iteration 185, loss = 1.30200135\n",
      "Iteration 186, loss = 1.30150297\n",
      "Iteration 187, loss = 1.30096358\n",
      "Iteration 188, loss = 1.30045327\n",
      "Iteration 189, loss = 1.29995639\n",
      "Iteration 190, loss = 1.29943758\n",
      "Iteration 191, loss = 1.29894781\n",
      "Iteration 192, loss = 1.29844315\n",
      "Iteration 193, loss = 1.29795911\n",
      "Iteration 194, loss = 1.29746132\n",
      "Iteration 195, loss = 1.29697962\n",
      "Iteration 196, loss = 1.29651006\n",
      "Iteration 197, loss = 1.29600811\n",
      "Iteration 198, loss = 1.29554009\n",
      "Iteration 199, loss = 1.29505848\n",
      "Iteration 200, loss = 1.29458171\n",
      "Iteration 201, loss = 1.29413921\n",
      "Iteration 202, loss = 1.29368402\n",
      "Iteration 203, loss = 1.29321164\n",
      "Iteration 204, loss = 1.29275047\n",
      "Iteration 205, loss = 1.29230134\n",
      "Iteration 206, loss = 1.29184284\n",
      "Iteration 207, loss = 1.29140857\n",
      "Iteration 208, loss = 1.29095459\n",
      "Iteration 209, loss = 1.29050919\n",
      "Iteration 210, loss = 1.29008657\n",
      "Iteration 211, loss = 1.28965146\n",
      "Iteration 212, loss = 1.28921491\n",
      "Iteration 213, loss = 1.28878066\n",
      "Iteration 214, loss = 1.28836071\n",
      "Iteration 215, loss = 1.28794026\n",
      "Iteration 216, loss = 1.28751771\n",
      "Iteration 217, loss = 1.28710617\n",
      "Iteration 218, loss = 1.28668989\n",
      "Iteration 219, loss = 1.28628227\n",
      "Iteration 220, loss = 1.28588253\n",
      "Iteration 221, loss = 1.28546569\n",
      "Iteration 222, loss = 1.28507494\n",
      "Iteration 223, loss = 1.28467111\n",
      "Iteration 224, loss = 1.28427510\n",
      "Iteration 225, loss = 1.28386651\n",
      "Iteration 226, loss = 1.28349140\n",
      "Iteration 227, loss = 1.28309978\n",
      "Iteration 228, loss = 1.28271492\n",
      "Iteration 229, loss = 1.28234303\n",
      "Iteration 230, loss = 1.28194157\n",
      "Iteration 231, loss = 1.28157326\n",
      "Iteration 232, loss = 1.28119589\n",
      "Iteration 233, loss = 1.28082116\n",
      "Iteration 234, loss = 1.28045801\n",
      "Iteration 235, loss = 1.28008878\n",
      "Iteration 236, loss = 1.27972563\n",
      "Iteration 237, loss = 1.27935609\n",
      "Iteration 238, loss = 1.27900571\n",
      "Iteration 239, loss = 1.27863471\n",
      "Iteration 240, loss = 1.27828594\n",
      "Iteration 241, loss = 1.27792607\n",
      "Iteration 242, loss = 1.27759236\n",
      "Iteration 243, loss = 1.27723664\n",
      "Iteration 244, loss = 1.27689004\n",
      "Iteration 245, loss = 1.27654650\n",
      "Iteration 246, loss = 1.27621747\n",
      "Iteration 247, loss = 1.27587232\n",
      "Iteration 248, loss = 1.27553429\n",
      "Iteration 249, loss = 1.27520271\n",
      "Iteration 250, loss = 1.27485920\n",
      "Iteration 251, loss = 1.27455103\n",
      "Iteration 252, loss = 1.27421436\n",
      "Iteration 253, loss = 1.27388925\n",
      "Iteration 254, loss = 1.27356874\n",
      "Iteration 255, loss = 1.27324332\n",
      "Iteration 256, loss = 1.27293847\n",
      "Iteration 257, loss = 1.27260803\n",
      "Iteration 258, loss = 1.27229288\n",
      "Iteration 259, loss = 1.27197132\n",
      "Iteration 260, loss = 1.27166816\n",
      "Iteration 261, loss = 1.27136268\n",
      "Iteration 262, loss = 1.27105725\n",
      "Iteration 263, loss = 1.27075071\n",
      "Iteration 264, loss = 1.27044757\n",
      "Iteration 265, loss = 1.27013548\n",
      "Iteration 266, loss = 1.26983557\n",
      "Iteration 267, loss = 1.26954103\n",
      "Iteration 268, loss = 1.26924429\n",
      "Iteration 269, loss = 1.26895443\n",
      "Iteration 270, loss = 1.26866656\n",
      "Iteration 271, loss = 1.26837044\n",
      "Iteration 272, loss = 1.26807669\n",
      "Iteration 273, loss = 1.26779218\n",
      "Iteration 274, loss = 1.26750425\n",
      "Iteration 275, loss = 1.26722300\n",
      "Iteration 276, loss = 1.26694097\n",
      "Iteration 277, loss = 1.26666268\n",
      "Iteration 278, loss = 1.26637481\n",
      "Iteration 279, loss = 1.26609834\n",
      "Iteration 280, loss = 1.26583058\n",
      "Iteration 281, loss = 1.26554806\n",
      "Iteration 282, loss = 1.26528235\n",
      "Iteration 283, loss = 1.26500910\n",
      "Iteration 284, loss = 1.26473651\n",
      "Iteration 285, loss = 1.26448225\n",
      "Iteration 286, loss = 1.26420233\n",
      "Iteration 287, loss = 1.26394306\n",
      "Iteration 288, loss = 1.26367374\n",
      "Iteration 289, loss = 1.26342016\n",
      "Iteration 290, loss = 1.26314979\n",
      "Iteration 291, loss = 1.26289370\n",
      "Iteration 292, loss = 1.26265047\n",
      "Iteration 293, loss = 1.26239810\n",
      "Iteration 294, loss = 1.26213017\n",
      "Iteration 295, loss = 1.26188403\n",
      "Iteration 296, loss = 1.26162572\n",
      "Iteration 297, loss = 1.26138034\n",
      "Iteration 298, loss = 1.26114286\n",
      "Iteration 299, loss = 1.26088900\n",
      "Iteration 300, loss = 1.26064421\n",
      "Iteration 1, loss = 1.51733147\n",
      "Iteration 2, loss = 1.51640786\n",
      "Iteration 3, loss = 1.51500472\n",
      "Iteration 4, loss = 1.51329856\n",
      "Iteration 5, loss = 1.51138214\n",
      "Iteration 6, loss = 1.50932341\n",
      "Iteration 7, loss = 1.50724672\n",
      "Iteration 8, loss = 1.50510173\n",
      "Iteration 9, loss = 1.50291713\n",
      "Iteration 10, loss = 1.50072855\n",
      "Iteration 11, loss = 1.49863191\n",
      "Iteration 12, loss = 1.49641030\n",
      "Iteration 13, loss = 1.49427528\n",
      "Iteration 14, loss = 1.49217983\n",
      "Iteration 15, loss = 1.48996964\n",
      "Iteration 16, loss = 1.48789065\n",
      "Iteration 17, loss = 1.48577255\n",
      "Iteration 18, loss = 1.48372250\n",
      "Iteration 19, loss = 1.48164400\n",
      "Iteration 20, loss = 1.47961764\n",
      "Iteration 21, loss = 1.47758380\n",
      "Iteration 22, loss = 1.47558994\n",
      "Iteration 23, loss = 1.47361342\n",
      "Iteration 24, loss = 1.47159766\n",
      "Iteration 25, loss = 1.46965390\n",
      "Iteration 26, loss = 1.46771276\n",
      "Iteration 27, loss = 1.46579489\n",
      "Iteration 28, loss = 1.46385164\n",
      "Iteration 29, loss = 1.46200313\n",
      "Iteration 30, loss = 1.46009750\n",
      "Iteration 31, loss = 1.45827837\n",
      "Iteration 32, loss = 1.45642748\n",
      "Iteration 33, loss = 1.45454835\n",
      "Iteration 34, loss = 1.45270548\n",
      "Iteration 35, loss = 1.45097123\n",
      "Iteration 36, loss = 1.44918581\n",
      "Iteration 37, loss = 1.44739472\n",
      "Iteration 38, loss = 1.44566381\n",
      "Iteration 39, loss = 1.44387130\n",
      "Iteration 40, loss = 1.44218331\n",
      "Iteration 41, loss = 1.44044449\n",
      "Iteration 42, loss = 1.43875995\n",
      "Iteration 43, loss = 1.43707609\n",
      "Iteration 44, loss = 1.43539608\n",
      "Iteration 45, loss = 1.43373958\n",
      "Iteration 46, loss = 1.43208876\n",
      "Iteration 47, loss = 1.43046999\n",
      "Iteration 48, loss = 1.42889030\n",
      "Iteration 49, loss = 1.42724527\n",
      "Iteration 50, loss = 1.42569023\n",
      "Iteration 51, loss = 1.42407889\n",
      "Iteration 52, loss = 1.42251842\n",
      "Iteration 53, loss = 1.42096546\n",
      "Iteration 54, loss = 1.41941319\n",
      "Iteration 55, loss = 1.41791355\n",
      "Iteration 56, loss = 1.41640140\n",
      "Iteration 57, loss = 1.41493414\n",
      "Iteration 58, loss = 1.41342042\n",
      "Iteration 59, loss = 1.41192901\n",
      "Iteration 60, loss = 1.41049686\n",
      "Iteration 61, loss = 1.40901003\n",
      "Iteration 62, loss = 1.40762310\n",
      "Iteration 63, loss = 1.40617118\n",
      "Iteration 64, loss = 1.40477739\n",
      "Iteration 65, loss = 1.40334572\n",
      "Iteration 66, loss = 1.40194946\n",
      "Iteration 67, loss = 1.40061320\n",
      "Iteration 68, loss = 1.39922579\n",
      "Iteration 69, loss = 1.39782814\n",
      "Iteration 70, loss = 1.39655156\n",
      "Iteration 71, loss = 1.39518645\n",
      "Iteration 72, loss = 1.39387312\n",
      "Iteration 73, loss = 1.39256649\n",
      "Iteration 74, loss = 1.39124839\n",
      "Iteration 75, loss = 1.38998437\n",
      "Iteration 76, loss = 1.38871178\n",
      "Iteration 77, loss = 1.38743053\n",
      "Iteration 78, loss = 1.38617293\n",
      "Iteration 79, loss = 1.38492831\n",
      "Iteration 80, loss = 1.38370508\n",
      "Iteration 81, loss = 1.38246804\n",
      "Iteration 82, loss = 1.38128021\n",
      "Iteration 83, loss = 1.38006253\n",
      "Iteration 84, loss = 1.37885558\n",
      "Iteration 85, loss = 1.37765348\n",
      "Iteration 86, loss = 1.37652790\n",
      "Iteration 87, loss = 1.37535891\n",
      "Iteration 88, loss = 1.37417813\n",
      "Iteration 89, loss = 1.37303938\n",
      "Iteration 90, loss = 1.37188406\n",
      "Iteration 91, loss = 1.37080022\n",
      "Iteration 92, loss = 1.36963786\n",
      "Iteration 93, loss = 1.36857212\n",
      "Iteration 94, loss = 1.36744267\n",
      "Iteration 95, loss = 1.36636305\n",
      "Iteration 96, loss = 1.36528622\n",
      "Iteration 97, loss = 1.36422691\n",
      "Iteration 98, loss = 1.36314118\n",
      "Iteration 99, loss = 1.36210794\n",
      "Iteration 100, loss = 1.36105258\n",
      "Iteration 101, loss = 1.36002358\n",
      "Iteration 102, loss = 1.35897624\n",
      "Iteration 103, loss = 1.35797196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 104, loss = 1.35694935\n",
      "Iteration 105, loss = 1.35596378\n",
      "Iteration 106, loss = 1.35497172\n",
      "Iteration 107, loss = 1.35397425\n",
      "Iteration 108, loss = 1.35299273\n",
      "Iteration 109, loss = 1.35201727\n",
      "Iteration 110, loss = 1.35104159\n",
      "Iteration 111, loss = 1.35011704\n",
      "Iteration 112, loss = 1.34916402\n",
      "Iteration 113, loss = 1.34823494\n",
      "Iteration 114, loss = 1.34728943\n",
      "Iteration 115, loss = 1.34637504\n",
      "Iteration 116, loss = 1.34545044\n",
      "Iteration 117, loss = 1.34455765\n",
      "Iteration 118, loss = 1.34364520\n",
      "Iteration 119, loss = 1.34276185\n",
      "Iteration 120, loss = 1.34187204\n",
      "Iteration 121, loss = 1.34099072\n",
      "Iteration 122, loss = 1.34011049\n",
      "Iteration 123, loss = 1.33923953\n",
      "Iteration 124, loss = 1.33840014\n",
      "Iteration 125, loss = 1.33755511\n",
      "Iteration 126, loss = 1.33670388\n",
      "Iteration 127, loss = 1.33586101\n",
      "Iteration 128, loss = 1.33504385\n",
      "Iteration 129, loss = 1.33421122\n",
      "Iteration 130, loss = 1.33339270\n",
      "Iteration 131, loss = 1.33257340\n",
      "Iteration 132, loss = 1.33178491\n",
      "Iteration 133, loss = 1.33097610\n",
      "Iteration 134, loss = 1.33020387\n",
      "Iteration 135, loss = 1.32939024\n",
      "Iteration 136, loss = 1.32861921\n",
      "Iteration 137, loss = 1.32785514\n",
      "Iteration 138, loss = 1.32709108\n",
      "Iteration 139, loss = 1.32631297\n",
      "Iteration 140, loss = 1.32556433\n",
      "Iteration 141, loss = 1.32482429\n",
      "Iteration 142, loss = 1.32406579\n",
      "Iteration 143, loss = 1.32334367\n",
      "Iteration 144, loss = 1.32260388\n",
      "Iteration 145, loss = 1.32189580\n",
      "Iteration 146, loss = 1.32116244\n",
      "Iteration 147, loss = 1.32044326\n",
      "Iteration 148, loss = 1.31972543\n",
      "Iteration 149, loss = 1.31903348\n",
      "Iteration 150, loss = 1.31833923\n",
      "Iteration 151, loss = 1.31764206\n",
      "Iteration 152, loss = 1.31697205\n",
      "Iteration 153, loss = 1.31628119\n",
      "Iteration 154, loss = 1.31559900\n",
      "Iteration 155, loss = 1.31492740\n",
      "Iteration 156, loss = 1.31423381\n",
      "Iteration 157, loss = 1.31358685\n",
      "Iteration 158, loss = 1.31293524\n",
      "Iteration 159, loss = 1.31229414\n",
      "Iteration 160, loss = 1.31165010\n",
      "Iteration 161, loss = 1.31100779\n",
      "Iteration 162, loss = 1.31035635\n",
      "Iteration 163, loss = 1.30973096\n",
      "Iteration 164, loss = 1.30908366\n",
      "Iteration 165, loss = 1.30847409\n",
      "Iteration 166, loss = 1.30786540\n",
      "Iteration 167, loss = 1.30724721\n",
      "Iteration 168, loss = 1.30663504\n",
      "Iteration 169, loss = 1.30602335\n",
      "Iteration 170, loss = 1.30542735\n",
      "Iteration 171, loss = 1.30483236\n",
      "Iteration 172, loss = 1.30423828\n",
      "Iteration 173, loss = 1.30365022\n",
      "Iteration 174, loss = 1.30307724\n",
      "Iteration 175, loss = 1.30249339\n",
      "Iteration 176, loss = 1.30192486\n",
      "Iteration 177, loss = 1.30134092\n",
      "Iteration 178, loss = 1.30078273\n",
      "Iteration 179, loss = 1.30021310\n",
      "Iteration 180, loss = 1.29966779\n",
      "Iteration 181, loss = 1.29909644\n",
      "Iteration 182, loss = 1.29855906\n",
      "Iteration 183, loss = 1.29802927\n",
      "Iteration 184, loss = 1.29747983\n",
      "Iteration 185, loss = 1.29692923\n",
      "Iteration 186, loss = 1.29641700\n",
      "Iteration 187, loss = 1.29588891\n",
      "Iteration 188, loss = 1.29534628\n",
      "Iteration 189, loss = 1.29484345\n",
      "Iteration 190, loss = 1.29430844\n",
      "Iteration 191, loss = 1.29379616\n",
      "Iteration 192, loss = 1.29328622\n",
      "Iteration 193, loss = 1.29278890\n",
      "Iteration 194, loss = 1.29228520\n",
      "Iteration 195, loss = 1.29179682\n",
      "Iteration 196, loss = 1.29128546\n",
      "Iteration 197, loss = 1.29078230\n",
      "Iteration 198, loss = 1.29032850\n",
      "Iteration 199, loss = 1.28982560\n",
      "Iteration 200, loss = 1.28934767\n",
      "Iteration 201, loss = 1.28886083\n",
      "Iteration 202, loss = 1.28838760\n",
      "Iteration 203, loss = 1.28791339\n",
      "Iteration 204, loss = 1.28745980\n",
      "Iteration 205, loss = 1.28698770\n",
      "Iteration 206, loss = 1.28652454\n",
      "Iteration 207, loss = 1.28605607\n",
      "Iteration 208, loss = 1.28561732\n",
      "Iteration 209, loss = 1.28515000\n",
      "Iteration 210, loss = 1.28472076\n",
      "Iteration 211, loss = 1.28426143\n",
      "Iteration 212, loss = 1.28382200\n",
      "Iteration 213, loss = 1.28337410\n",
      "Iteration 214, loss = 1.28294632\n",
      "Iteration 215, loss = 1.28252477\n",
      "Iteration 216, loss = 1.28207609\n",
      "Iteration 217, loss = 1.28166793\n",
      "Iteration 218, loss = 1.28121720\n",
      "Iteration 219, loss = 1.28080831\n",
      "Iteration 220, loss = 1.28040028\n",
      "Iteration 221, loss = 1.27995973\n",
      "Iteration 222, loss = 1.27955787\n",
      "Iteration 223, loss = 1.27915350\n",
      "Iteration 224, loss = 1.27876423\n",
      "Iteration 225, loss = 1.27833642\n",
      "Iteration 226, loss = 1.27793003\n",
      "Iteration 227, loss = 1.27754350\n",
      "Iteration 228, loss = 1.27713150\n",
      "Iteration 229, loss = 1.27673871\n",
      "Iteration 230, loss = 1.27636888\n",
      "Iteration 231, loss = 1.27596073\n",
      "Iteration 232, loss = 1.27557782\n",
      "Iteration 233, loss = 1.27519160\n",
      "Iteration 234, loss = 1.27481186\n",
      "Iteration 235, loss = 1.27443561\n",
      "Iteration 236, loss = 1.27405751\n",
      "Iteration 237, loss = 1.27368688\n",
      "Iteration 238, loss = 1.27332224\n",
      "Iteration 239, loss = 1.27293886\n",
      "Iteration 240, loss = 1.27257136\n",
      "Iteration 241, loss = 1.27222093\n",
      "Iteration 242, loss = 1.27185289\n",
      "Iteration 243, loss = 1.27149866\n",
      "Iteration 244, loss = 1.27114083\n",
      "Iteration 245, loss = 1.27078374\n",
      "Iteration 246, loss = 1.27042551\n",
      "Iteration 247, loss = 1.27007698\n",
      "Iteration 248, loss = 1.26971711\n",
      "Iteration 249, loss = 1.26938850\n",
      "Iteration 250, loss = 1.26904085\n",
      "Iteration 251, loss = 1.26869399\n",
      "Iteration 252, loss = 1.26835627\n",
      "Iteration 253, loss = 1.26802283\n",
      "Iteration 254, loss = 1.26768936\n",
      "Iteration 255, loss = 1.26736445\n",
      "Iteration 256, loss = 1.26701863\n",
      "Iteration 257, loss = 1.26668066\n",
      "Iteration 258, loss = 1.26637430\n",
      "Iteration 259, loss = 1.26603804\n",
      "Iteration 260, loss = 1.26571784\n",
      "Iteration 261, loss = 1.26539259\n",
      "Iteration 262, loss = 1.26506914\n",
      "Iteration 263, loss = 1.26475817\n",
      "Iteration 264, loss = 1.26444891\n",
      "Iteration 265, loss = 1.26413313\n",
      "Iteration 266, loss = 1.26381361\n",
      "Iteration 267, loss = 1.26351132\n",
      "Iteration 268, loss = 1.26320988\n",
      "Iteration 269, loss = 1.26289934\n",
      "Iteration 270, loss = 1.26258655\n",
      "Iteration 271, loss = 1.26227887\n",
      "Iteration 272, loss = 1.26198636\n",
      "Iteration 273, loss = 1.26170053\n",
      "Iteration 274, loss = 1.26138448\n",
      "Iteration 275, loss = 1.26110158\n",
      "Iteration 276, loss = 1.26080180\n",
      "Iteration 277, loss = 1.26051080\n",
      "Iteration 278, loss = 1.26022401\n",
      "Iteration 279, loss = 1.25992739\n",
      "Iteration 280, loss = 1.25965367\n",
      "Iteration 281, loss = 1.25936474\n",
      "Iteration 282, loss = 1.25908278\n",
      "Iteration 283, loss = 1.25878959\n",
      "Iteration 284, loss = 1.25852305\n",
      "Iteration 285, loss = 1.25823394\n",
      "Iteration 286, loss = 1.25795747\n",
      "Iteration 287, loss = 1.25768682\n",
      "Iteration 288, loss = 1.25741474\n",
      "Iteration 289, loss = 1.25713825\n",
      "Iteration 290, loss = 1.25687638\n",
      "Iteration 291, loss = 1.25660184\n",
      "Iteration 292, loss = 1.25634033\n",
      "Iteration 293, loss = 1.25606632\n",
      "Iteration 294, loss = 1.25581145\n",
      "Iteration 295, loss = 1.25554944\n",
      "Iteration 296, loss = 1.25528017\n",
      "Iteration 297, loss = 1.25502346\n",
      "Iteration 298, loss = 1.25475146\n",
      "Iteration 299, loss = 1.25450877\n",
      "Iteration 300, loss = 1.25425079\n",
      "Iteration 1, loss = 4.43345300\n",
      "Iteration 2, loss = 3.07029574\n",
      "Iteration 3, loss = 2.50808665\n",
      "Iteration 4, loss = 2.03708777\n",
      "Iteration 5, loss = 1.68507293\n",
      "Iteration 6, loss = 1.57597924\n",
      "Iteration 7, loss = 1.51822548\n",
      "Iteration 8, loss = 1.44598983\n",
      "Iteration 9, loss = 1.38503399\n",
      "Iteration 10, loss = 1.35231126\n",
      "Iteration 11, loss = 1.37573043\n",
      "Iteration 12, loss = 1.30694759\n",
      "Iteration 13, loss = 1.35396206\n",
      "Iteration 14, loss = 1.31161156\n",
      "Iteration 15, loss = 1.27676910\n",
      "Iteration 16, loss = 1.29359380\n",
      "Iteration 17, loss = 1.25726497\n",
      "Iteration 18, loss = 1.28365329\n",
      "Iteration 19, loss = 1.25803401\n",
      "Iteration 20, loss = 1.25036002\n",
      "Iteration 21, loss = 1.29774782\n",
      "Iteration 22, loss = 1.25911064\n",
      "Iteration 23, loss = 1.26921091\n",
      "Iteration 24, loss = 1.26514135\n",
      "Iteration 25, loss = 1.25450716\n",
      "Iteration 26, loss = 1.26916925\n",
      "Iteration 27, loss = 1.34516135\n",
      "Iteration 28, loss = 1.25843289\n",
      "Iteration 29, loss = 1.27344830\n",
      "Iteration 30, loss = 1.24343643\n",
      "Iteration 31, loss = 1.26699230\n",
      "Iteration 32, loss = 1.25267699\n",
      "Iteration 33, loss = 1.25526647\n",
      "Iteration 34, loss = 1.22883235\n",
      "Iteration 35, loss = 1.21913500\n",
      "Iteration 36, loss = 1.21472488\n",
      "Iteration 37, loss = 1.22706315\n",
      "Iteration 38, loss = 1.26404473\n",
      "Iteration 39, loss = 1.31419483\n",
      "Iteration 40, loss = 1.22620133\n",
      "Iteration 41, loss = 1.24723230\n",
      "Iteration 42, loss = 1.20274176\n",
      "Iteration 43, loss = 1.20699742\n",
      "Iteration 44, loss = 1.20035913\n",
      "Iteration 45, loss = 1.21347143\n",
      "Iteration 46, loss = 1.20566820\n",
      "Iteration 47, loss = 1.21848343\n",
      "Iteration 48, loss = 1.19731686\n",
      "Iteration 49, loss = 1.28100707\n",
      "Iteration 50, loss = 1.27519888\n",
      "Iteration 51, loss = 1.26266473\n",
      "Iteration 52, loss = 1.27835891\n",
      "Iteration 53, loss = 1.22881630\n",
      "Iteration 54, loss = 1.26607181\n",
      "Iteration 55, loss = 1.19440010\n",
      "Iteration 56, loss = 1.27278405\n",
      "Iteration 57, loss = 1.19212362\n",
      "Iteration 58, loss = 1.20113652\n",
      "Iteration 59, loss = 1.18287510\n",
      "Iteration 60, loss = 1.18706274\n",
      "Iteration 61, loss = 1.19357985\n",
      "Iteration 62, loss = 1.22880394\n",
      "Iteration 63, loss = 1.23260703\n",
      "Iteration 64, loss = 1.22011158\n",
      "Iteration 65, loss = 1.18406217\n",
      "Iteration 66, loss = 1.21498076\n",
      "Iteration 67, loss = 1.19838271\n",
      "Iteration 68, loss = 1.20448478\n",
      "Iteration 69, loss = 1.18936638\n",
      "Iteration 70, loss = 1.18997175\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.51614707\n",
      "Iteration 2, loss = 1.50721370\n",
      "Iteration 3, loss = 1.49366593\n",
      "Iteration 4, loss = 1.47830590\n",
      "Iteration 5, loss = 1.46163480\n",
      "Iteration 6, loss = 1.44540556\n",
      "Iteration 7, loss = 1.42881642\n",
      "Iteration 8, loss = 1.41402539\n",
      "Iteration 9, loss = 1.39931096\n",
      "Iteration 10, loss = 1.38640518\n",
      "Iteration 11, loss = 1.37387190\n",
      "Iteration 12, loss = 1.36242364\n",
      "Iteration 13, loss = 1.35171323\n",
      "Iteration 14, loss = 1.34208578\n",
      "Iteration 15, loss = 1.33350648\n",
      "Iteration 16, loss = 1.32532283\n",
      "Iteration 17, loss = 1.31795711\n",
      "Iteration 18, loss = 1.31128881\n",
      "Iteration 19, loss = 1.30493031\n",
      "Iteration 20, loss = 1.29912920\n",
      "Iteration 21, loss = 1.29398354\n",
      "Iteration 22, loss = 1.28897382\n",
      "Iteration 23, loss = 1.28460491\n",
      "Iteration 24, loss = 1.28019459\n",
      "Iteration 25, loss = 1.27652209\n",
      "Iteration 26, loss = 1.27284667\n",
      "Iteration 27, loss = 1.26951592\n",
      "Iteration 28, loss = 1.26630452\n",
      "Iteration 29, loss = 1.26335585\n",
      "Iteration 30, loss = 1.26063054\n",
      "Iteration 31, loss = 1.25793009\n",
      "Iteration 32, loss = 1.25556146\n",
      "Iteration 33, loss = 1.25325359\n",
      "Iteration 34, loss = 1.25103271\n",
      "Iteration 35, loss = 1.24898756\n",
      "Iteration 36, loss = 1.24705634\n",
      "Iteration 37, loss = 1.24517812\n",
      "Iteration 38, loss = 1.24339777\n",
      "Iteration 39, loss = 1.24178118\n",
      "Iteration 40, loss = 1.24009964\n",
      "Iteration 41, loss = 1.23862888\n",
      "Iteration 42, loss = 1.23714038\n",
      "Iteration 43, loss = 1.23571154\n",
      "Iteration 44, loss = 1.23444443\n",
      "Iteration 45, loss = 1.23311694\n",
      "Iteration 46, loss = 1.23193400\n",
      "Iteration 47, loss = 1.23072165\n",
      "Iteration 48, loss = 1.22958152\n",
      "Iteration 49, loss = 1.22848045\n",
      "Iteration 50, loss = 1.22745588\n",
      "Iteration 51, loss = 1.22650918\n",
      "Iteration 52, loss = 1.22548023\n",
      "Iteration 53, loss = 1.22455913\n",
      "Iteration 54, loss = 1.22364896\n",
      "Iteration 55, loss = 1.22276485\n",
      "Iteration 56, loss = 1.22189346\n",
      "Iteration 57, loss = 1.22117313\n",
      "Iteration 58, loss = 1.22033507\n",
      "Iteration 59, loss = 1.21960634\n",
      "Iteration 60, loss = 1.21885070\n",
      "Iteration 61, loss = 1.21808586\n",
      "Iteration 62, loss = 1.21740529\n",
      "Iteration 63, loss = 1.21673639\n",
      "Iteration 64, loss = 1.21606698\n",
      "Iteration 65, loss = 1.21543581\n",
      "Iteration 66, loss = 1.21481842\n",
      "Iteration 67, loss = 1.21426138\n",
      "Iteration 68, loss = 1.21360233\n",
      "Iteration 69, loss = 1.21302007\n",
      "Iteration 70, loss = 1.21245355\n",
      "Iteration 71, loss = 1.21188515\n",
      "Iteration 72, loss = 1.21141635\n",
      "Iteration 73, loss = 1.21087479\n",
      "Iteration 74, loss = 1.21035251\n",
      "Iteration 75, loss = 1.20982347\n",
      "Iteration 76, loss = 1.20936054\n",
      "Iteration 77, loss = 1.20885372\n",
      "Iteration 78, loss = 1.20839168\n",
      "Iteration 79, loss = 1.20795491\n",
      "Iteration 80, loss = 1.20753976\n",
      "Iteration 81, loss = 1.20706378\n",
      "Iteration 82, loss = 1.20659065\n",
      "Iteration 83, loss = 1.20619502\n",
      "Iteration 84, loss = 1.20577514\n",
      "Iteration 85, loss = 1.20537597\n",
      "Iteration 86, loss = 1.20496525\n",
      "Iteration 87, loss = 1.20455003\n",
      "Iteration 88, loss = 1.20420071\n",
      "Iteration 89, loss = 1.20381923\n",
      "Iteration 90, loss = 1.20342788\n",
      "Iteration 91, loss = 1.20306057\n",
      "Iteration 92, loss = 1.20269870\n",
      "Iteration 93, loss = 1.20231686\n",
      "Iteration 94, loss = 1.20201677\n",
      "Iteration 95, loss = 1.20164070\n",
      "Iteration 96, loss = 1.20128635\n",
      "Iteration 97, loss = 1.20100607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 98, loss = 1.20065113\n",
      "Iteration 99, loss = 1.20030527\n",
      "Iteration 100, loss = 1.19998327\n",
      "Iteration 1, loss = 1.51638664\n",
      "Iteration 2, loss = 1.50736711\n",
      "Iteration 3, loss = 1.49399211\n",
      "Iteration 4, loss = 1.47866355\n",
      "Iteration 5, loss = 1.46212073\n",
      "Iteration 6, loss = 1.44564054\n",
      "Iteration 7, loss = 1.42932412\n",
      "Iteration 8, loss = 1.41424433\n",
      "Iteration 9, loss = 1.39979458\n",
      "Iteration 10, loss = 1.38667448\n",
      "Iteration 11, loss = 1.37414017\n",
      "Iteration 12, loss = 1.36283308\n",
      "Iteration 13, loss = 1.35204893\n",
      "Iteration 14, loss = 1.34278183\n",
      "Iteration 15, loss = 1.33380194\n",
      "Iteration 16, loss = 1.32571878\n",
      "Iteration 17, loss = 1.31842617\n",
      "Iteration 18, loss = 1.31155628\n",
      "Iteration 19, loss = 1.30545361\n",
      "Iteration 20, loss = 1.29961253\n",
      "Iteration 21, loss = 1.29436458\n",
      "Iteration 22, loss = 1.28938742\n",
      "Iteration 23, loss = 1.28495989\n",
      "Iteration 24, loss = 1.28066616\n",
      "Iteration 25, loss = 1.27677221\n",
      "Iteration 26, loss = 1.27324415\n",
      "Iteration 27, loss = 1.26998107\n",
      "Iteration 28, loss = 1.26662618\n",
      "Iteration 29, loss = 1.26373827\n",
      "Iteration 30, loss = 1.26098675\n",
      "Iteration 31, loss = 1.25831170\n",
      "Iteration 32, loss = 1.25580962\n",
      "Iteration 33, loss = 1.25354954\n",
      "Iteration 34, loss = 1.25146682\n",
      "Iteration 35, loss = 1.24927663\n",
      "Iteration 36, loss = 1.24735165\n",
      "Iteration 37, loss = 1.24547506\n",
      "Iteration 38, loss = 1.24373358\n",
      "Iteration 39, loss = 1.24201156\n",
      "Iteration 40, loss = 1.24039051\n",
      "Iteration 41, loss = 1.23889676\n",
      "Iteration 42, loss = 1.23736829\n",
      "Iteration 43, loss = 1.23594653\n",
      "Iteration 44, loss = 1.23467784\n",
      "Iteration 45, loss = 1.23338779\n",
      "Iteration 46, loss = 1.23212356\n",
      "Iteration 47, loss = 1.23090864\n",
      "Iteration 48, loss = 1.22978273\n",
      "Iteration 49, loss = 1.22869094\n",
      "Iteration 50, loss = 1.22761497\n",
      "Iteration 51, loss = 1.22662817\n",
      "Iteration 52, loss = 1.22557882\n",
      "Iteration 53, loss = 1.22467948\n",
      "Iteration 54, loss = 1.22375150\n",
      "Iteration 55, loss = 1.22283722\n",
      "Iteration 56, loss = 1.22199215\n",
      "Iteration 57, loss = 1.22122344\n",
      "Iteration 58, loss = 1.22034182\n",
      "Iteration 59, loss = 1.21959151\n",
      "Iteration 60, loss = 1.21885717\n",
      "Iteration 61, loss = 1.21808774\n",
      "Iteration 62, loss = 1.21735140\n",
      "Iteration 63, loss = 1.21669819\n",
      "Iteration 64, loss = 1.21600008\n",
      "Iteration 65, loss = 1.21536084\n",
      "Iteration 66, loss = 1.21472486\n",
      "Iteration 67, loss = 1.21414005\n",
      "Iteration 68, loss = 1.21350336\n",
      "Iteration 69, loss = 1.21288226\n",
      "Iteration 70, loss = 1.21230509\n",
      "Iteration 71, loss = 1.21174854\n",
      "Iteration 72, loss = 1.21121852\n",
      "Iteration 73, loss = 1.21068557\n",
      "Iteration 74, loss = 1.21015102\n",
      "Iteration 75, loss = 1.20961978\n",
      "Iteration 76, loss = 1.20914392\n",
      "Iteration 77, loss = 1.20864670\n",
      "Iteration 78, loss = 1.20816865\n",
      "Iteration 79, loss = 1.20769960\n",
      "Iteration 80, loss = 1.20727359\n",
      "Iteration 81, loss = 1.20675701\n",
      "Iteration 82, loss = 1.20631014\n",
      "Iteration 83, loss = 1.20589430\n",
      "Iteration 84, loss = 1.20546034\n",
      "Iteration 85, loss = 1.20503216\n",
      "Iteration 86, loss = 1.20462141\n",
      "Iteration 87, loss = 1.20420067\n",
      "Iteration 88, loss = 1.20381766\n",
      "Iteration 89, loss = 1.20344437\n",
      "Iteration 90, loss = 1.20302754\n",
      "Iteration 91, loss = 1.20265079\n",
      "Iteration 92, loss = 1.20226791\n",
      "Iteration 93, loss = 1.20189039\n",
      "Iteration 94, loss = 1.20153299\n",
      "Iteration 95, loss = 1.20118813\n",
      "Iteration 96, loss = 1.20079019\n",
      "Iteration 97, loss = 1.20047542\n",
      "Iteration 98, loss = 1.20012295\n",
      "Iteration 99, loss = 1.19979416\n",
      "Iteration 100, loss = 1.19943650\n",
      "Iteration 1, loss = 1.51658759\n",
      "Iteration 2, loss = 1.50750331\n",
      "Iteration 3, loss = 1.49420954\n",
      "Iteration 4, loss = 1.47901319\n",
      "Iteration 5, loss = 1.46200911\n",
      "Iteration 6, loss = 1.44546808\n",
      "Iteration 7, loss = 1.42950421\n",
      "Iteration 8, loss = 1.41414199\n",
      "Iteration 9, loss = 1.39965147\n",
      "Iteration 10, loss = 1.38631706\n",
      "Iteration 11, loss = 1.37390963\n",
      "Iteration 12, loss = 1.36263602\n",
      "Iteration 13, loss = 1.35187948\n",
      "Iteration 14, loss = 1.34251327\n",
      "Iteration 15, loss = 1.33334527\n",
      "Iteration 16, loss = 1.32534597\n",
      "Iteration 17, loss = 1.31794115\n",
      "Iteration 18, loss = 1.31106900\n",
      "Iteration 19, loss = 1.30486388\n",
      "Iteration 20, loss = 1.29889565\n",
      "Iteration 21, loss = 1.29364508\n",
      "Iteration 22, loss = 1.28860833\n",
      "Iteration 23, loss = 1.28403840\n",
      "Iteration 24, loss = 1.27986732\n",
      "Iteration 25, loss = 1.27606983\n",
      "Iteration 26, loss = 1.27224618\n",
      "Iteration 27, loss = 1.26887457\n",
      "Iteration 28, loss = 1.26549662\n",
      "Iteration 29, loss = 1.26269167\n",
      "Iteration 30, loss = 1.25978358\n",
      "Iteration 31, loss = 1.25712210\n",
      "Iteration 32, loss = 1.25455278\n",
      "Iteration 33, loss = 1.25224685\n",
      "Iteration 34, loss = 1.25004769\n",
      "Iteration 35, loss = 1.24797648\n",
      "Iteration 36, loss = 1.24598040\n",
      "Iteration 37, loss = 1.24402850\n",
      "Iteration 38, loss = 1.24223934\n",
      "Iteration 39, loss = 1.24048241\n",
      "Iteration 40, loss = 1.23887414\n",
      "Iteration 41, loss = 1.23735564\n",
      "Iteration 42, loss = 1.23580608\n",
      "Iteration 43, loss = 1.23438923\n",
      "Iteration 44, loss = 1.23304879\n",
      "Iteration 45, loss = 1.23169700\n",
      "Iteration 46, loss = 1.23039959\n",
      "Iteration 47, loss = 1.22926273\n",
      "Iteration 48, loss = 1.22814803\n",
      "Iteration 49, loss = 1.22702474\n",
      "Iteration 50, loss = 1.22586926\n",
      "Iteration 51, loss = 1.22491483\n",
      "Iteration 52, loss = 1.22389634\n",
      "Iteration 53, loss = 1.22293036\n",
      "Iteration 54, loss = 1.22197797\n",
      "Iteration 55, loss = 1.22108776\n",
      "Iteration 56, loss = 1.22023966\n",
      "Iteration 57, loss = 1.21944647\n",
      "Iteration 58, loss = 1.21856300\n",
      "Iteration 59, loss = 1.21779182\n",
      "Iteration 60, loss = 1.21706912\n",
      "Iteration 61, loss = 1.21629368\n",
      "Iteration 62, loss = 1.21553760\n",
      "Iteration 63, loss = 1.21485706\n",
      "Iteration 64, loss = 1.21418196\n",
      "Iteration 65, loss = 1.21349631\n",
      "Iteration 66, loss = 1.21283445\n",
      "Iteration 67, loss = 1.21221404\n",
      "Iteration 68, loss = 1.21160077\n",
      "Iteration 69, loss = 1.21096500\n",
      "Iteration 70, loss = 1.21038836\n",
      "Iteration 71, loss = 1.20985542\n",
      "Iteration 72, loss = 1.20926367\n",
      "Iteration 73, loss = 1.20871479\n",
      "Iteration 74, loss = 1.20818492\n",
      "Iteration 75, loss = 1.20764297\n",
      "Iteration 76, loss = 1.20710579\n",
      "Iteration 77, loss = 1.20663355\n",
      "Iteration 78, loss = 1.20613394\n",
      "Iteration 79, loss = 1.20568674\n",
      "Iteration 80, loss = 1.20521060\n",
      "Iteration 81, loss = 1.20469524\n",
      "Iteration 82, loss = 1.20426732\n",
      "Iteration 83, loss = 1.20381093\n",
      "Iteration 84, loss = 1.20339973\n",
      "Iteration 85, loss = 1.20293864\n",
      "Iteration 86, loss = 1.20251024\n",
      "Iteration 87, loss = 1.20210004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 88, loss = 1.20168140\n",
      "Iteration 89, loss = 1.20127977\n",
      "Iteration 90, loss = 1.20089712\n",
      "Iteration 91, loss = 1.20047924\n",
      "Iteration 92, loss = 1.20009922\n",
      "Iteration 93, loss = 1.19973291\n",
      "Iteration 94, loss = 1.19933435\n",
      "Iteration 95, loss = 1.19897441\n",
      "Iteration 96, loss = 1.19859759\n",
      "Iteration 97, loss = 1.19822930\n",
      "Iteration 98, loss = 1.19786687\n",
      "Iteration 99, loss = 1.19751827\n",
      "Iteration 100, loss = 1.19720267\n",
      "Iteration 1, loss = 1.51671839\n",
      "Iteration 2, loss = 1.50780971\n",
      "Iteration 3, loss = 1.49449152\n",
      "Iteration 4, loss = 1.47918269\n",
      "Iteration 5, loss = 1.46241506\n",
      "Iteration 6, loss = 1.44606751\n",
      "Iteration 7, loss = 1.43024196\n",
      "Iteration 8, loss = 1.41477240\n",
      "Iteration 9, loss = 1.40071365\n",
      "Iteration 10, loss = 1.38724291\n",
      "Iteration 11, loss = 1.37491654\n",
      "Iteration 12, loss = 1.36376219\n",
      "Iteration 13, loss = 1.35325884\n",
      "Iteration 14, loss = 1.34409639\n",
      "Iteration 15, loss = 1.33486373\n",
      "Iteration 16, loss = 1.32704845\n",
      "Iteration 17, loss = 1.31979794\n",
      "Iteration 18, loss = 1.31287635\n",
      "Iteration 19, loss = 1.30702756\n",
      "Iteration 20, loss = 1.30096697\n",
      "Iteration 21, loss = 1.29590566\n",
      "Iteration 22, loss = 1.29115464\n",
      "Iteration 23, loss = 1.28644019\n",
      "Iteration 24, loss = 1.28244235\n",
      "Iteration 25, loss = 1.27861861\n",
      "Iteration 26, loss = 1.27499083\n",
      "Iteration 27, loss = 1.27171095\n",
      "Iteration 28, loss = 1.26853783\n",
      "Iteration 29, loss = 1.26568925\n",
      "Iteration 30, loss = 1.26297663\n",
      "Iteration 31, loss = 1.26037868\n",
      "Iteration 32, loss = 1.25791365\n",
      "Iteration 33, loss = 1.25561618\n",
      "Iteration 34, loss = 1.25365593\n",
      "Iteration 35, loss = 1.25155248\n",
      "Iteration 36, loss = 1.24958783\n",
      "Iteration 37, loss = 1.24775519\n",
      "Iteration 38, loss = 1.24609951\n",
      "Iteration 39, loss = 1.24439757\n",
      "Iteration 40, loss = 1.24287738\n",
      "Iteration 41, loss = 1.24127747\n",
      "Iteration 42, loss = 1.23986756\n",
      "Iteration 43, loss = 1.23848531\n",
      "Iteration 44, loss = 1.23719285\n",
      "Iteration 45, loss = 1.23594000\n",
      "Iteration 46, loss = 1.23470361\n",
      "Iteration 47, loss = 1.23361378\n",
      "Iteration 48, loss = 1.23253189\n",
      "Iteration 49, loss = 1.23142268\n",
      "Iteration 50, loss = 1.23034679\n",
      "Iteration 51, loss = 1.22942315\n",
      "Iteration 52, loss = 1.22844441\n",
      "Iteration 53, loss = 1.22753510\n",
      "Iteration 54, loss = 1.22665698\n",
      "Iteration 55, loss = 1.22578117\n",
      "Iteration 56, loss = 1.22496966\n",
      "Iteration 57, loss = 1.22420247\n",
      "Iteration 58, loss = 1.22339056\n",
      "Iteration 59, loss = 1.22265582\n",
      "Iteration 60, loss = 1.22199224\n",
      "Iteration 61, loss = 1.22125521\n",
      "Iteration 62, loss = 1.22059452\n",
      "Iteration 63, loss = 1.21992539\n",
      "Iteration 64, loss = 1.21926499\n",
      "Iteration 65, loss = 1.21865871\n",
      "Iteration 66, loss = 1.21805712\n",
      "Iteration 67, loss = 1.21745897\n",
      "Iteration 68, loss = 1.21688859\n",
      "Iteration 69, loss = 1.21635710\n",
      "Iteration 70, loss = 1.21578504\n",
      "Iteration 71, loss = 1.21530385\n",
      "Iteration 72, loss = 1.21474238\n",
      "Iteration 73, loss = 1.21425062\n",
      "Iteration 74, loss = 1.21380159\n",
      "Iteration 75, loss = 1.21327349\n",
      "Iteration 76, loss = 1.21280513\n",
      "Iteration 77, loss = 1.21235929\n",
      "Iteration 78, loss = 1.21193177\n",
      "Iteration 79, loss = 1.21149240\n",
      "Iteration 80, loss = 1.21108716\n",
      "Iteration 81, loss = 1.21061754\n",
      "Iteration 82, loss = 1.21021048\n",
      "Iteration 83, loss = 1.20983570\n",
      "Iteration 84, loss = 1.20944989\n",
      "Iteration 85, loss = 1.20904597\n",
      "Iteration 86, loss = 1.20867072\n",
      "Iteration 87, loss = 1.20830664\n",
      "Iteration 88, loss = 1.20790495\n",
      "Iteration 89, loss = 1.20759166\n",
      "Iteration 90, loss = 1.20721627\n",
      "Iteration 91, loss = 1.20686417\n",
      "Iteration 92, loss = 1.20650406\n",
      "Iteration 93, loss = 1.20617710\n",
      "Iteration 94, loss = 1.20584021\n",
      "Iteration 95, loss = 1.20553303\n",
      "Iteration 96, loss = 1.20518851\n",
      "Iteration 97, loss = 1.20487985\n",
      "Iteration 98, loss = 1.20455512\n",
      "Iteration 99, loss = 1.20424520\n",
      "Iteration 100, loss = 1.20392806\n",
      "Iteration 1, loss = 1.51551323\n",
      "Iteration 2, loss = 1.50643610\n",
      "Iteration 3, loss = 1.49303576\n",
      "Iteration 4, loss = 1.47739996\n",
      "Iteration 5, loss = 1.46068901\n",
      "Iteration 6, loss = 1.44371015\n",
      "Iteration 7, loss = 1.42770974\n",
      "Iteration 8, loss = 1.41221077\n",
      "Iteration 9, loss = 1.39751210\n",
      "Iteration 10, loss = 1.38386647\n",
      "Iteration 11, loss = 1.37192639\n",
      "Iteration 12, loss = 1.35995627\n",
      "Iteration 13, loss = 1.34952436\n",
      "Iteration 14, loss = 1.34013893\n",
      "Iteration 15, loss = 1.33072699\n",
      "Iteration 16, loss = 1.32270428\n",
      "Iteration 17, loss = 1.31511862\n",
      "Iteration 18, loss = 1.30838911\n",
      "Iteration 19, loss = 1.30197862\n",
      "Iteration 20, loss = 1.29619792\n",
      "Iteration 21, loss = 1.29076897\n",
      "Iteration 22, loss = 1.28591117\n",
      "Iteration 23, loss = 1.28135619\n",
      "Iteration 24, loss = 1.27690932\n",
      "Iteration 25, loss = 1.27305427\n",
      "Iteration 26, loss = 1.26933767\n",
      "Iteration 27, loss = 1.26583715\n",
      "Iteration 28, loss = 1.26255414\n",
      "Iteration 29, loss = 1.25964873\n",
      "Iteration 30, loss = 1.25675359\n",
      "Iteration 31, loss = 1.25417586\n",
      "Iteration 32, loss = 1.25163578\n",
      "Iteration 33, loss = 1.24916252\n",
      "Iteration 34, loss = 1.24689621\n",
      "Iteration 35, loss = 1.24486037\n",
      "Iteration 36, loss = 1.24284983\n",
      "Iteration 37, loss = 1.24089613\n",
      "Iteration 38, loss = 1.23912899\n",
      "Iteration 39, loss = 1.23729152\n",
      "Iteration 40, loss = 1.23572482\n",
      "Iteration 41, loss = 1.23406204\n",
      "Iteration 42, loss = 1.23257774\n",
      "Iteration 43, loss = 1.23114008\n",
      "Iteration 44, loss = 1.22977794\n",
      "Iteration 45, loss = 1.22840357\n",
      "Iteration 46, loss = 1.22715739\n",
      "Iteration 47, loss = 1.22592541\n",
      "Iteration 48, loss = 1.22475116\n",
      "Iteration 49, loss = 1.22359838\n",
      "Iteration 50, loss = 1.22253619\n",
      "Iteration 51, loss = 1.22140132\n",
      "Iteration 52, loss = 1.22045331\n",
      "Iteration 53, loss = 1.21944843\n",
      "Iteration 54, loss = 1.21845029\n",
      "Iteration 55, loss = 1.21757139\n",
      "Iteration 56, loss = 1.21666624\n",
      "Iteration 57, loss = 1.21582727\n",
      "Iteration 58, loss = 1.21495006\n",
      "Iteration 59, loss = 1.21416384\n",
      "Iteration 60, loss = 1.21342553\n",
      "Iteration 61, loss = 1.21257804\n",
      "Iteration 62, loss = 1.21185853\n",
      "Iteration 63, loss = 1.21113244\n",
      "Iteration 64, loss = 1.21044376\n",
      "Iteration 65, loss = 1.20971044\n",
      "Iteration 66, loss = 1.20906584\n",
      "Iteration 67, loss = 1.20842250\n",
      "Iteration 68, loss = 1.20776264\n",
      "Iteration 69, loss = 1.20711559\n",
      "Iteration 70, loss = 1.20658084\n",
      "Iteration 71, loss = 1.20596337\n",
      "Iteration 72, loss = 1.20537628\n",
      "Iteration 73, loss = 1.20483401\n",
      "Iteration 74, loss = 1.20423110\n",
      "Iteration 75, loss = 1.20369574\n",
      "Iteration 76, loss = 1.20318831\n",
      "Iteration 77, loss = 1.20264705\n",
      "Iteration 78, loss = 1.20210804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79, loss = 1.20160109\n",
      "Iteration 80, loss = 1.20116483\n",
      "Iteration 81, loss = 1.20066445\n",
      "Iteration 82, loss = 1.20016928\n",
      "Iteration 83, loss = 1.19971691\n",
      "Iteration 84, loss = 1.19925218\n",
      "Iteration 85, loss = 1.19876986\n",
      "Iteration 86, loss = 1.19832598\n",
      "Iteration 87, loss = 1.19791482\n",
      "Iteration 88, loss = 1.19743813\n",
      "Iteration 89, loss = 1.19702086\n",
      "Iteration 90, loss = 1.19663284\n",
      "Iteration 91, loss = 1.19619468\n",
      "Iteration 92, loss = 1.19575879\n",
      "Iteration 93, loss = 1.19536619\n",
      "Iteration 94, loss = 1.19496111\n",
      "Iteration 95, loss = 1.19458278\n",
      "Iteration 96, loss = 1.19416552\n",
      "Iteration 97, loss = 1.19378228\n",
      "Iteration 98, loss = 1.19340761\n",
      "Iteration 99, loss = 1.19301350\n",
      "Iteration 100, loss = 1.19264954\n",
      "Iteration 1, loss = 12.11038895\n",
      "Iteration 2, loss = 9.91049475\n",
      "Iteration 3, loss = 11.25404468\n",
      "Iteration 4, loss = 5.38341809\n",
      "Iteration 5, loss = 1.56362076\n",
      "Iteration 6, loss = 1.37188245\n",
      "Iteration 7, loss = 1.28953992\n",
      "Iteration 8, loss = 1.24433680\n",
      "Iteration 9, loss = 1.23540936\n",
      "Iteration 10, loss = 1.22911805\n",
      "Iteration 11, loss = 1.22353898\n",
      "Iteration 12, loss = 1.21969169\n",
      "Iteration 13, loss = 1.21433232\n",
      "Iteration 14, loss = 1.20967327\n",
      "Iteration 15, loss = 1.20688573\n",
      "Iteration 16, loss = 1.20727437\n",
      "Iteration 17, loss = 1.20453729\n",
      "Iteration 18, loss = 1.20218119\n",
      "Iteration 19, loss = 1.20011656\n",
      "Iteration 20, loss = 1.19904676\n",
      "Iteration 21, loss = 1.19849154\n",
      "Iteration 22, loss = 1.19683664\n",
      "Iteration 23, loss = 1.19500148\n",
      "Iteration 24, loss = 1.19382225\n",
      "Iteration 25, loss = 1.19322639\n",
      "Iteration 26, loss = 1.19129407\n",
      "Iteration 27, loss = 1.19032719\n",
      "Iteration 28, loss = 1.18726378\n",
      "Iteration 29, loss = 1.18720829\n",
      "Iteration 30, loss = 1.18730858\n",
      "Iteration 31, loss = 1.18680363\n",
      "Iteration 32, loss = 1.18461644\n",
      "Iteration 33, loss = 1.18452222\n",
      "Iteration 34, loss = 1.18253349\n",
      "Iteration 35, loss = 1.17998144\n",
      "Iteration 36, loss = 1.18057745\n",
      "Iteration 37, loss = 1.17913177\n",
      "Iteration 38, loss = 1.18041665\n",
      "Iteration 39, loss = 1.17718568\n",
      "Iteration 40, loss = 1.17794106\n",
      "Iteration 41, loss = 1.17605756\n",
      "Iteration 42, loss = 1.17566083\n",
      "Iteration 43, loss = 1.17744720\n",
      "Iteration 44, loss = 1.17506495\n",
      "Iteration 45, loss = 1.17163821\n",
      "Iteration 46, loss = 1.17414944\n",
      "Iteration 47, loss = 1.17157716\n",
      "Iteration 48, loss = 1.17140303\n",
      "Iteration 49, loss = 1.17323485\n",
      "Iteration 50, loss = 1.16843229\n",
      "Iteration 51, loss = 1.16762759\n",
      "Iteration 52, loss = 1.16959749\n",
      "Iteration 53, loss = 1.17053201\n",
      "Iteration 54, loss = 1.16464276\n",
      "Iteration 55, loss = 1.16688398\n",
      "Iteration 56, loss = 1.16495001\n",
      "Iteration 57, loss = 1.17012212\n",
      "Iteration 58, loss = 1.16556476\n",
      "Iteration 59, loss = 1.16306272\n",
      "Iteration 60, loss = 1.16244335\n",
      "Iteration 61, loss = 1.16320262\n",
      "Iteration 62, loss = 1.16477192\n",
      "Iteration 63, loss = 1.16231759\n",
      "Iteration 64, loss = 1.16160016\n",
      "Iteration 65, loss = 1.15711389\n",
      "Iteration 66, loss = 1.15947104\n",
      "Iteration 67, loss = 1.16155456\n",
      "Iteration 68, loss = 1.15870019\n",
      "Iteration 69, loss = 1.15990139\n",
      "Iteration 70, loss = 1.15571946\n",
      "Iteration 71, loss = 1.16094059\n",
      "Iteration 72, loss = 1.15609780\n",
      "Iteration 73, loss = 1.15478139\n",
      "Iteration 74, loss = 1.16152295\n",
      "Iteration 75, loss = 1.15618414\n",
      "Iteration 76, loss = 1.15383404\n",
      "Iteration 77, loss = 1.15257960\n",
      "Iteration 78, loss = 1.15113679\n",
      "Iteration 79, loss = 1.16270271\n",
      "Iteration 80, loss = 1.15610846\n",
      "Iteration 81, loss = 1.15194743\n",
      "Iteration 82, loss = 1.15550460\n",
      "Iteration 83, loss = 1.14991184\n",
      "Iteration 84, loss = 1.14828991\n",
      "Iteration 85, loss = 1.14869665\n",
      "Iteration 86, loss = 1.15788544\n",
      "Iteration 87, loss = 1.14676646\n",
      "Iteration 88, loss = 1.14442919\n",
      "Iteration 89, loss = 1.14706383\n",
      "Iteration 90, loss = 1.14594647\n",
      "Iteration 91, loss = 1.14727355\n",
      "Iteration 92, loss = 1.15460483\n",
      "Iteration 93, loss = 1.14658267\n",
      "Iteration 94, loss = 1.14841136\n",
      "Iteration 95, loss = 1.14308608\n",
      "Iteration 96, loss = 1.14797393\n",
      "Iteration 97, loss = 1.14660642\n",
      "Iteration 98, loss = 1.14263400\n",
      "Iteration 99, loss = 1.14438349\n",
      "Iteration 100, loss = 1.14869337\n",
      "Iteration 1, loss = 1.51614707\n",
      "Iteration 2, loss = 1.50721370\n",
      "Iteration 3, loss = 1.49366593\n",
      "Iteration 4, loss = 1.47830590\n",
      "Iteration 5, loss = 1.46163480\n",
      "Iteration 6, loss = 1.44540556\n",
      "Iteration 7, loss = 1.42881642\n",
      "Iteration 8, loss = 1.41402539\n",
      "Iteration 9, loss = 1.39931096\n",
      "Iteration 10, loss = 1.38640518\n",
      "Iteration 11, loss = 1.37387190\n",
      "Iteration 12, loss = 1.36242364\n",
      "Iteration 13, loss = 1.35171323\n",
      "Iteration 14, loss = 1.34208578\n",
      "Iteration 15, loss = 1.33350648\n",
      "Iteration 16, loss = 1.32532283\n",
      "Iteration 17, loss = 1.31795711\n",
      "Iteration 18, loss = 1.31128881\n",
      "Iteration 19, loss = 1.30493031\n",
      "Iteration 20, loss = 1.29912920\n",
      "Iteration 21, loss = 1.29398354\n",
      "Iteration 22, loss = 1.28897382\n",
      "Iteration 23, loss = 1.28460491\n",
      "Iteration 24, loss = 1.28019459\n",
      "Iteration 25, loss = 1.27652209\n",
      "Iteration 26, loss = 1.27284667\n",
      "Iteration 27, loss = 1.26951592\n",
      "Iteration 28, loss = 1.26630452\n",
      "Iteration 29, loss = 1.26335585\n",
      "Iteration 30, loss = 1.26063054\n",
      "Iteration 31, loss = 1.25793009\n",
      "Iteration 32, loss = 1.25556146\n",
      "Iteration 33, loss = 1.25325359\n",
      "Iteration 34, loss = 1.25103271\n",
      "Iteration 35, loss = 1.24898756\n",
      "Iteration 36, loss = 1.24705634\n",
      "Iteration 37, loss = 1.24517812\n",
      "Iteration 38, loss = 1.24339777\n",
      "Iteration 39, loss = 1.24178118\n",
      "Iteration 40, loss = 1.24009964\n",
      "Iteration 41, loss = 1.23862888\n",
      "Iteration 42, loss = 1.23714038\n",
      "Iteration 43, loss = 1.23571154\n",
      "Iteration 44, loss = 1.23444443\n",
      "Iteration 45, loss = 1.23311694\n",
      "Iteration 46, loss = 1.23193400\n",
      "Iteration 47, loss = 1.23072165\n",
      "Iteration 48, loss = 1.22958152\n",
      "Iteration 49, loss = 1.22848045\n",
      "Iteration 50, loss = 1.22745588\n",
      "Iteration 51, loss = 1.22650918\n",
      "Iteration 52, loss = 1.22548023\n",
      "Iteration 53, loss = 1.22455913\n",
      "Iteration 54, loss = 1.22364896\n",
      "Iteration 55, loss = 1.22276485\n",
      "Iteration 56, loss = 1.22189346\n",
      "Iteration 57, loss = 1.22117313\n",
      "Iteration 58, loss = 1.22033507\n",
      "Iteration 59, loss = 1.21960634\n",
      "Iteration 60, loss = 1.21885070\n",
      "Iteration 61, loss = 1.21808586\n",
      "Iteration 62, loss = 1.21740529\n",
      "Iteration 63, loss = 1.21673639\n",
      "Iteration 64, loss = 1.21606698\n",
      "Iteration 65, loss = 1.21543581\n",
      "Iteration 66, loss = 1.21481842\n",
      "Iteration 67, loss = 1.21426138\n",
      "Iteration 68, loss = 1.21360233\n",
      "Iteration 69, loss = 1.21302007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, loss = 1.21245355\n",
      "Iteration 71, loss = 1.21188515\n",
      "Iteration 72, loss = 1.21141635\n",
      "Iteration 73, loss = 1.21087479\n",
      "Iteration 74, loss = 1.21035251\n",
      "Iteration 75, loss = 1.20982347\n",
      "Iteration 76, loss = 1.20936054\n",
      "Iteration 77, loss = 1.20885372\n",
      "Iteration 78, loss = 1.20839168\n",
      "Iteration 79, loss = 1.20795491\n",
      "Iteration 80, loss = 1.20753976\n",
      "Iteration 81, loss = 1.20706378\n",
      "Iteration 82, loss = 1.20659065\n",
      "Iteration 83, loss = 1.20619502\n",
      "Iteration 84, loss = 1.20577514\n",
      "Iteration 85, loss = 1.20537597\n",
      "Iteration 86, loss = 1.20496525\n",
      "Iteration 87, loss = 1.20455003\n",
      "Iteration 88, loss = 1.20420071\n",
      "Iteration 89, loss = 1.20381923\n",
      "Iteration 90, loss = 1.20342788\n",
      "Iteration 91, loss = 1.20306057\n",
      "Iteration 92, loss = 1.20269870\n",
      "Iteration 93, loss = 1.20231686\n",
      "Iteration 94, loss = 1.20201677\n",
      "Iteration 95, loss = 1.20164070\n",
      "Iteration 96, loss = 1.20128635\n",
      "Iteration 97, loss = 1.20100607\n",
      "Iteration 98, loss = 1.20065113\n",
      "Iteration 99, loss = 1.20030527\n",
      "Iteration 100, loss = 1.19998327\n",
      "Iteration 101, loss = 1.19963053\n",
      "Iteration 102, loss = 1.19931357\n",
      "Iteration 103, loss = 1.19903537\n",
      "Iteration 104, loss = 1.19871298\n",
      "Iteration 105, loss = 1.19839488\n",
      "Iteration 106, loss = 1.19810615\n",
      "Iteration 107, loss = 1.19777999\n",
      "Iteration 108, loss = 1.19749155\n",
      "Iteration 109, loss = 1.19720085\n",
      "Iteration 110, loss = 1.19691246\n",
      "Iteration 111, loss = 1.19663178\n",
      "Iteration 112, loss = 1.19636148\n",
      "Iteration 113, loss = 1.19605222\n",
      "Iteration 114, loss = 1.19581603\n",
      "Iteration 115, loss = 1.19550698\n",
      "Iteration 116, loss = 1.19522825\n",
      "Iteration 117, loss = 1.19497739\n",
      "Iteration 118, loss = 1.19468549\n",
      "Iteration 119, loss = 1.19441812\n",
      "Iteration 120, loss = 1.19416940\n",
      "Iteration 121, loss = 1.19390975\n",
      "Iteration 122, loss = 1.19366236\n",
      "Iteration 123, loss = 1.19336666\n",
      "Iteration 124, loss = 1.19310914\n",
      "Iteration 125, loss = 1.19284857\n",
      "Iteration 126, loss = 1.19260524\n",
      "Iteration 127, loss = 1.19236922\n",
      "Iteration 128, loss = 1.19211692\n",
      "Iteration 129, loss = 1.19185681\n",
      "Iteration 130, loss = 1.19161279\n",
      "Iteration 131, loss = 1.19136112\n",
      "Iteration 132, loss = 1.19112999\n",
      "Iteration 133, loss = 1.19086806\n",
      "Iteration 134, loss = 1.19063088\n",
      "Iteration 135, loss = 1.19040505\n",
      "Iteration 136, loss = 1.19015827\n",
      "Iteration 137, loss = 1.18994166\n",
      "Iteration 138, loss = 1.18967102\n",
      "Iteration 139, loss = 1.18946751\n",
      "Iteration 140, loss = 1.18919825\n",
      "Iteration 141, loss = 1.18898083\n",
      "Iteration 142, loss = 1.18878422\n",
      "Iteration 143, loss = 1.18851367\n",
      "Iteration 144, loss = 1.18827296\n",
      "Iteration 145, loss = 1.18805304\n",
      "Iteration 146, loss = 1.18783363\n",
      "Iteration 147, loss = 1.18759620\n",
      "Iteration 148, loss = 1.18736440\n",
      "Iteration 149, loss = 1.18714017\n",
      "Iteration 150, loss = 1.18693721\n",
      "Iteration 151, loss = 1.18670256\n",
      "Iteration 152, loss = 1.18648864\n",
      "Iteration 153, loss = 1.18625504\n",
      "Iteration 154, loss = 1.18604874\n",
      "Iteration 155, loss = 1.18582074\n",
      "Iteration 156, loss = 1.18560143\n",
      "Iteration 157, loss = 1.18537446\n",
      "Iteration 158, loss = 1.18517930\n",
      "Iteration 159, loss = 1.18496795\n",
      "Iteration 160, loss = 1.18474191\n",
      "Iteration 161, loss = 1.18450497\n",
      "Iteration 162, loss = 1.18433643\n",
      "Iteration 163, loss = 1.18407724\n",
      "Iteration 164, loss = 1.18386220\n",
      "Iteration 165, loss = 1.18364737\n",
      "Iteration 166, loss = 1.18342182\n",
      "Iteration 167, loss = 1.18322256\n",
      "Iteration 168, loss = 1.18300571\n",
      "Iteration 169, loss = 1.18280281\n",
      "Iteration 170, loss = 1.18259165\n",
      "Iteration 171, loss = 1.18236053\n",
      "Iteration 172, loss = 1.18215545\n",
      "Iteration 173, loss = 1.18196001\n",
      "Iteration 174, loss = 1.18174329\n",
      "Iteration 175, loss = 1.18153501\n",
      "Iteration 176, loss = 1.18131907\n",
      "Iteration 177, loss = 1.18110242\n",
      "Iteration 178, loss = 1.18090540\n",
      "Iteration 179, loss = 1.18068460\n",
      "Iteration 180, loss = 1.18049240\n",
      "Iteration 181, loss = 1.18026973\n",
      "Iteration 182, loss = 1.18007844\n",
      "Iteration 183, loss = 1.17986504\n",
      "Iteration 184, loss = 1.17965568\n",
      "Iteration 185, loss = 1.17944166\n",
      "Iteration 186, loss = 1.17926265\n",
      "Iteration 187, loss = 1.17903280\n",
      "Iteration 188, loss = 1.17882751\n",
      "Iteration 189, loss = 1.17861540\n",
      "Iteration 190, loss = 1.17843515\n",
      "Iteration 191, loss = 1.17819670\n",
      "Iteration 192, loss = 1.17800674\n",
      "Iteration 193, loss = 1.17780447\n",
      "Iteration 194, loss = 1.17759053\n",
      "Iteration 195, loss = 1.17737661\n",
      "Iteration 196, loss = 1.17717063\n",
      "Iteration 197, loss = 1.17702012\n",
      "Iteration 198, loss = 1.17676597\n",
      "Iteration 199, loss = 1.17657803\n",
      "Iteration 200, loss = 1.17635302\n",
      "Iteration 1, loss = 1.51638664\n",
      "Iteration 2, loss = 1.50736711\n",
      "Iteration 3, loss = 1.49399211\n",
      "Iteration 4, loss = 1.47866355\n",
      "Iteration 5, loss = 1.46212073\n",
      "Iteration 6, loss = 1.44564054\n",
      "Iteration 7, loss = 1.42932412\n",
      "Iteration 8, loss = 1.41424433\n",
      "Iteration 9, loss = 1.39979458\n",
      "Iteration 10, loss = 1.38667448\n",
      "Iteration 11, loss = 1.37414017\n",
      "Iteration 12, loss = 1.36283308\n",
      "Iteration 13, loss = 1.35204893\n",
      "Iteration 14, loss = 1.34278183\n",
      "Iteration 15, loss = 1.33380194\n",
      "Iteration 16, loss = 1.32571878\n",
      "Iteration 17, loss = 1.31842617\n",
      "Iteration 18, loss = 1.31155628\n",
      "Iteration 19, loss = 1.30545361\n",
      "Iteration 20, loss = 1.29961253\n",
      "Iteration 21, loss = 1.29436458\n",
      "Iteration 22, loss = 1.28938742\n",
      "Iteration 23, loss = 1.28495989\n",
      "Iteration 24, loss = 1.28066616\n",
      "Iteration 25, loss = 1.27677221\n",
      "Iteration 26, loss = 1.27324415\n",
      "Iteration 27, loss = 1.26998107\n",
      "Iteration 28, loss = 1.26662618\n",
      "Iteration 29, loss = 1.26373827\n",
      "Iteration 30, loss = 1.26098675\n",
      "Iteration 31, loss = 1.25831170\n",
      "Iteration 32, loss = 1.25580962\n",
      "Iteration 33, loss = 1.25354954\n",
      "Iteration 34, loss = 1.25146682\n",
      "Iteration 35, loss = 1.24927663\n",
      "Iteration 36, loss = 1.24735165\n",
      "Iteration 37, loss = 1.24547506\n",
      "Iteration 38, loss = 1.24373358\n",
      "Iteration 39, loss = 1.24201156\n",
      "Iteration 40, loss = 1.24039051\n",
      "Iteration 41, loss = 1.23889676\n",
      "Iteration 42, loss = 1.23736829\n",
      "Iteration 43, loss = 1.23594653\n",
      "Iteration 44, loss = 1.23467784\n",
      "Iteration 45, loss = 1.23338779\n",
      "Iteration 46, loss = 1.23212356\n",
      "Iteration 47, loss = 1.23090864\n",
      "Iteration 48, loss = 1.22978273\n",
      "Iteration 49, loss = 1.22869094\n",
      "Iteration 50, loss = 1.22761497\n",
      "Iteration 51, loss = 1.22662817\n",
      "Iteration 52, loss = 1.22557882\n",
      "Iteration 53, loss = 1.22467948\n",
      "Iteration 54, loss = 1.22375150\n",
      "Iteration 55, loss = 1.22283722\n",
      "Iteration 56, loss = 1.22199215\n",
      "Iteration 57, loss = 1.22122344\n",
      "Iteration 58, loss = 1.22034182\n",
      "Iteration 59, loss = 1.21959151\n",
      "Iteration 60, loss = 1.21885717\n",
      "Iteration 61, loss = 1.21808774\n",
      "Iteration 62, loss = 1.21735140\n",
      "Iteration 63, loss = 1.21669819\n",
      "Iteration 64, loss = 1.21600008\n",
      "Iteration 65, loss = 1.21536084\n",
      "Iteration 66, loss = 1.21472486\n",
      "Iteration 67, loss = 1.21414005\n",
      "Iteration 68, loss = 1.21350336\n",
      "Iteration 69, loss = 1.21288226\n",
      "Iteration 70, loss = 1.21230509\n",
      "Iteration 71, loss = 1.21174854\n",
      "Iteration 72, loss = 1.21121852\n",
      "Iteration 73, loss = 1.21068557\n",
      "Iteration 74, loss = 1.21015102\n",
      "Iteration 75, loss = 1.20961978\n",
      "Iteration 76, loss = 1.20914392\n",
      "Iteration 77, loss = 1.20864670\n",
      "Iteration 78, loss = 1.20816865\n",
      "Iteration 79, loss = 1.20769960\n",
      "Iteration 80, loss = 1.20727359\n",
      "Iteration 81, loss = 1.20675701\n",
      "Iteration 82, loss = 1.20631014\n",
      "Iteration 83, loss = 1.20589430\n",
      "Iteration 84, loss = 1.20546034\n",
      "Iteration 85, loss = 1.20503216\n",
      "Iteration 86, loss = 1.20462141\n",
      "Iteration 87, loss = 1.20420067\n",
      "Iteration 88, loss = 1.20381766\n",
      "Iteration 89, loss = 1.20344437\n",
      "Iteration 90, loss = 1.20302754\n",
      "Iteration 91, loss = 1.20265079\n",
      "Iteration 92, loss = 1.20226791\n",
      "Iteration 93, loss = 1.20189039\n",
      "Iteration 94, loss = 1.20153299\n",
      "Iteration 95, loss = 1.20118813\n",
      "Iteration 96, loss = 1.20079019\n",
      "Iteration 97, loss = 1.20047542\n",
      "Iteration 98, loss = 1.20012295\n",
      "Iteration 99, loss = 1.19979416\n",
      "Iteration 100, loss = 1.19943650\n",
      "Iteration 101, loss = 1.19909239\n",
      "Iteration 102, loss = 1.19876665\n",
      "Iteration 103, loss = 1.19844299\n",
      "Iteration 104, loss = 1.19811443\n",
      "Iteration 105, loss = 1.19781541\n",
      "Iteration 106, loss = 1.19751354\n",
      "Iteration 107, loss = 1.19715487\n",
      "Iteration 108, loss = 1.19686972\n",
      "Iteration 109, loss = 1.19657150\n",
      "Iteration 110, loss = 1.19628996\n",
      "Iteration 111, loss = 1.19595900\n",
      "Iteration 112, loss = 1.19569273\n",
      "Iteration 113, loss = 1.19537267\n",
      "Iteration 114, loss = 1.19511326\n",
      "Iteration 115, loss = 1.19481468\n",
      "Iteration 116, loss = 1.19451579\n",
      "Iteration 117, loss = 1.19423607\n",
      "Iteration 118, loss = 1.19395466\n",
      "Iteration 119, loss = 1.19369406\n",
      "Iteration 120, loss = 1.19341590\n",
      "Iteration 121, loss = 1.19315540\n",
      "Iteration 122, loss = 1.19285538\n",
      "Iteration 123, loss = 1.19259814\n",
      "Iteration 124, loss = 1.19230916\n",
      "Iteration 125, loss = 1.19205427\n",
      "Iteration 126, loss = 1.19177397\n",
      "Iteration 127, loss = 1.19153418\n",
      "Iteration 128, loss = 1.19126595\n",
      "Iteration 129, loss = 1.19099623\n",
      "Iteration 130, loss = 1.19076477\n",
      "Iteration 131, loss = 1.19048884\n",
      "Iteration 132, loss = 1.19025824\n",
      "Iteration 133, loss = 1.18997654\n",
      "Iteration 134, loss = 1.18973099\n",
      "Iteration 135, loss = 1.18947710\n",
      "Iteration 136, loss = 1.18923922\n",
      "Iteration 137, loss = 1.18900249\n",
      "Iteration 138, loss = 1.18874980\n",
      "Iteration 139, loss = 1.18852398\n",
      "Iteration 140, loss = 1.18826021\n",
      "Iteration 141, loss = 1.18803932\n",
      "Iteration 142, loss = 1.18780082\n",
      "Iteration 143, loss = 1.18752533\n",
      "Iteration 144, loss = 1.18728533\n",
      "Iteration 145, loss = 1.18703318\n",
      "Iteration 146, loss = 1.18679896\n",
      "Iteration 147, loss = 1.18655655\n",
      "Iteration 148, loss = 1.18631157\n",
      "Iteration 149, loss = 1.18607708\n",
      "Iteration 150, loss = 1.18584929\n",
      "Iteration 151, loss = 1.18562971\n",
      "Iteration 152, loss = 1.18537110\n",
      "Iteration 153, loss = 1.18515248\n",
      "Iteration 154, loss = 1.18493780\n",
      "Iteration 155, loss = 1.18468088\n",
      "Iteration 156, loss = 1.18444944\n",
      "Iteration 157, loss = 1.18420689\n",
      "Iteration 158, loss = 1.18400202\n",
      "Iteration 159, loss = 1.18378254\n",
      "Iteration 160, loss = 1.18352596\n",
      "Iteration 161, loss = 1.18331420\n",
      "Iteration 162, loss = 1.18308328\n",
      "Iteration 163, loss = 1.18284087\n",
      "Iteration 164, loss = 1.18263013\n",
      "Iteration 165, loss = 1.18240594\n",
      "Iteration 166, loss = 1.18217907\n",
      "Iteration 167, loss = 1.18199020\n",
      "Iteration 168, loss = 1.18173797\n",
      "Iteration 169, loss = 1.18154802\n",
      "Iteration 170, loss = 1.18128393\n",
      "Iteration 171, loss = 1.18105682\n",
      "Iteration 172, loss = 1.18084288\n",
      "Iteration 173, loss = 1.18064645\n",
      "Iteration 174, loss = 1.18040649\n",
      "Iteration 175, loss = 1.18017699\n",
      "Iteration 176, loss = 1.17996309\n",
      "Iteration 177, loss = 1.17974423\n",
      "Iteration 178, loss = 1.17952181\n",
      "Iteration 179, loss = 1.17930225\n",
      "Iteration 180, loss = 1.17910442\n",
      "Iteration 181, loss = 1.17886317\n",
      "Iteration 182, loss = 1.17865450\n",
      "Iteration 183, loss = 1.17842708\n",
      "Iteration 184, loss = 1.17821198\n",
      "Iteration 185, loss = 1.17798294\n",
      "Iteration 186, loss = 1.17777015\n",
      "Iteration 187, loss = 1.17755661\n",
      "Iteration 188, loss = 1.17734780\n",
      "Iteration 189, loss = 1.17711704\n",
      "Iteration 190, loss = 1.17691918\n",
      "Iteration 191, loss = 1.17668224\n",
      "Iteration 192, loss = 1.17646758\n",
      "Iteration 193, loss = 1.17624725\n",
      "Iteration 194, loss = 1.17604411\n",
      "Iteration 195, loss = 1.17582276\n",
      "Iteration 196, loss = 1.17561319\n",
      "Iteration 197, loss = 1.17541319\n",
      "Iteration 198, loss = 1.17518506\n",
      "Iteration 199, loss = 1.17498751\n",
      "Iteration 200, loss = 1.17476091\n",
      "Iteration 1, loss = 1.51658759\n",
      "Iteration 2, loss = 1.50750331\n",
      "Iteration 3, loss = 1.49420954\n",
      "Iteration 4, loss = 1.47901319\n",
      "Iteration 5, loss = 1.46200911\n",
      "Iteration 6, loss = 1.44546808\n",
      "Iteration 7, loss = 1.42950421\n",
      "Iteration 8, loss = 1.41414199\n",
      "Iteration 9, loss = 1.39965147\n",
      "Iteration 10, loss = 1.38631706\n",
      "Iteration 11, loss = 1.37390963\n",
      "Iteration 12, loss = 1.36263602\n",
      "Iteration 13, loss = 1.35187948\n",
      "Iteration 14, loss = 1.34251327\n",
      "Iteration 15, loss = 1.33334527\n",
      "Iteration 16, loss = 1.32534597\n",
      "Iteration 17, loss = 1.31794115\n",
      "Iteration 18, loss = 1.31106900\n",
      "Iteration 19, loss = 1.30486388\n",
      "Iteration 20, loss = 1.29889565\n",
      "Iteration 21, loss = 1.29364508\n",
      "Iteration 22, loss = 1.28860833\n",
      "Iteration 23, loss = 1.28403840\n",
      "Iteration 24, loss = 1.27986732\n",
      "Iteration 25, loss = 1.27606983\n",
      "Iteration 26, loss = 1.27224618\n",
      "Iteration 27, loss = 1.26887457\n",
      "Iteration 28, loss = 1.26549662\n",
      "Iteration 29, loss = 1.26269167\n",
      "Iteration 30, loss = 1.25978358\n",
      "Iteration 31, loss = 1.25712210\n",
      "Iteration 32, loss = 1.25455278\n",
      "Iteration 33, loss = 1.25224685\n",
      "Iteration 34, loss = 1.25004769\n",
      "Iteration 35, loss = 1.24797648\n",
      "Iteration 36, loss = 1.24598040\n",
      "Iteration 37, loss = 1.24402850\n",
      "Iteration 38, loss = 1.24223934\n",
      "Iteration 39, loss = 1.24048241\n",
      "Iteration 40, loss = 1.23887414\n",
      "Iteration 41, loss = 1.23735564\n",
      "Iteration 42, loss = 1.23580608\n",
      "Iteration 43, loss = 1.23438923\n",
      "Iteration 44, loss = 1.23304879\n",
      "Iteration 45, loss = 1.23169700\n",
      "Iteration 46, loss = 1.23039959\n",
      "Iteration 47, loss = 1.22926273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 1.22814803\n",
      "Iteration 49, loss = 1.22702474\n",
      "Iteration 50, loss = 1.22586926\n",
      "Iteration 51, loss = 1.22491483\n",
      "Iteration 52, loss = 1.22389634\n",
      "Iteration 53, loss = 1.22293036\n",
      "Iteration 54, loss = 1.22197797\n",
      "Iteration 55, loss = 1.22108776\n",
      "Iteration 56, loss = 1.22023966\n",
      "Iteration 57, loss = 1.21944647\n",
      "Iteration 58, loss = 1.21856300\n",
      "Iteration 59, loss = 1.21779182\n",
      "Iteration 60, loss = 1.21706912\n",
      "Iteration 61, loss = 1.21629368\n",
      "Iteration 62, loss = 1.21553760\n",
      "Iteration 63, loss = 1.21485706\n",
      "Iteration 64, loss = 1.21418196\n",
      "Iteration 65, loss = 1.21349631\n",
      "Iteration 66, loss = 1.21283445\n",
      "Iteration 67, loss = 1.21221404\n",
      "Iteration 68, loss = 1.21160077\n",
      "Iteration 69, loss = 1.21096500\n",
      "Iteration 70, loss = 1.21038836\n",
      "Iteration 71, loss = 1.20985542\n",
      "Iteration 72, loss = 1.20926367\n",
      "Iteration 73, loss = 1.20871479\n",
      "Iteration 74, loss = 1.20818492\n",
      "Iteration 75, loss = 1.20764297\n",
      "Iteration 76, loss = 1.20710579\n",
      "Iteration 77, loss = 1.20663355\n",
      "Iteration 78, loss = 1.20613394\n",
      "Iteration 79, loss = 1.20568674\n",
      "Iteration 80, loss = 1.20521060\n",
      "Iteration 81, loss = 1.20469524\n",
      "Iteration 82, loss = 1.20426732\n",
      "Iteration 83, loss = 1.20381093\n",
      "Iteration 84, loss = 1.20339973\n",
      "Iteration 85, loss = 1.20293864\n",
      "Iteration 86, loss = 1.20251024\n",
      "Iteration 87, loss = 1.20210004\n",
      "Iteration 88, loss = 1.20168140\n",
      "Iteration 89, loss = 1.20127977\n",
      "Iteration 90, loss = 1.20089712\n",
      "Iteration 91, loss = 1.20047924\n",
      "Iteration 92, loss = 1.20009922\n",
      "Iteration 93, loss = 1.19973291\n",
      "Iteration 94, loss = 1.19933435\n",
      "Iteration 95, loss = 1.19897441\n",
      "Iteration 96, loss = 1.19859759\n",
      "Iteration 97, loss = 1.19822930\n",
      "Iteration 98, loss = 1.19786687\n",
      "Iteration 99, loss = 1.19751827\n",
      "Iteration 100, loss = 1.19720267\n",
      "Iteration 101, loss = 1.19682662\n",
      "Iteration 102, loss = 1.19648030\n",
      "Iteration 103, loss = 1.19614157\n",
      "Iteration 104, loss = 1.19581333\n",
      "Iteration 105, loss = 1.19547636\n",
      "Iteration 106, loss = 1.19513467\n",
      "Iteration 107, loss = 1.19479089\n",
      "Iteration 108, loss = 1.19449543\n",
      "Iteration 109, loss = 1.19420666\n",
      "Iteration 110, loss = 1.19386951\n",
      "Iteration 111, loss = 1.19352783\n",
      "Iteration 112, loss = 1.19324102\n",
      "Iteration 113, loss = 1.19293418\n",
      "Iteration 114, loss = 1.19262300\n",
      "Iteration 115, loss = 1.19233436\n",
      "Iteration 116, loss = 1.19201871\n",
      "Iteration 117, loss = 1.19173105\n",
      "Iteration 118, loss = 1.19145305\n",
      "Iteration 119, loss = 1.19114155\n",
      "Iteration 120, loss = 1.19085722\n",
      "Iteration 121, loss = 1.19056635\n",
      "Iteration 122, loss = 1.19027690\n",
      "Iteration 123, loss = 1.19001064\n",
      "Iteration 124, loss = 1.18974376\n",
      "Iteration 125, loss = 1.18945548\n",
      "Iteration 126, loss = 1.18915493\n",
      "Iteration 127, loss = 1.18888986\n",
      "Iteration 128, loss = 1.18861112\n",
      "Iteration 129, loss = 1.18833106\n",
      "Iteration 130, loss = 1.18808948\n",
      "Iteration 131, loss = 1.18777841\n",
      "Iteration 132, loss = 1.18752913\n",
      "Iteration 133, loss = 1.18725402\n",
      "Iteration 134, loss = 1.18698216\n",
      "Iteration 135, loss = 1.18671105\n",
      "Iteration 136, loss = 1.18646137\n",
      "Iteration 137, loss = 1.18621165\n",
      "Iteration 138, loss = 1.18594267\n",
      "Iteration 139, loss = 1.18566970\n",
      "Iteration 140, loss = 1.18542209\n",
      "Iteration 141, loss = 1.18516448\n",
      "Iteration 142, loss = 1.18491211\n",
      "Iteration 143, loss = 1.18463351\n",
      "Iteration 144, loss = 1.18438439\n",
      "Iteration 145, loss = 1.18414504\n",
      "Iteration 146, loss = 1.18389406\n",
      "Iteration 147, loss = 1.18363432\n",
      "Iteration 148, loss = 1.18336891\n",
      "Iteration 149, loss = 1.18315459\n",
      "Iteration 150, loss = 1.18287334\n",
      "Iteration 151, loss = 1.18264534\n",
      "Iteration 152, loss = 1.18238885\n",
      "Iteration 153, loss = 1.18213317\n",
      "Iteration 154, loss = 1.18189095\n",
      "Iteration 155, loss = 1.18163921\n",
      "Iteration 156, loss = 1.18139948\n",
      "Iteration 157, loss = 1.18115603\n",
      "Iteration 158, loss = 1.18090418\n",
      "Iteration 159, loss = 1.18066580\n",
      "Iteration 160, loss = 1.18043266\n",
      "Iteration 161, loss = 1.18019024\n",
      "Iteration 162, loss = 1.17996268\n",
      "Iteration 163, loss = 1.17969754\n",
      "Iteration 164, loss = 1.17946236\n",
      "Iteration 165, loss = 1.17922788\n",
      "Iteration 166, loss = 1.17898691\n",
      "Iteration 167, loss = 1.17876547\n",
      "Iteration 168, loss = 1.17851552\n",
      "Iteration 169, loss = 1.17827960\n",
      "Iteration 170, loss = 1.17804344\n",
      "Iteration 171, loss = 1.17779730\n",
      "Iteration 172, loss = 1.17756288\n",
      "Iteration 173, loss = 1.17735118\n",
      "Iteration 174, loss = 1.17709474\n",
      "Iteration 175, loss = 1.17686332\n",
      "Iteration 176, loss = 1.17662591\n",
      "Iteration 177, loss = 1.17639154\n",
      "Iteration 178, loss = 1.17617743\n",
      "Iteration 179, loss = 1.17594217\n",
      "Iteration 180, loss = 1.17574331\n",
      "Iteration 181, loss = 1.17548537\n",
      "Iteration 182, loss = 1.17525846\n",
      "Iteration 183, loss = 1.17503444\n",
      "Iteration 184, loss = 1.17478322\n",
      "Iteration 185, loss = 1.17454411\n",
      "Iteration 186, loss = 1.17432808\n",
      "Iteration 187, loss = 1.17411887\n",
      "Iteration 188, loss = 1.17389064\n",
      "Iteration 189, loss = 1.17364615\n",
      "Iteration 190, loss = 1.17343434\n",
      "Iteration 191, loss = 1.17319828\n",
      "Iteration 192, loss = 1.17296557\n",
      "Iteration 193, loss = 1.17275322\n",
      "Iteration 194, loss = 1.17253512\n",
      "Iteration 195, loss = 1.17233067\n",
      "Iteration 196, loss = 1.17208151\n",
      "Iteration 197, loss = 1.17186413\n",
      "Iteration 198, loss = 1.17162318\n",
      "Iteration 199, loss = 1.17141937\n",
      "Iteration 200, loss = 1.17120529\n",
      "Iteration 1, loss = 1.51671839\n",
      "Iteration 2, loss = 1.50780971\n",
      "Iteration 3, loss = 1.49449152\n",
      "Iteration 4, loss = 1.47918269\n",
      "Iteration 5, loss = 1.46241506\n",
      "Iteration 6, loss = 1.44606751\n",
      "Iteration 7, loss = 1.43024196\n",
      "Iteration 8, loss = 1.41477240\n",
      "Iteration 9, loss = 1.40071365\n",
      "Iteration 10, loss = 1.38724291\n",
      "Iteration 11, loss = 1.37491654\n",
      "Iteration 12, loss = 1.36376219\n",
      "Iteration 13, loss = 1.35325884\n",
      "Iteration 14, loss = 1.34409639\n",
      "Iteration 15, loss = 1.33486373\n",
      "Iteration 16, loss = 1.32704845\n",
      "Iteration 17, loss = 1.31979794\n",
      "Iteration 18, loss = 1.31287635\n",
      "Iteration 19, loss = 1.30702756\n",
      "Iteration 20, loss = 1.30096697\n",
      "Iteration 21, loss = 1.29590566\n",
      "Iteration 22, loss = 1.29115464\n",
      "Iteration 23, loss = 1.28644019\n",
      "Iteration 24, loss = 1.28244235\n",
      "Iteration 25, loss = 1.27861861\n",
      "Iteration 26, loss = 1.27499083\n",
      "Iteration 27, loss = 1.27171095\n",
      "Iteration 28, loss = 1.26853783\n",
      "Iteration 29, loss = 1.26568925\n",
      "Iteration 30, loss = 1.26297663\n",
      "Iteration 31, loss = 1.26037868\n",
      "Iteration 32, loss = 1.25791365\n",
      "Iteration 33, loss = 1.25561618\n",
      "Iteration 34, loss = 1.25365593\n",
      "Iteration 35, loss = 1.25155248\n",
      "Iteration 36, loss = 1.24958783\n",
      "Iteration 37, loss = 1.24775519\n",
      "Iteration 38, loss = 1.24609951\n",
      "Iteration 39, loss = 1.24439757\n",
      "Iteration 40, loss = 1.24287738\n",
      "Iteration 41, loss = 1.24127747\n",
      "Iteration 42, loss = 1.23986756\n",
      "Iteration 43, loss = 1.23848531\n",
      "Iteration 44, loss = 1.23719285\n",
      "Iteration 45, loss = 1.23594000\n",
      "Iteration 46, loss = 1.23470361\n",
      "Iteration 47, loss = 1.23361378\n",
      "Iteration 48, loss = 1.23253189\n",
      "Iteration 49, loss = 1.23142268\n",
      "Iteration 50, loss = 1.23034679\n",
      "Iteration 51, loss = 1.22942315\n",
      "Iteration 52, loss = 1.22844441\n",
      "Iteration 53, loss = 1.22753510\n",
      "Iteration 54, loss = 1.22665698\n",
      "Iteration 55, loss = 1.22578117\n",
      "Iteration 56, loss = 1.22496966\n",
      "Iteration 57, loss = 1.22420247\n",
      "Iteration 58, loss = 1.22339056\n",
      "Iteration 59, loss = 1.22265582\n",
      "Iteration 60, loss = 1.22199224\n",
      "Iteration 61, loss = 1.22125521\n",
      "Iteration 62, loss = 1.22059452\n",
      "Iteration 63, loss = 1.21992539\n",
      "Iteration 64, loss = 1.21926499\n",
      "Iteration 65, loss = 1.21865871\n",
      "Iteration 66, loss = 1.21805712\n",
      "Iteration 67, loss = 1.21745897\n",
      "Iteration 68, loss = 1.21688859\n",
      "Iteration 69, loss = 1.21635710\n",
      "Iteration 70, loss = 1.21578504\n",
      "Iteration 71, loss = 1.21530385\n",
      "Iteration 72, loss = 1.21474238\n",
      "Iteration 73, loss = 1.21425062\n",
      "Iteration 74, loss = 1.21380159\n",
      "Iteration 75, loss = 1.21327349\n",
      "Iteration 76, loss = 1.21280513\n",
      "Iteration 77, loss = 1.21235929\n",
      "Iteration 78, loss = 1.21193177\n",
      "Iteration 79, loss = 1.21149240\n",
      "Iteration 80, loss = 1.21108716\n",
      "Iteration 81, loss = 1.21061754\n",
      "Iteration 82, loss = 1.21021048\n",
      "Iteration 83, loss = 1.20983570\n",
      "Iteration 84, loss = 1.20944989\n",
      "Iteration 85, loss = 1.20904597\n",
      "Iteration 86, loss = 1.20867072\n",
      "Iteration 87, loss = 1.20830664\n",
      "Iteration 88, loss = 1.20790495\n",
      "Iteration 89, loss = 1.20759166\n",
      "Iteration 90, loss = 1.20721627\n",
      "Iteration 91, loss = 1.20686417\n",
      "Iteration 92, loss = 1.20650406\n",
      "Iteration 93, loss = 1.20617710\n",
      "Iteration 94, loss = 1.20584021\n",
      "Iteration 95, loss = 1.20553303\n",
      "Iteration 96, loss = 1.20518851\n",
      "Iteration 97, loss = 1.20487985\n",
      "Iteration 98, loss = 1.20455512\n",
      "Iteration 99, loss = 1.20424520\n",
      "Iteration 100, loss = 1.20392806\n",
      "Iteration 101, loss = 1.20363608\n",
      "Iteration 102, loss = 1.20334736\n",
      "Iteration 103, loss = 1.20304730\n",
      "Iteration 104, loss = 1.20275270\n",
      "Iteration 105, loss = 1.20245048\n",
      "Iteration 106, loss = 1.20217869\n",
      "Iteration 107, loss = 1.20188059\n",
      "Iteration 108, loss = 1.20161698\n",
      "Iteration 109, loss = 1.20136452\n",
      "Iteration 110, loss = 1.20107250\n",
      "Iteration 111, loss = 1.20078266\n",
      "Iteration 112, loss = 1.20052982\n",
      "Iteration 113, loss = 1.20027434\n",
      "Iteration 114, loss = 1.19998938\n",
      "Iteration 115, loss = 1.19975307\n",
      "Iteration 116, loss = 1.19948484\n",
      "Iteration 117, loss = 1.19924277\n",
      "Iteration 118, loss = 1.19898930\n",
      "Iteration 119, loss = 1.19873202\n",
      "Iteration 120, loss = 1.19849954\n",
      "Iteration 121, loss = 1.19824384\n",
      "Iteration 122, loss = 1.19799970\n",
      "Iteration 123, loss = 1.19777654\n",
      "Iteration 124, loss = 1.19753667\n",
      "Iteration 125, loss = 1.19730243\n",
      "Iteration 126, loss = 1.19706476\n",
      "Iteration 127, loss = 1.19681682\n",
      "Iteration 128, loss = 1.19657795\n",
      "Iteration 129, loss = 1.19636115\n",
      "Iteration 130, loss = 1.19612133\n",
      "Iteration 131, loss = 1.19588312\n",
      "Iteration 132, loss = 1.19567012\n",
      "Iteration 133, loss = 1.19542747\n",
      "Iteration 134, loss = 1.19519927\n",
      "Iteration 135, loss = 1.19497418\n",
      "Iteration 136, loss = 1.19476385\n",
      "Iteration 137, loss = 1.19453188\n",
      "Iteration 138, loss = 1.19430960\n",
      "Iteration 139, loss = 1.19408555\n",
      "Iteration 140, loss = 1.19390886\n",
      "Iteration 141, loss = 1.19366621\n",
      "Iteration 142, loss = 1.19347761\n",
      "Iteration 143, loss = 1.19322784\n",
      "Iteration 144, loss = 1.19302172\n",
      "Iteration 145, loss = 1.19281446\n",
      "Iteration 146, loss = 1.19258679\n",
      "Iteration 147, loss = 1.19238681\n",
      "Iteration 148, loss = 1.19218693\n",
      "Iteration 149, loss = 1.19196451\n",
      "Iteration 150, loss = 1.19176792\n",
      "Iteration 151, loss = 1.19157275\n",
      "Iteration 152, loss = 1.19133891\n",
      "Iteration 153, loss = 1.19113662\n",
      "Iteration 154, loss = 1.19093397\n",
      "Iteration 155, loss = 1.19071982\n",
      "Iteration 156, loss = 1.19052408\n",
      "Iteration 157, loss = 1.19032487\n",
      "Iteration 158, loss = 1.19011272\n",
      "Iteration 159, loss = 1.18990893\n",
      "Iteration 160, loss = 1.18971048\n",
      "Iteration 161, loss = 1.18952285\n",
      "Iteration 162, loss = 1.18932681\n",
      "Iteration 163, loss = 1.18911045\n",
      "Iteration 164, loss = 1.18891105\n",
      "Iteration 165, loss = 1.18873064\n",
      "Iteration 166, loss = 1.18853066\n",
      "Iteration 167, loss = 1.18833773\n",
      "Iteration 168, loss = 1.18814511\n",
      "Iteration 169, loss = 1.18793707\n",
      "Iteration 170, loss = 1.18774342\n",
      "Iteration 171, loss = 1.18753236\n",
      "Iteration 172, loss = 1.18734980\n",
      "Iteration 173, loss = 1.18716374\n",
      "Iteration 174, loss = 1.18695222\n",
      "Iteration 175, loss = 1.18676613\n",
      "Iteration 176, loss = 1.18658257\n",
      "Iteration 177, loss = 1.18636835\n",
      "Iteration 178, loss = 1.18619036\n",
      "Iteration 179, loss = 1.18601107\n",
      "Iteration 180, loss = 1.18580165\n",
      "Iteration 181, loss = 1.18560730\n",
      "Iteration 182, loss = 1.18544711\n",
      "Iteration 183, loss = 1.18525121\n",
      "Iteration 184, loss = 1.18504379\n",
      "Iteration 185, loss = 1.18484341\n",
      "Iteration 186, loss = 1.18467068\n",
      "Iteration 187, loss = 1.18446898\n",
      "Iteration 188, loss = 1.18428126\n",
      "Iteration 189, loss = 1.18409132\n",
      "Iteration 190, loss = 1.18392393\n",
      "Iteration 191, loss = 1.18372227\n",
      "Iteration 192, loss = 1.18353320\n",
      "Iteration 193, loss = 1.18337715\n",
      "Iteration 194, loss = 1.18317158\n",
      "Iteration 195, loss = 1.18298473\n",
      "Iteration 196, loss = 1.18281890\n",
      "Iteration 197, loss = 1.18261514\n",
      "Iteration 198, loss = 1.18243744\n",
      "Iteration 199, loss = 1.18225341\n",
      "Iteration 200, loss = 1.18207785\n",
      "Iteration 1, loss = 1.51551323\n",
      "Iteration 2, loss = 1.50643610\n",
      "Iteration 3, loss = 1.49303576\n",
      "Iteration 4, loss = 1.47739996\n",
      "Iteration 5, loss = 1.46068901\n",
      "Iteration 6, loss = 1.44371015\n",
      "Iteration 7, loss = 1.42770974\n",
      "Iteration 8, loss = 1.41221077\n",
      "Iteration 9, loss = 1.39751210\n",
      "Iteration 10, loss = 1.38386647\n",
      "Iteration 11, loss = 1.37192639\n",
      "Iteration 12, loss = 1.35995627\n",
      "Iteration 13, loss = 1.34952436\n",
      "Iteration 14, loss = 1.34013893\n",
      "Iteration 15, loss = 1.33072699\n",
      "Iteration 16, loss = 1.32270428\n",
      "Iteration 17, loss = 1.31511862\n",
      "Iteration 18, loss = 1.30838911\n",
      "Iteration 19, loss = 1.30197862\n",
      "Iteration 20, loss = 1.29619792\n",
      "Iteration 21, loss = 1.29076897\n",
      "Iteration 22, loss = 1.28591117\n",
      "Iteration 23, loss = 1.28135619\n",
      "Iteration 24, loss = 1.27690932\n",
      "Iteration 25, loss = 1.27305427\n",
      "Iteration 26, loss = 1.26933767\n",
      "Iteration 27, loss = 1.26583715\n",
      "Iteration 28, loss = 1.26255414\n",
      "Iteration 29, loss = 1.25964873\n",
      "Iteration 30, loss = 1.25675359\n",
      "Iteration 31, loss = 1.25417586\n",
      "Iteration 32, loss = 1.25163578\n",
      "Iteration 33, loss = 1.24916252\n",
      "Iteration 34, loss = 1.24689621\n",
      "Iteration 35, loss = 1.24486037\n",
      "Iteration 36, loss = 1.24284983\n",
      "Iteration 37, loss = 1.24089613\n",
      "Iteration 38, loss = 1.23912899\n",
      "Iteration 39, loss = 1.23729152\n",
      "Iteration 40, loss = 1.23572482\n",
      "Iteration 41, loss = 1.23406204\n",
      "Iteration 42, loss = 1.23257774\n",
      "Iteration 43, loss = 1.23114008\n",
      "Iteration 44, loss = 1.22977794\n",
      "Iteration 45, loss = 1.22840357\n",
      "Iteration 46, loss = 1.22715739\n",
      "Iteration 47, loss = 1.22592541\n",
      "Iteration 48, loss = 1.22475116\n",
      "Iteration 49, loss = 1.22359838\n",
      "Iteration 50, loss = 1.22253619\n",
      "Iteration 51, loss = 1.22140132\n",
      "Iteration 52, loss = 1.22045331\n",
      "Iteration 53, loss = 1.21944843\n",
      "Iteration 54, loss = 1.21845029\n",
      "Iteration 55, loss = 1.21757139\n",
      "Iteration 56, loss = 1.21666624\n",
      "Iteration 57, loss = 1.21582727\n",
      "Iteration 58, loss = 1.21495006\n",
      "Iteration 59, loss = 1.21416384\n",
      "Iteration 60, loss = 1.21342553\n",
      "Iteration 61, loss = 1.21257804\n",
      "Iteration 62, loss = 1.21185853\n",
      "Iteration 63, loss = 1.21113244\n",
      "Iteration 64, loss = 1.21044376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65, loss = 1.20971044\n",
      "Iteration 66, loss = 1.20906584\n",
      "Iteration 67, loss = 1.20842250\n",
      "Iteration 68, loss = 1.20776264\n",
      "Iteration 69, loss = 1.20711559\n",
      "Iteration 70, loss = 1.20658084\n",
      "Iteration 71, loss = 1.20596337\n",
      "Iteration 72, loss = 1.20537628\n",
      "Iteration 73, loss = 1.20483401\n",
      "Iteration 74, loss = 1.20423110\n",
      "Iteration 75, loss = 1.20369574\n",
      "Iteration 76, loss = 1.20318831\n",
      "Iteration 77, loss = 1.20264705\n",
      "Iteration 78, loss = 1.20210804\n",
      "Iteration 79, loss = 1.20160109\n",
      "Iteration 80, loss = 1.20116483\n",
      "Iteration 81, loss = 1.20066445\n",
      "Iteration 82, loss = 1.20016928\n",
      "Iteration 83, loss = 1.19971691\n",
      "Iteration 84, loss = 1.19925218\n",
      "Iteration 85, loss = 1.19876986\n",
      "Iteration 86, loss = 1.19832598\n",
      "Iteration 87, loss = 1.19791482\n",
      "Iteration 88, loss = 1.19743813\n",
      "Iteration 89, loss = 1.19702086\n",
      "Iteration 90, loss = 1.19663284\n",
      "Iteration 91, loss = 1.19619468\n",
      "Iteration 92, loss = 1.19575879\n",
      "Iteration 93, loss = 1.19536619\n",
      "Iteration 94, loss = 1.19496111\n",
      "Iteration 95, loss = 1.19458278\n",
      "Iteration 96, loss = 1.19416552\n",
      "Iteration 97, loss = 1.19378228\n",
      "Iteration 98, loss = 1.19340761\n",
      "Iteration 99, loss = 1.19301350\n",
      "Iteration 100, loss = 1.19264954\n",
      "Iteration 101, loss = 1.19226831\n",
      "Iteration 102, loss = 1.19194788\n",
      "Iteration 103, loss = 1.19153480\n",
      "Iteration 104, loss = 1.19118946\n",
      "Iteration 105, loss = 1.19084719\n",
      "Iteration 106, loss = 1.19047910\n",
      "Iteration 107, loss = 1.19013687\n",
      "Iteration 108, loss = 1.18977278\n",
      "Iteration 109, loss = 1.18945926\n",
      "Iteration 110, loss = 1.18909655\n",
      "Iteration 111, loss = 1.18877684\n",
      "Iteration 112, loss = 1.18842162\n",
      "Iteration 113, loss = 1.18807965\n",
      "Iteration 114, loss = 1.18776526\n",
      "Iteration 115, loss = 1.18742263\n",
      "Iteration 116, loss = 1.18710153\n",
      "Iteration 117, loss = 1.18679808\n",
      "Iteration 118, loss = 1.18645304\n",
      "Iteration 119, loss = 1.18614192\n",
      "Iteration 120, loss = 1.18580891\n",
      "Iteration 121, loss = 1.18551154\n",
      "Iteration 122, loss = 1.18518012\n",
      "Iteration 123, loss = 1.18488371\n",
      "Iteration 124, loss = 1.18457689\n",
      "Iteration 125, loss = 1.18427724\n",
      "Iteration 126, loss = 1.18395944\n",
      "Iteration 127, loss = 1.18364030\n",
      "Iteration 128, loss = 1.18338237\n",
      "Iteration 129, loss = 1.18303564\n",
      "Iteration 130, loss = 1.18273148\n",
      "Iteration 131, loss = 1.18244992\n",
      "Iteration 132, loss = 1.18214864\n",
      "Iteration 133, loss = 1.18184319\n",
      "Iteration 134, loss = 1.18155604\n",
      "Iteration 135, loss = 1.18126482\n",
      "Iteration 136, loss = 1.18098094\n",
      "Iteration 137, loss = 1.18067923\n",
      "Iteration 138, loss = 1.18040499\n",
      "Iteration 139, loss = 1.18011458\n",
      "Iteration 140, loss = 1.17981837\n",
      "Iteration 141, loss = 1.17955124\n",
      "Iteration 142, loss = 1.17927462\n",
      "Iteration 143, loss = 1.17898300\n",
      "Iteration 144, loss = 1.17868868\n",
      "Iteration 145, loss = 1.17842285\n",
      "Iteration 146, loss = 1.17814354\n",
      "Iteration 147, loss = 1.17787433\n",
      "Iteration 148, loss = 1.17759599\n",
      "Iteration 149, loss = 1.17731202\n",
      "Iteration 150, loss = 1.17703239\n",
      "Iteration 151, loss = 1.17677132\n",
      "Iteration 152, loss = 1.17650111\n",
      "Iteration 153, loss = 1.17621659\n",
      "Iteration 154, loss = 1.17594835\n",
      "Iteration 155, loss = 1.17567169\n",
      "Iteration 156, loss = 1.17545835\n",
      "Iteration 157, loss = 1.17514288\n",
      "Iteration 158, loss = 1.17490316\n",
      "Iteration 159, loss = 1.17460247\n",
      "Iteration 160, loss = 1.17433012\n",
      "Iteration 161, loss = 1.17407027\n",
      "Iteration 162, loss = 1.17379995\n",
      "Iteration 163, loss = 1.17353548\n",
      "Iteration 164, loss = 1.17328492\n",
      "Iteration 165, loss = 1.17300909\n",
      "Iteration 166, loss = 1.17275925\n",
      "Iteration 167, loss = 1.17248994\n",
      "Iteration 168, loss = 1.17223177\n",
      "Iteration 169, loss = 1.17197047\n",
      "Iteration 170, loss = 1.17171123\n",
      "Iteration 171, loss = 1.17147695\n",
      "Iteration 172, loss = 1.17118871\n",
      "Iteration 173, loss = 1.17094200\n",
      "Iteration 174, loss = 1.17070755\n",
      "Iteration 175, loss = 1.17042418\n",
      "Iteration 176, loss = 1.17019179\n",
      "Iteration 177, loss = 1.16992621\n",
      "Iteration 178, loss = 1.16966508\n",
      "Iteration 179, loss = 1.16941116\n",
      "Iteration 180, loss = 1.16915232\n",
      "Iteration 181, loss = 1.16893412\n",
      "Iteration 182, loss = 1.16867386\n",
      "Iteration 183, loss = 1.16841251\n",
      "Iteration 184, loss = 1.16813302\n",
      "Iteration 185, loss = 1.16790607\n",
      "Iteration 186, loss = 1.16765931\n",
      "Iteration 187, loss = 1.16740608\n",
      "Iteration 188, loss = 1.16715734\n",
      "Iteration 189, loss = 1.16689875\n",
      "Iteration 190, loss = 1.16664781\n",
      "Iteration 191, loss = 1.16638856\n",
      "Iteration 192, loss = 1.16614840\n",
      "Iteration 193, loss = 1.16591408\n",
      "Iteration 194, loss = 1.16562920\n",
      "Iteration 195, loss = 1.16540946\n",
      "Iteration 196, loss = 1.16513030\n",
      "Iteration 197, loss = 1.16493717\n",
      "Iteration 198, loss = 1.16465344\n",
      "Iteration 199, loss = 1.16439350\n",
      "Iteration 200, loss = 1.16415013\n",
      "Iteration 1, loss = 12.11038895\n",
      "Iteration 2, loss = 9.91049475\n",
      "Iteration 3, loss = 11.25404468\n",
      "Iteration 4, loss = 5.38341809\n",
      "Iteration 5, loss = 1.56362076\n",
      "Iteration 6, loss = 1.37188245\n",
      "Iteration 7, loss = 1.28953992\n",
      "Iteration 8, loss = 1.24433680\n",
      "Iteration 9, loss = 1.23540936\n",
      "Iteration 10, loss = 1.22911805\n",
      "Iteration 11, loss = 1.22353898\n",
      "Iteration 12, loss = 1.21969169\n",
      "Iteration 13, loss = 1.21433232\n",
      "Iteration 14, loss = 1.20967327\n",
      "Iteration 15, loss = 1.20688573\n",
      "Iteration 16, loss = 1.20727437\n",
      "Iteration 17, loss = 1.20453729\n",
      "Iteration 18, loss = 1.20218119\n",
      "Iteration 19, loss = 1.20011656\n",
      "Iteration 20, loss = 1.19904676\n",
      "Iteration 21, loss = 1.19849154\n",
      "Iteration 22, loss = 1.19683664\n",
      "Iteration 23, loss = 1.19500148\n",
      "Iteration 24, loss = 1.19382225\n",
      "Iteration 25, loss = 1.19322639\n",
      "Iteration 26, loss = 1.19129407\n",
      "Iteration 27, loss = 1.19032719\n",
      "Iteration 28, loss = 1.18726378\n",
      "Iteration 29, loss = 1.18720829\n",
      "Iteration 30, loss = 1.18730858\n",
      "Iteration 31, loss = 1.18680363\n",
      "Iteration 32, loss = 1.18461644\n",
      "Iteration 33, loss = 1.18452222\n",
      "Iteration 34, loss = 1.18253349\n",
      "Iteration 35, loss = 1.17998144\n",
      "Iteration 36, loss = 1.18057745\n",
      "Iteration 37, loss = 1.17913177\n",
      "Iteration 38, loss = 1.18041665\n",
      "Iteration 39, loss = 1.17718568\n",
      "Iteration 40, loss = 1.17794106\n",
      "Iteration 41, loss = 1.17605756\n",
      "Iteration 42, loss = 1.17566083\n",
      "Iteration 43, loss = 1.17744720\n",
      "Iteration 44, loss = 1.17506495\n",
      "Iteration 45, loss = 1.17163821\n",
      "Iteration 46, loss = 1.17414944\n",
      "Iteration 47, loss = 1.17157716\n",
      "Iteration 48, loss = 1.17140303\n",
      "Iteration 49, loss = 1.17323485\n",
      "Iteration 50, loss = 1.16843229\n",
      "Iteration 51, loss = 1.16762759\n",
      "Iteration 52, loss = 1.16959749\n",
      "Iteration 53, loss = 1.17053201\n",
      "Iteration 54, loss = 1.16464276\n",
      "Iteration 55, loss = 1.16688398\n",
      "Iteration 56, loss = 1.16495001\n",
      "Iteration 57, loss = 1.17012212\n",
      "Iteration 58, loss = 1.16556476\n",
      "Iteration 59, loss = 1.16306272\n",
      "Iteration 60, loss = 1.16244335\n",
      "Iteration 61, loss = 1.16320262\n",
      "Iteration 62, loss = 1.16477192\n",
      "Iteration 63, loss = 1.16231759\n",
      "Iteration 64, loss = 1.16160016\n",
      "Iteration 65, loss = 1.15711389\n",
      "Iteration 66, loss = 1.15947104\n",
      "Iteration 67, loss = 1.16155456\n",
      "Iteration 68, loss = 1.15870019\n",
      "Iteration 69, loss = 1.15990139\n",
      "Iteration 70, loss = 1.15571946\n",
      "Iteration 71, loss = 1.16094059\n",
      "Iteration 72, loss = 1.15609780\n",
      "Iteration 73, loss = 1.15478139\n",
      "Iteration 74, loss = 1.16152295\n",
      "Iteration 75, loss = 1.15618414\n",
      "Iteration 76, loss = 1.15383404\n",
      "Iteration 77, loss = 1.15257960\n",
      "Iteration 78, loss = 1.15113679\n",
      "Iteration 79, loss = 1.16270271\n",
      "Iteration 80, loss = 1.15610846\n",
      "Iteration 81, loss = 1.15194743\n",
      "Iteration 82, loss = 1.15550460\n",
      "Iteration 83, loss = 1.14991184\n",
      "Iteration 84, loss = 1.14828991\n",
      "Iteration 85, loss = 1.14869665\n",
      "Iteration 86, loss = 1.15788544\n",
      "Iteration 87, loss = 1.14676646\n",
      "Iteration 88, loss = 1.14442919\n",
      "Iteration 89, loss = 1.14706383\n",
      "Iteration 90, loss = 1.14594647\n",
      "Iteration 91, loss = 1.14727355\n",
      "Iteration 92, loss = 1.15460483\n",
      "Iteration 93, loss = 1.14658267\n",
      "Iteration 94, loss = 1.14841136\n",
      "Iteration 95, loss = 1.14308608\n",
      "Iteration 96, loss = 1.14797393\n",
      "Iteration 97, loss = 1.14660642\n",
      "Iteration 98, loss = 1.14263400\n",
      "Iteration 99, loss = 1.14438349\n",
      "Iteration 100, loss = 1.14869337\n",
      "Iteration 101, loss = 1.14014828\n",
      "Iteration 102, loss = 1.14467477\n",
      "Iteration 103, loss = 1.14821763\n",
      "Iteration 104, loss = 1.14355757\n",
      "Iteration 105, loss = 1.13651147\n",
      "Iteration 106, loss = 1.14673037\n",
      "Iteration 107, loss = 1.14050259\n",
      "Iteration 108, loss = 1.14057732\n",
      "Iteration 109, loss = 1.14084835\n",
      "Iteration 110, loss = 1.14279384\n",
      "Iteration 111, loss = 1.13859664\n",
      "Iteration 112, loss = 1.13647312\n",
      "Iteration 113, loss = 1.13641402\n",
      "Iteration 114, loss = 1.13555463\n",
      "Iteration 115, loss = 1.13912158\n",
      "Iteration 116, loss = 1.13489271\n",
      "Iteration 117, loss = 1.13424368\n",
      "Iteration 118, loss = 1.13268053\n",
      "Iteration 119, loss = 1.15028209\n",
      "Iteration 120, loss = 1.14300595\n",
      "Iteration 121, loss = 1.13341600\n",
      "Iteration 122, loss = 1.16596966\n",
      "Iteration 123, loss = 1.13279313\n",
      "Iteration 124, loss = 1.13412128\n",
      "Iteration 125, loss = 1.13323597\n",
      "Iteration 126, loss = 1.13203697\n",
      "Iteration 127, loss = 1.13537570\n",
      "Iteration 128, loss = 1.13647230\n",
      "Iteration 129, loss = 1.14737460\n",
      "Iteration 130, loss = 1.13565482\n",
      "Iteration 131, loss = 1.13194219\n",
      "Iteration 132, loss = 1.12928039\n",
      "Iteration 133, loss = 1.13597764\n",
      "Iteration 134, loss = 1.13211327\n",
      "Iteration 135, loss = 1.13352590\n",
      "Iteration 136, loss = 1.13538291\n",
      "Iteration 137, loss = 1.13448870\n",
      "Iteration 138, loss = 1.13031066\n",
      "Iteration 139, loss = 1.13292213\n",
      "Iteration 140, loss = 1.13422375\n",
      "Iteration 141, loss = 1.13072076\n",
      "Iteration 142, loss = 1.12852166\n",
      "Iteration 143, loss = 1.13219340\n",
      "Iteration 144, loss = 1.12611419\n",
      "Iteration 145, loss = 1.12797265\n",
      "Iteration 146, loss = 1.13493032\n",
      "Iteration 147, loss = 1.12192977\n",
      "Iteration 148, loss = 1.13056166\n",
      "Iteration 149, loss = 1.13446110\n",
      "Iteration 150, loss = 1.12587785\n",
      "Iteration 151, loss = 1.12874516\n",
      "Iteration 152, loss = 1.12126052\n",
      "Iteration 153, loss = 1.12198104\n",
      "Iteration 154, loss = 1.13442761\n",
      "Iteration 155, loss = 1.11891684\n",
      "Iteration 156, loss = 1.12632371\n",
      "Iteration 157, loss = 1.12441714\n",
      "Iteration 158, loss = 1.12392986\n",
      "Iteration 159, loss = 1.13364534\n",
      "Iteration 160, loss = 1.12302548\n",
      "Iteration 161, loss = 1.14166288\n",
      "Iteration 162, loss = 1.12496978\n",
      "Iteration 163, loss = 1.11927060\n",
      "Iteration 164, loss = 1.12420261\n",
      "Iteration 165, loss = 1.12924339\n",
      "Iteration 166, loss = 1.12228884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.51614707\n",
      "Iteration 2, loss = 1.50721370\n",
      "Iteration 3, loss = 1.49366593\n",
      "Iteration 4, loss = 1.47830590\n",
      "Iteration 5, loss = 1.46163480\n",
      "Iteration 6, loss = 1.44540556\n",
      "Iteration 7, loss = 1.42881642\n",
      "Iteration 8, loss = 1.41402539\n",
      "Iteration 9, loss = 1.39931096\n",
      "Iteration 10, loss = 1.38640518\n",
      "Iteration 11, loss = 1.37387190\n",
      "Iteration 12, loss = 1.36242364\n",
      "Iteration 13, loss = 1.35171323\n",
      "Iteration 14, loss = 1.34208578\n",
      "Iteration 15, loss = 1.33350648\n",
      "Iteration 16, loss = 1.32532283\n",
      "Iteration 17, loss = 1.31795711\n",
      "Iteration 18, loss = 1.31128881\n",
      "Iteration 19, loss = 1.30493031\n",
      "Iteration 20, loss = 1.29912920\n",
      "Iteration 21, loss = 1.29398354\n",
      "Iteration 22, loss = 1.28897382\n",
      "Iteration 23, loss = 1.28460491\n",
      "Iteration 24, loss = 1.28019459\n",
      "Iteration 25, loss = 1.27652209\n",
      "Iteration 26, loss = 1.27284667\n",
      "Iteration 27, loss = 1.26951592\n",
      "Iteration 28, loss = 1.26630452\n",
      "Iteration 29, loss = 1.26335585\n",
      "Iteration 30, loss = 1.26063054\n",
      "Iteration 31, loss = 1.25793009\n",
      "Iteration 32, loss = 1.25556146\n",
      "Iteration 33, loss = 1.25325359\n",
      "Iteration 34, loss = 1.25103271\n",
      "Iteration 35, loss = 1.24898756\n",
      "Iteration 36, loss = 1.24705634\n",
      "Iteration 37, loss = 1.24517812\n",
      "Iteration 38, loss = 1.24339777\n",
      "Iteration 39, loss = 1.24178118\n",
      "Iteration 40, loss = 1.24009964\n",
      "Iteration 41, loss = 1.23862888\n",
      "Iteration 42, loss = 1.23714038\n",
      "Iteration 43, loss = 1.23571154\n",
      "Iteration 44, loss = 1.23444443\n",
      "Iteration 45, loss = 1.23311694\n",
      "Iteration 46, loss = 1.23193400\n",
      "Iteration 47, loss = 1.23072165\n",
      "Iteration 48, loss = 1.22958152\n",
      "Iteration 49, loss = 1.22848045\n",
      "Iteration 50, loss = 1.22745588\n",
      "Iteration 51, loss = 1.22650918\n",
      "Iteration 52, loss = 1.22548023\n",
      "Iteration 53, loss = 1.22455913\n",
      "Iteration 54, loss = 1.22364896\n",
      "Iteration 55, loss = 1.22276485\n",
      "Iteration 56, loss = 1.22189346\n",
      "Iteration 57, loss = 1.22117313\n",
      "Iteration 58, loss = 1.22033507\n",
      "Iteration 59, loss = 1.21960634\n",
      "Iteration 60, loss = 1.21885070\n",
      "Iteration 61, loss = 1.21808586\n",
      "Iteration 62, loss = 1.21740529\n",
      "Iteration 63, loss = 1.21673639\n",
      "Iteration 64, loss = 1.21606698\n",
      "Iteration 65, loss = 1.21543581\n",
      "Iteration 66, loss = 1.21481842\n",
      "Iteration 67, loss = 1.21426138\n",
      "Iteration 68, loss = 1.21360233\n",
      "Iteration 69, loss = 1.21302007\n",
      "Iteration 70, loss = 1.21245355\n",
      "Iteration 71, loss = 1.21188515\n",
      "Iteration 72, loss = 1.21141635\n",
      "Iteration 73, loss = 1.21087479\n",
      "Iteration 74, loss = 1.21035251\n",
      "Iteration 75, loss = 1.20982347\n",
      "Iteration 76, loss = 1.20936054\n",
      "Iteration 77, loss = 1.20885372\n",
      "Iteration 78, loss = 1.20839168\n",
      "Iteration 79, loss = 1.20795491\n",
      "Iteration 80, loss = 1.20753976\n",
      "Iteration 81, loss = 1.20706378\n",
      "Iteration 82, loss = 1.20659065\n",
      "Iteration 83, loss = 1.20619502\n",
      "Iteration 84, loss = 1.20577514\n",
      "Iteration 85, loss = 1.20537597\n",
      "Iteration 86, loss = 1.20496525\n",
      "Iteration 87, loss = 1.20455003\n",
      "Iteration 88, loss = 1.20420071\n",
      "Iteration 89, loss = 1.20381923\n",
      "Iteration 90, loss = 1.20342788\n",
      "Iteration 91, loss = 1.20306057\n",
      "Iteration 92, loss = 1.20269870\n",
      "Iteration 93, loss = 1.20231686\n",
      "Iteration 94, loss = 1.20201677\n",
      "Iteration 95, loss = 1.20164070\n",
      "Iteration 96, loss = 1.20128635\n",
      "Iteration 97, loss = 1.20100607\n",
      "Iteration 98, loss = 1.20065113\n",
      "Iteration 99, loss = 1.20030527\n",
      "Iteration 100, loss = 1.19998327\n",
      "Iteration 101, loss = 1.19963053\n",
      "Iteration 102, loss = 1.19931357\n",
      "Iteration 103, loss = 1.19903537\n",
      "Iteration 104, loss = 1.19871298\n",
      "Iteration 105, loss = 1.19839488\n",
      "Iteration 106, loss = 1.19810615\n",
      "Iteration 107, loss = 1.19777999\n",
      "Iteration 108, loss = 1.19749155\n",
      "Iteration 109, loss = 1.19720085\n",
      "Iteration 110, loss = 1.19691246\n",
      "Iteration 111, loss = 1.19663178\n",
      "Iteration 112, loss = 1.19636148\n",
      "Iteration 113, loss = 1.19605222\n",
      "Iteration 114, loss = 1.19581603\n",
      "Iteration 115, loss = 1.19550698\n",
      "Iteration 116, loss = 1.19522825\n",
      "Iteration 117, loss = 1.19497739\n",
      "Iteration 118, loss = 1.19468549\n",
      "Iteration 119, loss = 1.19441812\n",
      "Iteration 120, loss = 1.19416940\n",
      "Iteration 121, loss = 1.19390975\n",
      "Iteration 122, loss = 1.19366236\n",
      "Iteration 123, loss = 1.19336666\n",
      "Iteration 124, loss = 1.19310914\n",
      "Iteration 125, loss = 1.19284857\n",
      "Iteration 126, loss = 1.19260524\n",
      "Iteration 127, loss = 1.19236922\n",
      "Iteration 128, loss = 1.19211692\n",
      "Iteration 129, loss = 1.19185681\n",
      "Iteration 130, loss = 1.19161279\n",
      "Iteration 131, loss = 1.19136112\n",
      "Iteration 132, loss = 1.19112999\n",
      "Iteration 133, loss = 1.19086806\n",
      "Iteration 134, loss = 1.19063088\n",
      "Iteration 135, loss = 1.19040505\n",
      "Iteration 136, loss = 1.19015827\n",
      "Iteration 137, loss = 1.18994166\n",
      "Iteration 138, loss = 1.18967102\n",
      "Iteration 139, loss = 1.18946751\n",
      "Iteration 140, loss = 1.18919825\n",
      "Iteration 141, loss = 1.18898083\n",
      "Iteration 142, loss = 1.18878422\n",
      "Iteration 143, loss = 1.18851367\n",
      "Iteration 144, loss = 1.18827296\n",
      "Iteration 145, loss = 1.18805304\n",
      "Iteration 146, loss = 1.18783363\n",
      "Iteration 147, loss = 1.18759620\n",
      "Iteration 148, loss = 1.18736440\n",
      "Iteration 149, loss = 1.18714017\n",
      "Iteration 150, loss = 1.18693721\n",
      "Iteration 151, loss = 1.18670256\n",
      "Iteration 152, loss = 1.18648864\n",
      "Iteration 153, loss = 1.18625504\n",
      "Iteration 154, loss = 1.18604874\n",
      "Iteration 155, loss = 1.18582074\n",
      "Iteration 156, loss = 1.18560143\n",
      "Iteration 157, loss = 1.18537446\n",
      "Iteration 158, loss = 1.18517930\n",
      "Iteration 159, loss = 1.18496795\n",
      "Iteration 160, loss = 1.18474191\n",
      "Iteration 161, loss = 1.18450497\n",
      "Iteration 162, loss = 1.18433643\n",
      "Iteration 163, loss = 1.18407724\n",
      "Iteration 164, loss = 1.18386220\n",
      "Iteration 165, loss = 1.18364737\n",
      "Iteration 166, loss = 1.18342182\n",
      "Iteration 167, loss = 1.18322256\n",
      "Iteration 168, loss = 1.18300571\n",
      "Iteration 169, loss = 1.18280281\n",
      "Iteration 170, loss = 1.18259165\n",
      "Iteration 171, loss = 1.18236053\n",
      "Iteration 172, loss = 1.18215545\n",
      "Iteration 173, loss = 1.18196001\n",
      "Iteration 174, loss = 1.18174329\n",
      "Iteration 175, loss = 1.18153501\n",
      "Iteration 176, loss = 1.18131907\n",
      "Iteration 177, loss = 1.18110242\n",
      "Iteration 178, loss = 1.18090540\n",
      "Iteration 179, loss = 1.18068460\n",
      "Iteration 180, loss = 1.18049240\n",
      "Iteration 181, loss = 1.18026973\n",
      "Iteration 182, loss = 1.18007844\n",
      "Iteration 183, loss = 1.17986504\n",
      "Iteration 184, loss = 1.17965568\n",
      "Iteration 185, loss = 1.17944166\n",
      "Iteration 186, loss = 1.17926265\n",
      "Iteration 187, loss = 1.17903280\n",
      "Iteration 188, loss = 1.17882751\n",
      "Iteration 189, loss = 1.17861540\n",
      "Iteration 190, loss = 1.17843515\n",
      "Iteration 191, loss = 1.17819670\n",
      "Iteration 192, loss = 1.17800674\n",
      "Iteration 193, loss = 1.17780447\n",
      "Iteration 194, loss = 1.17759053\n",
      "Iteration 195, loss = 1.17737661\n",
      "Iteration 196, loss = 1.17717063\n",
      "Iteration 197, loss = 1.17702012\n",
      "Iteration 198, loss = 1.17676597\n",
      "Iteration 199, loss = 1.17657803\n",
      "Iteration 200, loss = 1.17635302\n",
      "Iteration 201, loss = 1.17615258\n",
      "Iteration 202, loss = 1.17596994\n",
      "Iteration 203, loss = 1.17576789\n",
      "Iteration 204, loss = 1.17554834\n",
      "Iteration 205, loss = 1.17534580\n",
      "Iteration 206, loss = 1.17514872\n",
      "Iteration 207, loss = 1.17493727\n",
      "Iteration 208, loss = 1.17474800\n",
      "Iteration 209, loss = 1.17453363\n",
      "Iteration 210, loss = 1.17435676\n",
      "Iteration 211, loss = 1.17414601\n",
      "Iteration 212, loss = 1.17392892\n",
      "Iteration 213, loss = 1.17374627\n",
      "Iteration 214, loss = 1.17354987\n",
      "Iteration 215, loss = 1.17334070\n",
      "Iteration 216, loss = 1.17312936\n",
      "Iteration 217, loss = 1.17294638\n",
      "Iteration 218, loss = 1.17276457\n",
      "Iteration 219, loss = 1.17255212\n",
      "Iteration 220, loss = 1.17234492\n",
      "Iteration 221, loss = 1.17213163\n",
      "Iteration 222, loss = 1.17194032\n",
      "Iteration 223, loss = 1.17175741\n",
      "Iteration 224, loss = 1.17154581\n",
      "Iteration 225, loss = 1.17133407\n",
      "Iteration 226, loss = 1.17114996\n",
      "Iteration 227, loss = 1.17095364\n",
      "Iteration 228, loss = 1.17075624\n",
      "Iteration 229, loss = 1.17055417\n",
      "Iteration 230, loss = 1.17036473\n",
      "Iteration 231, loss = 1.17017709\n",
      "Iteration 232, loss = 1.16997200\n",
      "Iteration 233, loss = 1.16977909\n",
      "Iteration 234, loss = 1.16957686\n",
      "Iteration 235, loss = 1.16940468\n",
      "Iteration 236, loss = 1.16919268\n",
      "Iteration 237, loss = 1.16901217\n",
      "Iteration 238, loss = 1.16880157\n",
      "Iteration 239, loss = 1.16861548\n",
      "Iteration 240, loss = 1.16842561\n",
      "Iteration 241, loss = 1.16823257\n",
      "Iteration 242, loss = 1.16803437\n",
      "Iteration 243, loss = 1.16784714\n",
      "Iteration 244, loss = 1.16764111\n",
      "Iteration 245, loss = 1.16746555\n",
      "Iteration 246, loss = 1.16725929\n",
      "Iteration 247, loss = 1.16706433\n",
      "Iteration 248, loss = 1.16688970\n",
      "Iteration 249, loss = 1.16667182\n",
      "Iteration 250, loss = 1.16647918\n",
      "Iteration 251, loss = 1.16629945\n",
      "Iteration 252, loss = 1.16611540\n",
      "Iteration 253, loss = 1.16591973\n",
      "Iteration 254, loss = 1.16571491\n",
      "Iteration 255, loss = 1.16553785\n",
      "Iteration 256, loss = 1.16534520\n",
      "Iteration 257, loss = 1.16516442\n",
      "Iteration 258, loss = 1.16495267\n",
      "Iteration 259, loss = 1.16480927\n",
      "Iteration 260, loss = 1.16457935\n",
      "Iteration 261, loss = 1.16440124\n",
      "Iteration 262, loss = 1.16419637\n",
      "Iteration 263, loss = 1.16401759\n",
      "Iteration 264, loss = 1.16386847\n",
      "Iteration 265, loss = 1.16364464\n",
      "Iteration 266, loss = 1.16346421\n",
      "Iteration 267, loss = 1.16329248\n",
      "Iteration 268, loss = 1.16309439\n",
      "Iteration 269, loss = 1.16290520\n",
      "Iteration 270, loss = 1.16270532\n",
      "Iteration 271, loss = 1.16254442\n",
      "Iteration 272, loss = 1.16234472\n",
      "Iteration 273, loss = 1.16215813\n",
      "Iteration 274, loss = 1.16200046\n",
      "Iteration 275, loss = 1.16180316\n",
      "Iteration 276, loss = 1.16160563\n",
      "Iteration 277, loss = 1.16141488\n",
      "Iteration 278, loss = 1.16121568\n",
      "Iteration 279, loss = 1.16103118\n",
      "Iteration 280, loss = 1.16087270\n",
      "Iteration 281, loss = 1.16069638\n",
      "Iteration 282, loss = 1.16049750\n",
      "Iteration 283, loss = 1.16029656\n",
      "Iteration 284, loss = 1.16013398\n",
      "Iteration 285, loss = 1.15994699\n",
      "Iteration 286, loss = 1.15979964\n",
      "Iteration 287, loss = 1.15957517\n",
      "Iteration 288, loss = 1.15941173\n",
      "Iteration 289, loss = 1.15920934\n",
      "Iteration 290, loss = 1.15903482\n",
      "Iteration 291, loss = 1.15888724\n",
      "Iteration 292, loss = 1.15874076\n",
      "Iteration 293, loss = 1.15848537\n",
      "Iteration 294, loss = 1.15830900\n",
      "Iteration 295, loss = 1.15813124\n",
      "Iteration 296, loss = 1.15794242\n",
      "Iteration 297, loss = 1.15776915\n",
      "Iteration 298, loss = 1.15760076\n",
      "Iteration 299, loss = 1.15741944\n",
      "Iteration 300, loss = 1.15724740\n",
      "Iteration 1, loss = 1.51638664\n",
      "Iteration 2, loss = 1.50736711\n",
      "Iteration 3, loss = 1.49399211\n",
      "Iteration 4, loss = 1.47866355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 1.46212073\n",
      "Iteration 6, loss = 1.44564054\n",
      "Iteration 7, loss = 1.42932412\n",
      "Iteration 8, loss = 1.41424433\n",
      "Iteration 9, loss = 1.39979458\n",
      "Iteration 10, loss = 1.38667448\n",
      "Iteration 11, loss = 1.37414017\n",
      "Iteration 12, loss = 1.36283308\n",
      "Iteration 13, loss = 1.35204893\n",
      "Iteration 14, loss = 1.34278183\n",
      "Iteration 15, loss = 1.33380194\n",
      "Iteration 16, loss = 1.32571878\n",
      "Iteration 17, loss = 1.31842617\n",
      "Iteration 18, loss = 1.31155628\n",
      "Iteration 19, loss = 1.30545361\n",
      "Iteration 20, loss = 1.29961253\n",
      "Iteration 21, loss = 1.29436458\n",
      "Iteration 22, loss = 1.28938742\n",
      "Iteration 23, loss = 1.28495989\n",
      "Iteration 24, loss = 1.28066616\n",
      "Iteration 25, loss = 1.27677221\n",
      "Iteration 26, loss = 1.27324415\n",
      "Iteration 27, loss = 1.26998107\n",
      "Iteration 28, loss = 1.26662618\n",
      "Iteration 29, loss = 1.26373827\n",
      "Iteration 30, loss = 1.26098675\n",
      "Iteration 31, loss = 1.25831170\n",
      "Iteration 32, loss = 1.25580962\n",
      "Iteration 33, loss = 1.25354954\n",
      "Iteration 34, loss = 1.25146682\n",
      "Iteration 35, loss = 1.24927663\n",
      "Iteration 36, loss = 1.24735165\n",
      "Iteration 37, loss = 1.24547506\n",
      "Iteration 38, loss = 1.24373358\n",
      "Iteration 39, loss = 1.24201156\n",
      "Iteration 40, loss = 1.24039051\n",
      "Iteration 41, loss = 1.23889676\n",
      "Iteration 42, loss = 1.23736829\n",
      "Iteration 43, loss = 1.23594653\n",
      "Iteration 44, loss = 1.23467784\n",
      "Iteration 45, loss = 1.23338779\n",
      "Iteration 46, loss = 1.23212356\n",
      "Iteration 47, loss = 1.23090864\n",
      "Iteration 48, loss = 1.22978273\n",
      "Iteration 49, loss = 1.22869094\n",
      "Iteration 50, loss = 1.22761497\n",
      "Iteration 51, loss = 1.22662817\n",
      "Iteration 52, loss = 1.22557882\n",
      "Iteration 53, loss = 1.22467948\n",
      "Iteration 54, loss = 1.22375150\n",
      "Iteration 55, loss = 1.22283722\n",
      "Iteration 56, loss = 1.22199215\n",
      "Iteration 57, loss = 1.22122344\n",
      "Iteration 58, loss = 1.22034182\n",
      "Iteration 59, loss = 1.21959151\n",
      "Iteration 60, loss = 1.21885717\n",
      "Iteration 61, loss = 1.21808774\n",
      "Iteration 62, loss = 1.21735140\n",
      "Iteration 63, loss = 1.21669819\n",
      "Iteration 64, loss = 1.21600008\n",
      "Iteration 65, loss = 1.21536084\n",
      "Iteration 66, loss = 1.21472486\n",
      "Iteration 67, loss = 1.21414005\n",
      "Iteration 68, loss = 1.21350336\n",
      "Iteration 69, loss = 1.21288226\n",
      "Iteration 70, loss = 1.21230509\n",
      "Iteration 71, loss = 1.21174854\n",
      "Iteration 72, loss = 1.21121852\n",
      "Iteration 73, loss = 1.21068557\n",
      "Iteration 74, loss = 1.21015102\n",
      "Iteration 75, loss = 1.20961978\n",
      "Iteration 76, loss = 1.20914392\n",
      "Iteration 77, loss = 1.20864670\n",
      "Iteration 78, loss = 1.20816865\n",
      "Iteration 79, loss = 1.20769960\n",
      "Iteration 80, loss = 1.20727359\n",
      "Iteration 81, loss = 1.20675701\n",
      "Iteration 82, loss = 1.20631014\n",
      "Iteration 83, loss = 1.20589430\n",
      "Iteration 84, loss = 1.20546034\n",
      "Iteration 85, loss = 1.20503216\n",
      "Iteration 86, loss = 1.20462141\n",
      "Iteration 87, loss = 1.20420067\n",
      "Iteration 88, loss = 1.20381766\n",
      "Iteration 89, loss = 1.20344437\n",
      "Iteration 90, loss = 1.20302754\n",
      "Iteration 91, loss = 1.20265079\n",
      "Iteration 92, loss = 1.20226791\n",
      "Iteration 93, loss = 1.20189039\n",
      "Iteration 94, loss = 1.20153299\n",
      "Iteration 95, loss = 1.20118813\n",
      "Iteration 96, loss = 1.20079019\n",
      "Iteration 97, loss = 1.20047542\n",
      "Iteration 98, loss = 1.20012295\n",
      "Iteration 99, loss = 1.19979416\n",
      "Iteration 100, loss = 1.19943650\n",
      "Iteration 101, loss = 1.19909239\n",
      "Iteration 102, loss = 1.19876665\n",
      "Iteration 103, loss = 1.19844299\n",
      "Iteration 104, loss = 1.19811443\n",
      "Iteration 105, loss = 1.19781541\n",
      "Iteration 106, loss = 1.19751354\n",
      "Iteration 107, loss = 1.19715487\n",
      "Iteration 108, loss = 1.19686972\n",
      "Iteration 109, loss = 1.19657150\n",
      "Iteration 110, loss = 1.19628996\n",
      "Iteration 111, loss = 1.19595900\n",
      "Iteration 112, loss = 1.19569273\n",
      "Iteration 113, loss = 1.19537267\n",
      "Iteration 114, loss = 1.19511326\n",
      "Iteration 115, loss = 1.19481468\n",
      "Iteration 116, loss = 1.19451579\n",
      "Iteration 117, loss = 1.19423607\n",
      "Iteration 118, loss = 1.19395466\n",
      "Iteration 119, loss = 1.19369406\n",
      "Iteration 120, loss = 1.19341590\n",
      "Iteration 121, loss = 1.19315540\n",
      "Iteration 122, loss = 1.19285538\n",
      "Iteration 123, loss = 1.19259814\n",
      "Iteration 124, loss = 1.19230916\n",
      "Iteration 125, loss = 1.19205427\n",
      "Iteration 126, loss = 1.19177397\n",
      "Iteration 127, loss = 1.19153418\n",
      "Iteration 128, loss = 1.19126595\n",
      "Iteration 129, loss = 1.19099623\n",
      "Iteration 130, loss = 1.19076477\n",
      "Iteration 131, loss = 1.19048884\n",
      "Iteration 132, loss = 1.19025824\n",
      "Iteration 133, loss = 1.18997654\n",
      "Iteration 134, loss = 1.18973099\n",
      "Iteration 135, loss = 1.18947710\n",
      "Iteration 136, loss = 1.18923922\n",
      "Iteration 137, loss = 1.18900249\n",
      "Iteration 138, loss = 1.18874980\n",
      "Iteration 139, loss = 1.18852398\n",
      "Iteration 140, loss = 1.18826021\n",
      "Iteration 141, loss = 1.18803932\n",
      "Iteration 142, loss = 1.18780082\n",
      "Iteration 143, loss = 1.18752533\n",
      "Iteration 144, loss = 1.18728533\n",
      "Iteration 145, loss = 1.18703318\n",
      "Iteration 146, loss = 1.18679896\n",
      "Iteration 147, loss = 1.18655655\n",
      "Iteration 148, loss = 1.18631157\n",
      "Iteration 149, loss = 1.18607708\n",
      "Iteration 150, loss = 1.18584929\n",
      "Iteration 151, loss = 1.18562971\n",
      "Iteration 152, loss = 1.18537110\n",
      "Iteration 153, loss = 1.18515248\n",
      "Iteration 154, loss = 1.18493780\n",
      "Iteration 155, loss = 1.18468088\n",
      "Iteration 156, loss = 1.18444944\n",
      "Iteration 157, loss = 1.18420689\n",
      "Iteration 158, loss = 1.18400202\n",
      "Iteration 159, loss = 1.18378254\n",
      "Iteration 160, loss = 1.18352596\n",
      "Iteration 161, loss = 1.18331420\n",
      "Iteration 162, loss = 1.18308328\n",
      "Iteration 163, loss = 1.18284087\n",
      "Iteration 164, loss = 1.18263013\n",
      "Iteration 165, loss = 1.18240594\n",
      "Iteration 166, loss = 1.18217907\n",
      "Iteration 167, loss = 1.18199020\n",
      "Iteration 168, loss = 1.18173797\n",
      "Iteration 169, loss = 1.18154802\n",
      "Iteration 170, loss = 1.18128393\n",
      "Iteration 171, loss = 1.18105682\n",
      "Iteration 172, loss = 1.18084288\n",
      "Iteration 173, loss = 1.18064645\n",
      "Iteration 174, loss = 1.18040649\n",
      "Iteration 175, loss = 1.18017699\n",
      "Iteration 176, loss = 1.17996309\n",
      "Iteration 177, loss = 1.17974423\n",
      "Iteration 178, loss = 1.17952181\n",
      "Iteration 179, loss = 1.17930225\n",
      "Iteration 180, loss = 1.17910442\n",
      "Iteration 181, loss = 1.17886317\n",
      "Iteration 182, loss = 1.17865450\n",
      "Iteration 183, loss = 1.17842708\n",
      "Iteration 184, loss = 1.17821198\n",
      "Iteration 185, loss = 1.17798294\n",
      "Iteration 186, loss = 1.17777015\n",
      "Iteration 187, loss = 1.17755661\n",
      "Iteration 188, loss = 1.17734780\n",
      "Iteration 189, loss = 1.17711704\n",
      "Iteration 190, loss = 1.17691918\n",
      "Iteration 191, loss = 1.17668224\n",
      "Iteration 192, loss = 1.17646758\n",
      "Iteration 193, loss = 1.17624725\n",
      "Iteration 194, loss = 1.17604411\n",
      "Iteration 195, loss = 1.17582276\n",
      "Iteration 196, loss = 1.17561319\n",
      "Iteration 197, loss = 1.17541319\n",
      "Iteration 198, loss = 1.17518506\n",
      "Iteration 199, loss = 1.17498751\n",
      "Iteration 200, loss = 1.17476091\n",
      "Iteration 201, loss = 1.17454530\n",
      "Iteration 202, loss = 1.17435853\n",
      "Iteration 203, loss = 1.17414943\n",
      "Iteration 204, loss = 1.17390971\n",
      "Iteration 205, loss = 1.17371772\n",
      "Iteration 206, loss = 1.17349747\n",
      "Iteration 207, loss = 1.17328341\n",
      "Iteration 208, loss = 1.17308974\n",
      "Iteration 209, loss = 1.17286757\n",
      "Iteration 210, loss = 1.17268243\n",
      "Iteration 211, loss = 1.17245506\n",
      "Iteration 212, loss = 1.17225280\n",
      "Iteration 213, loss = 1.17202701\n",
      "Iteration 214, loss = 1.17181397\n",
      "Iteration 215, loss = 1.17162103\n",
      "Iteration 216, loss = 1.17140344\n",
      "Iteration 217, loss = 1.17119497\n",
      "Iteration 218, loss = 1.17098965\n",
      "Iteration 219, loss = 1.17080814\n",
      "Iteration 220, loss = 1.17057886\n",
      "Iteration 221, loss = 1.17035480\n",
      "Iteration 222, loss = 1.17014791\n",
      "Iteration 223, loss = 1.16995202\n",
      "Iteration 224, loss = 1.16973640\n",
      "Iteration 225, loss = 1.16952672\n",
      "Iteration 226, loss = 1.16931837\n",
      "Iteration 227, loss = 1.16911530\n",
      "Iteration 228, loss = 1.16888644\n",
      "Iteration 229, loss = 1.16870053\n",
      "Iteration 230, loss = 1.16851038\n",
      "Iteration 231, loss = 1.16829400\n",
      "Iteration 232, loss = 1.16807606\n",
      "Iteration 233, loss = 1.16787263\n",
      "Iteration 234, loss = 1.16766006\n",
      "Iteration 235, loss = 1.16747473\n",
      "Iteration 236, loss = 1.16727038\n",
      "Iteration 237, loss = 1.16705963\n",
      "Iteration 238, loss = 1.16683790\n",
      "Iteration 239, loss = 1.16664426\n",
      "Iteration 240, loss = 1.16644116\n",
      "Iteration 241, loss = 1.16623961\n",
      "Iteration 242, loss = 1.16604142\n",
      "Iteration 243, loss = 1.16586138\n",
      "Iteration 244, loss = 1.16563940\n",
      "Iteration 245, loss = 1.16543605\n",
      "Iteration 246, loss = 1.16525330\n",
      "Iteration 247, loss = 1.16502319\n",
      "Iteration 248, loss = 1.16481880\n",
      "Iteration 249, loss = 1.16461451\n",
      "Iteration 250, loss = 1.16440926\n",
      "Iteration 251, loss = 1.16422756\n",
      "Iteration 252, loss = 1.16402071\n",
      "Iteration 253, loss = 1.16383798\n",
      "Iteration 254, loss = 1.16361268\n",
      "Iteration 255, loss = 1.16341931\n",
      "Iteration 256, loss = 1.16323002\n",
      "Iteration 257, loss = 1.16302273\n",
      "Iteration 258, loss = 1.16282344\n",
      "Iteration 259, loss = 1.16266676\n",
      "Iteration 260, loss = 1.16243844\n",
      "Iteration 261, loss = 1.16223656\n",
      "Iteration 262, loss = 1.16202370\n",
      "Iteration 263, loss = 1.16183494\n",
      "Iteration 264, loss = 1.16171032\n",
      "Iteration 265, loss = 1.16145349\n",
      "Iteration 266, loss = 1.16124467\n",
      "Iteration 267, loss = 1.16105528\n",
      "Iteration 268, loss = 1.16085284\n",
      "Iteration 269, loss = 1.16065363\n",
      "Iteration 270, loss = 1.16048718\n",
      "Iteration 271, loss = 1.16026342\n",
      "Iteration 272, loss = 1.16006648\n",
      "Iteration 273, loss = 1.15988383\n",
      "Iteration 274, loss = 1.15969390\n",
      "Iteration 275, loss = 1.15951799\n",
      "Iteration 276, loss = 1.15929604\n",
      "Iteration 277, loss = 1.15910661\n",
      "Iteration 278, loss = 1.15888288\n",
      "Iteration 279, loss = 1.15871476\n",
      "Iteration 280, loss = 1.15849857\n",
      "Iteration 281, loss = 1.15830791\n",
      "Iteration 282, loss = 1.15811798\n",
      "Iteration 283, loss = 1.15790877\n",
      "Iteration 284, loss = 1.15773049\n",
      "Iteration 285, loss = 1.15752573\n",
      "Iteration 286, loss = 1.15733518\n",
      "Iteration 287, loss = 1.15711822\n",
      "Iteration 288, loss = 1.15697638\n",
      "Iteration 289, loss = 1.15674161\n",
      "Iteration 290, loss = 1.15655483\n",
      "Iteration 291, loss = 1.15637997\n",
      "Iteration 292, loss = 1.15617564\n",
      "Iteration 293, loss = 1.15597103\n",
      "Iteration 294, loss = 1.15580166\n",
      "Iteration 295, loss = 1.15560030\n",
      "Iteration 296, loss = 1.15538211\n",
      "Iteration 297, loss = 1.15519344\n",
      "Iteration 298, loss = 1.15499665\n",
      "Iteration 299, loss = 1.15480856\n",
      "Iteration 300, loss = 1.15464792\n",
      "Iteration 1, loss = 1.51658759\n",
      "Iteration 2, loss = 1.50750331\n",
      "Iteration 3, loss = 1.49420954\n",
      "Iteration 4, loss = 1.47901319\n",
      "Iteration 5, loss = 1.46200911\n",
      "Iteration 6, loss = 1.44546808\n",
      "Iteration 7, loss = 1.42950421\n",
      "Iteration 8, loss = 1.41414199\n",
      "Iteration 9, loss = 1.39965147\n",
      "Iteration 10, loss = 1.38631706\n",
      "Iteration 11, loss = 1.37390963\n",
      "Iteration 12, loss = 1.36263602\n",
      "Iteration 13, loss = 1.35187948\n",
      "Iteration 14, loss = 1.34251327\n",
      "Iteration 15, loss = 1.33334527\n",
      "Iteration 16, loss = 1.32534597\n",
      "Iteration 17, loss = 1.31794115\n",
      "Iteration 18, loss = 1.31106900\n",
      "Iteration 19, loss = 1.30486388\n",
      "Iteration 20, loss = 1.29889565\n",
      "Iteration 21, loss = 1.29364508\n",
      "Iteration 22, loss = 1.28860833\n",
      "Iteration 23, loss = 1.28403840\n",
      "Iteration 24, loss = 1.27986732\n",
      "Iteration 25, loss = 1.27606983\n",
      "Iteration 26, loss = 1.27224618\n",
      "Iteration 27, loss = 1.26887457\n",
      "Iteration 28, loss = 1.26549662\n",
      "Iteration 29, loss = 1.26269167\n",
      "Iteration 30, loss = 1.25978358\n",
      "Iteration 31, loss = 1.25712210\n",
      "Iteration 32, loss = 1.25455278\n",
      "Iteration 33, loss = 1.25224685\n",
      "Iteration 34, loss = 1.25004769\n",
      "Iteration 35, loss = 1.24797648\n",
      "Iteration 36, loss = 1.24598040\n",
      "Iteration 37, loss = 1.24402850\n",
      "Iteration 38, loss = 1.24223934\n",
      "Iteration 39, loss = 1.24048241\n",
      "Iteration 40, loss = 1.23887414\n",
      "Iteration 41, loss = 1.23735564\n",
      "Iteration 42, loss = 1.23580608\n",
      "Iteration 43, loss = 1.23438923\n",
      "Iteration 44, loss = 1.23304879\n",
      "Iteration 45, loss = 1.23169700\n",
      "Iteration 46, loss = 1.23039959\n",
      "Iteration 47, loss = 1.22926273\n",
      "Iteration 48, loss = 1.22814803\n",
      "Iteration 49, loss = 1.22702474\n",
      "Iteration 50, loss = 1.22586926\n",
      "Iteration 51, loss = 1.22491483\n",
      "Iteration 52, loss = 1.22389634\n",
      "Iteration 53, loss = 1.22293036\n",
      "Iteration 54, loss = 1.22197797\n",
      "Iteration 55, loss = 1.22108776\n",
      "Iteration 56, loss = 1.22023966\n",
      "Iteration 57, loss = 1.21944647\n",
      "Iteration 58, loss = 1.21856300\n",
      "Iteration 59, loss = 1.21779182\n",
      "Iteration 60, loss = 1.21706912\n",
      "Iteration 61, loss = 1.21629368\n",
      "Iteration 62, loss = 1.21553760\n",
      "Iteration 63, loss = 1.21485706\n",
      "Iteration 64, loss = 1.21418196\n",
      "Iteration 65, loss = 1.21349631\n",
      "Iteration 66, loss = 1.21283445\n",
      "Iteration 67, loss = 1.21221404\n",
      "Iteration 68, loss = 1.21160077\n",
      "Iteration 69, loss = 1.21096500\n",
      "Iteration 70, loss = 1.21038836\n",
      "Iteration 71, loss = 1.20985542\n",
      "Iteration 72, loss = 1.20926367\n",
      "Iteration 73, loss = 1.20871479\n",
      "Iteration 74, loss = 1.20818492\n",
      "Iteration 75, loss = 1.20764297\n",
      "Iteration 76, loss = 1.20710579\n",
      "Iteration 77, loss = 1.20663355\n",
      "Iteration 78, loss = 1.20613394\n",
      "Iteration 79, loss = 1.20568674\n",
      "Iteration 80, loss = 1.20521060\n",
      "Iteration 81, loss = 1.20469524\n",
      "Iteration 82, loss = 1.20426732\n",
      "Iteration 83, loss = 1.20381093\n",
      "Iteration 84, loss = 1.20339973\n",
      "Iteration 85, loss = 1.20293864\n",
      "Iteration 86, loss = 1.20251024\n",
      "Iteration 87, loss = 1.20210004\n",
      "Iteration 88, loss = 1.20168140\n",
      "Iteration 89, loss = 1.20127977\n",
      "Iteration 90, loss = 1.20089712\n",
      "Iteration 91, loss = 1.20047924\n",
      "Iteration 92, loss = 1.20009922\n",
      "Iteration 93, loss = 1.19973291\n",
      "Iteration 94, loss = 1.19933435\n",
      "Iteration 95, loss = 1.19897441\n",
      "Iteration 96, loss = 1.19859759\n",
      "Iteration 97, loss = 1.19822930\n",
      "Iteration 98, loss = 1.19786687\n",
      "Iteration 99, loss = 1.19751827\n",
      "Iteration 100, loss = 1.19720267\n",
      "Iteration 101, loss = 1.19682662\n",
      "Iteration 102, loss = 1.19648030\n",
      "Iteration 103, loss = 1.19614157\n",
      "Iteration 104, loss = 1.19581333\n",
      "Iteration 105, loss = 1.19547636\n",
      "Iteration 106, loss = 1.19513467\n",
      "Iteration 107, loss = 1.19479089\n",
      "Iteration 108, loss = 1.19449543\n",
      "Iteration 109, loss = 1.19420666\n",
      "Iteration 110, loss = 1.19386951\n",
      "Iteration 111, loss = 1.19352783\n",
      "Iteration 112, loss = 1.19324102\n",
      "Iteration 113, loss = 1.19293418\n",
      "Iteration 114, loss = 1.19262300\n",
      "Iteration 115, loss = 1.19233436\n",
      "Iteration 116, loss = 1.19201871\n",
      "Iteration 117, loss = 1.19173105\n",
      "Iteration 118, loss = 1.19145305\n",
      "Iteration 119, loss = 1.19114155\n",
      "Iteration 120, loss = 1.19085722\n",
      "Iteration 121, loss = 1.19056635\n",
      "Iteration 122, loss = 1.19027690\n",
      "Iteration 123, loss = 1.19001064\n",
      "Iteration 124, loss = 1.18974376\n",
      "Iteration 125, loss = 1.18945548\n",
      "Iteration 126, loss = 1.18915493\n",
      "Iteration 127, loss = 1.18888986\n",
      "Iteration 128, loss = 1.18861112\n",
      "Iteration 129, loss = 1.18833106\n",
      "Iteration 130, loss = 1.18808948\n",
      "Iteration 131, loss = 1.18777841\n",
      "Iteration 132, loss = 1.18752913\n",
      "Iteration 133, loss = 1.18725402\n",
      "Iteration 134, loss = 1.18698216\n",
      "Iteration 135, loss = 1.18671105\n",
      "Iteration 136, loss = 1.18646137\n",
      "Iteration 137, loss = 1.18621165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 138, loss = 1.18594267\n",
      "Iteration 139, loss = 1.18566970\n",
      "Iteration 140, loss = 1.18542209\n",
      "Iteration 141, loss = 1.18516448\n",
      "Iteration 142, loss = 1.18491211\n",
      "Iteration 143, loss = 1.18463351\n",
      "Iteration 144, loss = 1.18438439\n",
      "Iteration 145, loss = 1.18414504\n",
      "Iteration 146, loss = 1.18389406\n",
      "Iteration 147, loss = 1.18363432\n",
      "Iteration 148, loss = 1.18336891\n",
      "Iteration 149, loss = 1.18315459\n",
      "Iteration 150, loss = 1.18287334\n",
      "Iteration 151, loss = 1.18264534\n",
      "Iteration 152, loss = 1.18238885\n",
      "Iteration 153, loss = 1.18213317\n",
      "Iteration 154, loss = 1.18189095\n",
      "Iteration 155, loss = 1.18163921\n",
      "Iteration 156, loss = 1.18139948\n",
      "Iteration 157, loss = 1.18115603\n",
      "Iteration 158, loss = 1.18090418\n",
      "Iteration 159, loss = 1.18066580\n",
      "Iteration 160, loss = 1.18043266\n",
      "Iteration 161, loss = 1.18019024\n",
      "Iteration 162, loss = 1.17996268\n",
      "Iteration 163, loss = 1.17969754\n",
      "Iteration 164, loss = 1.17946236\n",
      "Iteration 165, loss = 1.17922788\n",
      "Iteration 166, loss = 1.17898691\n",
      "Iteration 167, loss = 1.17876547\n",
      "Iteration 168, loss = 1.17851552\n",
      "Iteration 169, loss = 1.17827960\n",
      "Iteration 170, loss = 1.17804344\n",
      "Iteration 171, loss = 1.17779730\n",
      "Iteration 172, loss = 1.17756288\n",
      "Iteration 173, loss = 1.17735118\n",
      "Iteration 174, loss = 1.17709474\n",
      "Iteration 175, loss = 1.17686332\n",
      "Iteration 176, loss = 1.17662591\n",
      "Iteration 177, loss = 1.17639154\n",
      "Iteration 178, loss = 1.17617743\n",
      "Iteration 179, loss = 1.17594217\n",
      "Iteration 180, loss = 1.17574331\n",
      "Iteration 181, loss = 1.17548537\n",
      "Iteration 182, loss = 1.17525846\n",
      "Iteration 183, loss = 1.17503444\n",
      "Iteration 184, loss = 1.17478322\n",
      "Iteration 185, loss = 1.17454411\n",
      "Iteration 186, loss = 1.17432808\n",
      "Iteration 187, loss = 1.17411887\n",
      "Iteration 188, loss = 1.17389064\n",
      "Iteration 189, loss = 1.17364615\n",
      "Iteration 190, loss = 1.17343434\n",
      "Iteration 191, loss = 1.17319828\n",
      "Iteration 192, loss = 1.17296557\n",
      "Iteration 193, loss = 1.17275322\n",
      "Iteration 194, loss = 1.17253512\n",
      "Iteration 195, loss = 1.17233067\n",
      "Iteration 196, loss = 1.17208151\n",
      "Iteration 197, loss = 1.17186413\n",
      "Iteration 198, loss = 1.17162318\n",
      "Iteration 199, loss = 1.17141937\n",
      "Iteration 200, loss = 1.17120529\n",
      "Iteration 201, loss = 1.17096099\n",
      "Iteration 202, loss = 1.17075283\n",
      "Iteration 203, loss = 1.17055360\n",
      "Iteration 204, loss = 1.17030244\n",
      "Iteration 205, loss = 1.17009158\n",
      "Iteration 206, loss = 1.16985911\n",
      "Iteration 207, loss = 1.16963315\n",
      "Iteration 208, loss = 1.16941925\n",
      "Iteration 209, loss = 1.16921079\n",
      "Iteration 210, loss = 1.16900245\n",
      "Iteration 211, loss = 1.16876733\n",
      "Iteration 212, loss = 1.16854623\n",
      "Iteration 213, loss = 1.16833689\n",
      "Iteration 214, loss = 1.16813387\n",
      "Iteration 215, loss = 1.16789365\n",
      "Iteration 216, loss = 1.16766427\n",
      "Iteration 217, loss = 1.16744186\n",
      "Iteration 218, loss = 1.16722844\n",
      "Iteration 219, loss = 1.16702531\n",
      "Iteration 220, loss = 1.16680710\n",
      "Iteration 221, loss = 1.16658186\n",
      "Iteration 222, loss = 1.16636172\n",
      "Iteration 223, loss = 1.16614789\n",
      "Iteration 224, loss = 1.16595057\n",
      "Iteration 225, loss = 1.16572446\n",
      "Iteration 226, loss = 1.16550554\n",
      "Iteration 227, loss = 1.16529085\n",
      "Iteration 228, loss = 1.16506825\n",
      "Iteration 229, loss = 1.16486294\n",
      "Iteration 230, loss = 1.16464596\n",
      "Iteration 231, loss = 1.16444203\n",
      "Iteration 232, loss = 1.16423464\n",
      "Iteration 233, loss = 1.16399717\n",
      "Iteration 234, loss = 1.16381126\n",
      "Iteration 235, loss = 1.16358433\n",
      "Iteration 236, loss = 1.16339018\n",
      "Iteration 237, loss = 1.16317700\n",
      "Iteration 238, loss = 1.16294420\n",
      "Iteration 239, loss = 1.16273709\n",
      "Iteration 240, loss = 1.16253154\n",
      "Iteration 241, loss = 1.16234567\n",
      "Iteration 242, loss = 1.16209645\n",
      "Iteration 243, loss = 1.16189201\n",
      "Iteration 244, loss = 1.16167867\n",
      "Iteration 245, loss = 1.16147522\n",
      "Iteration 246, loss = 1.16126520\n",
      "Iteration 247, loss = 1.16105123\n",
      "Iteration 248, loss = 1.16083651\n",
      "Iteration 249, loss = 1.16062115\n",
      "Iteration 250, loss = 1.16041425\n",
      "Iteration 251, loss = 1.16022252\n",
      "Iteration 252, loss = 1.16000345\n",
      "Iteration 253, loss = 1.15979241\n",
      "Iteration 254, loss = 1.15957750\n",
      "Iteration 255, loss = 1.15938564\n",
      "Iteration 256, loss = 1.15919059\n",
      "Iteration 257, loss = 1.15896899\n",
      "Iteration 258, loss = 1.15875993\n",
      "Iteration 259, loss = 1.15856517\n",
      "Iteration 260, loss = 1.15835857\n",
      "Iteration 261, loss = 1.15814632\n",
      "Iteration 262, loss = 1.15792953\n",
      "Iteration 263, loss = 1.15772471\n",
      "Iteration 264, loss = 1.15754873\n",
      "Iteration 265, loss = 1.15732795\n",
      "Iteration 266, loss = 1.15710279\n",
      "Iteration 267, loss = 1.15690717\n",
      "Iteration 268, loss = 1.15670155\n",
      "Iteration 269, loss = 1.15649503\n",
      "Iteration 270, loss = 1.15631269\n",
      "Iteration 271, loss = 1.15608197\n",
      "Iteration 272, loss = 1.15589289\n",
      "Iteration 273, loss = 1.15568775\n",
      "Iteration 274, loss = 1.15550508\n",
      "Iteration 275, loss = 1.15529680\n",
      "Iteration 276, loss = 1.15507915\n",
      "Iteration 277, loss = 1.15488211\n",
      "Iteration 278, loss = 1.15467844\n",
      "Iteration 279, loss = 1.15448720\n",
      "Iteration 280, loss = 1.15426154\n",
      "Iteration 281, loss = 1.15408624\n",
      "Iteration 282, loss = 1.15387834\n",
      "Iteration 283, loss = 1.15368087\n",
      "Iteration 284, loss = 1.15347947\n",
      "Iteration 285, loss = 1.15328033\n",
      "Iteration 286, loss = 1.15307387\n",
      "Iteration 287, loss = 1.15287523\n",
      "Iteration 288, loss = 1.15271323\n",
      "Iteration 289, loss = 1.15246866\n",
      "Iteration 290, loss = 1.15227856\n",
      "Iteration 291, loss = 1.15207958\n",
      "Iteration 292, loss = 1.15189369\n",
      "Iteration 293, loss = 1.15167336\n",
      "Iteration 294, loss = 1.15150427\n",
      "Iteration 295, loss = 1.15129206\n",
      "Iteration 296, loss = 1.15107651\n",
      "Iteration 297, loss = 1.15088713\n",
      "Iteration 298, loss = 1.15072851\n",
      "Iteration 299, loss = 1.15052377\n",
      "Iteration 300, loss = 1.15031857\n",
      "Iteration 1, loss = 1.51671839\n",
      "Iteration 2, loss = 1.50780971\n",
      "Iteration 3, loss = 1.49449152\n",
      "Iteration 4, loss = 1.47918269\n",
      "Iteration 5, loss = 1.46241506\n",
      "Iteration 6, loss = 1.44606751\n",
      "Iteration 7, loss = 1.43024196\n",
      "Iteration 8, loss = 1.41477240\n",
      "Iteration 9, loss = 1.40071365\n",
      "Iteration 10, loss = 1.38724291\n",
      "Iteration 11, loss = 1.37491654\n",
      "Iteration 12, loss = 1.36376219\n",
      "Iteration 13, loss = 1.35325884\n",
      "Iteration 14, loss = 1.34409639\n",
      "Iteration 15, loss = 1.33486373\n",
      "Iteration 16, loss = 1.32704845\n",
      "Iteration 17, loss = 1.31979794\n",
      "Iteration 18, loss = 1.31287635\n",
      "Iteration 19, loss = 1.30702756\n",
      "Iteration 20, loss = 1.30096697\n",
      "Iteration 21, loss = 1.29590566\n",
      "Iteration 22, loss = 1.29115464\n",
      "Iteration 23, loss = 1.28644019\n",
      "Iteration 24, loss = 1.28244235\n",
      "Iteration 25, loss = 1.27861861\n",
      "Iteration 26, loss = 1.27499083\n",
      "Iteration 27, loss = 1.27171095\n",
      "Iteration 28, loss = 1.26853783\n",
      "Iteration 29, loss = 1.26568925\n",
      "Iteration 30, loss = 1.26297663\n",
      "Iteration 31, loss = 1.26037868\n",
      "Iteration 32, loss = 1.25791365\n",
      "Iteration 33, loss = 1.25561618\n",
      "Iteration 34, loss = 1.25365593\n",
      "Iteration 35, loss = 1.25155248\n",
      "Iteration 36, loss = 1.24958783\n",
      "Iteration 37, loss = 1.24775519\n",
      "Iteration 38, loss = 1.24609951\n",
      "Iteration 39, loss = 1.24439757\n",
      "Iteration 40, loss = 1.24287738\n",
      "Iteration 41, loss = 1.24127747\n",
      "Iteration 42, loss = 1.23986756\n",
      "Iteration 43, loss = 1.23848531\n",
      "Iteration 44, loss = 1.23719285\n",
      "Iteration 45, loss = 1.23594000\n",
      "Iteration 46, loss = 1.23470361\n",
      "Iteration 47, loss = 1.23361378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 1.23253189\n",
      "Iteration 49, loss = 1.23142268\n",
      "Iteration 50, loss = 1.23034679\n",
      "Iteration 51, loss = 1.22942315\n",
      "Iteration 52, loss = 1.22844441\n",
      "Iteration 53, loss = 1.22753510\n",
      "Iteration 54, loss = 1.22665698\n",
      "Iteration 55, loss = 1.22578117\n",
      "Iteration 56, loss = 1.22496966\n",
      "Iteration 57, loss = 1.22420247\n",
      "Iteration 58, loss = 1.22339056\n",
      "Iteration 59, loss = 1.22265582\n",
      "Iteration 60, loss = 1.22199224\n",
      "Iteration 61, loss = 1.22125521\n",
      "Iteration 62, loss = 1.22059452\n",
      "Iteration 63, loss = 1.21992539\n",
      "Iteration 64, loss = 1.21926499\n",
      "Iteration 65, loss = 1.21865871\n",
      "Iteration 66, loss = 1.21805712\n",
      "Iteration 67, loss = 1.21745897\n",
      "Iteration 68, loss = 1.21688859\n",
      "Iteration 69, loss = 1.21635710\n",
      "Iteration 70, loss = 1.21578504\n",
      "Iteration 71, loss = 1.21530385\n",
      "Iteration 72, loss = 1.21474238\n",
      "Iteration 73, loss = 1.21425062\n",
      "Iteration 74, loss = 1.21380159\n",
      "Iteration 75, loss = 1.21327349\n",
      "Iteration 76, loss = 1.21280513\n",
      "Iteration 77, loss = 1.21235929\n",
      "Iteration 78, loss = 1.21193177\n",
      "Iteration 79, loss = 1.21149240\n",
      "Iteration 80, loss = 1.21108716\n",
      "Iteration 81, loss = 1.21061754\n",
      "Iteration 82, loss = 1.21021048\n",
      "Iteration 83, loss = 1.20983570\n",
      "Iteration 84, loss = 1.20944989\n",
      "Iteration 85, loss = 1.20904597\n",
      "Iteration 86, loss = 1.20867072\n",
      "Iteration 87, loss = 1.20830664\n",
      "Iteration 88, loss = 1.20790495\n",
      "Iteration 89, loss = 1.20759166\n",
      "Iteration 90, loss = 1.20721627\n",
      "Iteration 91, loss = 1.20686417\n",
      "Iteration 92, loss = 1.20650406\n",
      "Iteration 93, loss = 1.20617710\n",
      "Iteration 94, loss = 1.20584021\n",
      "Iteration 95, loss = 1.20553303\n",
      "Iteration 96, loss = 1.20518851\n",
      "Iteration 97, loss = 1.20487985\n",
      "Iteration 98, loss = 1.20455512\n",
      "Iteration 99, loss = 1.20424520\n",
      "Iteration 100, loss = 1.20392806\n",
      "Iteration 101, loss = 1.20363608\n",
      "Iteration 102, loss = 1.20334736\n",
      "Iteration 103, loss = 1.20304730\n",
      "Iteration 104, loss = 1.20275270\n",
      "Iteration 105, loss = 1.20245048\n",
      "Iteration 106, loss = 1.20217869\n",
      "Iteration 107, loss = 1.20188059\n",
      "Iteration 108, loss = 1.20161698\n",
      "Iteration 109, loss = 1.20136452\n",
      "Iteration 110, loss = 1.20107250\n",
      "Iteration 111, loss = 1.20078266\n",
      "Iteration 112, loss = 1.20052982\n",
      "Iteration 113, loss = 1.20027434\n",
      "Iteration 114, loss = 1.19998938\n",
      "Iteration 115, loss = 1.19975307\n",
      "Iteration 116, loss = 1.19948484\n",
      "Iteration 117, loss = 1.19924277\n",
      "Iteration 118, loss = 1.19898930\n",
      "Iteration 119, loss = 1.19873202\n",
      "Iteration 120, loss = 1.19849954\n",
      "Iteration 121, loss = 1.19824384\n",
      "Iteration 122, loss = 1.19799970\n",
      "Iteration 123, loss = 1.19777654\n",
      "Iteration 124, loss = 1.19753667\n",
      "Iteration 125, loss = 1.19730243\n",
      "Iteration 126, loss = 1.19706476\n",
      "Iteration 127, loss = 1.19681682\n",
      "Iteration 128, loss = 1.19657795\n",
      "Iteration 129, loss = 1.19636115\n",
      "Iteration 130, loss = 1.19612133\n",
      "Iteration 131, loss = 1.19588312\n",
      "Iteration 132, loss = 1.19567012\n",
      "Iteration 133, loss = 1.19542747\n",
      "Iteration 134, loss = 1.19519927\n",
      "Iteration 135, loss = 1.19497418\n",
      "Iteration 136, loss = 1.19476385\n",
      "Iteration 137, loss = 1.19453188\n",
      "Iteration 138, loss = 1.19430960\n",
      "Iteration 139, loss = 1.19408555\n",
      "Iteration 140, loss = 1.19390886\n",
      "Iteration 141, loss = 1.19366621\n",
      "Iteration 142, loss = 1.19347761\n",
      "Iteration 143, loss = 1.19322784\n",
      "Iteration 144, loss = 1.19302172\n",
      "Iteration 145, loss = 1.19281446\n",
      "Iteration 146, loss = 1.19258679\n",
      "Iteration 147, loss = 1.19238681\n",
      "Iteration 148, loss = 1.19218693\n",
      "Iteration 149, loss = 1.19196451\n",
      "Iteration 150, loss = 1.19176792\n",
      "Iteration 151, loss = 1.19157275\n",
      "Iteration 152, loss = 1.19133891\n",
      "Iteration 153, loss = 1.19113662\n",
      "Iteration 154, loss = 1.19093397\n",
      "Iteration 155, loss = 1.19071982\n",
      "Iteration 156, loss = 1.19052408\n",
      "Iteration 157, loss = 1.19032487\n",
      "Iteration 158, loss = 1.19011272\n",
      "Iteration 159, loss = 1.18990893\n",
      "Iteration 160, loss = 1.18971048\n",
      "Iteration 161, loss = 1.18952285\n",
      "Iteration 162, loss = 1.18932681\n",
      "Iteration 163, loss = 1.18911045\n",
      "Iteration 164, loss = 1.18891105\n",
      "Iteration 165, loss = 1.18873064\n",
      "Iteration 166, loss = 1.18853066\n",
      "Iteration 167, loss = 1.18833773\n",
      "Iteration 168, loss = 1.18814511\n",
      "Iteration 169, loss = 1.18793707\n",
      "Iteration 170, loss = 1.18774342\n",
      "Iteration 171, loss = 1.18753236\n",
      "Iteration 172, loss = 1.18734980\n",
      "Iteration 173, loss = 1.18716374\n",
      "Iteration 174, loss = 1.18695222\n",
      "Iteration 175, loss = 1.18676613\n",
      "Iteration 176, loss = 1.18658257\n",
      "Iteration 177, loss = 1.18636835\n",
      "Iteration 178, loss = 1.18619036\n",
      "Iteration 179, loss = 1.18601107\n",
      "Iteration 180, loss = 1.18580165\n",
      "Iteration 181, loss = 1.18560730\n",
      "Iteration 182, loss = 1.18544711\n",
      "Iteration 183, loss = 1.18525121\n",
      "Iteration 184, loss = 1.18504379\n",
      "Iteration 185, loss = 1.18484341\n",
      "Iteration 186, loss = 1.18467068\n",
      "Iteration 187, loss = 1.18446898\n",
      "Iteration 188, loss = 1.18428126\n",
      "Iteration 189, loss = 1.18409132\n",
      "Iteration 190, loss = 1.18392393\n",
      "Iteration 191, loss = 1.18372227\n",
      "Iteration 192, loss = 1.18353320\n",
      "Iteration 193, loss = 1.18337715\n",
      "Iteration 194, loss = 1.18317158\n",
      "Iteration 195, loss = 1.18298473\n",
      "Iteration 196, loss = 1.18281890\n",
      "Iteration 197, loss = 1.18261514\n",
      "Iteration 198, loss = 1.18243744\n",
      "Iteration 199, loss = 1.18225341\n",
      "Iteration 200, loss = 1.18207785\n",
      "Iteration 201, loss = 1.18187713\n",
      "Iteration 202, loss = 1.18171401\n",
      "Iteration 203, loss = 1.18151501\n",
      "Iteration 204, loss = 1.18133657\n",
      "Iteration 205, loss = 1.18118346\n",
      "Iteration 206, loss = 1.18098405\n",
      "Iteration 207, loss = 1.18080338\n",
      "Iteration 208, loss = 1.18062025\n",
      "Iteration 209, loss = 1.18042482\n",
      "Iteration 210, loss = 1.18026136\n",
      "Iteration 211, loss = 1.18008400\n",
      "Iteration 212, loss = 1.17990040\n",
      "Iteration 213, loss = 1.17970515\n",
      "Iteration 214, loss = 1.17953348\n",
      "Iteration 215, loss = 1.17940575\n",
      "Iteration 216, loss = 1.17917689\n",
      "Iteration 217, loss = 1.17899458\n",
      "Iteration 218, loss = 1.17881899\n",
      "Iteration 219, loss = 1.17863232\n",
      "Iteration 220, loss = 1.17847073\n",
      "Iteration 221, loss = 1.17828847\n",
      "Iteration 222, loss = 1.17811361\n",
      "Iteration 223, loss = 1.17792317\n",
      "Iteration 224, loss = 1.17777481\n",
      "Iteration 225, loss = 1.17758261\n",
      "Iteration 226, loss = 1.17739949\n",
      "Iteration 227, loss = 1.17723676\n",
      "Iteration 228, loss = 1.17704009\n",
      "Iteration 229, loss = 1.17689530\n",
      "Iteration 230, loss = 1.17670998\n",
      "Iteration 231, loss = 1.17651201\n",
      "Iteration 232, loss = 1.17632889\n",
      "Iteration 233, loss = 1.17614459\n",
      "Iteration 234, loss = 1.17598048\n",
      "Iteration 235, loss = 1.17580006\n",
      "Iteration 236, loss = 1.17561433\n",
      "Iteration 237, loss = 1.17544696\n",
      "Iteration 238, loss = 1.17527822\n",
      "Iteration 239, loss = 1.17508002\n",
      "Iteration 240, loss = 1.17490720\n",
      "Iteration 241, loss = 1.17472926\n",
      "Iteration 242, loss = 1.17456135\n",
      "Iteration 243, loss = 1.17437374\n",
      "Iteration 244, loss = 1.17421336\n",
      "Iteration 245, loss = 1.17403022\n",
      "Iteration 246, loss = 1.17386646\n",
      "Iteration 247, loss = 1.17367897\n",
      "Iteration 248, loss = 1.17350742\n",
      "Iteration 249, loss = 1.17332103\n",
      "Iteration 250, loss = 1.17315759\n",
      "Iteration 251, loss = 1.17299584\n",
      "Iteration 252, loss = 1.17282041\n",
      "Iteration 253, loss = 1.17263879\n",
      "Iteration 254, loss = 1.17246099\n",
      "Iteration 255, loss = 1.17229873\n",
      "Iteration 256, loss = 1.17213460\n",
      "Iteration 257, loss = 1.17193471\n",
      "Iteration 258, loss = 1.17176507\n",
      "Iteration 259, loss = 1.17160556\n",
      "Iteration 260, loss = 1.17142586\n",
      "Iteration 261, loss = 1.17126726\n",
      "Iteration 262, loss = 1.17108413\n",
      "Iteration 263, loss = 1.17090197\n",
      "Iteration 264, loss = 1.17075555\n",
      "Iteration 265, loss = 1.17055834\n",
      "Iteration 266, loss = 1.17039704\n",
      "Iteration 267, loss = 1.17021507\n",
      "Iteration 268, loss = 1.17006065\n",
      "Iteration 269, loss = 1.16988650\n",
      "Iteration 270, loss = 1.16970831\n",
      "Iteration 271, loss = 1.16954154\n",
      "Iteration 272, loss = 1.16936584\n",
      "Iteration 273, loss = 1.16921146\n",
      "Iteration 274, loss = 1.16903855\n",
      "Iteration 275, loss = 1.16886745\n",
      "Iteration 276, loss = 1.16868822\n",
      "Iteration 277, loss = 1.16851844\n",
      "Iteration 278, loss = 1.16834654\n",
      "Iteration 279, loss = 1.16821266\n",
      "Iteration 280, loss = 1.16800675\n",
      "Iteration 281, loss = 1.16784347\n",
      "Iteration 282, loss = 1.16765616\n",
      "Iteration 283, loss = 1.16750789\n",
      "Iteration 284, loss = 1.16734794\n",
      "Iteration 285, loss = 1.16717288\n",
      "Iteration 286, loss = 1.16700329\n",
      "Iteration 287, loss = 1.16682268\n",
      "Iteration 288, loss = 1.16669703\n",
      "Iteration 289, loss = 1.16647900\n",
      "Iteration 290, loss = 1.16633874\n",
      "Iteration 291, loss = 1.16615302\n",
      "Iteration 292, loss = 1.16598520\n",
      "Iteration 293, loss = 1.16582608\n",
      "Iteration 294, loss = 1.16565277\n",
      "Iteration 295, loss = 1.16548588\n",
      "Iteration 296, loss = 1.16531252\n",
      "Iteration 297, loss = 1.16515553\n",
      "Iteration 298, loss = 1.16499289\n",
      "Iteration 299, loss = 1.16481460\n",
      "Iteration 300, loss = 1.16465870\n",
      "Iteration 1, loss = 1.51551323\n",
      "Iteration 2, loss = 1.50643610\n",
      "Iteration 3, loss = 1.49303576\n",
      "Iteration 4, loss = 1.47739996\n",
      "Iteration 5, loss = 1.46068901\n",
      "Iteration 6, loss = 1.44371015\n",
      "Iteration 7, loss = 1.42770974\n",
      "Iteration 8, loss = 1.41221077\n",
      "Iteration 9, loss = 1.39751210\n",
      "Iteration 10, loss = 1.38386647\n",
      "Iteration 11, loss = 1.37192639\n",
      "Iteration 12, loss = 1.35995627\n",
      "Iteration 13, loss = 1.34952436\n",
      "Iteration 14, loss = 1.34013893\n",
      "Iteration 15, loss = 1.33072699\n",
      "Iteration 16, loss = 1.32270428\n",
      "Iteration 17, loss = 1.31511862\n",
      "Iteration 18, loss = 1.30838911\n",
      "Iteration 19, loss = 1.30197862\n",
      "Iteration 20, loss = 1.29619792\n",
      "Iteration 21, loss = 1.29076897\n",
      "Iteration 22, loss = 1.28591117\n",
      "Iteration 23, loss = 1.28135619\n",
      "Iteration 24, loss = 1.27690932\n",
      "Iteration 25, loss = 1.27305427\n",
      "Iteration 26, loss = 1.26933767\n",
      "Iteration 27, loss = 1.26583715\n",
      "Iteration 28, loss = 1.26255414\n",
      "Iteration 29, loss = 1.25964873\n",
      "Iteration 30, loss = 1.25675359\n",
      "Iteration 31, loss = 1.25417586\n",
      "Iteration 32, loss = 1.25163578\n",
      "Iteration 33, loss = 1.24916252\n",
      "Iteration 34, loss = 1.24689621\n",
      "Iteration 35, loss = 1.24486037\n",
      "Iteration 36, loss = 1.24284983\n",
      "Iteration 37, loss = 1.24089613\n",
      "Iteration 38, loss = 1.23912899\n",
      "Iteration 39, loss = 1.23729152\n",
      "Iteration 40, loss = 1.23572482\n",
      "Iteration 41, loss = 1.23406204\n",
      "Iteration 42, loss = 1.23257774\n",
      "Iteration 43, loss = 1.23114008\n",
      "Iteration 44, loss = 1.22977794\n",
      "Iteration 45, loss = 1.22840357\n",
      "Iteration 46, loss = 1.22715739\n",
      "Iteration 47, loss = 1.22592541\n",
      "Iteration 48, loss = 1.22475116\n",
      "Iteration 49, loss = 1.22359838\n",
      "Iteration 50, loss = 1.22253619\n",
      "Iteration 51, loss = 1.22140132\n",
      "Iteration 52, loss = 1.22045331\n",
      "Iteration 53, loss = 1.21944843\n",
      "Iteration 54, loss = 1.21845029\n",
      "Iteration 55, loss = 1.21757139\n",
      "Iteration 56, loss = 1.21666624\n",
      "Iteration 57, loss = 1.21582727\n",
      "Iteration 58, loss = 1.21495006\n",
      "Iteration 59, loss = 1.21416384\n",
      "Iteration 60, loss = 1.21342553\n",
      "Iteration 61, loss = 1.21257804\n",
      "Iteration 62, loss = 1.21185853\n",
      "Iteration 63, loss = 1.21113244\n",
      "Iteration 64, loss = 1.21044376\n",
      "Iteration 65, loss = 1.20971044\n",
      "Iteration 66, loss = 1.20906584\n",
      "Iteration 67, loss = 1.20842250\n",
      "Iteration 68, loss = 1.20776264\n",
      "Iteration 69, loss = 1.20711559\n",
      "Iteration 70, loss = 1.20658084\n",
      "Iteration 71, loss = 1.20596337\n",
      "Iteration 72, loss = 1.20537628\n",
      "Iteration 73, loss = 1.20483401\n",
      "Iteration 74, loss = 1.20423110\n",
      "Iteration 75, loss = 1.20369574\n",
      "Iteration 76, loss = 1.20318831\n",
      "Iteration 77, loss = 1.20264705\n",
      "Iteration 78, loss = 1.20210804\n",
      "Iteration 79, loss = 1.20160109\n",
      "Iteration 80, loss = 1.20116483\n",
      "Iteration 81, loss = 1.20066445\n",
      "Iteration 82, loss = 1.20016928\n",
      "Iteration 83, loss = 1.19971691\n",
      "Iteration 84, loss = 1.19925218\n",
      "Iteration 85, loss = 1.19876986\n",
      "Iteration 86, loss = 1.19832598\n",
      "Iteration 87, loss = 1.19791482\n",
      "Iteration 88, loss = 1.19743813\n",
      "Iteration 89, loss = 1.19702086\n",
      "Iteration 90, loss = 1.19663284\n",
      "Iteration 91, loss = 1.19619468\n",
      "Iteration 92, loss = 1.19575879\n",
      "Iteration 93, loss = 1.19536619\n",
      "Iteration 94, loss = 1.19496111\n",
      "Iteration 95, loss = 1.19458278\n",
      "Iteration 96, loss = 1.19416552\n",
      "Iteration 97, loss = 1.19378228\n",
      "Iteration 98, loss = 1.19340761\n",
      "Iteration 99, loss = 1.19301350\n",
      "Iteration 100, loss = 1.19264954\n",
      "Iteration 101, loss = 1.19226831\n",
      "Iteration 102, loss = 1.19194788\n",
      "Iteration 103, loss = 1.19153480\n",
      "Iteration 104, loss = 1.19118946\n",
      "Iteration 105, loss = 1.19084719\n",
      "Iteration 106, loss = 1.19047910\n",
      "Iteration 107, loss = 1.19013687\n",
      "Iteration 108, loss = 1.18977278\n",
      "Iteration 109, loss = 1.18945926\n",
      "Iteration 110, loss = 1.18909655\n",
      "Iteration 111, loss = 1.18877684\n",
      "Iteration 112, loss = 1.18842162\n",
      "Iteration 113, loss = 1.18807965\n",
      "Iteration 114, loss = 1.18776526\n",
      "Iteration 115, loss = 1.18742263\n",
      "Iteration 116, loss = 1.18710153\n",
      "Iteration 117, loss = 1.18679808\n",
      "Iteration 118, loss = 1.18645304\n",
      "Iteration 119, loss = 1.18614192\n",
      "Iteration 120, loss = 1.18580891\n",
      "Iteration 121, loss = 1.18551154\n",
      "Iteration 122, loss = 1.18518012\n",
      "Iteration 123, loss = 1.18488371\n",
      "Iteration 124, loss = 1.18457689\n",
      "Iteration 125, loss = 1.18427724\n",
      "Iteration 126, loss = 1.18395944\n",
      "Iteration 127, loss = 1.18364030\n",
      "Iteration 128, loss = 1.18338237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 129, loss = 1.18303564\n",
      "Iteration 130, loss = 1.18273148\n",
      "Iteration 131, loss = 1.18244992\n",
      "Iteration 132, loss = 1.18214864\n",
      "Iteration 133, loss = 1.18184319\n",
      "Iteration 134, loss = 1.18155604\n",
      "Iteration 135, loss = 1.18126482\n",
      "Iteration 136, loss = 1.18098094\n",
      "Iteration 137, loss = 1.18067923\n",
      "Iteration 138, loss = 1.18040499\n",
      "Iteration 139, loss = 1.18011458\n",
      "Iteration 140, loss = 1.17981837\n",
      "Iteration 141, loss = 1.17955124\n",
      "Iteration 142, loss = 1.17927462\n",
      "Iteration 143, loss = 1.17898300\n",
      "Iteration 144, loss = 1.17868868\n",
      "Iteration 145, loss = 1.17842285\n",
      "Iteration 146, loss = 1.17814354\n",
      "Iteration 147, loss = 1.17787433\n",
      "Iteration 148, loss = 1.17759599\n",
      "Iteration 149, loss = 1.17731202\n",
      "Iteration 150, loss = 1.17703239\n",
      "Iteration 151, loss = 1.17677132\n",
      "Iteration 152, loss = 1.17650111\n",
      "Iteration 153, loss = 1.17621659\n",
      "Iteration 154, loss = 1.17594835\n",
      "Iteration 155, loss = 1.17567169\n",
      "Iteration 156, loss = 1.17545835\n",
      "Iteration 157, loss = 1.17514288\n",
      "Iteration 158, loss = 1.17490316\n",
      "Iteration 159, loss = 1.17460247\n",
      "Iteration 160, loss = 1.17433012\n",
      "Iteration 161, loss = 1.17407027\n",
      "Iteration 162, loss = 1.17379995\n",
      "Iteration 163, loss = 1.17353548\n",
      "Iteration 164, loss = 1.17328492\n",
      "Iteration 165, loss = 1.17300909\n",
      "Iteration 166, loss = 1.17275925\n",
      "Iteration 167, loss = 1.17248994\n",
      "Iteration 168, loss = 1.17223177\n",
      "Iteration 169, loss = 1.17197047\n",
      "Iteration 170, loss = 1.17171123\n",
      "Iteration 171, loss = 1.17147695\n",
      "Iteration 172, loss = 1.17118871\n",
      "Iteration 173, loss = 1.17094200\n",
      "Iteration 174, loss = 1.17070755\n",
      "Iteration 175, loss = 1.17042418\n",
      "Iteration 176, loss = 1.17019179\n",
      "Iteration 177, loss = 1.16992621\n",
      "Iteration 178, loss = 1.16966508\n",
      "Iteration 179, loss = 1.16941116\n",
      "Iteration 180, loss = 1.16915232\n",
      "Iteration 181, loss = 1.16893412\n",
      "Iteration 182, loss = 1.16867386\n",
      "Iteration 183, loss = 1.16841251\n",
      "Iteration 184, loss = 1.16813302\n",
      "Iteration 185, loss = 1.16790607\n",
      "Iteration 186, loss = 1.16765931\n",
      "Iteration 187, loss = 1.16740608\n",
      "Iteration 188, loss = 1.16715734\n",
      "Iteration 189, loss = 1.16689875\n",
      "Iteration 190, loss = 1.16664781\n",
      "Iteration 191, loss = 1.16638856\n",
      "Iteration 192, loss = 1.16614840\n",
      "Iteration 193, loss = 1.16591408\n",
      "Iteration 194, loss = 1.16562920\n",
      "Iteration 195, loss = 1.16540946\n",
      "Iteration 196, loss = 1.16513030\n",
      "Iteration 197, loss = 1.16493717\n",
      "Iteration 198, loss = 1.16465344\n",
      "Iteration 199, loss = 1.16439350\n",
      "Iteration 200, loss = 1.16415013\n",
      "Iteration 201, loss = 1.16390954\n",
      "Iteration 202, loss = 1.16366772\n",
      "Iteration 203, loss = 1.16342046\n",
      "Iteration 204, loss = 1.16317814\n",
      "Iteration 205, loss = 1.16292943\n",
      "Iteration 206, loss = 1.16265254\n",
      "Iteration 207, loss = 1.16243609\n",
      "Iteration 208, loss = 1.16217216\n",
      "Iteration 209, loss = 1.16198481\n",
      "Iteration 210, loss = 1.16167835\n",
      "Iteration 211, loss = 1.16144490\n",
      "Iteration 212, loss = 1.16117768\n",
      "Iteration 213, loss = 1.16093918\n",
      "Iteration 214, loss = 1.16070515\n",
      "Iteration 215, loss = 1.16047064\n",
      "Iteration 216, loss = 1.16021915\n",
      "Iteration 217, loss = 1.15995158\n",
      "Iteration 218, loss = 1.15973514\n",
      "Iteration 219, loss = 1.15945793\n",
      "Iteration 220, loss = 1.15921598\n",
      "Iteration 221, loss = 1.15896556\n",
      "Iteration 222, loss = 1.15875288\n",
      "Iteration 223, loss = 1.15849687\n",
      "Iteration 224, loss = 1.15830053\n",
      "Iteration 225, loss = 1.15800550\n",
      "Iteration 226, loss = 1.15775075\n",
      "Iteration 227, loss = 1.15750234\n",
      "Iteration 228, loss = 1.15725845\n",
      "Iteration 229, loss = 1.15701744\n",
      "Iteration 230, loss = 1.15678922\n",
      "Iteration 231, loss = 1.15651618\n",
      "Iteration 232, loss = 1.15627995\n",
      "Iteration 233, loss = 1.15607892\n",
      "Iteration 234, loss = 1.15580086\n",
      "Iteration 235, loss = 1.15555192\n",
      "Iteration 236, loss = 1.15530912\n",
      "Iteration 237, loss = 1.15507703\n",
      "Iteration 238, loss = 1.15482796\n",
      "Iteration 239, loss = 1.15459629\n",
      "Iteration 240, loss = 1.15434046\n",
      "Iteration 241, loss = 1.15411134\n",
      "Iteration 242, loss = 1.15386811\n",
      "Iteration 243, loss = 1.15363039\n",
      "Iteration 244, loss = 1.15338230\n",
      "Iteration 245, loss = 1.15315122\n",
      "Iteration 246, loss = 1.15292780\n",
      "Iteration 247, loss = 1.15267414\n",
      "Iteration 248, loss = 1.15247911\n",
      "Iteration 249, loss = 1.15221035\n",
      "Iteration 250, loss = 1.15195521\n",
      "Iteration 251, loss = 1.15171593\n",
      "Iteration 252, loss = 1.15148354\n",
      "Iteration 253, loss = 1.15123633\n",
      "Iteration 254, loss = 1.15099992\n",
      "Iteration 255, loss = 1.15077354\n",
      "Iteration 256, loss = 1.15052593\n",
      "Iteration 257, loss = 1.15031084\n",
      "Iteration 258, loss = 1.15008937\n",
      "Iteration 259, loss = 1.14981725\n",
      "Iteration 260, loss = 1.14957362\n",
      "Iteration 261, loss = 1.14934357\n",
      "Iteration 262, loss = 1.14911726\n",
      "Iteration 263, loss = 1.14886438\n",
      "Iteration 264, loss = 1.14863035\n",
      "Iteration 265, loss = 1.14839366\n",
      "Iteration 266, loss = 1.14814767\n",
      "Iteration 267, loss = 1.14792181\n",
      "Iteration 268, loss = 1.14767730\n",
      "Iteration 269, loss = 1.14744304\n",
      "Iteration 270, loss = 1.14720455\n",
      "Iteration 271, loss = 1.14697913\n",
      "Iteration 272, loss = 1.14673048\n",
      "Iteration 273, loss = 1.14650128\n",
      "Iteration 274, loss = 1.14627952\n",
      "Iteration 275, loss = 1.14603617\n",
      "Iteration 276, loss = 1.14581827\n",
      "Iteration 277, loss = 1.14556878\n",
      "Iteration 278, loss = 1.14532466\n",
      "Iteration 279, loss = 1.14509981\n",
      "Iteration 280, loss = 1.14487548\n",
      "Iteration 281, loss = 1.14464170\n",
      "Iteration 282, loss = 1.14439381\n",
      "Iteration 283, loss = 1.14416295\n",
      "Iteration 284, loss = 1.14393363\n",
      "Iteration 285, loss = 1.14371114\n",
      "Iteration 286, loss = 1.14347682\n",
      "Iteration 287, loss = 1.14324069\n",
      "Iteration 288, loss = 1.14301466\n",
      "Iteration 289, loss = 1.14279184\n",
      "Iteration 290, loss = 1.14257981\n",
      "Iteration 291, loss = 1.14232062\n",
      "Iteration 292, loss = 1.14210836\n",
      "Iteration 293, loss = 1.14187980\n",
      "Iteration 294, loss = 1.14164657\n",
      "Iteration 295, loss = 1.14143543\n",
      "Iteration 296, loss = 1.14118743\n",
      "Iteration 297, loss = 1.14095938\n",
      "Iteration 298, loss = 1.14075915\n",
      "Iteration 299, loss = 1.14051212\n",
      "Iteration 300, loss = 1.14028897\n",
      "Iteration 1, loss = 12.11038895\n",
      "Iteration 2, loss = 9.91049475\n",
      "Iteration 3, loss = 11.25404468\n",
      "Iteration 4, loss = 5.38341809\n",
      "Iteration 5, loss = 1.56362076\n",
      "Iteration 6, loss = 1.37188245\n",
      "Iteration 7, loss = 1.28953992\n",
      "Iteration 8, loss = 1.24433680\n",
      "Iteration 9, loss = 1.23540936\n",
      "Iteration 10, loss = 1.22911805\n",
      "Iteration 11, loss = 1.22353898\n",
      "Iteration 12, loss = 1.21969169\n",
      "Iteration 13, loss = 1.21433232\n",
      "Iteration 14, loss = 1.20967327\n",
      "Iteration 15, loss = 1.20688573\n",
      "Iteration 16, loss = 1.20727437\n",
      "Iteration 17, loss = 1.20453729\n",
      "Iteration 18, loss = 1.20218119\n",
      "Iteration 19, loss = 1.20011656\n",
      "Iteration 20, loss = 1.19904676\n",
      "Iteration 21, loss = 1.19849154\n",
      "Iteration 22, loss = 1.19683664\n",
      "Iteration 23, loss = 1.19500148\n",
      "Iteration 24, loss = 1.19382225\n",
      "Iteration 25, loss = 1.19322639\n",
      "Iteration 26, loss = 1.19129407\n",
      "Iteration 27, loss = 1.19032719\n",
      "Iteration 28, loss = 1.18726378\n",
      "Iteration 29, loss = 1.18720829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 1.18730858\n",
      "Iteration 31, loss = 1.18680363\n",
      "Iteration 32, loss = 1.18461644\n",
      "Iteration 33, loss = 1.18452222\n",
      "Iteration 34, loss = 1.18253349\n",
      "Iteration 35, loss = 1.17998144\n",
      "Iteration 36, loss = 1.18057745\n",
      "Iteration 37, loss = 1.17913177\n",
      "Iteration 38, loss = 1.18041665\n",
      "Iteration 39, loss = 1.17718568\n",
      "Iteration 40, loss = 1.17794106\n",
      "Iteration 41, loss = 1.17605756\n",
      "Iteration 42, loss = 1.17566083\n",
      "Iteration 43, loss = 1.17744720\n",
      "Iteration 44, loss = 1.17506495\n",
      "Iteration 45, loss = 1.17163821\n",
      "Iteration 46, loss = 1.17414944\n",
      "Iteration 47, loss = 1.17157716\n",
      "Iteration 48, loss = 1.17140303\n",
      "Iteration 49, loss = 1.17323485\n",
      "Iteration 50, loss = 1.16843229\n",
      "Iteration 51, loss = 1.16762759\n",
      "Iteration 52, loss = 1.16959749\n",
      "Iteration 53, loss = 1.17053201\n",
      "Iteration 54, loss = 1.16464276\n",
      "Iteration 55, loss = 1.16688398\n",
      "Iteration 56, loss = 1.16495001\n",
      "Iteration 57, loss = 1.17012212\n",
      "Iteration 58, loss = 1.16556476\n",
      "Iteration 59, loss = 1.16306272\n",
      "Iteration 60, loss = 1.16244335\n",
      "Iteration 61, loss = 1.16320262\n",
      "Iteration 62, loss = 1.16477192\n",
      "Iteration 63, loss = 1.16231759\n",
      "Iteration 64, loss = 1.16160016\n",
      "Iteration 65, loss = 1.15711389\n",
      "Iteration 66, loss = 1.15947104\n",
      "Iteration 67, loss = 1.16155456\n",
      "Iteration 68, loss = 1.15870019\n",
      "Iteration 69, loss = 1.15990139\n",
      "Iteration 70, loss = 1.15571946\n",
      "Iteration 71, loss = 1.16094059\n",
      "Iteration 72, loss = 1.15609780\n",
      "Iteration 73, loss = 1.15478139\n",
      "Iteration 74, loss = 1.16152295\n",
      "Iteration 75, loss = 1.15618414\n",
      "Iteration 76, loss = 1.15383404\n",
      "Iteration 77, loss = 1.15257960\n",
      "Iteration 78, loss = 1.15113679\n",
      "Iteration 79, loss = 1.16270271\n",
      "Iteration 80, loss = 1.15610846\n",
      "Iteration 81, loss = 1.15194743\n",
      "Iteration 82, loss = 1.15550460\n",
      "Iteration 83, loss = 1.14991184\n",
      "Iteration 84, loss = 1.14828991\n",
      "Iteration 85, loss = 1.14869665\n",
      "Iteration 86, loss = 1.15788544\n",
      "Iteration 87, loss = 1.14676646\n",
      "Iteration 88, loss = 1.14442919\n",
      "Iteration 89, loss = 1.14706383\n",
      "Iteration 90, loss = 1.14594647\n",
      "Iteration 91, loss = 1.14727355\n",
      "Iteration 92, loss = 1.15460483\n",
      "Iteration 93, loss = 1.14658267\n",
      "Iteration 94, loss = 1.14841136\n",
      "Iteration 95, loss = 1.14308608\n",
      "Iteration 96, loss = 1.14797393\n",
      "Iteration 97, loss = 1.14660642\n",
      "Iteration 98, loss = 1.14263400\n",
      "Iteration 99, loss = 1.14438349\n",
      "Iteration 100, loss = 1.14869337\n",
      "Iteration 101, loss = 1.14014828\n",
      "Iteration 102, loss = 1.14467477\n",
      "Iteration 103, loss = 1.14821763\n",
      "Iteration 104, loss = 1.14355757\n",
      "Iteration 105, loss = 1.13651147\n",
      "Iteration 106, loss = 1.14673037\n",
      "Iteration 107, loss = 1.14050259\n",
      "Iteration 108, loss = 1.14057732\n",
      "Iteration 109, loss = 1.14084835\n",
      "Iteration 110, loss = 1.14279384\n",
      "Iteration 111, loss = 1.13859664\n",
      "Iteration 112, loss = 1.13647312\n",
      "Iteration 113, loss = 1.13641402\n",
      "Iteration 114, loss = 1.13555463\n",
      "Iteration 115, loss = 1.13912158\n",
      "Iteration 116, loss = 1.13489271\n",
      "Iteration 117, loss = 1.13424368\n",
      "Iteration 118, loss = 1.13268053\n",
      "Iteration 119, loss = 1.15028209\n",
      "Iteration 120, loss = 1.14300595\n",
      "Iteration 121, loss = 1.13341600\n",
      "Iteration 122, loss = 1.16596966\n",
      "Iteration 123, loss = 1.13279313\n",
      "Iteration 124, loss = 1.13412128\n",
      "Iteration 125, loss = 1.13323597\n",
      "Iteration 126, loss = 1.13203697\n",
      "Iteration 127, loss = 1.13537570\n",
      "Iteration 128, loss = 1.13647230\n",
      "Iteration 129, loss = 1.14737460\n",
      "Iteration 130, loss = 1.13565482\n",
      "Iteration 131, loss = 1.13194219\n",
      "Iteration 132, loss = 1.12928039\n",
      "Iteration 133, loss = 1.13597764\n",
      "Iteration 134, loss = 1.13211327\n",
      "Iteration 135, loss = 1.13352590\n",
      "Iteration 136, loss = 1.13538291\n",
      "Iteration 137, loss = 1.13448870\n",
      "Iteration 138, loss = 1.13031066\n",
      "Iteration 139, loss = 1.13292213\n",
      "Iteration 140, loss = 1.13422375\n",
      "Iteration 141, loss = 1.13072076\n",
      "Iteration 142, loss = 1.12852166\n",
      "Iteration 143, loss = 1.13219340\n",
      "Iteration 144, loss = 1.12611419\n",
      "Iteration 145, loss = 1.12797265\n",
      "Iteration 146, loss = 1.13493032\n",
      "Iteration 147, loss = 1.12192977\n",
      "Iteration 148, loss = 1.13056166\n",
      "Iteration 149, loss = 1.13446110\n",
      "Iteration 150, loss = 1.12587785\n",
      "Iteration 151, loss = 1.12874516\n",
      "Iteration 152, loss = 1.12126052\n",
      "Iteration 153, loss = 1.12198104\n",
      "Iteration 154, loss = 1.13442761\n",
      "Iteration 155, loss = 1.11891684\n",
      "Iteration 156, loss = 1.12632371\n",
      "Iteration 157, loss = 1.12441714\n",
      "Iteration 158, loss = 1.12392986\n",
      "Iteration 159, loss = 1.13364534\n",
      "Iteration 160, loss = 1.12302548\n",
      "Iteration 161, loss = 1.14166288\n",
      "Iteration 162, loss = 1.12496978\n",
      "Iteration 163, loss = 1.11927060\n",
      "Iteration 164, loss = 1.12420261\n",
      "Iteration 165, loss = 1.12924339\n",
      "Iteration 166, loss = 1.12228884\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49877010\n",
      "Iteration 2, loss = 1.42394318\n",
      "Iteration 3, loss = 1.34119410\n",
      "Iteration 4, loss = 1.28498078\n",
      "Iteration 5, loss = 1.25153773\n",
      "Iteration 6, loss = 1.23571073\n",
      "Iteration 7, loss = 1.22350347\n",
      "Iteration 8, loss = 1.21574579\n",
      "Iteration 9, loss = 1.21003156\n",
      "Iteration 10, loss = 1.20583339\n",
      "Iteration 11, loss = 1.20249108\n",
      "Iteration 12, loss = 1.19921498\n",
      "Iteration 13, loss = 1.19690082\n",
      "Iteration 14, loss = 1.19451029\n",
      "Iteration 15, loss = 1.19209021\n",
      "Iteration 16, loss = 1.19005400\n",
      "Iteration 17, loss = 1.18837096\n",
      "Iteration 18, loss = 1.18617293\n",
      "Iteration 19, loss = 1.18418046\n",
      "Iteration 20, loss = 1.18247231\n",
      "Iteration 21, loss = 1.18042635\n",
      "Iteration 22, loss = 1.17874462\n",
      "Iteration 23, loss = 1.17650435\n",
      "Iteration 24, loss = 1.17481981\n",
      "Iteration 25, loss = 1.17290113\n",
      "Iteration 26, loss = 1.17084780\n",
      "Iteration 27, loss = 1.16937153\n",
      "Iteration 28, loss = 1.16712519\n",
      "Iteration 29, loss = 1.16525700\n",
      "Iteration 30, loss = 1.16363399\n",
      "Iteration 31, loss = 1.16155187\n",
      "Iteration 32, loss = 1.15987037\n",
      "Iteration 33, loss = 1.15808800\n",
      "Iteration 34, loss = 1.15647633\n",
      "Iteration 35, loss = 1.15457149\n",
      "Iteration 36, loss = 1.15286976\n",
      "Iteration 37, loss = 1.15120144\n",
      "Iteration 38, loss = 1.14946293\n",
      "Iteration 39, loss = 1.14791988\n",
      "Iteration 40, loss = 1.14630636\n",
      "Iteration 41, loss = 1.14495326\n",
      "Iteration 42, loss = 1.14342064\n",
      "Iteration 43, loss = 1.14144499\n",
      "Iteration 44, loss = 1.14041578\n",
      "Iteration 45, loss = 1.13856614\n",
      "Iteration 46, loss = 1.13712823\n",
      "Iteration 47, loss = 1.13569953\n",
      "Iteration 48, loss = 1.13420544\n",
      "Iteration 49, loss = 1.13275052\n",
      "Iteration 50, loss = 1.13146173\n",
      "Iteration 51, loss = 1.13019700\n",
      "Iteration 52, loss = 1.12900778\n",
      "Iteration 53, loss = 1.12740129\n",
      "Iteration 54, loss = 1.12602723\n",
      "Iteration 55, loss = 1.12470829\n",
      "Iteration 56, loss = 1.12357860\n",
      "Iteration 57, loss = 1.12233616\n",
      "Iteration 58, loss = 1.12101670\n",
      "Iteration 59, loss = 1.11992827\n",
      "Iteration 60, loss = 1.11853886\n",
      "Iteration 61, loss = 1.11741575\n",
      "Iteration 62, loss = 1.11622408\n",
      "Iteration 63, loss = 1.11506399\n",
      "Iteration 64, loss = 1.11393119\n",
      "Iteration 65, loss = 1.11301474\n",
      "Iteration 66, loss = 1.11185256\n",
      "Iteration 67, loss = 1.11098063\n",
      "Iteration 68, loss = 1.10982537\n",
      "Iteration 69, loss = 1.10861399\n",
      "Iteration 70, loss = 1.10774232\n",
      "Iteration 71, loss = 1.10668960\n",
      "Iteration 72, loss = 1.10597830\n",
      "Iteration 73, loss = 1.10491288\n",
      "Iteration 74, loss = 1.10394688\n",
      "Iteration 75, loss = 1.10303745\n",
      "Iteration 76, loss = 1.10209595\n",
      "Iteration 77, loss = 1.10122015\n",
      "Iteration 78, loss = 1.10025264\n",
      "Iteration 79, loss = 1.09944313\n",
      "Iteration 80, loss = 1.09883685\n",
      "Iteration 81, loss = 1.09790346\n",
      "Iteration 82, loss = 1.09690954\n",
      "Iteration 83, loss = 1.09638237\n",
      "Iteration 84, loss = 1.09529422\n",
      "Iteration 85, loss = 1.09457674\n",
      "Iteration 86, loss = 1.09373414\n",
      "Iteration 87, loss = 1.09301784\n",
      "Iteration 88, loss = 1.09227291\n",
      "Iteration 89, loss = 1.09140879\n",
      "Iteration 90, loss = 1.09069689\n",
      "Iteration 91, loss = 1.08995829\n",
      "Iteration 92, loss = 1.08931233\n",
      "Iteration 93, loss = 1.08867704\n",
      "Iteration 94, loss = 1.08800688\n",
      "Iteration 95, loss = 1.08703739\n",
      "Iteration 96, loss = 1.08650581\n",
      "Iteration 97, loss = 1.08653622\n",
      "Iteration 98, loss = 1.08568888\n",
      "Iteration 99, loss = 1.08476333\n",
      "Iteration 100, loss = 1.08399303\n",
      "Iteration 1, loss = 1.49908819\n",
      "Iteration 2, loss = 1.42332086\n",
      "Iteration 3, loss = 1.34163044\n",
      "Iteration 4, loss = 1.28521406\n",
      "Iteration 5, loss = 1.25234688\n",
      "Iteration 6, loss = 1.23565166\n",
      "Iteration 7, loss = 1.22404055\n",
      "Iteration 8, loss = 1.21572726\n",
      "Iteration 9, loss = 1.21006083\n",
      "Iteration 10, loss = 1.20576524\n",
      "Iteration 11, loss = 1.20195414\n",
      "Iteration 12, loss = 1.19866271\n",
      "Iteration 13, loss = 1.19633471\n",
      "Iteration 14, loss = 1.19346505\n",
      "Iteration 15, loss = 1.19120732\n",
      "Iteration 16, loss = 1.18935925\n",
      "Iteration 17, loss = 1.18740936\n",
      "Iteration 18, loss = 1.18517371\n",
      "Iteration 19, loss = 1.18331336\n",
      "Iteration 20, loss = 1.18127180\n",
      "Iteration 21, loss = 1.17917906\n",
      "Iteration 22, loss = 1.17733991\n",
      "Iteration 23, loss = 1.17506201\n",
      "Iteration 24, loss = 1.17326842\n",
      "Iteration 25, loss = 1.17127106\n",
      "Iteration 26, loss = 1.16920098\n",
      "Iteration 27, loss = 1.16790199\n",
      "Iteration 28, loss = 1.16533732\n",
      "Iteration 29, loss = 1.16328821\n",
      "Iteration 30, loss = 1.16176242\n",
      "Iteration 31, loss = 1.15942983\n",
      "Iteration 32, loss = 1.15764938\n",
      "Iteration 33, loss = 1.15568411\n",
      "Iteration 34, loss = 1.15383611\n",
      "Iteration 35, loss = 1.15201811\n",
      "Iteration 36, loss = 1.15017699\n",
      "Iteration 37, loss = 1.14837700\n",
      "Iteration 38, loss = 1.14649081\n",
      "Iteration 39, loss = 1.14474955\n",
      "Iteration 40, loss = 1.14292172\n",
      "Iteration 41, loss = 1.14139153\n",
      "Iteration 42, loss = 1.13958302\n",
      "Iteration 43, loss = 1.13788771\n",
      "Iteration 44, loss = 1.13625339\n",
      "Iteration 45, loss = 1.13445816\n",
      "Iteration 46, loss = 1.13272506\n",
      "Iteration 47, loss = 1.13136890\n",
      "Iteration 48, loss = 1.12977353\n",
      "Iteration 49, loss = 1.12822840\n",
      "Iteration 50, loss = 1.12659356\n",
      "Iteration 51, loss = 1.12504655\n",
      "Iteration 52, loss = 1.12386442\n",
      "Iteration 53, loss = 1.12204734\n",
      "Iteration 54, loss = 1.12069872\n",
      "Iteration 55, loss = 1.11933247\n",
      "Iteration 56, loss = 1.11798130\n",
      "Iteration 57, loss = 1.11657911\n",
      "Iteration 58, loss = 1.11517779\n",
      "Iteration 59, loss = 1.11385272\n",
      "Iteration 60, loss = 1.11247553\n",
      "Iteration 61, loss = 1.11144913\n",
      "Iteration 62, loss = 1.11008281\n",
      "Iteration 63, loss = 1.10869610\n",
      "Iteration 64, loss = 1.10740776\n",
      "Iteration 65, loss = 1.10651976\n",
      "Iteration 66, loss = 1.10521962\n",
      "Iteration 67, loss = 1.10404739\n",
      "Iteration 68, loss = 1.10291673\n",
      "Iteration 69, loss = 1.10180466\n",
      "Iteration 70, loss = 1.10050414\n",
      "Iteration 71, loss = 1.09940301\n",
      "Iteration 72, loss = 1.09860258\n",
      "Iteration 73, loss = 1.09749046\n",
      "Iteration 74, loss = 1.09640875\n",
      "Iteration 75, loss = 1.09547987\n",
      "Iteration 76, loss = 1.09461095\n",
      "Iteration 77, loss = 1.09362025\n",
      "Iteration 78, loss = 1.09269018\n",
      "Iteration 79, loss = 1.09168983\n",
      "Iteration 80, loss = 1.09102697\n",
      "Iteration 81, loss = 1.08971929\n",
      "Iteration 82, loss = 1.08875089\n",
      "Iteration 83, loss = 1.08811939\n",
      "Iteration 84, loss = 1.08698501\n",
      "Iteration 85, loss = 1.08635557\n",
      "Iteration 86, loss = 1.08546708\n",
      "Iteration 87, loss = 1.08477211\n",
      "Iteration 88, loss = 1.08378598\n",
      "Iteration 89, loss = 1.08322349\n",
      "Iteration 90, loss = 1.08221743\n",
      "Iteration 91, loss = 1.08146266\n",
      "Iteration 92, loss = 1.08064838\n",
      "Iteration 93, loss = 1.08042361\n",
      "Iteration 94, loss = 1.07929674\n",
      "Iteration 95, loss = 1.07850579\n",
      "Iteration 96, loss = 1.07780113\n",
      "Iteration 97, loss = 1.07732816\n",
      "Iteration 98, loss = 1.07660085\n",
      "Iteration 99, loss = 1.07606939\n",
      "Iteration 100, loss = 1.07509805\n",
      "Iteration 1, loss = 1.49923727\n",
      "Iteration 2, loss = 1.42299377\n",
      "Iteration 3, loss = 1.34202567\n",
      "Iteration 4, loss = 1.28679915\n",
      "Iteration 5, loss = 1.25108990\n",
      "Iteration 6, loss = 1.23405464\n",
      "Iteration 7, loss = 1.22292696\n",
      "Iteration 8, loss = 1.21394865\n",
      "Iteration 9, loss = 1.20861756\n",
      "Iteration 10, loss = 1.20388773\n",
      "Iteration 11, loss = 1.19979396\n",
      "Iteration 12, loss = 1.19644209\n",
      "Iteration 13, loss = 1.19370771\n",
      "Iteration 14, loss = 1.19105836\n",
      "Iteration 15, loss = 1.18864421\n",
      "Iteration 16, loss = 1.18655215\n",
      "Iteration 17, loss = 1.18448150\n",
      "Iteration 18, loss = 1.18199345\n",
      "Iteration 19, loss = 1.17992997\n",
      "Iteration 20, loss = 1.17771381\n",
      "Iteration 21, loss = 1.17559532\n",
      "Iteration 22, loss = 1.17343029\n",
      "Iteration 23, loss = 1.17136724\n",
      "Iteration 24, loss = 1.16946825\n",
      "Iteration 25, loss = 1.16757638\n",
      "Iteration 26, loss = 1.16512556\n",
      "Iteration 27, loss = 1.16332170\n",
      "Iteration 28, loss = 1.16132321\n",
      "Iteration 29, loss = 1.15908280\n",
      "Iteration 30, loss = 1.15698671\n",
      "Iteration 31, loss = 1.15496340\n",
      "Iteration 32, loss = 1.15332187\n",
      "Iteration 33, loss = 1.15106046\n",
      "Iteration 34, loss = 1.14924663\n",
      "Iteration 35, loss = 1.14745642\n",
      "Iteration 36, loss = 1.14561081\n",
      "Iteration 37, loss = 1.14370721\n",
      "Iteration 38, loss = 1.14175784\n",
      "Iteration 39, loss = 1.14000123\n",
      "Iteration 40, loss = 1.13828863\n",
      "Iteration 41, loss = 1.13665436\n",
      "Iteration 42, loss = 1.13510325\n",
      "Iteration 43, loss = 1.13320068\n",
      "Iteration 44, loss = 1.13152034\n",
      "Iteration 45, loss = 1.12987894\n",
      "Iteration 46, loss = 1.12827999\n",
      "Iteration 47, loss = 1.12671934\n",
      "Iteration 48, loss = 1.12528830\n",
      "Iteration 49, loss = 1.12394611\n",
      "Iteration 50, loss = 1.12244943\n",
      "Iteration 51, loss = 1.12083801\n",
      "Iteration 52, loss = 1.11982670\n",
      "Iteration 53, loss = 1.11828239\n",
      "Iteration 54, loss = 1.11695346\n",
      "Iteration 55, loss = 1.11566887\n",
      "Iteration 56, loss = 1.11432172\n",
      "Iteration 57, loss = 1.11314367\n",
      "Iteration 58, loss = 1.11196685\n",
      "Iteration 59, loss = 1.11085319\n",
      "Iteration 60, loss = 1.10977080\n",
      "Iteration 61, loss = 1.10889616\n",
      "Iteration 62, loss = 1.10726085\n",
      "Iteration 63, loss = 1.10656140\n",
      "Iteration 64, loss = 1.10529195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65, loss = 1.10418169\n",
      "Iteration 66, loss = 1.10296441\n",
      "Iteration 67, loss = 1.10199996\n",
      "Iteration 68, loss = 1.10130565\n",
      "Iteration 69, loss = 1.10003285\n",
      "Iteration 70, loss = 1.09912893\n",
      "Iteration 71, loss = 1.09833414\n",
      "Iteration 72, loss = 1.09731422\n",
      "Iteration 73, loss = 1.09636466\n",
      "Iteration 74, loss = 1.09553965\n",
      "Iteration 75, loss = 1.09495089\n",
      "Iteration 76, loss = 1.09385803\n",
      "Iteration 77, loss = 1.09306523\n",
      "Iteration 78, loss = 1.09217621\n",
      "Iteration 79, loss = 1.09164752\n",
      "Iteration 80, loss = 1.09065440\n",
      "Iteration 81, loss = 1.08976123\n",
      "Iteration 82, loss = 1.08915268\n",
      "Iteration 83, loss = 1.08829057\n",
      "Iteration 84, loss = 1.08780120\n",
      "Iteration 85, loss = 1.08681043\n",
      "Iteration 86, loss = 1.08616233\n",
      "Iteration 87, loss = 1.08582023\n",
      "Iteration 88, loss = 1.08473916\n",
      "Iteration 89, loss = 1.08410801\n",
      "Iteration 90, loss = 1.08349611\n",
      "Iteration 91, loss = 1.08280474\n",
      "Iteration 92, loss = 1.08215946\n",
      "Iteration 93, loss = 1.08178509\n",
      "Iteration 94, loss = 1.08092636\n",
      "Iteration 95, loss = 1.08024795\n",
      "Iteration 96, loss = 1.07989735\n",
      "Iteration 97, loss = 1.07923266\n",
      "Iteration 98, loss = 1.07856904\n",
      "Iteration 99, loss = 1.07790168\n",
      "Iteration 100, loss = 1.07765122\n",
      "Iteration 1, loss = 1.49943611\n",
      "Iteration 2, loss = 1.42457960\n",
      "Iteration 3, loss = 1.34336208\n",
      "Iteration 4, loss = 1.28753208\n",
      "Iteration 5, loss = 1.25371366\n",
      "Iteration 6, loss = 1.23774789\n",
      "Iteration 7, loss = 1.22801699\n",
      "Iteration 8, loss = 1.21912371\n",
      "Iteration 9, loss = 1.21345391\n",
      "Iteration 10, loss = 1.20939172\n",
      "Iteration 11, loss = 1.20575896\n",
      "Iteration 12, loss = 1.20312205\n",
      "Iteration 13, loss = 1.20092866\n",
      "Iteration 14, loss = 1.19832460\n",
      "Iteration 15, loss = 1.19649262\n",
      "Iteration 16, loss = 1.19450072\n",
      "Iteration 17, loss = 1.19288875\n",
      "Iteration 18, loss = 1.19090354\n",
      "Iteration 19, loss = 1.18925464\n",
      "Iteration 20, loss = 1.18736623\n",
      "Iteration 21, loss = 1.18561718\n",
      "Iteration 22, loss = 1.18402260\n",
      "Iteration 23, loss = 1.18198905\n",
      "Iteration 24, loss = 1.18044697\n",
      "Iteration 25, loss = 1.17858017\n",
      "Iteration 26, loss = 1.17678144\n",
      "Iteration 27, loss = 1.17513278\n",
      "Iteration 28, loss = 1.17357532\n",
      "Iteration 29, loss = 1.17174587\n",
      "Iteration 30, loss = 1.17004257\n",
      "Iteration 31, loss = 1.16843293\n",
      "Iteration 32, loss = 1.16724621\n",
      "Iteration 33, loss = 1.16522135\n",
      "Iteration 34, loss = 1.16363177\n",
      "Iteration 35, loss = 1.16205485\n",
      "Iteration 36, loss = 1.16063158\n",
      "Iteration 37, loss = 1.15882263\n",
      "Iteration 38, loss = 1.15725388\n",
      "Iteration 39, loss = 1.15603247\n",
      "Iteration 40, loss = 1.15423614\n",
      "Iteration 41, loss = 1.15260265\n",
      "Iteration 42, loss = 1.15117961\n",
      "Iteration 43, loss = 1.14967494\n",
      "Iteration 44, loss = 1.14817895\n",
      "Iteration 45, loss = 1.14672477\n",
      "Iteration 46, loss = 1.14552394\n",
      "Iteration 47, loss = 1.14396942\n",
      "Iteration 48, loss = 1.14251539\n",
      "Iteration 49, loss = 1.14138583\n",
      "Iteration 50, loss = 1.13982252\n",
      "Iteration 51, loss = 1.13849011\n",
      "Iteration 52, loss = 1.13717931\n",
      "Iteration 53, loss = 1.13590067\n",
      "Iteration 54, loss = 1.13471115\n",
      "Iteration 55, loss = 1.13362208\n",
      "Iteration 56, loss = 1.13210712\n",
      "Iteration 57, loss = 1.13094481\n",
      "Iteration 58, loss = 1.12976965\n",
      "Iteration 59, loss = 1.12883168\n",
      "Iteration 60, loss = 1.12768493\n",
      "Iteration 61, loss = 1.12652316\n",
      "Iteration 62, loss = 1.12539036\n",
      "Iteration 63, loss = 1.12452339\n",
      "Iteration 64, loss = 1.12304162\n",
      "Iteration 65, loss = 1.12202532\n",
      "Iteration 66, loss = 1.12099639\n",
      "Iteration 67, loss = 1.11994606\n",
      "Iteration 68, loss = 1.11908530\n",
      "Iteration 69, loss = 1.11816273\n",
      "Iteration 70, loss = 1.11707542\n",
      "Iteration 71, loss = 1.11615279\n",
      "Iteration 72, loss = 1.11501043\n",
      "Iteration 73, loss = 1.11408118\n",
      "Iteration 74, loss = 1.11340005\n",
      "Iteration 75, loss = 1.11252678\n",
      "Iteration 76, loss = 1.11140372\n",
      "Iteration 77, loss = 1.11072695\n",
      "Iteration 78, loss = 1.10981419\n",
      "Iteration 79, loss = 1.10900917\n",
      "Iteration 80, loss = 1.10808753\n",
      "Iteration 81, loss = 1.10722416\n",
      "Iteration 82, loss = 1.10650826\n",
      "Iteration 83, loss = 1.10573628\n",
      "Iteration 84, loss = 1.10489628\n",
      "Iteration 85, loss = 1.10428797\n",
      "Iteration 86, loss = 1.10359373\n",
      "Iteration 87, loss = 1.10293248\n",
      "Iteration 88, loss = 1.10183855\n",
      "Iteration 89, loss = 1.10155432\n",
      "Iteration 90, loss = 1.10054645\n",
      "Iteration 91, loss = 1.09993169\n",
      "Iteration 92, loss = 1.09926913\n",
      "Iteration 93, loss = 1.09868532\n",
      "Iteration 94, loss = 1.09794506\n",
      "Iteration 95, loss = 1.09725257\n",
      "Iteration 96, loss = 1.09661535\n",
      "Iteration 97, loss = 1.09628142\n",
      "Iteration 98, loss = 1.09535977\n",
      "Iteration 99, loss = 1.09473611\n",
      "Iteration 100, loss = 1.09406605\n",
      "Iteration 1, loss = 1.49807517\n",
      "Iteration 2, loss = 1.42189236\n",
      "Iteration 3, loss = 1.34026881\n",
      "Iteration 4, loss = 1.28253382\n",
      "Iteration 5, loss = 1.24947911\n",
      "Iteration 6, loss = 1.23011747\n",
      "Iteration 7, loss = 1.22057111\n",
      "Iteration 8, loss = 1.21131264\n",
      "Iteration 9, loss = 1.20460531\n",
      "Iteration 10, loss = 1.20035572\n",
      "Iteration 11, loss = 1.19590278\n",
      "Iteration 12, loss = 1.19214745\n",
      "Iteration 13, loss = 1.18914157\n",
      "Iteration 14, loss = 1.18637494\n",
      "Iteration 15, loss = 1.18404079\n",
      "Iteration 16, loss = 1.18130405\n",
      "Iteration 17, loss = 1.17888284\n",
      "Iteration 18, loss = 1.17649243\n",
      "Iteration 19, loss = 1.17400746\n",
      "Iteration 20, loss = 1.17153964\n",
      "Iteration 21, loss = 1.16923138\n",
      "Iteration 22, loss = 1.16696224\n",
      "Iteration 23, loss = 1.16453613\n",
      "Iteration 24, loss = 1.16229642\n",
      "Iteration 25, loss = 1.15986241\n",
      "Iteration 26, loss = 1.15738219\n",
      "Iteration 27, loss = 1.15514071\n",
      "Iteration 28, loss = 1.15290489\n",
      "Iteration 29, loss = 1.15041634\n",
      "Iteration 30, loss = 1.14819239\n",
      "Iteration 31, loss = 1.14587743\n",
      "Iteration 32, loss = 1.14365922\n",
      "Iteration 33, loss = 1.14154958\n",
      "Iteration 34, loss = 1.13959070\n",
      "Iteration 35, loss = 1.13700201\n",
      "Iteration 36, loss = 1.13490860\n",
      "Iteration 37, loss = 1.13259729\n",
      "Iteration 38, loss = 1.13054747\n",
      "Iteration 39, loss = 1.12855966\n",
      "Iteration 40, loss = 1.12644935\n",
      "Iteration 41, loss = 1.12450660\n",
      "Iteration 42, loss = 1.12244593\n",
      "Iteration 43, loss = 1.12053680\n",
      "Iteration 44, loss = 1.11873013\n",
      "Iteration 45, loss = 1.11680025\n",
      "Iteration 46, loss = 1.11516635\n",
      "Iteration 47, loss = 1.11323901\n",
      "Iteration 48, loss = 1.11189668\n",
      "Iteration 49, loss = 1.10976950\n",
      "Iteration 50, loss = 1.10814222\n",
      "Iteration 51, loss = 1.10632653\n",
      "Iteration 52, loss = 1.10492890\n",
      "Iteration 53, loss = 1.10325056\n",
      "Iteration 54, loss = 1.10164049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55, loss = 1.10028564\n",
      "Iteration 56, loss = 1.09863323\n",
      "Iteration 57, loss = 1.09727083\n",
      "Iteration 58, loss = 1.09572172\n",
      "Iteration 59, loss = 1.09440372\n",
      "Iteration 60, loss = 1.09352364\n",
      "Iteration 61, loss = 1.09180537\n",
      "Iteration 62, loss = 1.09043338\n",
      "Iteration 63, loss = 1.08896739\n",
      "Iteration 64, loss = 1.08792410\n",
      "Iteration 65, loss = 1.08647909\n",
      "Iteration 66, loss = 1.08523542\n",
      "Iteration 67, loss = 1.08429361\n",
      "Iteration 68, loss = 1.08300171\n",
      "Iteration 69, loss = 1.08195146\n",
      "Iteration 70, loss = 1.08076961\n",
      "Iteration 71, loss = 1.07964249\n",
      "Iteration 72, loss = 1.07863600\n",
      "Iteration 73, loss = 1.07764179\n",
      "Iteration 74, loss = 1.07664894\n",
      "Iteration 75, loss = 1.07553062\n",
      "Iteration 76, loss = 1.07465482\n",
      "Iteration 77, loss = 1.07373274\n",
      "Iteration 78, loss = 1.07267282\n",
      "Iteration 79, loss = 1.07177092\n",
      "Iteration 80, loss = 1.07125957\n",
      "Iteration 81, loss = 1.07014525\n",
      "Iteration 82, loss = 1.06938552\n",
      "Iteration 83, loss = 1.06843274\n",
      "Iteration 84, loss = 1.06783804\n",
      "Iteration 85, loss = 1.06693387\n",
      "Iteration 86, loss = 1.06576176\n",
      "Iteration 87, loss = 1.06524972\n",
      "Iteration 88, loss = 1.06413055\n",
      "Iteration 89, loss = 1.06357361\n",
      "Iteration 90, loss = 1.06304314\n",
      "Iteration 91, loss = 1.06199438\n",
      "Iteration 92, loss = 1.06123150\n",
      "Iteration 93, loss = 1.06066506\n",
      "Iteration 94, loss = 1.05979770\n",
      "Iteration 95, loss = 1.05914475\n",
      "Iteration 96, loss = 1.05835927\n",
      "Iteration 97, loss = 1.05769492\n",
      "Iteration 98, loss = 1.05701908\n",
      "Iteration 99, loss = 1.05637307\n",
      "Iteration 100, loss = 1.05578366\n",
      "Iteration 1, loss = 11.14936199\n",
      "Iteration 2, loss = 1.50582905\n",
      "Iteration 3, loss = 1.27998454\n",
      "Iteration 4, loss = 1.25078880\n",
      "Iteration 5, loss = 1.24822459\n",
      "Iteration 6, loss = 1.24987482\n",
      "Iteration 7, loss = 1.25257571\n",
      "Iteration 8, loss = 1.25546561\n",
      "Iteration 9, loss = 1.24545958\n",
      "Iteration 10, loss = 1.25084855\n",
      "Iteration 11, loss = 1.25223439\n",
      "Iteration 12, loss = 1.24784159\n",
      "Iteration 13, loss = 1.25224773\n",
      "Iteration 14, loss = 1.24340789\n",
      "Iteration 15, loss = 1.24774694\n",
      "Iteration 16, loss = 1.24313022\n",
      "Iteration 17, loss = 1.24097861\n",
      "Iteration 18, loss = 1.24423703\n",
      "Iteration 19, loss = 1.24113315\n",
      "Iteration 20, loss = 1.24799952\n",
      "Iteration 21, loss = 1.24023395\n",
      "Iteration 22, loss = 1.24069864\n",
      "Iteration 23, loss = 1.23727807\n",
      "Iteration 24, loss = 1.23907916\n",
      "Iteration 25, loss = 1.23769386\n",
      "Iteration 26, loss = 1.24770867\n",
      "Iteration 27, loss = 1.23908269\n",
      "Iteration 28, loss = 1.23659692\n",
      "Iteration 29, loss = 1.23477750\n",
      "Iteration 30, loss = 1.24419515\n",
      "Iteration 31, loss = 1.26552632\n",
      "Iteration 32, loss = 1.23354135\n",
      "Iteration 33, loss = 1.23548321\n",
      "Iteration 34, loss = 1.23702292\n",
      "Iteration 35, loss = 1.23351557\n",
      "Iteration 36, loss = 1.23811509\n",
      "Iteration 37, loss = 1.23238570\n",
      "Iteration 38, loss = 1.23236190\n",
      "Iteration 39, loss = 1.23530774\n",
      "Iteration 40, loss = 1.23152532\n",
      "Iteration 41, loss = 1.24722650\n",
      "Iteration 42, loss = 1.23009184\n",
      "Iteration 43, loss = 1.23780052\n",
      "Iteration 44, loss = 1.23280336\n",
      "Iteration 45, loss = 1.23663597\n",
      "Iteration 46, loss = 1.23759432\n",
      "Iteration 47, loss = 1.23590851\n",
      "Iteration 48, loss = 1.22511542\n",
      "Iteration 49, loss = 1.22973226\n",
      "Iteration 50, loss = 1.22488350\n",
      "Iteration 51, loss = 1.22880051\n",
      "Iteration 52, loss = 1.23619943\n",
      "Iteration 53, loss = 1.25721589\n",
      "Iteration 54, loss = 1.23033706\n",
      "Iteration 55, loss = 1.24315972\n",
      "Iteration 56, loss = 1.22823840\n",
      "Iteration 57, loss = 1.23421545\n",
      "Iteration 58, loss = 1.23507971\n",
      "Iteration 59, loss = 1.23886090\n",
      "Iteration 60, loss = 1.23732968\n",
      "Iteration 61, loss = 1.22870140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49877010\n",
      "Iteration 2, loss = 1.42394318\n",
      "Iteration 3, loss = 1.34119410\n",
      "Iteration 4, loss = 1.28498078\n",
      "Iteration 5, loss = 1.25153773\n",
      "Iteration 6, loss = 1.23571073\n",
      "Iteration 7, loss = 1.22350347\n",
      "Iteration 8, loss = 1.21574579\n",
      "Iteration 9, loss = 1.21003156\n",
      "Iteration 10, loss = 1.20583339\n",
      "Iteration 11, loss = 1.20249108\n",
      "Iteration 12, loss = 1.19921498\n",
      "Iteration 13, loss = 1.19690082\n",
      "Iteration 14, loss = 1.19451029\n",
      "Iteration 15, loss = 1.19209021\n",
      "Iteration 16, loss = 1.19005400\n",
      "Iteration 17, loss = 1.18837096\n",
      "Iteration 18, loss = 1.18617293\n",
      "Iteration 19, loss = 1.18418046\n",
      "Iteration 20, loss = 1.18247231\n",
      "Iteration 21, loss = 1.18042635\n",
      "Iteration 22, loss = 1.17874462\n",
      "Iteration 23, loss = 1.17650435\n",
      "Iteration 24, loss = 1.17481981\n",
      "Iteration 25, loss = 1.17290113\n",
      "Iteration 26, loss = 1.17084780\n",
      "Iteration 27, loss = 1.16937153\n",
      "Iteration 28, loss = 1.16712519\n",
      "Iteration 29, loss = 1.16525700\n",
      "Iteration 30, loss = 1.16363399\n",
      "Iteration 31, loss = 1.16155187\n",
      "Iteration 32, loss = 1.15987037\n",
      "Iteration 33, loss = 1.15808800\n",
      "Iteration 34, loss = 1.15647633\n",
      "Iteration 35, loss = 1.15457149\n",
      "Iteration 36, loss = 1.15286976\n",
      "Iteration 37, loss = 1.15120144\n",
      "Iteration 38, loss = 1.14946293\n",
      "Iteration 39, loss = 1.14791988\n",
      "Iteration 40, loss = 1.14630636\n",
      "Iteration 41, loss = 1.14495326\n",
      "Iteration 42, loss = 1.14342064\n",
      "Iteration 43, loss = 1.14144499\n",
      "Iteration 44, loss = 1.14041578\n",
      "Iteration 45, loss = 1.13856614\n",
      "Iteration 46, loss = 1.13712823\n",
      "Iteration 47, loss = 1.13569953\n",
      "Iteration 48, loss = 1.13420544\n",
      "Iteration 49, loss = 1.13275052\n",
      "Iteration 50, loss = 1.13146173\n",
      "Iteration 51, loss = 1.13019700\n",
      "Iteration 52, loss = 1.12900778\n",
      "Iteration 53, loss = 1.12740129\n",
      "Iteration 54, loss = 1.12602723\n",
      "Iteration 55, loss = 1.12470829\n",
      "Iteration 56, loss = 1.12357860\n",
      "Iteration 57, loss = 1.12233616\n",
      "Iteration 58, loss = 1.12101670\n",
      "Iteration 59, loss = 1.11992827\n",
      "Iteration 60, loss = 1.11853886\n",
      "Iteration 61, loss = 1.11741575\n",
      "Iteration 62, loss = 1.11622408\n",
      "Iteration 63, loss = 1.11506399\n",
      "Iteration 64, loss = 1.11393119\n",
      "Iteration 65, loss = 1.11301474\n",
      "Iteration 66, loss = 1.11185256\n",
      "Iteration 67, loss = 1.11098063\n",
      "Iteration 68, loss = 1.10982537\n",
      "Iteration 69, loss = 1.10861399\n",
      "Iteration 70, loss = 1.10774232\n",
      "Iteration 71, loss = 1.10668960\n",
      "Iteration 72, loss = 1.10597830\n",
      "Iteration 73, loss = 1.10491288\n",
      "Iteration 74, loss = 1.10394688\n",
      "Iteration 75, loss = 1.10303745\n",
      "Iteration 76, loss = 1.10209595\n",
      "Iteration 77, loss = 1.10122015\n",
      "Iteration 78, loss = 1.10025264\n",
      "Iteration 79, loss = 1.09944313\n",
      "Iteration 80, loss = 1.09883685\n",
      "Iteration 81, loss = 1.09790346\n",
      "Iteration 82, loss = 1.09690954\n",
      "Iteration 83, loss = 1.09638237\n",
      "Iteration 84, loss = 1.09529422\n",
      "Iteration 85, loss = 1.09457674\n",
      "Iteration 86, loss = 1.09373414\n",
      "Iteration 87, loss = 1.09301784\n",
      "Iteration 88, loss = 1.09227291\n",
      "Iteration 89, loss = 1.09140879\n",
      "Iteration 90, loss = 1.09069689\n",
      "Iteration 91, loss = 1.08995829\n",
      "Iteration 92, loss = 1.08931233\n",
      "Iteration 93, loss = 1.08867704\n",
      "Iteration 94, loss = 1.08800688\n",
      "Iteration 95, loss = 1.08703739\n",
      "Iteration 96, loss = 1.08650581\n",
      "Iteration 97, loss = 1.08653622\n",
      "Iteration 98, loss = 1.08568888\n",
      "Iteration 99, loss = 1.08476333\n",
      "Iteration 100, loss = 1.08399303\n",
      "Iteration 101, loss = 1.08344057\n",
      "Iteration 102, loss = 1.08265686\n",
      "Iteration 103, loss = 1.08210377\n",
      "Iteration 104, loss = 1.08151378\n",
      "Iteration 105, loss = 1.08073670\n",
      "Iteration 106, loss = 1.08021981\n",
      "Iteration 107, loss = 1.07960830\n",
      "Iteration 108, loss = 1.07898663\n",
      "Iteration 109, loss = 1.07835263\n",
      "Iteration 110, loss = 1.07776582\n",
      "Iteration 111, loss = 1.07772294\n",
      "Iteration 112, loss = 1.07706524\n",
      "Iteration 113, loss = 1.07616911\n",
      "Iteration 114, loss = 1.07598677\n",
      "Iteration 115, loss = 1.07545796\n",
      "Iteration 116, loss = 1.07460220\n",
      "Iteration 117, loss = 1.07433354\n",
      "Iteration 118, loss = 1.07356399\n",
      "Iteration 119, loss = 1.07318146\n",
      "Iteration 120, loss = 1.07268971\n",
      "Iteration 121, loss = 1.07219349\n",
      "Iteration 122, loss = 1.07219180\n",
      "Iteration 123, loss = 1.07117702\n",
      "Iteration 124, loss = 1.07065959\n",
      "Iteration 125, loss = 1.07010745\n",
      "Iteration 126, loss = 1.06983060\n",
      "Iteration 127, loss = 1.06948716\n",
      "Iteration 128, loss = 1.06893749\n",
      "Iteration 129, loss = 1.06868046\n",
      "Iteration 130, loss = 1.06777238\n",
      "Iteration 131, loss = 1.06733571\n",
      "Iteration 132, loss = 1.06704931\n",
      "Iteration 133, loss = 1.06659103\n",
      "Iteration 134, loss = 1.06620499\n",
      "Iteration 135, loss = 1.06579363\n",
      "Iteration 136, loss = 1.06530422\n",
      "Iteration 137, loss = 1.06495858\n",
      "Iteration 138, loss = 1.06442278\n",
      "Iteration 139, loss = 1.06443890\n",
      "Iteration 140, loss = 1.06385421\n",
      "Iteration 141, loss = 1.06341915\n",
      "Iteration 142, loss = 1.06353180\n",
      "Iteration 143, loss = 1.06257647\n",
      "Iteration 144, loss = 1.06214198\n",
      "Iteration 145, loss = 1.06177092\n",
      "Iteration 146, loss = 1.06148763\n",
      "Iteration 147, loss = 1.06123225\n",
      "Iteration 148, loss = 1.06067453\n",
      "Iteration 149, loss = 1.06027519\n",
      "Iteration 150, loss = 1.06029783\n",
      "Iteration 151, loss = 1.05954151\n",
      "Iteration 152, loss = 1.05978251\n",
      "Iteration 153, loss = 1.05896441\n",
      "Iteration 154, loss = 1.05882242\n",
      "Iteration 155, loss = 1.05844338\n",
      "Iteration 156, loss = 1.05803132\n",
      "Iteration 157, loss = 1.05758672\n",
      "Iteration 158, loss = 1.05752468\n",
      "Iteration 159, loss = 1.05689535\n",
      "Iteration 160, loss = 1.05689604\n",
      "Iteration 161, loss = 1.05628963\n",
      "Iteration 162, loss = 1.05653448\n",
      "Iteration 163, loss = 1.05584419\n",
      "Iteration 164, loss = 1.05535169\n",
      "Iteration 165, loss = 1.05507367\n",
      "Iteration 166, loss = 1.05463439\n",
      "Iteration 167, loss = 1.05451866\n",
      "Iteration 168, loss = 1.05415555\n",
      "Iteration 169, loss = 1.05392565\n",
      "Iteration 170, loss = 1.05362984\n",
      "Iteration 171, loss = 1.05326905\n",
      "Iteration 172, loss = 1.05307949\n",
      "Iteration 173, loss = 1.05291733\n",
      "Iteration 174, loss = 1.05259919\n",
      "Iteration 175, loss = 1.05220067\n",
      "Iteration 176, loss = 1.05196177\n",
      "Iteration 177, loss = 1.05159595\n",
      "Iteration 178, loss = 1.05145925\n",
      "Iteration 179, loss = 1.05108838\n",
      "Iteration 180, loss = 1.05089472\n",
      "Iteration 181, loss = 1.05055844\n",
      "Iteration 182, loss = 1.05047530\n",
      "Iteration 183, loss = 1.05015490\n",
      "Iteration 184, loss = 1.05002922\n",
      "Iteration 185, loss = 1.04958919\n",
      "Iteration 186, loss = 1.04966571\n",
      "Iteration 187, loss = 1.04916446\n",
      "Iteration 188, loss = 1.04900671\n",
      "Iteration 189, loss = 1.04863194\n",
      "Iteration 190, loss = 1.04855778\n",
      "Iteration 191, loss = 1.04837363\n",
      "Iteration 192, loss = 1.04805515\n",
      "Iteration 193, loss = 1.04808644\n",
      "Iteration 194, loss = 1.04755786\n",
      "Iteration 195, loss = 1.04726901\n",
      "Iteration 196, loss = 1.04695477\n",
      "Iteration 197, loss = 1.04766166\n",
      "Iteration 198, loss = 1.04655542\n",
      "Iteration 199, loss = 1.04659171\n",
      "Iteration 200, loss = 1.04610991\n",
      "Iteration 1, loss = 1.49908819\n",
      "Iteration 2, loss = 1.42332086\n",
      "Iteration 3, loss = 1.34163044\n",
      "Iteration 4, loss = 1.28521406\n",
      "Iteration 5, loss = 1.25234688\n",
      "Iteration 6, loss = 1.23565166\n",
      "Iteration 7, loss = 1.22404055\n",
      "Iteration 8, loss = 1.21572726\n",
      "Iteration 9, loss = 1.21006083\n",
      "Iteration 10, loss = 1.20576524\n",
      "Iteration 11, loss = 1.20195414\n",
      "Iteration 12, loss = 1.19866271\n",
      "Iteration 13, loss = 1.19633471\n",
      "Iteration 14, loss = 1.19346505\n",
      "Iteration 15, loss = 1.19120732\n",
      "Iteration 16, loss = 1.18935925\n",
      "Iteration 17, loss = 1.18740936\n",
      "Iteration 18, loss = 1.18517371\n",
      "Iteration 19, loss = 1.18331336\n",
      "Iteration 20, loss = 1.18127180\n",
      "Iteration 21, loss = 1.17917906\n",
      "Iteration 22, loss = 1.17733991\n",
      "Iteration 23, loss = 1.17506201\n",
      "Iteration 24, loss = 1.17326842\n",
      "Iteration 25, loss = 1.17127106\n",
      "Iteration 26, loss = 1.16920098\n",
      "Iteration 27, loss = 1.16790199\n",
      "Iteration 28, loss = 1.16533732\n",
      "Iteration 29, loss = 1.16328821\n",
      "Iteration 30, loss = 1.16176242\n",
      "Iteration 31, loss = 1.15942983\n",
      "Iteration 32, loss = 1.15764938\n",
      "Iteration 33, loss = 1.15568411\n",
      "Iteration 34, loss = 1.15383611\n",
      "Iteration 35, loss = 1.15201811\n",
      "Iteration 36, loss = 1.15017699\n",
      "Iteration 37, loss = 1.14837700\n",
      "Iteration 38, loss = 1.14649081\n",
      "Iteration 39, loss = 1.14474955\n",
      "Iteration 40, loss = 1.14292172\n",
      "Iteration 41, loss = 1.14139153\n",
      "Iteration 42, loss = 1.13958302\n",
      "Iteration 43, loss = 1.13788771\n",
      "Iteration 44, loss = 1.13625339\n",
      "Iteration 45, loss = 1.13445816\n",
      "Iteration 46, loss = 1.13272506\n",
      "Iteration 47, loss = 1.13136890\n",
      "Iteration 48, loss = 1.12977353\n",
      "Iteration 49, loss = 1.12822840\n",
      "Iteration 50, loss = 1.12659356\n",
      "Iteration 51, loss = 1.12504655\n",
      "Iteration 52, loss = 1.12386442\n",
      "Iteration 53, loss = 1.12204734\n",
      "Iteration 54, loss = 1.12069872\n",
      "Iteration 55, loss = 1.11933247\n",
      "Iteration 56, loss = 1.11798130\n",
      "Iteration 57, loss = 1.11657911\n",
      "Iteration 58, loss = 1.11517779\n",
      "Iteration 59, loss = 1.11385272\n",
      "Iteration 60, loss = 1.11247553\n",
      "Iteration 61, loss = 1.11144913\n",
      "Iteration 62, loss = 1.11008281\n",
      "Iteration 63, loss = 1.10869610\n",
      "Iteration 64, loss = 1.10740776\n",
      "Iteration 65, loss = 1.10651976\n",
      "Iteration 66, loss = 1.10521962\n",
      "Iteration 67, loss = 1.10404739\n",
      "Iteration 68, loss = 1.10291673\n",
      "Iteration 69, loss = 1.10180466\n",
      "Iteration 70, loss = 1.10050414\n",
      "Iteration 71, loss = 1.09940301\n",
      "Iteration 72, loss = 1.09860258\n",
      "Iteration 73, loss = 1.09749046\n",
      "Iteration 74, loss = 1.09640875\n",
      "Iteration 75, loss = 1.09547987\n",
      "Iteration 76, loss = 1.09461095\n",
      "Iteration 77, loss = 1.09362025\n",
      "Iteration 78, loss = 1.09269018\n",
      "Iteration 79, loss = 1.09168983\n",
      "Iteration 80, loss = 1.09102697\n",
      "Iteration 81, loss = 1.08971929\n",
      "Iteration 82, loss = 1.08875089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 83, loss = 1.08811939\n",
      "Iteration 84, loss = 1.08698501\n",
      "Iteration 85, loss = 1.08635557\n",
      "Iteration 86, loss = 1.08546708\n",
      "Iteration 87, loss = 1.08477211\n",
      "Iteration 88, loss = 1.08378598\n",
      "Iteration 89, loss = 1.08322349\n",
      "Iteration 90, loss = 1.08221743\n",
      "Iteration 91, loss = 1.08146266\n",
      "Iteration 92, loss = 1.08064838\n",
      "Iteration 93, loss = 1.08042361\n",
      "Iteration 94, loss = 1.07929674\n",
      "Iteration 95, loss = 1.07850579\n",
      "Iteration 96, loss = 1.07780113\n",
      "Iteration 97, loss = 1.07732816\n",
      "Iteration 98, loss = 1.07660085\n",
      "Iteration 99, loss = 1.07606939\n",
      "Iteration 100, loss = 1.07509805\n",
      "Iteration 101, loss = 1.07436025\n",
      "Iteration 102, loss = 1.07399418\n",
      "Iteration 103, loss = 1.07319585\n",
      "Iteration 104, loss = 1.07244974\n",
      "Iteration 105, loss = 1.07195547\n",
      "Iteration 106, loss = 1.07141605\n",
      "Iteration 107, loss = 1.07086147\n",
      "Iteration 108, loss = 1.07004548\n",
      "Iteration 109, loss = 1.06951657\n",
      "Iteration 110, loss = 1.06931163\n",
      "Iteration 111, loss = 1.06853758\n",
      "Iteration 112, loss = 1.06797489\n",
      "Iteration 113, loss = 1.06717965\n",
      "Iteration 114, loss = 1.06684877\n",
      "Iteration 115, loss = 1.06666787\n",
      "Iteration 116, loss = 1.06559471\n",
      "Iteration 117, loss = 1.06500543\n",
      "Iteration 118, loss = 1.06447560\n",
      "Iteration 119, loss = 1.06457156\n",
      "Iteration 120, loss = 1.06393468\n",
      "Iteration 121, loss = 1.06344650\n",
      "Iteration 122, loss = 1.06264646\n",
      "Iteration 123, loss = 1.06221128\n",
      "Iteration 124, loss = 1.06155844\n",
      "Iteration 125, loss = 1.06116513\n",
      "Iteration 126, loss = 1.06068919\n",
      "Iteration 127, loss = 1.06034617\n",
      "Iteration 128, loss = 1.05989334\n",
      "Iteration 129, loss = 1.05940417\n",
      "Iteration 130, loss = 1.05905861\n",
      "Iteration 131, loss = 1.05848310\n",
      "Iteration 132, loss = 1.05829618\n",
      "Iteration 133, loss = 1.05766454\n",
      "Iteration 134, loss = 1.05732272\n",
      "Iteration 135, loss = 1.05688139\n",
      "Iteration 136, loss = 1.05655387\n",
      "Iteration 137, loss = 1.05607491\n",
      "Iteration 138, loss = 1.05587440\n",
      "Iteration 139, loss = 1.05560230\n",
      "Iteration 140, loss = 1.05525476\n",
      "Iteration 141, loss = 1.05521643\n",
      "Iteration 142, loss = 1.05489333\n",
      "Iteration 143, loss = 1.05407478\n",
      "Iteration 144, loss = 1.05347795\n",
      "Iteration 145, loss = 1.05307020\n",
      "Iteration 146, loss = 1.05268499\n",
      "Iteration 147, loss = 1.05235024\n",
      "Iteration 148, loss = 1.05199986\n",
      "Iteration 149, loss = 1.05169041\n",
      "Iteration 150, loss = 1.05141852\n",
      "Iteration 151, loss = 1.05144885\n",
      "Iteration 152, loss = 1.05071687\n",
      "Iteration 153, loss = 1.05047851\n",
      "Iteration 154, loss = 1.05039874\n",
      "Iteration 155, loss = 1.04999647\n",
      "Iteration 156, loss = 1.04960162\n",
      "Iteration 157, loss = 1.04917727\n",
      "Iteration 158, loss = 1.04898735\n",
      "Iteration 159, loss = 1.04862601\n",
      "Iteration 160, loss = 1.04832603\n",
      "Iteration 161, loss = 1.04802512\n",
      "Iteration 162, loss = 1.04780428\n",
      "Iteration 163, loss = 1.04745265\n",
      "Iteration 164, loss = 1.04728726\n",
      "Iteration 165, loss = 1.04691086\n",
      "Iteration 166, loss = 1.04664828\n",
      "Iteration 167, loss = 1.04665131\n",
      "Iteration 168, loss = 1.04614946\n",
      "Iteration 169, loss = 1.04616084\n",
      "Iteration 170, loss = 1.04536704\n",
      "Iteration 171, loss = 1.04535617\n",
      "Iteration 172, loss = 1.04500138\n",
      "Iteration 173, loss = 1.04500399\n",
      "Iteration 174, loss = 1.04450555\n",
      "Iteration 175, loss = 1.04421977\n",
      "Iteration 176, loss = 1.04396589\n",
      "Iteration 177, loss = 1.04379077\n",
      "Iteration 178, loss = 1.04341583\n",
      "Iteration 179, loss = 1.04329753\n",
      "Iteration 180, loss = 1.04338831\n",
      "Iteration 181, loss = 1.04275782\n",
      "Iteration 182, loss = 1.04259767\n",
      "Iteration 183, loss = 1.04234229\n",
      "Iteration 184, loss = 1.04232238\n",
      "Iteration 185, loss = 1.04201587\n",
      "Iteration 186, loss = 1.04163748\n",
      "Iteration 187, loss = 1.04155228\n",
      "Iteration 188, loss = 1.04126985\n",
      "Iteration 189, loss = 1.04087042\n",
      "Iteration 190, loss = 1.04081829\n",
      "Iteration 191, loss = 1.04064327\n",
      "Iteration 192, loss = 1.04032555\n",
      "Iteration 193, loss = 1.03991599\n",
      "Iteration 194, loss = 1.04007844\n",
      "Iteration 195, loss = 1.03963409\n",
      "Iteration 196, loss = 1.03942706\n",
      "Iteration 197, loss = 1.03927856\n",
      "Iteration 198, loss = 1.03905684\n",
      "Iteration 199, loss = 1.03887880\n",
      "Iteration 200, loss = 1.03852904\n",
      "Iteration 1, loss = 1.49923727\n",
      "Iteration 2, loss = 1.42299377\n",
      "Iteration 3, loss = 1.34202567\n",
      "Iteration 4, loss = 1.28679915\n",
      "Iteration 5, loss = 1.25108990\n",
      "Iteration 6, loss = 1.23405464\n",
      "Iteration 7, loss = 1.22292696\n",
      "Iteration 8, loss = 1.21394865\n",
      "Iteration 9, loss = 1.20861756\n",
      "Iteration 10, loss = 1.20388773\n",
      "Iteration 11, loss = 1.19979396\n",
      "Iteration 12, loss = 1.19644209\n",
      "Iteration 13, loss = 1.19370771\n",
      "Iteration 14, loss = 1.19105836\n",
      "Iteration 15, loss = 1.18864421\n",
      "Iteration 16, loss = 1.18655215\n",
      "Iteration 17, loss = 1.18448150\n",
      "Iteration 18, loss = 1.18199345\n",
      "Iteration 19, loss = 1.17992997\n",
      "Iteration 20, loss = 1.17771381\n",
      "Iteration 21, loss = 1.17559532\n",
      "Iteration 22, loss = 1.17343029\n",
      "Iteration 23, loss = 1.17136724\n",
      "Iteration 24, loss = 1.16946825\n",
      "Iteration 25, loss = 1.16757638\n",
      "Iteration 26, loss = 1.16512556\n",
      "Iteration 27, loss = 1.16332170\n",
      "Iteration 28, loss = 1.16132321\n",
      "Iteration 29, loss = 1.15908280\n",
      "Iteration 30, loss = 1.15698671\n",
      "Iteration 31, loss = 1.15496340\n",
      "Iteration 32, loss = 1.15332187\n",
      "Iteration 33, loss = 1.15106046\n",
      "Iteration 34, loss = 1.14924663\n",
      "Iteration 35, loss = 1.14745642\n",
      "Iteration 36, loss = 1.14561081\n",
      "Iteration 37, loss = 1.14370721\n",
      "Iteration 38, loss = 1.14175784\n",
      "Iteration 39, loss = 1.14000123\n",
      "Iteration 40, loss = 1.13828863\n",
      "Iteration 41, loss = 1.13665436\n",
      "Iteration 42, loss = 1.13510325\n",
      "Iteration 43, loss = 1.13320068\n",
      "Iteration 44, loss = 1.13152034\n",
      "Iteration 45, loss = 1.12987894\n",
      "Iteration 46, loss = 1.12827999\n",
      "Iteration 47, loss = 1.12671934\n",
      "Iteration 48, loss = 1.12528830\n",
      "Iteration 49, loss = 1.12394611\n",
      "Iteration 50, loss = 1.12244943\n",
      "Iteration 51, loss = 1.12083801\n",
      "Iteration 52, loss = 1.11982670\n",
      "Iteration 53, loss = 1.11828239\n",
      "Iteration 54, loss = 1.11695346\n",
      "Iteration 55, loss = 1.11566887\n",
      "Iteration 56, loss = 1.11432172\n",
      "Iteration 57, loss = 1.11314367\n",
      "Iteration 58, loss = 1.11196685\n",
      "Iteration 59, loss = 1.11085319\n",
      "Iteration 60, loss = 1.10977080\n",
      "Iteration 61, loss = 1.10889616\n",
      "Iteration 62, loss = 1.10726085\n",
      "Iteration 63, loss = 1.10656140\n",
      "Iteration 64, loss = 1.10529195\n",
      "Iteration 65, loss = 1.10418169\n",
      "Iteration 66, loss = 1.10296441\n",
      "Iteration 67, loss = 1.10199996\n",
      "Iteration 68, loss = 1.10130565\n",
      "Iteration 69, loss = 1.10003285\n",
      "Iteration 70, loss = 1.09912893\n",
      "Iteration 71, loss = 1.09833414\n",
      "Iteration 72, loss = 1.09731422\n",
      "Iteration 73, loss = 1.09636466\n",
      "Iteration 74, loss = 1.09553965\n",
      "Iteration 75, loss = 1.09495089\n",
      "Iteration 76, loss = 1.09385803\n",
      "Iteration 77, loss = 1.09306523\n",
      "Iteration 78, loss = 1.09217621\n",
      "Iteration 79, loss = 1.09164752\n",
      "Iteration 80, loss = 1.09065440\n",
      "Iteration 81, loss = 1.08976123\n",
      "Iteration 82, loss = 1.08915268\n",
      "Iteration 83, loss = 1.08829057\n",
      "Iteration 84, loss = 1.08780120\n",
      "Iteration 85, loss = 1.08681043\n",
      "Iteration 86, loss = 1.08616233\n",
      "Iteration 87, loss = 1.08582023\n",
      "Iteration 88, loss = 1.08473916\n",
      "Iteration 89, loss = 1.08410801\n",
      "Iteration 90, loss = 1.08349611\n",
      "Iteration 91, loss = 1.08280474\n",
      "Iteration 92, loss = 1.08215946\n",
      "Iteration 93, loss = 1.08178509\n",
      "Iteration 94, loss = 1.08092636\n",
      "Iteration 95, loss = 1.08024795\n",
      "Iteration 96, loss = 1.07989735\n",
      "Iteration 97, loss = 1.07923266\n",
      "Iteration 98, loss = 1.07856904\n",
      "Iteration 99, loss = 1.07790168\n",
      "Iteration 100, loss = 1.07765122\n",
      "Iteration 101, loss = 1.07699704\n",
      "Iteration 102, loss = 1.07639223\n",
      "Iteration 103, loss = 1.07572040\n",
      "Iteration 104, loss = 1.07554642\n",
      "Iteration 105, loss = 1.07479976\n",
      "Iteration 106, loss = 1.07402082\n",
      "Iteration 107, loss = 1.07375004\n",
      "Iteration 108, loss = 1.07329060\n",
      "Iteration 109, loss = 1.07272676\n",
      "Iteration 110, loss = 1.07231480\n",
      "Iteration 111, loss = 1.07155007\n",
      "Iteration 112, loss = 1.07115348\n",
      "Iteration 113, loss = 1.07075107\n",
      "Iteration 114, loss = 1.07012603\n",
      "Iteration 115, loss = 1.06975445\n",
      "Iteration 116, loss = 1.06945358\n",
      "Iteration 117, loss = 1.06873283\n",
      "Iteration 118, loss = 1.06860425\n",
      "Iteration 119, loss = 1.06820380\n",
      "Iteration 120, loss = 1.06753168\n",
      "Iteration 121, loss = 1.06723037\n",
      "Iteration 122, loss = 1.06661850\n",
      "Iteration 123, loss = 1.06633110\n",
      "Iteration 124, loss = 1.06618044\n",
      "Iteration 125, loss = 1.06588474\n",
      "Iteration 126, loss = 1.06510317\n",
      "Iteration 127, loss = 1.06494272\n",
      "Iteration 128, loss = 1.06432860\n",
      "Iteration 129, loss = 1.06371323\n",
      "Iteration 130, loss = 1.06381755\n",
      "Iteration 131, loss = 1.06302762\n",
      "Iteration 132, loss = 1.06273816\n",
      "Iteration 133, loss = 1.06242299\n",
      "Iteration 134, loss = 1.06197285\n",
      "Iteration 135, loss = 1.06162477\n",
      "Iteration 136, loss = 1.06138476\n",
      "Iteration 137, loss = 1.06113583\n",
      "Iteration 138, loss = 1.06096246\n",
      "Iteration 139, loss = 1.06041134\n",
      "Iteration 140, loss = 1.05998865\n",
      "Iteration 141, loss = 1.05977299\n",
      "Iteration 142, loss = 1.05966958\n",
      "Iteration 143, loss = 1.05907304\n",
      "Iteration 144, loss = 1.05866946\n",
      "Iteration 145, loss = 1.05834896\n",
      "Iteration 146, loss = 1.05808432\n",
      "Iteration 147, loss = 1.05770150\n",
      "Iteration 148, loss = 1.05737359\n",
      "Iteration 149, loss = 1.05729400\n",
      "Iteration 150, loss = 1.05666845\n",
      "Iteration 151, loss = 1.05675533\n",
      "Iteration 152, loss = 1.05629524\n",
      "Iteration 153, loss = 1.05593325\n",
      "Iteration 154, loss = 1.05555829\n",
      "Iteration 155, loss = 1.05528242\n",
      "Iteration 156, loss = 1.05497019\n",
      "Iteration 157, loss = 1.05483839\n",
      "Iteration 158, loss = 1.05450037\n",
      "Iteration 159, loss = 1.05412572\n",
      "Iteration 160, loss = 1.05387157\n",
      "Iteration 161, loss = 1.05363719\n",
      "Iteration 162, loss = 1.05341641\n",
      "Iteration 163, loss = 1.05325306\n",
      "Iteration 164, loss = 1.05299429\n",
      "Iteration 165, loss = 1.05253963\n",
      "Iteration 166, loss = 1.05237689\n",
      "Iteration 167, loss = 1.05218832\n",
      "Iteration 168, loss = 1.05181107\n",
      "Iteration 169, loss = 1.05153052\n",
      "Iteration 170, loss = 1.05150842\n",
      "Iteration 171, loss = 1.05110832\n",
      "Iteration 172, loss = 1.05094372\n",
      "Iteration 173, loss = 1.05078376\n",
      "Iteration 174, loss = 1.05033148\n",
      "Iteration 175, loss = 1.05009856\n",
      "Iteration 176, loss = 1.04988539\n",
      "Iteration 177, loss = 1.04964739\n",
      "Iteration 178, loss = 1.04953145\n",
      "Iteration 179, loss = 1.04954585\n",
      "Iteration 180, loss = 1.04958356\n",
      "Iteration 181, loss = 1.04902700\n",
      "Iteration 182, loss = 1.04887154\n",
      "Iteration 183, loss = 1.04868862\n",
      "Iteration 184, loss = 1.04824231\n",
      "Iteration 185, loss = 1.04798652\n",
      "Iteration 186, loss = 1.04766512\n",
      "Iteration 187, loss = 1.04775798\n",
      "Iteration 188, loss = 1.04744888\n",
      "Iteration 189, loss = 1.04705432\n",
      "Iteration 190, loss = 1.04700450\n",
      "Iteration 191, loss = 1.04691787\n",
      "Iteration 192, loss = 1.04634289\n",
      "Iteration 193, loss = 1.04623385\n",
      "Iteration 194, loss = 1.04629973\n",
      "Iteration 195, loss = 1.04649283\n",
      "Iteration 196, loss = 1.04581602\n",
      "Iteration 197, loss = 1.04546521\n",
      "Iteration 198, loss = 1.04525139\n",
      "Iteration 199, loss = 1.04504215\n",
      "Iteration 200, loss = 1.04510753\n",
      "Iteration 1, loss = 1.49943611\n",
      "Iteration 2, loss = 1.42457960\n",
      "Iteration 3, loss = 1.34336208\n",
      "Iteration 4, loss = 1.28753208\n",
      "Iteration 5, loss = 1.25371366\n",
      "Iteration 6, loss = 1.23774789\n",
      "Iteration 7, loss = 1.22801699\n",
      "Iteration 8, loss = 1.21912371\n",
      "Iteration 9, loss = 1.21345391\n",
      "Iteration 10, loss = 1.20939172\n",
      "Iteration 11, loss = 1.20575896\n",
      "Iteration 12, loss = 1.20312205\n",
      "Iteration 13, loss = 1.20092866\n",
      "Iteration 14, loss = 1.19832460\n",
      "Iteration 15, loss = 1.19649262\n",
      "Iteration 16, loss = 1.19450072\n",
      "Iteration 17, loss = 1.19288875\n",
      "Iteration 18, loss = 1.19090354\n",
      "Iteration 19, loss = 1.18925464\n",
      "Iteration 20, loss = 1.18736623\n",
      "Iteration 21, loss = 1.18561718\n",
      "Iteration 22, loss = 1.18402260\n",
      "Iteration 23, loss = 1.18198905\n",
      "Iteration 24, loss = 1.18044697\n",
      "Iteration 25, loss = 1.17858017\n",
      "Iteration 26, loss = 1.17678144\n",
      "Iteration 27, loss = 1.17513278\n",
      "Iteration 28, loss = 1.17357532\n",
      "Iteration 29, loss = 1.17174587\n",
      "Iteration 30, loss = 1.17004257\n",
      "Iteration 31, loss = 1.16843293\n",
      "Iteration 32, loss = 1.16724621\n",
      "Iteration 33, loss = 1.16522135\n",
      "Iteration 34, loss = 1.16363177\n",
      "Iteration 35, loss = 1.16205485\n",
      "Iteration 36, loss = 1.16063158\n",
      "Iteration 37, loss = 1.15882263\n",
      "Iteration 38, loss = 1.15725388\n",
      "Iteration 39, loss = 1.15603247\n",
      "Iteration 40, loss = 1.15423614\n",
      "Iteration 41, loss = 1.15260265\n",
      "Iteration 42, loss = 1.15117961\n",
      "Iteration 43, loss = 1.14967494\n",
      "Iteration 44, loss = 1.14817895\n",
      "Iteration 45, loss = 1.14672477\n",
      "Iteration 46, loss = 1.14552394\n",
      "Iteration 47, loss = 1.14396942\n",
      "Iteration 48, loss = 1.14251539\n",
      "Iteration 49, loss = 1.14138583\n",
      "Iteration 50, loss = 1.13982252\n",
      "Iteration 51, loss = 1.13849011\n",
      "Iteration 52, loss = 1.13717931\n",
      "Iteration 53, loss = 1.13590067\n",
      "Iteration 54, loss = 1.13471115\n",
      "Iteration 55, loss = 1.13362208\n",
      "Iteration 56, loss = 1.13210712\n",
      "Iteration 57, loss = 1.13094481\n",
      "Iteration 58, loss = 1.12976965\n",
      "Iteration 59, loss = 1.12883168\n",
      "Iteration 60, loss = 1.12768493\n",
      "Iteration 61, loss = 1.12652316\n",
      "Iteration 62, loss = 1.12539036\n",
      "Iteration 63, loss = 1.12452339\n",
      "Iteration 64, loss = 1.12304162\n",
      "Iteration 65, loss = 1.12202532\n",
      "Iteration 66, loss = 1.12099639\n",
      "Iteration 67, loss = 1.11994606\n",
      "Iteration 68, loss = 1.11908530\n",
      "Iteration 69, loss = 1.11816273\n",
      "Iteration 70, loss = 1.11707542\n",
      "Iteration 71, loss = 1.11615279\n",
      "Iteration 72, loss = 1.11501043\n",
      "Iteration 73, loss = 1.11408118\n",
      "Iteration 74, loss = 1.11340005\n",
      "Iteration 75, loss = 1.11252678\n",
      "Iteration 76, loss = 1.11140372\n",
      "Iteration 77, loss = 1.11072695\n",
      "Iteration 78, loss = 1.10981419\n",
      "Iteration 79, loss = 1.10900917\n",
      "Iteration 80, loss = 1.10808753\n",
      "Iteration 81, loss = 1.10722416\n",
      "Iteration 82, loss = 1.10650826\n",
      "Iteration 83, loss = 1.10573628\n",
      "Iteration 84, loss = 1.10489628\n",
      "Iteration 85, loss = 1.10428797\n",
      "Iteration 86, loss = 1.10359373\n",
      "Iteration 87, loss = 1.10293248\n",
      "Iteration 88, loss = 1.10183855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 89, loss = 1.10155432\n",
      "Iteration 90, loss = 1.10054645\n",
      "Iteration 91, loss = 1.09993169\n",
      "Iteration 92, loss = 1.09926913\n",
      "Iteration 93, loss = 1.09868532\n",
      "Iteration 94, loss = 1.09794506\n",
      "Iteration 95, loss = 1.09725257\n",
      "Iteration 96, loss = 1.09661535\n",
      "Iteration 97, loss = 1.09628142\n",
      "Iteration 98, loss = 1.09535977\n",
      "Iteration 99, loss = 1.09473611\n",
      "Iteration 100, loss = 1.09406605\n",
      "Iteration 101, loss = 1.09356483\n",
      "Iteration 102, loss = 1.09321538\n",
      "Iteration 103, loss = 1.09243863\n",
      "Iteration 104, loss = 1.09194545\n",
      "Iteration 105, loss = 1.09118424\n",
      "Iteration 106, loss = 1.09058185\n",
      "Iteration 107, loss = 1.09005877\n",
      "Iteration 108, loss = 1.08968295\n",
      "Iteration 109, loss = 1.08922379\n",
      "Iteration 110, loss = 1.08869055\n",
      "Iteration 111, loss = 1.08792558\n",
      "Iteration 112, loss = 1.08757396\n",
      "Iteration 113, loss = 1.08713979\n",
      "Iteration 114, loss = 1.08647504\n",
      "Iteration 115, loss = 1.08614719\n",
      "Iteration 116, loss = 1.08555477\n",
      "Iteration 117, loss = 1.08497027\n",
      "Iteration 118, loss = 1.08479061\n",
      "Iteration 119, loss = 1.08416595\n",
      "Iteration 120, loss = 1.08372732\n",
      "Iteration 121, loss = 1.08330277\n",
      "Iteration 122, loss = 1.08281889\n",
      "Iteration 123, loss = 1.08247960\n",
      "Iteration 124, loss = 1.08212960\n",
      "Iteration 125, loss = 1.08189937\n",
      "Iteration 126, loss = 1.08140670\n",
      "Iteration 127, loss = 1.08095222\n",
      "Iteration 128, loss = 1.08010862\n",
      "Iteration 129, loss = 1.07978300\n",
      "Iteration 130, loss = 1.07946667\n",
      "Iteration 131, loss = 1.07908674\n",
      "Iteration 132, loss = 1.07885278\n",
      "Iteration 133, loss = 1.07831234\n",
      "Iteration 134, loss = 1.07791530\n",
      "Iteration 135, loss = 1.07748841\n",
      "Iteration 136, loss = 1.07702511\n",
      "Iteration 137, loss = 1.07669996\n",
      "Iteration 138, loss = 1.07635783\n",
      "Iteration 139, loss = 1.07592187\n",
      "Iteration 140, loss = 1.07596711\n",
      "Iteration 141, loss = 1.07554537\n",
      "Iteration 142, loss = 1.07529802\n",
      "Iteration 143, loss = 1.07478281\n",
      "Iteration 144, loss = 1.07432057\n",
      "Iteration 145, loss = 1.07399312\n",
      "Iteration 146, loss = 1.07369672\n",
      "Iteration 147, loss = 1.07338641\n",
      "Iteration 148, loss = 1.07299610\n",
      "Iteration 149, loss = 1.07259116\n",
      "Iteration 150, loss = 1.07224273\n",
      "Iteration 151, loss = 1.07229031\n",
      "Iteration 152, loss = 1.07194865\n",
      "Iteration 153, loss = 1.07121870\n",
      "Iteration 154, loss = 1.07099776\n",
      "Iteration 155, loss = 1.07077948\n",
      "Iteration 156, loss = 1.07054682\n",
      "Iteration 157, loss = 1.07026259\n",
      "Iteration 158, loss = 1.06989198\n",
      "Iteration 159, loss = 1.06955632\n",
      "Iteration 160, loss = 1.06929865\n",
      "Iteration 161, loss = 1.06910477\n",
      "Iteration 162, loss = 1.06878813\n",
      "Iteration 163, loss = 1.06851587\n",
      "Iteration 164, loss = 1.06817569\n",
      "Iteration 165, loss = 1.06802464\n",
      "Iteration 166, loss = 1.06770460\n",
      "Iteration 167, loss = 1.06769632\n",
      "Iteration 168, loss = 1.06735298\n",
      "Iteration 169, loss = 1.06669309\n",
      "Iteration 170, loss = 1.06650619\n",
      "Iteration 171, loss = 1.06646415\n",
      "Iteration 172, loss = 1.06615328\n",
      "Iteration 173, loss = 1.06584059\n",
      "Iteration 174, loss = 1.06549840\n",
      "Iteration 175, loss = 1.06538232\n",
      "Iteration 176, loss = 1.06510018\n",
      "Iteration 177, loss = 1.06481560\n",
      "Iteration 178, loss = 1.06453050\n",
      "Iteration 179, loss = 1.06463013\n",
      "Iteration 180, loss = 1.06415756\n",
      "Iteration 181, loss = 1.06381768\n",
      "Iteration 182, loss = 1.06431304\n",
      "Iteration 183, loss = 1.06375666\n",
      "Iteration 184, loss = 1.06340026\n",
      "Iteration 185, loss = 1.06305771\n",
      "Iteration 186, loss = 1.06289469\n",
      "Iteration 187, loss = 1.06251541\n",
      "Iteration 188, loss = 1.06231271\n",
      "Iteration 189, loss = 1.06205791\n",
      "Iteration 190, loss = 1.06204239\n",
      "Iteration 191, loss = 1.06168831\n",
      "Iteration 192, loss = 1.06142895\n",
      "Iteration 193, loss = 1.06145134\n",
      "Iteration 194, loss = 1.06138965\n",
      "Iteration 195, loss = 1.06099818\n",
      "Iteration 196, loss = 1.06088923\n",
      "Iteration 197, loss = 1.06042222\n",
      "Iteration 198, loss = 1.06041560\n",
      "Iteration 199, loss = 1.06009043\n",
      "Iteration 200, loss = 1.06007547\n",
      "Iteration 1, loss = 1.49807517\n",
      "Iteration 2, loss = 1.42189236\n",
      "Iteration 3, loss = 1.34026881\n",
      "Iteration 4, loss = 1.28253382\n",
      "Iteration 5, loss = 1.24947911\n",
      "Iteration 6, loss = 1.23011747\n",
      "Iteration 7, loss = 1.22057111\n",
      "Iteration 8, loss = 1.21131264\n",
      "Iteration 9, loss = 1.20460531\n",
      "Iteration 10, loss = 1.20035572\n",
      "Iteration 11, loss = 1.19590278\n",
      "Iteration 12, loss = 1.19214745\n",
      "Iteration 13, loss = 1.18914157\n",
      "Iteration 14, loss = 1.18637494\n",
      "Iteration 15, loss = 1.18404079\n",
      "Iteration 16, loss = 1.18130405\n",
      "Iteration 17, loss = 1.17888284\n",
      "Iteration 18, loss = 1.17649243\n",
      "Iteration 19, loss = 1.17400746\n",
      "Iteration 20, loss = 1.17153964\n",
      "Iteration 21, loss = 1.16923138\n",
      "Iteration 22, loss = 1.16696224\n",
      "Iteration 23, loss = 1.16453613\n",
      "Iteration 24, loss = 1.16229642\n",
      "Iteration 25, loss = 1.15986241\n",
      "Iteration 26, loss = 1.15738219\n",
      "Iteration 27, loss = 1.15514071\n",
      "Iteration 28, loss = 1.15290489\n",
      "Iteration 29, loss = 1.15041634\n",
      "Iteration 30, loss = 1.14819239\n",
      "Iteration 31, loss = 1.14587743\n",
      "Iteration 32, loss = 1.14365922\n",
      "Iteration 33, loss = 1.14154958\n",
      "Iteration 34, loss = 1.13959070\n",
      "Iteration 35, loss = 1.13700201\n",
      "Iteration 36, loss = 1.13490860\n",
      "Iteration 37, loss = 1.13259729\n",
      "Iteration 38, loss = 1.13054747\n",
      "Iteration 39, loss = 1.12855966\n",
      "Iteration 40, loss = 1.12644935\n",
      "Iteration 41, loss = 1.12450660\n",
      "Iteration 42, loss = 1.12244593\n",
      "Iteration 43, loss = 1.12053680\n",
      "Iteration 44, loss = 1.11873013\n",
      "Iteration 45, loss = 1.11680025\n",
      "Iteration 46, loss = 1.11516635\n",
      "Iteration 47, loss = 1.11323901\n",
      "Iteration 48, loss = 1.11189668\n",
      "Iteration 49, loss = 1.10976950\n",
      "Iteration 50, loss = 1.10814222\n",
      "Iteration 51, loss = 1.10632653\n",
      "Iteration 52, loss = 1.10492890\n",
      "Iteration 53, loss = 1.10325056\n",
      "Iteration 54, loss = 1.10164049\n",
      "Iteration 55, loss = 1.10028564\n",
      "Iteration 56, loss = 1.09863323\n",
      "Iteration 57, loss = 1.09727083\n",
      "Iteration 58, loss = 1.09572172\n",
      "Iteration 59, loss = 1.09440372\n",
      "Iteration 60, loss = 1.09352364\n",
      "Iteration 61, loss = 1.09180537\n",
      "Iteration 62, loss = 1.09043338\n",
      "Iteration 63, loss = 1.08896739\n",
      "Iteration 64, loss = 1.08792410\n",
      "Iteration 65, loss = 1.08647909\n",
      "Iteration 66, loss = 1.08523542\n",
      "Iteration 67, loss = 1.08429361\n",
      "Iteration 68, loss = 1.08300171\n",
      "Iteration 69, loss = 1.08195146\n",
      "Iteration 70, loss = 1.08076961\n",
      "Iteration 71, loss = 1.07964249\n",
      "Iteration 72, loss = 1.07863600\n",
      "Iteration 73, loss = 1.07764179\n",
      "Iteration 74, loss = 1.07664894\n",
      "Iteration 75, loss = 1.07553062\n",
      "Iteration 76, loss = 1.07465482\n",
      "Iteration 77, loss = 1.07373274\n",
      "Iteration 78, loss = 1.07267282\n",
      "Iteration 79, loss = 1.07177092\n",
      "Iteration 80, loss = 1.07125957\n",
      "Iteration 81, loss = 1.07014525\n",
      "Iteration 82, loss = 1.06938552\n",
      "Iteration 83, loss = 1.06843274\n",
      "Iteration 84, loss = 1.06783804\n",
      "Iteration 85, loss = 1.06693387\n",
      "Iteration 86, loss = 1.06576176\n",
      "Iteration 87, loss = 1.06524972\n",
      "Iteration 88, loss = 1.06413055\n",
      "Iteration 89, loss = 1.06357361\n",
      "Iteration 90, loss = 1.06304314\n",
      "Iteration 91, loss = 1.06199438\n",
      "Iteration 92, loss = 1.06123150\n",
      "Iteration 93, loss = 1.06066506\n",
      "Iteration 94, loss = 1.05979770\n",
      "Iteration 95, loss = 1.05914475\n",
      "Iteration 96, loss = 1.05835927\n",
      "Iteration 97, loss = 1.05769492\n",
      "Iteration 98, loss = 1.05701908\n",
      "Iteration 99, loss = 1.05637307\n",
      "Iteration 100, loss = 1.05578366\n",
      "Iteration 101, loss = 1.05500249\n",
      "Iteration 102, loss = 1.05466340\n",
      "Iteration 103, loss = 1.05369853\n",
      "Iteration 104, loss = 1.05321081\n",
      "Iteration 105, loss = 1.05283878\n",
      "Iteration 106, loss = 1.05196186\n",
      "Iteration 107, loss = 1.05142639\n",
      "Iteration 108, loss = 1.05085543\n",
      "Iteration 109, loss = 1.05042896\n",
      "Iteration 110, loss = 1.04977683\n",
      "Iteration 111, loss = 1.04935082\n",
      "Iteration 112, loss = 1.04856065\n",
      "Iteration 113, loss = 1.04807730\n",
      "Iteration 114, loss = 1.04750950\n",
      "Iteration 115, loss = 1.04721838\n",
      "Iteration 116, loss = 1.04644860\n",
      "Iteration 117, loss = 1.04608059\n",
      "Iteration 118, loss = 1.04547481\n",
      "Iteration 119, loss = 1.04497842\n",
      "Iteration 120, loss = 1.04461814\n",
      "Iteration 121, loss = 1.04398921\n",
      "Iteration 122, loss = 1.04331669\n",
      "Iteration 123, loss = 1.04311572\n",
      "Iteration 124, loss = 1.04276254\n",
      "Iteration 125, loss = 1.04248435\n",
      "Iteration 126, loss = 1.04166792\n",
      "Iteration 127, loss = 1.04108000\n",
      "Iteration 128, loss = 1.04097684\n",
      "Iteration 129, loss = 1.04028625\n",
      "Iteration 130, loss = 1.03980091\n",
      "Iteration 131, loss = 1.03936536\n",
      "Iteration 132, loss = 1.03902790\n",
      "Iteration 133, loss = 1.03854171\n",
      "Iteration 134, loss = 1.03818365\n",
      "Iteration 135, loss = 1.03760000\n",
      "Iteration 136, loss = 1.03752303\n",
      "Iteration 137, loss = 1.03688994\n",
      "Iteration 138, loss = 1.03653145\n",
      "Iteration 139, loss = 1.03628151\n",
      "Iteration 140, loss = 1.03568310\n",
      "Iteration 141, loss = 1.03538044\n",
      "Iteration 142, loss = 1.03523716\n",
      "Iteration 143, loss = 1.03466415\n",
      "Iteration 144, loss = 1.03421997\n",
      "Iteration 145, loss = 1.03396777\n",
      "Iteration 146, loss = 1.03354286\n",
      "Iteration 147, loss = 1.03317117\n",
      "Iteration 148, loss = 1.03283634\n",
      "Iteration 149, loss = 1.03250731\n",
      "Iteration 150, loss = 1.03208558\n",
      "Iteration 151, loss = 1.03176953\n",
      "Iteration 152, loss = 1.03148183\n",
      "Iteration 153, loss = 1.03107278\n",
      "Iteration 154, loss = 1.03078986\n",
      "Iteration 155, loss = 1.03039405\n",
      "Iteration 156, loss = 1.03052369\n",
      "Iteration 157, loss = 1.02981182\n",
      "Iteration 158, loss = 1.02959392\n",
      "Iteration 159, loss = 1.02894432\n",
      "Iteration 160, loss = 1.02873527\n",
      "Iteration 161, loss = 1.02848125\n",
      "Iteration 162, loss = 1.02798872\n",
      "Iteration 163, loss = 1.02769179\n",
      "Iteration 164, loss = 1.02764741\n",
      "Iteration 165, loss = 1.02719104\n",
      "Iteration 166, loss = 1.02686696\n",
      "Iteration 167, loss = 1.02681980\n",
      "Iteration 168, loss = 1.02628668\n",
      "Iteration 169, loss = 1.02606112\n",
      "Iteration 170, loss = 1.02578901\n",
      "Iteration 171, loss = 1.02557367\n",
      "Iteration 172, loss = 1.02512904\n",
      "Iteration 173, loss = 1.02491462\n",
      "Iteration 174, loss = 1.02487914\n",
      "Iteration 175, loss = 1.02434762\n",
      "Iteration 176, loss = 1.02418505\n",
      "Iteration 177, loss = 1.02396765\n",
      "Iteration 178, loss = 1.02378385\n",
      "Iteration 179, loss = 1.02330366\n",
      "Iteration 180, loss = 1.02295283\n",
      "Iteration 181, loss = 1.02309896\n",
      "Iteration 182, loss = 1.02279553\n",
      "Iteration 183, loss = 1.02250965\n",
      "Iteration 184, loss = 1.02207796\n",
      "Iteration 185, loss = 1.02221547\n",
      "Iteration 186, loss = 1.02180581\n",
      "Iteration 187, loss = 1.02156124\n",
      "Iteration 188, loss = 1.02121395\n",
      "Iteration 189, loss = 1.02080256\n",
      "Iteration 190, loss = 1.02076106\n",
      "Iteration 191, loss = 1.02035582\n",
      "Iteration 192, loss = 1.02015482\n",
      "Iteration 193, loss = 1.02018918\n",
      "Iteration 194, loss = 1.01980908\n",
      "Iteration 195, loss = 1.01973532\n",
      "Iteration 196, loss = 1.01929080\n",
      "Iteration 197, loss = 1.01961558\n",
      "Iteration 198, loss = 1.01892209\n",
      "Iteration 199, loss = 1.01920658\n",
      "Iteration 200, loss = 1.01854167\n",
      "Iteration 1, loss = 11.14936199\n",
      "Iteration 2, loss = 1.50582905\n",
      "Iteration 3, loss = 1.27998454\n",
      "Iteration 4, loss = 1.25078880\n",
      "Iteration 5, loss = 1.24822459\n",
      "Iteration 6, loss = 1.24987482\n",
      "Iteration 7, loss = 1.25257571\n",
      "Iteration 8, loss = 1.25546561\n",
      "Iteration 9, loss = 1.24545958\n",
      "Iteration 10, loss = 1.25084855\n",
      "Iteration 11, loss = 1.25223439\n",
      "Iteration 12, loss = 1.24784159\n",
      "Iteration 13, loss = 1.25224773\n",
      "Iteration 14, loss = 1.24340789\n",
      "Iteration 15, loss = 1.24774694\n",
      "Iteration 16, loss = 1.24313022\n",
      "Iteration 17, loss = 1.24097861\n",
      "Iteration 18, loss = 1.24423703\n",
      "Iteration 19, loss = 1.24113315\n",
      "Iteration 20, loss = 1.24799952\n",
      "Iteration 21, loss = 1.24023395\n",
      "Iteration 22, loss = 1.24069864\n",
      "Iteration 23, loss = 1.23727807\n",
      "Iteration 24, loss = 1.23907916\n",
      "Iteration 25, loss = 1.23769386\n",
      "Iteration 26, loss = 1.24770867\n",
      "Iteration 27, loss = 1.23908269\n",
      "Iteration 28, loss = 1.23659692\n",
      "Iteration 29, loss = 1.23477750\n",
      "Iteration 30, loss = 1.24419515\n",
      "Iteration 31, loss = 1.26552632\n",
      "Iteration 32, loss = 1.23354135\n",
      "Iteration 33, loss = 1.23548321\n",
      "Iteration 34, loss = 1.23702292\n",
      "Iteration 35, loss = 1.23351557\n",
      "Iteration 36, loss = 1.23811509\n",
      "Iteration 37, loss = 1.23238570\n",
      "Iteration 38, loss = 1.23236190\n",
      "Iteration 39, loss = 1.23530774\n",
      "Iteration 40, loss = 1.23152532\n",
      "Iteration 41, loss = 1.24722650\n",
      "Iteration 42, loss = 1.23009184\n",
      "Iteration 43, loss = 1.23780052\n",
      "Iteration 44, loss = 1.23280336\n",
      "Iteration 45, loss = 1.23663597\n",
      "Iteration 46, loss = 1.23759432\n",
      "Iteration 47, loss = 1.23590851\n",
      "Iteration 48, loss = 1.22511542\n",
      "Iteration 49, loss = 1.22973226\n",
      "Iteration 50, loss = 1.22488350\n",
      "Iteration 51, loss = 1.22880051\n",
      "Iteration 52, loss = 1.23619943\n",
      "Iteration 53, loss = 1.25721589\n",
      "Iteration 54, loss = 1.23033706\n",
      "Iteration 55, loss = 1.24315972\n",
      "Iteration 56, loss = 1.22823840\n",
      "Iteration 57, loss = 1.23421545\n",
      "Iteration 58, loss = 1.23507971\n",
      "Iteration 59, loss = 1.23886090\n",
      "Iteration 60, loss = 1.23732968\n",
      "Iteration 61, loss = 1.22870140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49877010\n",
      "Iteration 2, loss = 1.42394318\n",
      "Iteration 3, loss = 1.34119410\n",
      "Iteration 4, loss = 1.28498078\n",
      "Iteration 5, loss = 1.25153773\n",
      "Iteration 6, loss = 1.23571073\n",
      "Iteration 7, loss = 1.22350347\n",
      "Iteration 8, loss = 1.21574579\n",
      "Iteration 9, loss = 1.21003156\n",
      "Iteration 10, loss = 1.20583339\n",
      "Iteration 11, loss = 1.20249108\n",
      "Iteration 12, loss = 1.19921498\n",
      "Iteration 13, loss = 1.19690082\n",
      "Iteration 14, loss = 1.19451029\n",
      "Iteration 15, loss = 1.19209021\n",
      "Iteration 16, loss = 1.19005400\n",
      "Iteration 17, loss = 1.18837096\n",
      "Iteration 18, loss = 1.18617293\n",
      "Iteration 19, loss = 1.18418046\n",
      "Iteration 20, loss = 1.18247231\n",
      "Iteration 21, loss = 1.18042635\n",
      "Iteration 22, loss = 1.17874462\n",
      "Iteration 23, loss = 1.17650435\n",
      "Iteration 24, loss = 1.17481981\n",
      "Iteration 25, loss = 1.17290113\n",
      "Iteration 26, loss = 1.17084780\n",
      "Iteration 27, loss = 1.16937153\n",
      "Iteration 28, loss = 1.16712519\n",
      "Iteration 29, loss = 1.16525700\n",
      "Iteration 30, loss = 1.16363399\n",
      "Iteration 31, loss = 1.16155187\n",
      "Iteration 32, loss = 1.15987037\n",
      "Iteration 33, loss = 1.15808800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 1.15647633\n",
      "Iteration 35, loss = 1.15457149\n",
      "Iteration 36, loss = 1.15286976\n",
      "Iteration 37, loss = 1.15120144\n",
      "Iteration 38, loss = 1.14946293\n",
      "Iteration 39, loss = 1.14791988\n",
      "Iteration 40, loss = 1.14630636\n",
      "Iteration 41, loss = 1.14495326\n",
      "Iteration 42, loss = 1.14342064\n",
      "Iteration 43, loss = 1.14144499\n",
      "Iteration 44, loss = 1.14041578\n",
      "Iteration 45, loss = 1.13856614\n",
      "Iteration 46, loss = 1.13712823\n",
      "Iteration 47, loss = 1.13569953\n",
      "Iteration 48, loss = 1.13420544\n",
      "Iteration 49, loss = 1.13275052\n",
      "Iteration 50, loss = 1.13146173\n",
      "Iteration 51, loss = 1.13019700\n",
      "Iteration 52, loss = 1.12900778\n",
      "Iteration 53, loss = 1.12740129\n",
      "Iteration 54, loss = 1.12602723\n",
      "Iteration 55, loss = 1.12470829\n",
      "Iteration 56, loss = 1.12357860\n",
      "Iteration 57, loss = 1.12233616\n",
      "Iteration 58, loss = 1.12101670\n",
      "Iteration 59, loss = 1.11992827\n",
      "Iteration 60, loss = 1.11853886\n",
      "Iteration 61, loss = 1.11741575\n",
      "Iteration 62, loss = 1.11622408\n",
      "Iteration 63, loss = 1.11506399\n",
      "Iteration 64, loss = 1.11393119\n",
      "Iteration 65, loss = 1.11301474\n",
      "Iteration 66, loss = 1.11185256\n",
      "Iteration 67, loss = 1.11098063\n",
      "Iteration 68, loss = 1.10982537\n",
      "Iteration 69, loss = 1.10861399\n",
      "Iteration 70, loss = 1.10774232\n",
      "Iteration 71, loss = 1.10668960\n",
      "Iteration 72, loss = 1.10597830\n",
      "Iteration 73, loss = 1.10491288\n",
      "Iteration 74, loss = 1.10394688\n",
      "Iteration 75, loss = 1.10303745\n",
      "Iteration 76, loss = 1.10209595\n",
      "Iteration 77, loss = 1.10122015\n",
      "Iteration 78, loss = 1.10025264\n",
      "Iteration 79, loss = 1.09944313\n",
      "Iteration 80, loss = 1.09883685\n",
      "Iteration 81, loss = 1.09790346\n",
      "Iteration 82, loss = 1.09690954\n",
      "Iteration 83, loss = 1.09638237\n",
      "Iteration 84, loss = 1.09529422\n",
      "Iteration 85, loss = 1.09457674\n",
      "Iteration 86, loss = 1.09373414\n",
      "Iteration 87, loss = 1.09301784\n",
      "Iteration 88, loss = 1.09227291\n",
      "Iteration 89, loss = 1.09140879\n",
      "Iteration 90, loss = 1.09069689\n",
      "Iteration 91, loss = 1.08995829\n",
      "Iteration 92, loss = 1.08931233\n",
      "Iteration 93, loss = 1.08867704\n",
      "Iteration 94, loss = 1.08800688\n",
      "Iteration 95, loss = 1.08703739\n",
      "Iteration 96, loss = 1.08650581\n",
      "Iteration 97, loss = 1.08653622\n",
      "Iteration 98, loss = 1.08568888\n",
      "Iteration 99, loss = 1.08476333\n",
      "Iteration 100, loss = 1.08399303\n",
      "Iteration 101, loss = 1.08344057\n",
      "Iteration 102, loss = 1.08265686\n",
      "Iteration 103, loss = 1.08210377\n",
      "Iteration 104, loss = 1.08151378\n",
      "Iteration 105, loss = 1.08073670\n",
      "Iteration 106, loss = 1.08021981\n",
      "Iteration 107, loss = 1.07960830\n",
      "Iteration 108, loss = 1.07898663\n",
      "Iteration 109, loss = 1.07835263\n",
      "Iteration 110, loss = 1.07776582\n",
      "Iteration 111, loss = 1.07772294\n",
      "Iteration 112, loss = 1.07706524\n",
      "Iteration 113, loss = 1.07616911\n",
      "Iteration 114, loss = 1.07598677\n",
      "Iteration 115, loss = 1.07545796\n",
      "Iteration 116, loss = 1.07460220\n",
      "Iteration 117, loss = 1.07433354\n",
      "Iteration 118, loss = 1.07356399\n",
      "Iteration 119, loss = 1.07318146\n",
      "Iteration 120, loss = 1.07268971\n",
      "Iteration 121, loss = 1.07219349\n",
      "Iteration 122, loss = 1.07219180\n",
      "Iteration 123, loss = 1.07117702\n",
      "Iteration 124, loss = 1.07065959\n",
      "Iteration 125, loss = 1.07010745\n",
      "Iteration 126, loss = 1.06983060\n",
      "Iteration 127, loss = 1.06948716\n",
      "Iteration 128, loss = 1.06893749\n",
      "Iteration 129, loss = 1.06868046\n",
      "Iteration 130, loss = 1.06777238\n",
      "Iteration 131, loss = 1.06733571\n",
      "Iteration 132, loss = 1.06704931\n",
      "Iteration 133, loss = 1.06659103\n",
      "Iteration 134, loss = 1.06620499\n",
      "Iteration 135, loss = 1.06579363\n",
      "Iteration 136, loss = 1.06530422\n",
      "Iteration 137, loss = 1.06495858\n",
      "Iteration 138, loss = 1.06442278\n",
      "Iteration 139, loss = 1.06443890\n",
      "Iteration 140, loss = 1.06385421\n",
      "Iteration 141, loss = 1.06341915\n",
      "Iteration 142, loss = 1.06353180\n",
      "Iteration 143, loss = 1.06257647\n",
      "Iteration 144, loss = 1.06214198\n",
      "Iteration 145, loss = 1.06177092\n",
      "Iteration 146, loss = 1.06148763\n",
      "Iteration 147, loss = 1.06123225\n",
      "Iteration 148, loss = 1.06067453\n",
      "Iteration 149, loss = 1.06027519\n",
      "Iteration 150, loss = 1.06029783\n",
      "Iteration 151, loss = 1.05954151\n",
      "Iteration 152, loss = 1.05978251\n",
      "Iteration 153, loss = 1.05896441\n",
      "Iteration 154, loss = 1.05882242\n",
      "Iteration 155, loss = 1.05844338\n",
      "Iteration 156, loss = 1.05803132\n",
      "Iteration 157, loss = 1.05758672\n",
      "Iteration 158, loss = 1.05752468\n",
      "Iteration 159, loss = 1.05689535\n",
      "Iteration 160, loss = 1.05689604\n",
      "Iteration 161, loss = 1.05628963\n",
      "Iteration 162, loss = 1.05653448\n",
      "Iteration 163, loss = 1.05584419\n",
      "Iteration 164, loss = 1.05535169\n",
      "Iteration 165, loss = 1.05507367\n",
      "Iteration 166, loss = 1.05463439\n",
      "Iteration 167, loss = 1.05451866\n",
      "Iteration 168, loss = 1.05415555\n",
      "Iteration 169, loss = 1.05392565\n",
      "Iteration 170, loss = 1.05362984\n",
      "Iteration 171, loss = 1.05326905\n",
      "Iteration 172, loss = 1.05307949\n",
      "Iteration 173, loss = 1.05291733\n",
      "Iteration 174, loss = 1.05259919\n",
      "Iteration 175, loss = 1.05220067\n",
      "Iteration 176, loss = 1.05196177\n",
      "Iteration 177, loss = 1.05159595\n",
      "Iteration 178, loss = 1.05145925\n",
      "Iteration 179, loss = 1.05108838\n",
      "Iteration 180, loss = 1.05089472\n",
      "Iteration 181, loss = 1.05055844\n",
      "Iteration 182, loss = 1.05047530\n",
      "Iteration 183, loss = 1.05015490\n",
      "Iteration 184, loss = 1.05002922\n",
      "Iteration 185, loss = 1.04958919\n",
      "Iteration 186, loss = 1.04966571\n",
      "Iteration 187, loss = 1.04916446\n",
      "Iteration 188, loss = 1.04900671\n",
      "Iteration 189, loss = 1.04863194\n",
      "Iteration 190, loss = 1.04855778\n",
      "Iteration 191, loss = 1.04837363\n",
      "Iteration 192, loss = 1.04805515\n",
      "Iteration 193, loss = 1.04808644\n",
      "Iteration 194, loss = 1.04755786\n",
      "Iteration 195, loss = 1.04726901\n",
      "Iteration 196, loss = 1.04695477\n",
      "Iteration 197, loss = 1.04766166\n",
      "Iteration 198, loss = 1.04655542\n",
      "Iteration 199, loss = 1.04659171\n",
      "Iteration 200, loss = 1.04610991\n",
      "Iteration 201, loss = 1.04602173\n",
      "Iteration 202, loss = 1.04603459\n",
      "Iteration 203, loss = 1.04583328\n",
      "Iteration 204, loss = 1.04534852\n",
      "Iteration 205, loss = 1.04519155\n",
      "Iteration 206, loss = 1.04476749\n",
      "Iteration 207, loss = 1.04457867\n",
      "Iteration 208, loss = 1.04462758\n",
      "Iteration 209, loss = 1.04408635\n",
      "Iteration 210, loss = 1.04419867\n",
      "Iteration 211, loss = 1.04409488\n",
      "Iteration 212, loss = 1.04355789\n",
      "Iteration 213, loss = 1.04369533\n",
      "Iteration 214, loss = 1.04383189\n",
      "Iteration 215, loss = 1.04294367\n",
      "Iteration 216, loss = 1.04282015\n",
      "Iteration 217, loss = 1.04282543\n",
      "Iteration 218, loss = 1.04266992\n",
      "Iteration 219, loss = 1.04236167\n",
      "Iteration 220, loss = 1.04221247\n",
      "Iteration 221, loss = 1.04190637\n",
      "Iteration 222, loss = 1.04180244\n",
      "Iteration 223, loss = 1.04213881\n",
      "Iteration 224, loss = 1.04142016\n",
      "Iteration 225, loss = 1.04114754\n",
      "Iteration 226, loss = 1.04091278\n",
      "Iteration 227, loss = 1.04076953\n",
      "Iteration 228, loss = 1.04060077\n",
      "Iteration 229, loss = 1.04042623\n",
      "Iteration 230, loss = 1.04036871\n",
      "Iteration 231, loss = 1.04029530\n",
      "Iteration 232, loss = 1.04011713\n",
      "Iteration 233, loss = 1.03971975\n",
      "Iteration 234, loss = 1.03950096\n",
      "Iteration 235, loss = 1.03959183\n",
      "Iteration 236, loss = 1.03936437\n",
      "Iteration 237, loss = 1.03907971\n",
      "Iteration 238, loss = 1.03888067\n",
      "Iteration 239, loss = 1.03875388\n",
      "Iteration 240, loss = 1.03883289\n",
      "Iteration 241, loss = 1.03868625\n",
      "Iteration 242, loss = 1.03847307\n",
      "Iteration 243, loss = 1.03856566\n",
      "Iteration 244, loss = 1.03784476\n",
      "Iteration 245, loss = 1.03800784\n",
      "Iteration 246, loss = 1.03764885\n",
      "Iteration 247, loss = 1.03739522\n",
      "Iteration 248, loss = 1.03753231\n",
      "Iteration 249, loss = 1.03705686\n",
      "Iteration 250, loss = 1.03691868\n",
      "Iteration 251, loss = 1.03676071\n",
      "Iteration 252, loss = 1.03674828\n",
      "Iteration 253, loss = 1.03643318\n",
      "Iteration 254, loss = 1.03627487\n",
      "Iteration 255, loss = 1.03628508\n",
      "Iteration 256, loss = 1.03592983\n",
      "Iteration 257, loss = 1.03634337\n",
      "Iteration 258, loss = 1.03568779\n",
      "Iteration 259, loss = 1.03600524\n",
      "Iteration 260, loss = 1.03549451\n",
      "Iteration 261, loss = 1.03526307\n",
      "Iteration 262, loss = 1.03506269\n",
      "Iteration 263, loss = 1.03514449\n",
      "Iteration 264, loss = 1.03516644\n",
      "Iteration 265, loss = 1.03480701\n",
      "Iteration 266, loss = 1.03464285\n",
      "Iteration 267, loss = 1.03481076\n",
      "Iteration 268, loss = 1.03453247\n",
      "Iteration 269, loss = 1.03451291\n",
      "Iteration 270, loss = 1.03395403\n",
      "Iteration 271, loss = 1.03450350\n",
      "Iteration 272, loss = 1.03405057\n",
      "Iteration 273, loss = 1.03369993\n",
      "Iteration 274, loss = 1.03415143\n",
      "Iteration 275, loss = 1.03343380\n",
      "Iteration 276, loss = 1.03335555\n",
      "Iteration 277, loss = 1.03321861\n",
      "Iteration 278, loss = 1.03292254\n",
      "Iteration 279, loss = 1.03267735\n",
      "Iteration 280, loss = 1.03292462\n",
      "Iteration 281, loss = 1.03292653\n",
      "Iteration 282, loss = 1.03250101\n",
      "Iteration 283, loss = 1.03213758\n",
      "Iteration 284, loss = 1.03232958\n",
      "Iteration 285, loss = 1.03210311\n",
      "Iteration 286, loss = 1.03290286\n",
      "Iteration 287, loss = 1.03175567\n",
      "Iteration 288, loss = 1.03208861\n",
      "Iteration 289, loss = 1.03145541\n",
      "Iteration 290, loss = 1.03147581\n",
      "Iteration 291, loss = 1.03176950\n",
      "Iteration 292, loss = 1.03262694\n",
      "Iteration 293, loss = 1.03097920\n",
      "Iteration 294, loss = 1.03088454\n",
      "Iteration 295, loss = 1.03103882\n",
      "Iteration 296, loss = 1.03060530\n",
      "Iteration 297, loss = 1.03055867\n",
      "Iteration 298, loss = 1.03059559\n",
      "Iteration 299, loss = 1.03033055\n",
      "Iteration 300, loss = 1.03025810\n",
      "Iteration 1, loss = 1.49908819\n",
      "Iteration 2, loss = 1.42332086\n",
      "Iteration 3, loss = 1.34163044\n",
      "Iteration 4, loss = 1.28521406\n",
      "Iteration 5, loss = 1.25234688\n",
      "Iteration 6, loss = 1.23565166\n",
      "Iteration 7, loss = 1.22404055\n",
      "Iteration 8, loss = 1.21572726\n",
      "Iteration 9, loss = 1.21006083\n",
      "Iteration 10, loss = 1.20576524\n",
      "Iteration 11, loss = 1.20195414\n",
      "Iteration 12, loss = 1.19866271\n",
      "Iteration 13, loss = 1.19633471\n",
      "Iteration 14, loss = 1.19346505\n",
      "Iteration 15, loss = 1.19120732\n",
      "Iteration 16, loss = 1.18935925\n",
      "Iteration 17, loss = 1.18740936\n",
      "Iteration 18, loss = 1.18517371\n",
      "Iteration 19, loss = 1.18331336\n",
      "Iteration 20, loss = 1.18127180\n",
      "Iteration 21, loss = 1.17917906\n",
      "Iteration 22, loss = 1.17733991\n",
      "Iteration 23, loss = 1.17506201\n",
      "Iteration 24, loss = 1.17326842\n",
      "Iteration 25, loss = 1.17127106\n",
      "Iteration 26, loss = 1.16920098\n",
      "Iteration 27, loss = 1.16790199\n",
      "Iteration 28, loss = 1.16533732\n",
      "Iteration 29, loss = 1.16328821\n",
      "Iteration 30, loss = 1.16176242\n",
      "Iteration 31, loss = 1.15942983\n",
      "Iteration 32, loss = 1.15764938\n",
      "Iteration 33, loss = 1.15568411\n",
      "Iteration 34, loss = 1.15383611\n",
      "Iteration 35, loss = 1.15201811\n",
      "Iteration 36, loss = 1.15017699\n",
      "Iteration 37, loss = 1.14837700\n",
      "Iteration 38, loss = 1.14649081\n",
      "Iteration 39, loss = 1.14474955\n",
      "Iteration 40, loss = 1.14292172\n",
      "Iteration 41, loss = 1.14139153\n",
      "Iteration 42, loss = 1.13958302\n",
      "Iteration 43, loss = 1.13788771\n",
      "Iteration 44, loss = 1.13625339\n",
      "Iteration 45, loss = 1.13445816\n",
      "Iteration 46, loss = 1.13272506\n",
      "Iteration 47, loss = 1.13136890\n",
      "Iteration 48, loss = 1.12977353\n",
      "Iteration 49, loss = 1.12822840\n",
      "Iteration 50, loss = 1.12659356\n",
      "Iteration 51, loss = 1.12504655\n",
      "Iteration 52, loss = 1.12386442\n",
      "Iteration 53, loss = 1.12204734\n",
      "Iteration 54, loss = 1.12069872\n",
      "Iteration 55, loss = 1.11933247\n",
      "Iteration 56, loss = 1.11798130\n",
      "Iteration 57, loss = 1.11657911\n",
      "Iteration 58, loss = 1.11517779\n",
      "Iteration 59, loss = 1.11385272\n",
      "Iteration 60, loss = 1.11247553\n",
      "Iteration 61, loss = 1.11144913\n",
      "Iteration 62, loss = 1.11008281\n",
      "Iteration 63, loss = 1.10869610\n",
      "Iteration 64, loss = 1.10740776\n",
      "Iteration 65, loss = 1.10651976\n",
      "Iteration 66, loss = 1.10521962\n",
      "Iteration 67, loss = 1.10404739\n",
      "Iteration 68, loss = 1.10291673\n",
      "Iteration 69, loss = 1.10180466\n",
      "Iteration 70, loss = 1.10050414\n",
      "Iteration 71, loss = 1.09940301\n",
      "Iteration 72, loss = 1.09860258\n",
      "Iteration 73, loss = 1.09749046\n",
      "Iteration 74, loss = 1.09640875\n",
      "Iteration 75, loss = 1.09547987\n",
      "Iteration 76, loss = 1.09461095\n",
      "Iteration 77, loss = 1.09362025\n",
      "Iteration 78, loss = 1.09269018\n",
      "Iteration 79, loss = 1.09168983\n",
      "Iteration 80, loss = 1.09102697\n",
      "Iteration 81, loss = 1.08971929\n",
      "Iteration 82, loss = 1.08875089\n",
      "Iteration 83, loss = 1.08811939\n",
      "Iteration 84, loss = 1.08698501\n",
      "Iteration 85, loss = 1.08635557\n",
      "Iteration 86, loss = 1.08546708\n",
      "Iteration 87, loss = 1.08477211\n",
      "Iteration 88, loss = 1.08378598\n",
      "Iteration 89, loss = 1.08322349\n",
      "Iteration 90, loss = 1.08221743\n",
      "Iteration 91, loss = 1.08146266\n",
      "Iteration 92, loss = 1.08064838\n",
      "Iteration 93, loss = 1.08042361\n",
      "Iteration 94, loss = 1.07929674\n",
      "Iteration 95, loss = 1.07850579\n",
      "Iteration 96, loss = 1.07780113\n",
      "Iteration 97, loss = 1.07732816\n",
      "Iteration 98, loss = 1.07660085\n",
      "Iteration 99, loss = 1.07606939\n",
      "Iteration 100, loss = 1.07509805\n",
      "Iteration 101, loss = 1.07436025\n",
      "Iteration 102, loss = 1.07399418\n",
      "Iteration 103, loss = 1.07319585\n",
      "Iteration 104, loss = 1.07244974\n",
      "Iteration 105, loss = 1.07195547\n",
      "Iteration 106, loss = 1.07141605\n",
      "Iteration 107, loss = 1.07086147\n",
      "Iteration 108, loss = 1.07004548\n",
      "Iteration 109, loss = 1.06951657\n",
      "Iteration 110, loss = 1.06931163\n",
      "Iteration 111, loss = 1.06853758\n",
      "Iteration 112, loss = 1.06797489\n",
      "Iteration 113, loss = 1.06717965\n",
      "Iteration 114, loss = 1.06684877\n",
      "Iteration 115, loss = 1.06666787\n",
      "Iteration 116, loss = 1.06559471\n",
      "Iteration 117, loss = 1.06500543\n",
      "Iteration 118, loss = 1.06447560\n",
      "Iteration 119, loss = 1.06457156\n",
      "Iteration 120, loss = 1.06393468\n",
      "Iteration 121, loss = 1.06344650\n",
      "Iteration 122, loss = 1.06264646\n",
      "Iteration 123, loss = 1.06221128\n",
      "Iteration 124, loss = 1.06155844\n",
      "Iteration 125, loss = 1.06116513\n",
      "Iteration 126, loss = 1.06068919\n",
      "Iteration 127, loss = 1.06034617\n",
      "Iteration 128, loss = 1.05989334\n",
      "Iteration 129, loss = 1.05940417\n",
      "Iteration 130, loss = 1.05905861\n",
      "Iteration 131, loss = 1.05848310\n",
      "Iteration 132, loss = 1.05829618\n",
      "Iteration 133, loss = 1.05766454\n",
      "Iteration 134, loss = 1.05732272\n",
      "Iteration 135, loss = 1.05688139\n",
      "Iteration 136, loss = 1.05655387\n",
      "Iteration 137, loss = 1.05607491\n",
      "Iteration 138, loss = 1.05587440\n",
      "Iteration 139, loss = 1.05560230\n",
      "Iteration 140, loss = 1.05525476\n",
      "Iteration 141, loss = 1.05521643\n",
      "Iteration 142, loss = 1.05489333\n",
      "Iteration 143, loss = 1.05407478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 144, loss = 1.05347795\n",
      "Iteration 145, loss = 1.05307020\n",
      "Iteration 146, loss = 1.05268499\n",
      "Iteration 147, loss = 1.05235024\n",
      "Iteration 148, loss = 1.05199986\n",
      "Iteration 149, loss = 1.05169041\n",
      "Iteration 150, loss = 1.05141852\n",
      "Iteration 151, loss = 1.05144885\n",
      "Iteration 152, loss = 1.05071687\n",
      "Iteration 153, loss = 1.05047851\n",
      "Iteration 154, loss = 1.05039874\n",
      "Iteration 155, loss = 1.04999647\n",
      "Iteration 156, loss = 1.04960162\n",
      "Iteration 157, loss = 1.04917727\n",
      "Iteration 158, loss = 1.04898735\n",
      "Iteration 159, loss = 1.04862601\n",
      "Iteration 160, loss = 1.04832603\n",
      "Iteration 161, loss = 1.04802512\n",
      "Iteration 162, loss = 1.04780428\n",
      "Iteration 163, loss = 1.04745265\n",
      "Iteration 164, loss = 1.04728726\n",
      "Iteration 165, loss = 1.04691086\n",
      "Iteration 166, loss = 1.04664828\n",
      "Iteration 167, loss = 1.04665131\n",
      "Iteration 168, loss = 1.04614946\n",
      "Iteration 169, loss = 1.04616084\n",
      "Iteration 170, loss = 1.04536704\n",
      "Iteration 171, loss = 1.04535617\n",
      "Iteration 172, loss = 1.04500138\n",
      "Iteration 173, loss = 1.04500399\n",
      "Iteration 174, loss = 1.04450555\n",
      "Iteration 175, loss = 1.04421977\n",
      "Iteration 176, loss = 1.04396589\n",
      "Iteration 177, loss = 1.04379077\n",
      "Iteration 178, loss = 1.04341583\n",
      "Iteration 179, loss = 1.04329753\n",
      "Iteration 180, loss = 1.04338831\n",
      "Iteration 181, loss = 1.04275782\n",
      "Iteration 182, loss = 1.04259767\n",
      "Iteration 183, loss = 1.04234229\n",
      "Iteration 184, loss = 1.04232238\n",
      "Iteration 185, loss = 1.04201587\n",
      "Iteration 186, loss = 1.04163748\n",
      "Iteration 187, loss = 1.04155228\n",
      "Iteration 188, loss = 1.04126985\n",
      "Iteration 189, loss = 1.04087042\n",
      "Iteration 190, loss = 1.04081829\n",
      "Iteration 191, loss = 1.04064327\n",
      "Iteration 192, loss = 1.04032555\n",
      "Iteration 193, loss = 1.03991599\n",
      "Iteration 194, loss = 1.04007844\n",
      "Iteration 195, loss = 1.03963409\n",
      "Iteration 196, loss = 1.03942706\n",
      "Iteration 197, loss = 1.03927856\n",
      "Iteration 198, loss = 1.03905684\n",
      "Iteration 199, loss = 1.03887880\n",
      "Iteration 200, loss = 1.03852904\n",
      "Iteration 201, loss = 1.03832632\n",
      "Iteration 202, loss = 1.03842825\n",
      "Iteration 203, loss = 1.03828444\n",
      "Iteration 204, loss = 1.03762876\n",
      "Iteration 205, loss = 1.03786046\n",
      "Iteration 206, loss = 1.03724642\n",
      "Iteration 207, loss = 1.03703141\n",
      "Iteration 208, loss = 1.03746158\n",
      "Iteration 209, loss = 1.03679523\n",
      "Iteration 210, loss = 1.03685185\n",
      "Iteration 211, loss = 1.03657966\n",
      "Iteration 212, loss = 1.03641953\n",
      "Iteration 213, loss = 1.03599312\n",
      "Iteration 214, loss = 1.03598822\n",
      "Iteration 215, loss = 1.03574852\n",
      "Iteration 216, loss = 1.03576088\n",
      "Iteration 217, loss = 1.03531299\n",
      "Iteration 218, loss = 1.03536583\n",
      "Iteration 219, loss = 1.03546067\n",
      "Iteration 220, loss = 1.03485751\n",
      "Iteration 221, loss = 1.03476755\n",
      "Iteration 222, loss = 1.03446337\n",
      "Iteration 223, loss = 1.03452310\n",
      "Iteration 224, loss = 1.03413248\n",
      "Iteration 225, loss = 1.03401644\n",
      "Iteration 226, loss = 1.03380593\n",
      "Iteration 227, loss = 1.03357632\n",
      "Iteration 228, loss = 1.03333316\n",
      "Iteration 229, loss = 1.03318855\n",
      "Iteration 230, loss = 1.03333237\n",
      "Iteration 231, loss = 1.03300419\n",
      "Iteration 232, loss = 1.03278674\n",
      "Iteration 233, loss = 1.03257983\n",
      "Iteration 234, loss = 1.03233820\n",
      "Iteration 235, loss = 1.03254177\n",
      "Iteration 236, loss = 1.03236085\n",
      "Iteration 237, loss = 1.03196432\n",
      "Iteration 238, loss = 1.03163865\n",
      "Iteration 239, loss = 1.03158639\n",
      "Iteration 240, loss = 1.03166505\n",
      "Iteration 241, loss = 1.03138563\n",
      "Iteration 242, loss = 1.03121586\n",
      "Iteration 243, loss = 1.03178366\n",
      "Iteration 244, loss = 1.03105256\n",
      "Iteration 245, loss = 1.03067702\n",
      "Iteration 246, loss = 1.03113224\n",
      "Iteration 247, loss = 1.03054615\n",
      "Iteration 248, loss = 1.03017212\n",
      "Iteration 249, loss = 1.03009601\n",
      "Iteration 250, loss = 1.02996542\n",
      "Iteration 251, loss = 1.02982627\n",
      "Iteration 252, loss = 1.02965162\n",
      "Iteration 253, loss = 1.02976197\n",
      "Iteration 254, loss = 1.02936005\n",
      "Iteration 255, loss = 1.02923111\n",
      "Iteration 256, loss = 1.02914256\n",
      "Iteration 257, loss = 1.02897252\n",
      "Iteration 258, loss = 1.02872694\n",
      "Iteration 259, loss = 1.02912858\n",
      "Iteration 260, loss = 1.02868066\n",
      "Iteration 261, loss = 1.02832093\n",
      "Iteration 262, loss = 1.02809977\n",
      "Iteration 263, loss = 1.02828630\n",
      "Iteration 264, loss = 1.02873172\n",
      "Iteration 265, loss = 1.02799405\n",
      "Iteration 266, loss = 1.02776561\n",
      "Iteration 267, loss = 1.02769487\n",
      "Iteration 268, loss = 1.02744044\n",
      "Iteration 269, loss = 1.02738136\n",
      "Iteration 270, loss = 1.02736143\n",
      "Iteration 271, loss = 1.02698556\n",
      "Iteration 272, loss = 1.02695939\n",
      "Iteration 273, loss = 1.02691218\n",
      "Iteration 274, loss = 1.02699291\n",
      "Iteration 275, loss = 1.02719916\n",
      "Iteration 276, loss = 1.02659056\n",
      "Iteration 277, loss = 1.02653307\n",
      "Iteration 278, loss = 1.02615683\n",
      "Iteration 279, loss = 1.02636155\n",
      "Iteration 280, loss = 1.02588250\n",
      "Iteration 281, loss = 1.02583586\n",
      "Iteration 282, loss = 1.02585717\n",
      "Iteration 283, loss = 1.02545578\n",
      "Iteration 284, loss = 1.02554038\n",
      "Iteration 285, loss = 1.02536913\n",
      "Iteration 286, loss = 1.02541268\n",
      "Iteration 287, loss = 1.02485625\n",
      "Iteration 288, loss = 1.02545676\n",
      "Iteration 289, loss = 1.02473608\n",
      "Iteration 290, loss = 1.02473070\n",
      "Iteration 291, loss = 1.02492490\n",
      "Iteration 292, loss = 1.02455517\n",
      "Iteration 293, loss = 1.02458152\n",
      "Iteration 294, loss = 1.02475846\n",
      "Iteration 295, loss = 1.02450848\n",
      "Iteration 296, loss = 1.02379719\n",
      "Iteration 297, loss = 1.02378315\n",
      "Iteration 298, loss = 1.02368086\n",
      "Iteration 299, loss = 1.02346084\n",
      "Iteration 300, loss = 1.02396999\n",
      "Iteration 1, loss = 1.49923727\n",
      "Iteration 2, loss = 1.42299377\n",
      "Iteration 3, loss = 1.34202567\n",
      "Iteration 4, loss = 1.28679915\n",
      "Iteration 5, loss = 1.25108990\n",
      "Iteration 6, loss = 1.23405464\n",
      "Iteration 7, loss = 1.22292696\n",
      "Iteration 8, loss = 1.21394865\n",
      "Iteration 9, loss = 1.20861756\n",
      "Iteration 10, loss = 1.20388773\n",
      "Iteration 11, loss = 1.19979396\n",
      "Iteration 12, loss = 1.19644209\n",
      "Iteration 13, loss = 1.19370771\n",
      "Iteration 14, loss = 1.19105836\n",
      "Iteration 15, loss = 1.18864421\n",
      "Iteration 16, loss = 1.18655215\n",
      "Iteration 17, loss = 1.18448150\n",
      "Iteration 18, loss = 1.18199345\n",
      "Iteration 19, loss = 1.17992997\n",
      "Iteration 20, loss = 1.17771381\n",
      "Iteration 21, loss = 1.17559532\n",
      "Iteration 22, loss = 1.17343029\n",
      "Iteration 23, loss = 1.17136724\n",
      "Iteration 24, loss = 1.16946825\n",
      "Iteration 25, loss = 1.16757638\n",
      "Iteration 26, loss = 1.16512556\n",
      "Iteration 27, loss = 1.16332170\n",
      "Iteration 28, loss = 1.16132321\n",
      "Iteration 29, loss = 1.15908280\n",
      "Iteration 30, loss = 1.15698671\n",
      "Iteration 31, loss = 1.15496340\n",
      "Iteration 32, loss = 1.15332187\n",
      "Iteration 33, loss = 1.15106046\n",
      "Iteration 34, loss = 1.14924663\n",
      "Iteration 35, loss = 1.14745642\n",
      "Iteration 36, loss = 1.14561081\n",
      "Iteration 37, loss = 1.14370721\n",
      "Iteration 38, loss = 1.14175784\n",
      "Iteration 39, loss = 1.14000123\n",
      "Iteration 40, loss = 1.13828863\n",
      "Iteration 41, loss = 1.13665436\n",
      "Iteration 42, loss = 1.13510325\n",
      "Iteration 43, loss = 1.13320068\n",
      "Iteration 44, loss = 1.13152034\n",
      "Iteration 45, loss = 1.12987894\n",
      "Iteration 46, loss = 1.12827999\n",
      "Iteration 47, loss = 1.12671934\n",
      "Iteration 48, loss = 1.12528830\n",
      "Iteration 49, loss = 1.12394611\n",
      "Iteration 50, loss = 1.12244943\n",
      "Iteration 51, loss = 1.12083801\n",
      "Iteration 52, loss = 1.11982670\n",
      "Iteration 53, loss = 1.11828239\n",
      "Iteration 54, loss = 1.11695346\n",
      "Iteration 55, loss = 1.11566887\n",
      "Iteration 56, loss = 1.11432172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57, loss = 1.11314367\n",
      "Iteration 58, loss = 1.11196685\n",
      "Iteration 59, loss = 1.11085319\n",
      "Iteration 60, loss = 1.10977080\n",
      "Iteration 61, loss = 1.10889616\n",
      "Iteration 62, loss = 1.10726085\n",
      "Iteration 63, loss = 1.10656140\n",
      "Iteration 64, loss = 1.10529195\n",
      "Iteration 65, loss = 1.10418169\n",
      "Iteration 66, loss = 1.10296441\n",
      "Iteration 67, loss = 1.10199996\n",
      "Iteration 68, loss = 1.10130565\n",
      "Iteration 69, loss = 1.10003285\n",
      "Iteration 70, loss = 1.09912893\n",
      "Iteration 71, loss = 1.09833414\n",
      "Iteration 72, loss = 1.09731422\n",
      "Iteration 73, loss = 1.09636466\n",
      "Iteration 74, loss = 1.09553965\n",
      "Iteration 75, loss = 1.09495089\n",
      "Iteration 76, loss = 1.09385803\n",
      "Iteration 77, loss = 1.09306523\n",
      "Iteration 78, loss = 1.09217621\n",
      "Iteration 79, loss = 1.09164752\n",
      "Iteration 80, loss = 1.09065440\n",
      "Iteration 81, loss = 1.08976123\n",
      "Iteration 82, loss = 1.08915268\n",
      "Iteration 83, loss = 1.08829057\n",
      "Iteration 84, loss = 1.08780120\n",
      "Iteration 85, loss = 1.08681043\n",
      "Iteration 86, loss = 1.08616233\n",
      "Iteration 87, loss = 1.08582023\n",
      "Iteration 88, loss = 1.08473916\n",
      "Iteration 89, loss = 1.08410801\n",
      "Iteration 90, loss = 1.08349611\n",
      "Iteration 91, loss = 1.08280474\n",
      "Iteration 92, loss = 1.08215946\n",
      "Iteration 93, loss = 1.08178509\n",
      "Iteration 94, loss = 1.08092636\n",
      "Iteration 95, loss = 1.08024795\n",
      "Iteration 96, loss = 1.07989735\n",
      "Iteration 97, loss = 1.07923266\n",
      "Iteration 98, loss = 1.07856904\n",
      "Iteration 99, loss = 1.07790168\n",
      "Iteration 100, loss = 1.07765122\n",
      "Iteration 101, loss = 1.07699704\n",
      "Iteration 102, loss = 1.07639223\n",
      "Iteration 103, loss = 1.07572040\n",
      "Iteration 104, loss = 1.07554642\n",
      "Iteration 105, loss = 1.07479976\n",
      "Iteration 106, loss = 1.07402082\n",
      "Iteration 107, loss = 1.07375004\n",
      "Iteration 108, loss = 1.07329060\n",
      "Iteration 109, loss = 1.07272676\n",
      "Iteration 110, loss = 1.07231480\n",
      "Iteration 111, loss = 1.07155007\n",
      "Iteration 112, loss = 1.07115348\n",
      "Iteration 113, loss = 1.07075107\n",
      "Iteration 114, loss = 1.07012603\n",
      "Iteration 115, loss = 1.06975445\n",
      "Iteration 116, loss = 1.06945358\n",
      "Iteration 117, loss = 1.06873283\n",
      "Iteration 118, loss = 1.06860425\n",
      "Iteration 119, loss = 1.06820380\n",
      "Iteration 120, loss = 1.06753168\n",
      "Iteration 121, loss = 1.06723037\n",
      "Iteration 122, loss = 1.06661850\n",
      "Iteration 123, loss = 1.06633110\n",
      "Iteration 124, loss = 1.06618044\n",
      "Iteration 125, loss = 1.06588474\n",
      "Iteration 126, loss = 1.06510317\n",
      "Iteration 127, loss = 1.06494272\n",
      "Iteration 128, loss = 1.06432860\n",
      "Iteration 129, loss = 1.06371323\n",
      "Iteration 130, loss = 1.06381755\n",
      "Iteration 131, loss = 1.06302762\n",
      "Iteration 132, loss = 1.06273816\n",
      "Iteration 133, loss = 1.06242299\n",
      "Iteration 134, loss = 1.06197285\n",
      "Iteration 135, loss = 1.06162477\n",
      "Iteration 136, loss = 1.06138476\n",
      "Iteration 137, loss = 1.06113583\n",
      "Iteration 138, loss = 1.06096246\n",
      "Iteration 139, loss = 1.06041134\n",
      "Iteration 140, loss = 1.05998865\n",
      "Iteration 141, loss = 1.05977299\n",
      "Iteration 142, loss = 1.05966958\n",
      "Iteration 143, loss = 1.05907304\n",
      "Iteration 144, loss = 1.05866946\n",
      "Iteration 145, loss = 1.05834896\n",
      "Iteration 146, loss = 1.05808432\n",
      "Iteration 147, loss = 1.05770150\n",
      "Iteration 148, loss = 1.05737359\n",
      "Iteration 149, loss = 1.05729400\n",
      "Iteration 150, loss = 1.05666845\n",
      "Iteration 151, loss = 1.05675533\n",
      "Iteration 152, loss = 1.05629524\n",
      "Iteration 153, loss = 1.05593325\n",
      "Iteration 154, loss = 1.05555829\n",
      "Iteration 155, loss = 1.05528242\n",
      "Iteration 156, loss = 1.05497019\n",
      "Iteration 157, loss = 1.05483839\n",
      "Iteration 158, loss = 1.05450037\n",
      "Iteration 159, loss = 1.05412572\n",
      "Iteration 160, loss = 1.05387157\n",
      "Iteration 161, loss = 1.05363719\n",
      "Iteration 162, loss = 1.05341641\n",
      "Iteration 163, loss = 1.05325306\n",
      "Iteration 164, loss = 1.05299429\n",
      "Iteration 165, loss = 1.05253963\n",
      "Iteration 166, loss = 1.05237689\n",
      "Iteration 167, loss = 1.05218832\n",
      "Iteration 168, loss = 1.05181107\n",
      "Iteration 169, loss = 1.05153052\n",
      "Iteration 170, loss = 1.05150842\n",
      "Iteration 171, loss = 1.05110832\n",
      "Iteration 172, loss = 1.05094372\n",
      "Iteration 173, loss = 1.05078376\n",
      "Iteration 174, loss = 1.05033148\n",
      "Iteration 175, loss = 1.05009856\n",
      "Iteration 176, loss = 1.04988539\n",
      "Iteration 177, loss = 1.04964739\n",
      "Iteration 178, loss = 1.04953145\n",
      "Iteration 179, loss = 1.04954585\n",
      "Iteration 180, loss = 1.04958356\n",
      "Iteration 181, loss = 1.04902700\n",
      "Iteration 182, loss = 1.04887154\n",
      "Iteration 183, loss = 1.04868862\n",
      "Iteration 184, loss = 1.04824231\n",
      "Iteration 185, loss = 1.04798652\n",
      "Iteration 186, loss = 1.04766512\n",
      "Iteration 187, loss = 1.04775798\n",
      "Iteration 188, loss = 1.04744888\n",
      "Iteration 189, loss = 1.04705432\n",
      "Iteration 190, loss = 1.04700450\n",
      "Iteration 191, loss = 1.04691787\n",
      "Iteration 192, loss = 1.04634289\n",
      "Iteration 193, loss = 1.04623385\n",
      "Iteration 194, loss = 1.04629973\n",
      "Iteration 195, loss = 1.04649283\n",
      "Iteration 196, loss = 1.04581602\n",
      "Iteration 197, loss = 1.04546521\n",
      "Iteration 198, loss = 1.04525139\n",
      "Iteration 199, loss = 1.04504215\n",
      "Iteration 200, loss = 1.04510753\n",
      "Iteration 201, loss = 1.04462901\n",
      "Iteration 202, loss = 1.04473137\n",
      "Iteration 203, loss = 1.04476852\n",
      "Iteration 204, loss = 1.04426245\n",
      "Iteration 205, loss = 1.04405742\n",
      "Iteration 206, loss = 1.04381759\n",
      "Iteration 207, loss = 1.04360648\n",
      "Iteration 208, loss = 1.04338920\n",
      "Iteration 209, loss = 1.04335059\n",
      "Iteration 210, loss = 1.04350553\n",
      "Iteration 211, loss = 1.04320593\n",
      "Iteration 212, loss = 1.04277092\n",
      "Iteration 213, loss = 1.04266318\n",
      "Iteration 214, loss = 1.04267051\n",
      "Iteration 215, loss = 1.04225295\n",
      "Iteration 216, loss = 1.04201859\n",
      "Iteration 217, loss = 1.04166805\n",
      "Iteration 218, loss = 1.04178829\n",
      "Iteration 219, loss = 1.04169238\n",
      "Iteration 220, loss = 1.04130854\n",
      "Iteration 221, loss = 1.04114221\n",
      "Iteration 222, loss = 1.04093729\n",
      "Iteration 223, loss = 1.04076926\n",
      "Iteration 224, loss = 1.04085683\n",
      "Iteration 225, loss = 1.04052674\n",
      "Iteration 226, loss = 1.04042067\n",
      "Iteration 227, loss = 1.04005844\n",
      "Iteration 228, loss = 1.04001142\n",
      "Iteration 229, loss = 1.03984045\n",
      "Iteration 230, loss = 1.03971102\n",
      "Iteration 231, loss = 1.03958585\n",
      "Iteration 232, loss = 1.03968343\n",
      "Iteration 233, loss = 1.03913908\n",
      "Iteration 234, loss = 1.03942731\n",
      "Iteration 235, loss = 1.03888207\n",
      "Iteration 236, loss = 1.03903998\n",
      "Iteration 237, loss = 1.03895475\n",
      "Iteration 238, loss = 1.03839937\n",
      "Iteration 239, loss = 1.03840860\n",
      "Iteration 240, loss = 1.03849422\n",
      "Iteration 241, loss = 1.03841595\n",
      "Iteration 242, loss = 1.03785015\n",
      "Iteration 243, loss = 1.03786848\n",
      "Iteration 244, loss = 1.03758079\n",
      "Iteration 245, loss = 1.03738556\n",
      "Iteration 246, loss = 1.03732231\n",
      "Iteration 247, loss = 1.03735407\n",
      "Iteration 248, loss = 1.03690486\n",
      "Iteration 249, loss = 1.03674502\n",
      "Iteration 250, loss = 1.03671045\n",
      "Iteration 251, loss = 1.03663427\n",
      "Iteration 252, loss = 1.03632995\n",
      "Iteration 253, loss = 1.03616259\n",
      "Iteration 254, loss = 1.03618009\n",
      "Iteration 255, loss = 1.03609446\n",
      "Iteration 256, loss = 1.03607378\n",
      "Iteration 257, loss = 1.03569430\n",
      "Iteration 258, loss = 1.03547152\n",
      "Iteration 259, loss = 1.03567805\n",
      "Iteration 260, loss = 1.03546076\n",
      "Iteration 261, loss = 1.03511722\n",
      "Iteration 262, loss = 1.03497503\n",
      "Iteration 263, loss = 1.03498453\n",
      "Iteration 264, loss = 1.03495392\n",
      "Iteration 265, loss = 1.03469059\n",
      "Iteration 266, loss = 1.03446166\n",
      "Iteration 267, loss = 1.03459072\n",
      "Iteration 268, loss = 1.03418903\n",
      "Iteration 269, loss = 1.03409145\n",
      "Iteration 270, loss = 1.03414119\n",
      "Iteration 271, loss = 1.03369797\n",
      "Iteration 272, loss = 1.03384575\n",
      "Iteration 273, loss = 1.03353191\n",
      "Iteration 274, loss = 1.03407258\n",
      "Iteration 275, loss = 1.03419815\n",
      "Iteration 276, loss = 1.03366992\n",
      "Iteration 277, loss = 1.03321903\n",
      "Iteration 278, loss = 1.03311452\n",
      "Iteration 279, loss = 1.03307191\n",
      "Iteration 280, loss = 1.03259324\n",
      "Iteration 281, loss = 1.03266303\n",
      "Iteration 282, loss = 1.03258982\n",
      "Iteration 283, loss = 1.03249237\n",
      "Iteration 284, loss = 1.03242242\n",
      "Iteration 285, loss = 1.03219095\n",
      "Iteration 286, loss = 1.03209329\n",
      "Iteration 287, loss = 1.03181298\n",
      "Iteration 288, loss = 1.03215135\n",
      "Iteration 289, loss = 1.03157089\n",
      "Iteration 290, loss = 1.03173602\n",
      "Iteration 291, loss = 1.03125052\n",
      "Iteration 292, loss = 1.03135436\n",
      "Iteration 293, loss = 1.03100662\n",
      "Iteration 294, loss = 1.03130744\n",
      "Iteration 295, loss = 1.03092093\n",
      "Iteration 296, loss = 1.03063337\n",
      "Iteration 297, loss = 1.03042373\n",
      "Iteration 298, loss = 1.03096640\n",
      "Iteration 299, loss = 1.03053195\n",
      "Iteration 300, loss = 1.03051297\n",
      "Iteration 1, loss = 1.49943611\n",
      "Iteration 2, loss = 1.42457960\n",
      "Iteration 3, loss = 1.34336208\n",
      "Iteration 4, loss = 1.28753208\n",
      "Iteration 5, loss = 1.25371366\n",
      "Iteration 6, loss = 1.23774789\n",
      "Iteration 7, loss = 1.22801699\n",
      "Iteration 8, loss = 1.21912371\n",
      "Iteration 9, loss = 1.21345391\n",
      "Iteration 10, loss = 1.20939172\n",
      "Iteration 11, loss = 1.20575896\n",
      "Iteration 12, loss = 1.20312205\n",
      "Iteration 13, loss = 1.20092866\n",
      "Iteration 14, loss = 1.19832460\n",
      "Iteration 15, loss = 1.19649262\n",
      "Iteration 16, loss = 1.19450072\n",
      "Iteration 17, loss = 1.19288875\n",
      "Iteration 18, loss = 1.19090354\n",
      "Iteration 19, loss = 1.18925464\n",
      "Iteration 20, loss = 1.18736623\n",
      "Iteration 21, loss = 1.18561718\n",
      "Iteration 22, loss = 1.18402260\n",
      "Iteration 23, loss = 1.18198905\n",
      "Iteration 24, loss = 1.18044697\n",
      "Iteration 25, loss = 1.17858017\n",
      "Iteration 26, loss = 1.17678144\n",
      "Iteration 27, loss = 1.17513278\n",
      "Iteration 28, loss = 1.17357532\n",
      "Iteration 29, loss = 1.17174587\n",
      "Iteration 30, loss = 1.17004257\n",
      "Iteration 31, loss = 1.16843293\n",
      "Iteration 32, loss = 1.16724621\n",
      "Iteration 33, loss = 1.16522135\n",
      "Iteration 34, loss = 1.16363177\n",
      "Iteration 35, loss = 1.16205485\n",
      "Iteration 36, loss = 1.16063158\n",
      "Iteration 37, loss = 1.15882263\n",
      "Iteration 38, loss = 1.15725388\n",
      "Iteration 39, loss = 1.15603247\n",
      "Iteration 40, loss = 1.15423614\n",
      "Iteration 41, loss = 1.15260265\n",
      "Iteration 42, loss = 1.15117961\n",
      "Iteration 43, loss = 1.14967494\n",
      "Iteration 44, loss = 1.14817895\n",
      "Iteration 45, loss = 1.14672477\n",
      "Iteration 46, loss = 1.14552394\n",
      "Iteration 47, loss = 1.14396942\n",
      "Iteration 48, loss = 1.14251539\n",
      "Iteration 49, loss = 1.14138583\n",
      "Iteration 50, loss = 1.13982252\n",
      "Iteration 51, loss = 1.13849011\n",
      "Iteration 52, loss = 1.13717931\n",
      "Iteration 53, loss = 1.13590067\n",
      "Iteration 54, loss = 1.13471115\n",
      "Iteration 55, loss = 1.13362208\n",
      "Iteration 56, loss = 1.13210712\n",
      "Iteration 57, loss = 1.13094481\n",
      "Iteration 58, loss = 1.12976965\n",
      "Iteration 59, loss = 1.12883168\n",
      "Iteration 60, loss = 1.12768493\n",
      "Iteration 61, loss = 1.12652316\n",
      "Iteration 62, loss = 1.12539036\n",
      "Iteration 63, loss = 1.12452339\n",
      "Iteration 64, loss = 1.12304162\n",
      "Iteration 65, loss = 1.12202532\n",
      "Iteration 66, loss = 1.12099639\n",
      "Iteration 67, loss = 1.11994606\n",
      "Iteration 68, loss = 1.11908530\n",
      "Iteration 69, loss = 1.11816273\n",
      "Iteration 70, loss = 1.11707542\n",
      "Iteration 71, loss = 1.11615279\n",
      "Iteration 72, loss = 1.11501043\n",
      "Iteration 73, loss = 1.11408118\n",
      "Iteration 74, loss = 1.11340005\n",
      "Iteration 75, loss = 1.11252678\n",
      "Iteration 76, loss = 1.11140372\n",
      "Iteration 77, loss = 1.11072695\n",
      "Iteration 78, loss = 1.10981419\n",
      "Iteration 79, loss = 1.10900917\n",
      "Iteration 80, loss = 1.10808753\n",
      "Iteration 81, loss = 1.10722416\n",
      "Iteration 82, loss = 1.10650826\n",
      "Iteration 83, loss = 1.10573628\n",
      "Iteration 84, loss = 1.10489628\n",
      "Iteration 85, loss = 1.10428797\n",
      "Iteration 86, loss = 1.10359373\n",
      "Iteration 87, loss = 1.10293248\n",
      "Iteration 88, loss = 1.10183855\n",
      "Iteration 89, loss = 1.10155432\n",
      "Iteration 90, loss = 1.10054645\n",
      "Iteration 91, loss = 1.09993169\n",
      "Iteration 92, loss = 1.09926913\n",
      "Iteration 93, loss = 1.09868532\n",
      "Iteration 94, loss = 1.09794506\n",
      "Iteration 95, loss = 1.09725257\n",
      "Iteration 96, loss = 1.09661535\n",
      "Iteration 97, loss = 1.09628142\n",
      "Iteration 98, loss = 1.09535977\n",
      "Iteration 99, loss = 1.09473611\n",
      "Iteration 100, loss = 1.09406605\n",
      "Iteration 101, loss = 1.09356483\n",
      "Iteration 102, loss = 1.09321538\n",
      "Iteration 103, loss = 1.09243863\n",
      "Iteration 104, loss = 1.09194545\n",
      "Iteration 105, loss = 1.09118424\n",
      "Iteration 106, loss = 1.09058185\n",
      "Iteration 107, loss = 1.09005877\n",
      "Iteration 108, loss = 1.08968295\n",
      "Iteration 109, loss = 1.08922379\n",
      "Iteration 110, loss = 1.08869055\n",
      "Iteration 111, loss = 1.08792558\n",
      "Iteration 112, loss = 1.08757396\n",
      "Iteration 113, loss = 1.08713979\n",
      "Iteration 114, loss = 1.08647504\n",
      "Iteration 115, loss = 1.08614719\n",
      "Iteration 116, loss = 1.08555477\n",
      "Iteration 117, loss = 1.08497027\n",
      "Iteration 118, loss = 1.08479061\n",
      "Iteration 119, loss = 1.08416595\n",
      "Iteration 120, loss = 1.08372732\n",
      "Iteration 121, loss = 1.08330277\n",
      "Iteration 122, loss = 1.08281889\n",
      "Iteration 123, loss = 1.08247960\n",
      "Iteration 124, loss = 1.08212960\n",
      "Iteration 125, loss = 1.08189937\n",
      "Iteration 126, loss = 1.08140670\n",
      "Iteration 127, loss = 1.08095222\n",
      "Iteration 128, loss = 1.08010862\n",
      "Iteration 129, loss = 1.07978300\n",
      "Iteration 130, loss = 1.07946667\n",
      "Iteration 131, loss = 1.07908674\n",
      "Iteration 132, loss = 1.07885278\n",
      "Iteration 133, loss = 1.07831234\n",
      "Iteration 134, loss = 1.07791530\n",
      "Iteration 135, loss = 1.07748841\n",
      "Iteration 136, loss = 1.07702511\n",
      "Iteration 137, loss = 1.07669996\n",
      "Iteration 138, loss = 1.07635783\n",
      "Iteration 139, loss = 1.07592187\n",
      "Iteration 140, loss = 1.07596711\n",
      "Iteration 141, loss = 1.07554537\n",
      "Iteration 142, loss = 1.07529802\n",
      "Iteration 143, loss = 1.07478281\n",
      "Iteration 144, loss = 1.07432057\n",
      "Iteration 145, loss = 1.07399312\n",
      "Iteration 146, loss = 1.07369672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 147, loss = 1.07338641\n",
      "Iteration 148, loss = 1.07299610\n",
      "Iteration 149, loss = 1.07259116\n",
      "Iteration 150, loss = 1.07224273\n",
      "Iteration 151, loss = 1.07229031\n",
      "Iteration 152, loss = 1.07194865\n",
      "Iteration 153, loss = 1.07121870\n",
      "Iteration 154, loss = 1.07099776\n",
      "Iteration 155, loss = 1.07077948\n",
      "Iteration 156, loss = 1.07054682\n",
      "Iteration 157, loss = 1.07026259\n",
      "Iteration 158, loss = 1.06989198\n",
      "Iteration 159, loss = 1.06955632\n",
      "Iteration 160, loss = 1.06929865\n",
      "Iteration 161, loss = 1.06910477\n",
      "Iteration 162, loss = 1.06878813\n",
      "Iteration 163, loss = 1.06851587\n",
      "Iteration 164, loss = 1.06817569\n",
      "Iteration 165, loss = 1.06802464\n",
      "Iteration 166, loss = 1.06770460\n",
      "Iteration 167, loss = 1.06769632\n",
      "Iteration 168, loss = 1.06735298\n",
      "Iteration 169, loss = 1.06669309\n",
      "Iteration 170, loss = 1.06650619\n",
      "Iteration 171, loss = 1.06646415\n",
      "Iteration 172, loss = 1.06615328\n",
      "Iteration 173, loss = 1.06584059\n",
      "Iteration 174, loss = 1.06549840\n",
      "Iteration 175, loss = 1.06538232\n",
      "Iteration 176, loss = 1.06510018\n",
      "Iteration 177, loss = 1.06481560\n",
      "Iteration 178, loss = 1.06453050\n",
      "Iteration 179, loss = 1.06463013\n",
      "Iteration 180, loss = 1.06415756\n",
      "Iteration 181, loss = 1.06381768\n",
      "Iteration 182, loss = 1.06431304\n",
      "Iteration 183, loss = 1.06375666\n",
      "Iteration 184, loss = 1.06340026\n",
      "Iteration 185, loss = 1.06305771\n",
      "Iteration 186, loss = 1.06289469\n",
      "Iteration 187, loss = 1.06251541\n",
      "Iteration 188, loss = 1.06231271\n",
      "Iteration 189, loss = 1.06205791\n",
      "Iteration 190, loss = 1.06204239\n",
      "Iteration 191, loss = 1.06168831\n",
      "Iteration 192, loss = 1.06142895\n",
      "Iteration 193, loss = 1.06145134\n",
      "Iteration 194, loss = 1.06138965\n",
      "Iteration 195, loss = 1.06099818\n",
      "Iteration 196, loss = 1.06088923\n",
      "Iteration 197, loss = 1.06042222\n",
      "Iteration 198, loss = 1.06041560\n",
      "Iteration 199, loss = 1.06009043\n",
      "Iteration 200, loss = 1.06007547\n",
      "Iteration 201, loss = 1.05955390\n",
      "Iteration 202, loss = 1.05980212\n",
      "Iteration 203, loss = 1.05924354\n",
      "Iteration 204, loss = 1.05919400\n",
      "Iteration 205, loss = 1.05955403\n",
      "Iteration 206, loss = 1.05897553\n",
      "Iteration 207, loss = 1.05879848\n",
      "Iteration 208, loss = 1.05853292\n",
      "Iteration 209, loss = 1.05812028\n",
      "Iteration 210, loss = 1.05810842\n",
      "Iteration 211, loss = 1.05802774\n",
      "Iteration 212, loss = 1.05769043\n",
      "Iteration 213, loss = 1.05735992\n",
      "Iteration 214, loss = 1.05727348\n",
      "Iteration 215, loss = 1.05799365\n",
      "Iteration 216, loss = 1.05704288\n",
      "Iteration 217, loss = 1.05674539\n",
      "Iteration 218, loss = 1.05678884\n",
      "Iteration 219, loss = 1.05640086\n",
      "Iteration 220, loss = 1.05627928\n",
      "Iteration 221, loss = 1.05614093\n",
      "Iteration 222, loss = 1.05613645\n",
      "Iteration 223, loss = 1.05579188\n",
      "Iteration 224, loss = 1.05590000\n",
      "Iteration 225, loss = 1.05551377\n",
      "Iteration 226, loss = 1.05525964\n",
      "Iteration 227, loss = 1.05524850\n",
      "Iteration 228, loss = 1.05508090\n",
      "Iteration 229, loss = 1.05520966\n",
      "Iteration 230, loss = 1.05489722\n",
      "Iteration 231, loss = 1.05445672\n",
      "Iteration 232, loss = 1.05425161\n",
      "Iteration 233, loss = 1.05410038\n",
      "Iteration 234, loss = 1.05418021\n",
      "Iteration 235, loss = 1.05394100\n",
      "Iteration 236, loss = 1.05369096\n",
      "Iteration 237, loss = 1.05368449\n",
      "Iteration 238, loss = 1.05360590\n",
      "Iteration 239, loss = 1.05314154\n",
      "Iteration 240, loss = 1.05311770\n",
      "Iteration 241, loss = 1.05295624\n",
      "Iteration 242, loss = 1.05277224\n",
      "Iteration 243, loss = 1.05252918\n",
      "Iteration 244, loss = 1.05259038\n",
      "Iteration 245, loss = 1.05220098\n",
      "Iteration 246, loss = 1.05232913\n",
      "Iteration 247, loss = 1.05203450\n",
      "Iteration 248, loss = 1.05171169\n",
      "Iteration 249, loss = 1.05161049\n",
      "Iteration 250, loss = 1.05170245\n",
      "Iteration 251, loss = 1.05153364\n",
      "Iteration 252, loss = 1.05139168\n",
      "Iteration 253, loss = 1.05102380\n",
      "Iteration 254, loss = 1.05100309\n",
      "Iteration 255, loss = 1.05096289\n",
      "Iteration 256, loss = 1.05090732\n",
      "Iteration 257, loss = 1.05048461\n",
      "Iteration 258, loss = 1.05029333\n",
      "Iteration 259, loss = 1.05035740\n",
      "Iteration 260, loss = 1.05014015\n",
      "Iteration 261, loss = 1.05006447\n",
      "Iteration 262, loss = 1.04995528\n",
      "Iteration 263, loss = 1.04972067\n",
      "Iteration 264, loss = 1.04968789\n",
      "Iteration 265, loss = 1.04933442\n",
      "Iteration 266, loss = 1.04946998\n",
      "Iteration 267, loss = 1.04923486\n",
      "Iteration 268, loss = 1.04909013\n",
      "Iteration 269, loss = 1.04894973\n",
      "Iteration 270, loss = 1.04870295\n",
      "Iteration 271, loss = 1.04868969\n",
      "Iteration 272, loss = 1.04850341\n",
      "Iteration 273, loss = 1.04868322\n",
      "Iteration 274, loss = 1.04876767\n",
      "Iteration 275, loss = 1.04875281\n",
      "Iteration 276, loss = 1.04800252\n",
      "Iteration 277, loss = 1.04790570\n",
      "Iteration 278, loss = 1.04776673\n",
      "Iteration 279, loss = 1.04819276\n",
      "Iteration 280, loss = 1.04752358\n",
      "Iteration 281, loss = 1.04749008\n",
      "Iteration 282, loss = 1.04715997\n",
      "Iteration 283, loss = 1.04731932\n",
      "Iteration 284, loss = 1.04733336\n",
      "Iteration 285, loss = 1.04707684\n",
      "Iteration 286, loss = 1.04702445\n",
      "Iteration 287, loss = 1.04648228\n",
      "Iteration 288, loss = 1.04692337\n",
      "Iteration 289, loss = 1.04622876\n",
      "Iteration 290, loss = 1.04668299\n",
      "Iteration 291, loss = 1.04605882\n",
      "Iteration 292, loss = 1.04602538\n",
      "Iteration 293, loss = 1.04599206\n",
      "Iteration 294, loss = 1.04570434\n",
      "Iteration 295, loss = 1.04573225\n",
      "Iteration 296, loss = 1.04560125\n",
      "Iteration 297, loss = 1.04538501\n",
      "Iteration 298, loss = 1.04564207\n",
      "Iteration 299, loss = 1.04516739\n",
      "Iteration 300, loss = 1.04534575\n",
      "Iteration 1, loss = 1.49807517\n",
      "Iteration 2, loss = 1.42189236\n",
      "Iteration 3, loss = 1.34026881\n",
      "Iteration 4, loss = 1.28253382\n",
      "Iteration 5, loss = 1.24947911\n",
      "Iteration 6, loss = 1.23011747\n",
      "Iteration 7, loss = 1.22057111\n",
      "Iteration 8, loss = 1.21131264\n",
      "Iteration 9, loss = 1.20460531\n",
      "Iteration 10, loss = 1.20035572\n",
      "Iteration 11, loss = 1.19590278\n",
      "Iteration 12, loss = 1.19214745\n",
      "Iteration 13, loss = 1.18914157\n",
      "Iteration 14, loss = 1.18637494\n",
      "Iteration 15, loss = 1.18404079\n",
      "Iteration 16, loss = 1.18130405\n",
      "Iteration 17, loss = 1.17888284\n",
      "Iteration 18, loss = 1.17649243\n",
      "Iteration 19, loss = 1.17400746\n",
      "Iteration 20, loss = 1.17153964\n",
      "Iteration 21, loss = 1.16923138\n",
      "Iteration 22, loss = 1.16696224\n",
      "Iteration 23, loss = 1.16453613\n",
      "Iteration 24, loss = 1.16229642\n",
      "Iteration 25, loss = 1.15986241\n",
      "Iteration 26, loss = 1.15738219\n",
      "Iteration 27, loss = 1.15514071\n",
      "Iteration 28, loss = 1.15290489\n",
      "Iteration 29, loss = 1.15041634\n",
      "Iteration 30, loss = 1.14819239\n",
      "Iteration 31, loss = 1.14587743\n",
      "Iteration 32, loss = 1.14365922\n",
      "Iteration 33, loss = 1.14154958\n",
      "Iteration 34, loss = 1.13959070\n",
      "Iteration 35, loss = 1.13700201\n",
      "Iteration 36, loss = 1.13490860\n",
      "Iteration 37, loss = 1.13259729\n",
      "Iteration 38, loss = 1.13054747\n",
      "Iteration 39, loss = 1.12855966\n",
      "Iteration 40, loss = 1.12644935\n",
      "Iteration 41, loss = 1.12450660\n",
      "Iteration 42, loss = 1.12244593\n",
      "Iteration 43, loss = 1.12053680\n",
      "Iteration 44, loss = 1.11873013\n",
      "Iteration 45, loss = 1.11680025\n",
      "Iteration 46, loss = 1.11516635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47, loss = 1.11323901\n",
      "Iteration 48, loss = 1.11189668\n",
      "Iteration 49, loss = 1.10976950\n",
      "Iteration 50, loss = 1.10814222\n",
      "Iteration 51, loss = 1.10632653\n",
      "Iteration 52, loss = 1.10492890\n",
      "Iteration 53, loss = 1.10325056\n",
      "Iteration 54, loss = 1.10164049\n",
      "Iteration 55, loss = 1.10028564\n",
      "Iteration 56, loss = 1.09863323\n",
      "Iteration 57, loss = 1.09727083\n",
      "Iteration 58, loss = 1.09572172\n",
      "Iteration 59, loss = 1.09440372\n",
      "Iteration 60, loss = 1.09352364\n",
      "Iteration 61, loss = 1.09180537\n",
      "Iteration 62, loss = 1.09043338\n",
      "Iteration 63, loss = 1.08896739\n",
      "Iteration 64, loss = 1.08792410\n",
      "Iteration 65, loss = 1.08647909\n",
      "Iteration 66, loss = 1.08523542\n",
      "Iteration 67, loss = 1.08429361\n",
      "Iteration 68, loss = 1.08300171\n",
      "Iteration 69, loss = 1.08195146\n",
      "Iteration 70, loss = 1.08076961\n",
      "Iteration 71, loss = 1.07964249\n",
      "Iteration 72, loss = 1.07863600\n",
      "Iteration 73, loss = 1.07764179\n",
      "Iteration 74, loss = 1.07664894\n",
      "Iteration 75, loss = 1.07553062\n",
      "Iteration 76, loss = 1.07465482\n",
      "Iteration 77, loss = 1.07373274\n",
      "Iteration 78, loss = 1.07267282\n",
      "Iteration 79, loss = 1.07177092\n",
      "Iteration 80, loss = 1.07125957\n",
      "Iteration 81, loss = 1.07014525\n",
      "Iteration 82, loss = 1.06938552\n",
      "Iteration 83, loss = 1.06843274\n",
      "Iteration 84, loss = 1.06783804\n",
      "Iteration 85, loss = 1.06693387\n",
      "Iteration 86, loss = 1.06576176\n",
      "Iteration 87, loss = 1.06524972\n",
      "Iteration 88, loss = 1.06413055\n",
      "Iteration 89, loss = 1.06357361\n",
      "Iteration 90, loss = 1.06304314\n",
      "Iteration 91, loss = 1.06199438\n",
      "Iteration 92, loss = 1.06123150\n",
      "Iteration 93, loss = 1.06066506\n",
      "Iteration 94, loss = 1.05979770\n",
      "Iteration 95, loss = 1.05914475\n",
      "Iteration 96, loss = 1.05835927\n",
      "Iteration 97, loss = 1.05769492\n",
      "Iteration 98, loss = 1.05701908\n",
      "Iteration 99, loss = 1.05637307\n",
      "Iteration 100, loss = 1.05578366\n",
      "Iteration 101, loss = 1.05500249\n",
      "Iteration 102, loss = 1.05466340\n",
      "Iteration 103, loss = 1.05369853\n",
      "Iteration 104, loss = 1.05321081\n",
      "Iteration 105, loss = 1.05283878\n",
      "Iteration 106, loss = 1.05196186\n",
      "Iteration 107, loss = 1.05142639\n",
      "Iteration 108, loss = 1.05085543\n",
      "Iteration 109, loss = 1.05042896\n",
      "Iteration 110, loss = 1.04977683\n",
      "Iteration 111, loss = 1.04935082\n",
      "Iteration 112, loss = 1.04856065\n",
      "Iteration 113, loss = 1.04807730\n",
      "Iteration 114, loss = 1.04750950\n",
      "Iteration 115, loss = 1.04721838\n",
      "Iteration 116, loss = 1.04644860\n",
      "Iteration 117, loss = 1.04608059\n",
      "Iteration 118, loss = 1.04547481\n",
      "Iteration 119, loss = 1.04497842\n",
      "Iteration 120, loss = 1.04461814\n",
      "Iteration 121, loss = 1.04398921\n",
      "Iteration 122, loss = 1.04331669\n",
      "Iteration 123, loss = 1.04311572\n",
      "Iteration 124, loss = 1.04276254\n",
      "Iteration 125, loss = 1.04248435\n",
      "Iteration 126, loss = 1.04166792\n",
      "Iteration 127, loss = 1.04108000\n",
      "Iteration 128, loss = 1.04097684\n",
      "Iteration 129, loss = 1.04028625\n",
      "Iteration 130, loss = 1.03980091\n",
      "Iteration 131, loss = 1.03936536\n",
      "Iteration 132, loss = 1.03902790\n",
      "Iteration 133, loss = 1.03854171\n",
      "Iteration 134, loss = 1.03818365\n",
      "Iteration 135, loss = 1.03760000\n",
      "Iteration 136, loss = 1.03752303\n",
      "Iteration 137, loss = 1.03688994\n",
      "Iteration 138, loss = 1.03653145\n",
      "Iteration 139, loss = 1.03628151\n",
      "Iteration 140, loss = 1.03568310\n",
      "Iteration 141, loss = 1.03538044\n",
      "Iteration 142, loss = 1.03523716\n",
      "Iteration 143, loss = 1.03466415\n",
      "Iteration 144, loss = 1.03421997\n",
      "Iteration 145, loss = 1.03396777\n",
      "Iteration 146, loss = 1.03354286\n",
      "Iteration 147, loss = 1.03317117\n",
      "Iteration 148, loss = 1.03283634\n",
      "Iteration 149, loss = 1.03250731\n",
      "Iteration 150, loss = 1.03208558\n",
      "Iteration 151, loss = 1.03176953\n",
      "Iteration 152, loss = 1.03148183\n",
      "Iteration 153, loss = 1.03107278\n",
      "Iteration 154, loss = 1.03078986\n",
      "Iteration 155, loss = 1.03039405\n",
      "Iteration 156, loss = 1.03052369\n",
      "Iteration 157, loss = 1.02981182\n",
      "Iteration 158, loss = 1.02959392\n",
      "Iteration 159, loss = 1.02894432\n",
      "Iteration 160, loss = 1.02873527\n",
      "Iteration 161, loss = 1.02848125\n",
      "Iteration 162, loss = 1.02798872\n",
      "Iteration 163, loss = 1.02769179\n",
      "Iteration 164, loss = 1.02764741\n",
      "Iteration 165, loss = 1.02719104\n",
      "Iteration 166, loss = 1.02686696\n",
      "Iteration 167, loss = 1.02681980\n",
      "Iteration 168, loss = 1.02628668\n",
      "Iteration 169, loss = 1.02606112\n",
      "Iteration 170, loss = 1.02578901\n",
      "Iteration 171, loss = 1.02557367\n",
      "Iteration 172, loss = 1.02512904\n",
      "Iteration 173, loss = 1.02491462\n",
      "Iteration 174, loss = 1.02487914\n",
      "Iteration 175, loss = 1.02434762\n",
      "Iteration 176, loss = 1.02418505\n",
      "Iteration 177, loss = 1.02396765\n",
      "Iteration 178, loss = 1.02378385\n",
      "Iteration 179, loss = 1.02330366\n",
      "Iteration 180, loss = 1.02295283\n",
      "Iteration 181, loss = 1.02309896\n",
      "Iteration 182, loss = 1.02279553\n",
      "Iteration 183, loss = 1.02250965\n",
      "Iteration 184, loss = 1.02207796\n",
      "Iteration 185, loss = 1.02221547\n",
      "Iteration 186, loss = 1.02180581\n",
      "Iteration 187, loss = 1.02156124\n",
      "Iteration 188, loss = 1.02121395\n",
      "Iteration 189, loss = 1.02080256\n",
      "Iteration 190, loss = 1.02076106\n",
      "Iteration 191, loss = 1.02035582\n",
      "Iteration 192, loss = 1.02015482\n",
      "Iteration 193, loss = 1.02018918\n",
      "Iteration 194, loss = 1.01980908\n",
      "Iteration 195, loss = 1.01973532\n",
      "Iteration 196, loss = 1.01929080\n",
      "Iteration 197, loss = 1.01961558\n",
      "Iteration 198, loss = 1.01892209\n",
      "Iteration 199, loss = 1.01920658\n",
      "Iteration 200, loss = 1.01854167\n",
      "Iteration 201, loss = 1.01833376\n",
      "Iteration 202, loss = 1.01820056\n",
      "Iteration 203, loss = 1.01826934\n",
      "Iteration 204, loss = 1.01800831\n",
      "Iteration 205, loss = 1.01803908\n",
      "Iteration 206, loss = 1.01710823\n",
      "Iteration 207, loss = 1.01711133\n",
      "Iteration 208, loss = 1.01703603\n",
      "Iteration 209, loss = 1.01728325\n",
      "Iteration 210, loss = 1.01667976\n",
      "Iteration 211, loss = 1.01623560\n",
      "Iteration 212, loss = 1.01612338\n",
      "Iteration 213, loss = 1.01597864\n",
      "Iteration 214, loss = 1.01572732\n",
      "Iteration 215, loss = 1.01607463\n",
      "Iteration 216, loss = 1.01553789\n",
      "Iteration 217, loss = 1.01500297\n",
      "Iteration 218, loss = 1.01520583\n",
      "Iteration 219, loss = 1.01472496\n",
      "Iteration 220, loss = 1.01456525\n",
      "Iteration 221, loss = 1.01429182\n",
      "Iteration 222, loss = 1.01432957\n",
      "Iteration 223, loss = 1.01416640\n",
      "Iteration 224, loss = 1.01451010\n",
      "Iteration 225, loss = 1.01369836\n",
      "Iteration 226, loss = 1.01342414\n",
      "Iteration 227, loss = 1.01313339\n",
      "Iteration 228, loss = 1.01302588\n",
      "Iteration 229, loss = 1.01292672\n",
      "Iteration 230, loss = 1.01300641\n",
      "Iteration 231, loss = 1.01258567\n",
      "Iteration 232, loss = 1.01233705\n",
      "Iteration 233, loss = 1.01253420\n",
      "Iteration 234, loss = 1.01215602\n",
      "Iteration 235, loss = 1.01187444\n",
      "Iteration 236, loss = 1.01161030\n",
      "Iteration 237, loss = 1.01157780\n",
      "Iteration 238, loss = 1.01133888\n",
      "Iteration 239, loss = 1.01130783\n",
      "Iteration 240, loss = 1.01105690\n",
      "Iteration 241, loss = 1.01089374\n",
      "Iteration 242, loss = 1.01058223\n",
      "Iteration 243, loss = 1.01058131\n",
      "Iteration 244, loss = 1.01031344\n",
      "Iteration 245, loss = 1.01029673\n",
      "Iteration 246, loss = 1.01045606\n",
      "Iteration 247, loss = 1.00987826\n",
      "Iteration 248, loss = 1.01028515\n",
      "Iteration 249, loss = 1.00974598\n",
      "Iteration 250, loss = 1.00955772\n",
      "Iteration 251, loss = 1.00921612\n",
      "Iteration 252, loss = 1.00925133\n",
      "Iteration 253, loss = 1.00900848\n",
      "Iteration 254, loss = 1.00880643\n",
      "Iteration 255, loss = 1.00883977\n",
      "Iteration 256, loss = 1.00845297\n",
      "Iteration 257, loss = 1.00911640\n",
      "Iteration 258, loss = 1.00880962\n",
      "Iteration 259, loss = 1.00817083\n",
      "Iteration 260, loss = 1.00791392\n",
      "Iteration 261, loss = 1.00785984\n",
      "Iteration 262, loss = 1.00781403\n",
      "Iteration 263, loss = 1.00762294\n",
      "Iteration 264, loss = 1.00778594\n",
      "Iteration 265, loss = 1.00773508\n",
      "Iteration 266, loss = 1.00701173\n",
      "Iteration 267, loss = 1.00681894\n",
      "Iteration 268, loss = 1.00699803\n",
      "Iteration 269, loss = 1.00675504\n",
      "Iteration 270, loss = 1.00642642\n",
      "Iteration 271, loss = 1.00649221\n",
      "Iteration 272, loss = 1.00624524\n",
      "Iteration 273, loss = 1.00608091\n",
      "Iteration 274, loss = 1.00596829\n",
      "Iteration 275, loss = 1.00596373\n",
      "Iteration 276, loss = 1.00577512\n",
      "Iteration 277, loss = 1.00577428\n",
      "Iteration 278, loss = 1.00533546\n",
      "Iteration 279, loss = 1.00543299\n",
      "Iteration 280, loss = 1.00542115\n",
      "Iteration 281, loss = 1.00509239\n",
      "Iteration 282, loss = 1.00479970\n",
      "Iteration 283, loss = 1.00472674\n",
      "Iteration 284, loss = 1.00460718\n",
      "Iteration 285, loss = 1.00443848\n",
      "Iteration 286, loss = 1.00428324\n",
      "Iteration 287, loss = 1.00408556\n",
      "Iteration 288, loss = 1.00398561\n",
      "Iteration 289, loss = 1.00405378\n",
      "Iteration 290, loss = 1.00433009\n",
      "Iteration 291, loss = 1.00359108\n",
      "Iteration 292, loss = 1.00366206\n",
      "Iteration 293, loss = 1.00356442\n",
      "Iteration 294, loss = 1.00329068\n",
      "Iteration 295, loss = 1.00334667\n",
      "Iteration 296, loss = 1.00299876\n",
      "Iteration 297, loss = 1.00289992\n",
      "Iteration 298, loss = 1.00325379\n",
      "Iteration 299, loss = 1.00255699\n",
      "Iteration 300, loss = 1.00269152\n",
      "Iteration 1, loss = 11.14936199\n",
      "Iteration 2, loss = 1.50582905\n",
      "Iteration 3, loss = 1.27998454\n",
      "Iteration 4, loss = 1.25078880\n",
      "Iteration 5, loss = 1.24822459\n",
      "Iteration 6, loss = 1.24987482\n",
      "Iteration 7, loss = 1.25257571\n",
      "Iteration 8, loss = 1.25546561\n",
      "Iteration 9, loss = 1.24545958\n",
      "Iteration 10, loss = 1.25084855\n",
      "Iteration 11, loss = 1.25223439\n",
      "Iteration 12, loss = 1.24784159\n",
      "Iteration 13, loss = 1.25224773\n",
      "Iteration 14, loss = 1.24340789\n",
      "Iteration 15, loss = 1.24774694\n",
      "Iteration 16, loss = 1.24313022\n",
      "Iteration 17, loss = 1.24097861\n",
      "Iteration 18, loss = 1.24423703\n",
      "Iteration 19, loss = 1.24113315\n",
      "Iteration 20, loss = 1.24799952\n",
      "Iteration 21, loss = 1.24023395\n",
      "Iteration 22, loss = 1.24069864\n",
      "Iteration 23, loss = 1.23727807\n",
      "Iteration 24, loss = 1.23907916\n",
      "Iteration 25, loss = 1.23769386\n",
      "Iteration 26, loss = 1.24770867\n",
      "Iteration 27, loss = 1.23908269\n",
      "Iteration 28, loss = 1.23659692\n",
      "Iteration 29, loss = 1.23477750\n",
      "Iteration 30, loss = 1.24419515\n",
      "Iteration 31, loss = 1.26552632\n",
      "Iteration 32, loss = 1.23354135\n",
      "Iteration 33, loss = 1.23548321\n",
      "Iteration 34, loss = 1.23702292\n",
      "Iteration 35, loss = 1.23351557\n",
      "Iteration 36, loss = 1.23811509\n",
      "Iteration 37, loss = 1.23238570\n",
      "Iteration 38, loss = 1.23236190\n",
      "Iteration 39, loss = 1.23530774\n",
      "Iteration 40, loss = 1.23152532\n",
      "Iteration 41, loss = 1.24722650\n",
      "Iteration 42, loss = 1.23009184\n",
      "Iteration 43, loss = 1.23780052\n",
      "Iteration 44, loss = 1.23280336\n",
      "Iteration 45, loss = 1.23663597\n",
      "Iteration 46, loss = 1.23759432\n",
      "Iteration 47, loss = 1.23590851\n",
      "Iteration 48, loss = 1.22511542\n",
      "Iteration 49, loss = 1.22973226\n",
      "Iteration 50, loss = 1.22488350\n",
      "Iteration 51, loss = 1.22880051\n",
      "Iteration 52, loss = 1.23619943\n",
      "Iteration 53, loss = 1.25721589\n",
      "Iteration 54, loss = 1.23033706\n",
      "Iteration 55, loss = 1.24315972\n",
      "Iteration 56, loss = 1.22823840\n",
      "Iteration 57, loss = 1.23421545\n",
      "Iteration 58, loss = 1.23507971\n",
      "Iteration 59, loss = 1.23886090\n",
      "Iteration 60, loss = 1.23732968\n",
      "Iteration 61, loss = 1.22870140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#looping over different hyperparameters with 'relu' activator with num_mneurons = 128\n",
    "\n",
    "num_neurons = 128 \n",
    "activator = 'relu'\n",
    "\n",
    "#activators = ['relu', 'tanh', 'logistic', 'identity']\n",
    "#lrate_type = [ 'constantâ€™, â€˜invscalingâ€™, â€˜adaptiveâ€™ ]\n",
    "lrate_type = ['constant', 'invscaling', 'adaptive']\n",
    "lrate_init = [0.0001, 0.001, 0.01]\n",
    "max_iterations = [100, 200, 300]\n",
    "\n",
    "\n",
    "#variables to comapare results\n",
    "cv_mean_act = []\n",
    "cv_std_act = []\n",
    "test_scores_act = []\n",
    "\n",
    "for i in range(len(lrate_init)):\n",
    "    row_scores = []\n",
    "    for j in range(len(max_iterations)):\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(num_neurons,), # hidden layer with num_neurons number of neurons\n",
    "                            activation = activator,  # taking different activator types as input\n",
    "                            solver='sgd',  # default is Adam\n",
    "                            #earning_rate = lrate_type[i],\n",
    "                            alpha=0.01,  # regularization parameter, default is 0.0001 (increase up to 1.0 for stronger regularization)\n",
    "                            learning_rate_init = lrate_init[i] ,  # initial step-size for updating the weights, default is 0.001\n",
    "                            max_iter = max_iterations[j],  # number of epochs, default=200\n",
    "                            random_state=seed,\n",
    "                            verbose=1, \n",
    "                            )\n",
    "        scores = cross_val_score(mlp, X_train_scaled, y_train, cv=5, scoring=scoring_method)\n",
    "        cv_mean_act.append(scores.mean())\n",
    "        cv_std_act.append(scores.std())\n",
    "    \n",
    "        #train the classifier and tesing it\n",
    "        mlp.fit(X_train, y_train)\n",
    "        test_acc = accuracy_score(y_test, mlp.predict(X_test))\n",
    "        row_scores.append(test_acc)\n",
    "    test_scores_act.append(row_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b26bd5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44344703770197486, 0.44344703770197486, 0.44344703770197486], [0.45370607848166195, 0.45370607848166195, 0.45370607848166195], [0.4467812259553732, 0.4467812259553732, 0.4467812259553732]]\n",
      "Learning rate: 0.0001, max_iterations: 100, score: 0.44344703770197486 \n",
      "Learning rate: 0.0001, max_iterations: 200, score: 0.44344703770197486 \n",
      "Learning rate: 0.0001, max_iterations: 300, score: 0.44344703770197486 \n",
      "Learning rate: 0.001, max_iterations: 100, score: 0.45370607848166195 \n",
      "Learning rate: 0.001, max_iterations: 200, score: 0.45370607848166195 \n",
      "Learning rate: 0.001, max_iterations: 300, score: 0.45370607848166195 \n",
      "Learning rate: 0.01, max_iterations: 100, score: 0.4467812259553732 \n",
      "Learning rate: 0.01, max_iterations: 200, score: 0.4467812259553732 \n",
      "Learning rate: 0.01, max_iterations: 300, score: 0.4467812259553732 \n"
     ]
    }
   ],
   "source": [
    "print(f'{test_scores_act}')\n",
    "\n",
    "max_score = 0 \n",
    " \n",
    "for i in range(len(lrate_init)):\n",
    "    for j in range(len(max_iterations)):\n",
    "        if test_scores_act[i][j] > max_score:\n",
    "            max_score = test_scores_act[i][j]\n",
    "            a = i\n",
    "            b = j\n",
    "        print(f'Learning rate: {lrate_init[i]}, max_iterations: {max_iterations[j]}, score: {test_scores_act[i][j]} ')\n",
    "#print(f'{max_score}, {lrate_init[a]}, {max_iterations[b]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67330779",
   "metadata": {},
   "source": [
    "# The model performance changes with different hyperparameters. For the selected hyperparameters, the best performance was observed for Learning rate: 0.001 and the change in max_iterations did not affect the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca08cb",
   "metadata": {},
   "source": [
    "# 5. Now use cross-validation to train and tune an MLPClassifier model that adds a second hidden layer to the best model from parts 2 - 4. Report and discuss how model performance changes across the range of new hyperparameter values (i.e., number of neurons for the second layer) and compare it to the single layer model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b6723571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.35207746\n",
      "Iteration 2, loss = 1.20114026\n",
      "Iteration 3, loss = 1.19430311\n",
      "Iteration 4, loss = 1.17601712\n",
      "Iteration 5, loss = 1.14028711\n",
      "Iteration 6, loss = 1.12092398\n",
      "Iteration 7, loss = 1.10877618\n",
      "Iteration 8, loss = 1.09239377\n",
      "Iteration 9, loss = 1.08328871\n",
      "Iteration 10, loss = 1.07707000\n",
      "Iteration 11, loss = 1.06316849\n",
      "Iteration 12, loss = 1.06006602\n",
      "Iteration 13, loss = 1.05095738\n",
      "Iteration 14, loss = 1.05052041\n",
      "Iteration 15, loss = 1.04370316\n",
      "Iteration 16, loss = 1.04083608\n",
      "Iteration 17, loss = 1.03626173\n",
      "Iteration 18, loss = 1.03036468\n",
      "Iteration 19, loss = 1.02984548\n",
      "Iteration 20, loss = 1.02418689\n",
      "Iteration 21, loss = 1.02296479\n",
      "Iteration 22, loss = 1.02000471\n",
      "Iteration 23, loss = 1.01654628\n",
      "Iteration 24, loss = 1.01531598\n",
      "Iteration 25, loss = 1.01205348\n",
      "Iteration 26, loss = 1.01130755\n",
      "Iteration 27, loss = 1.01466938\n",
      "Iteration 28, loss = 1.01074140\n",
      "Iteration 29, loss = 1.00268170\n",
      "Iteration 30, loss = 1.00546369\n",
      "Iteration 31, loss = 1.00016727\n",
      "Iteration 32, loss = 0.99453130\n",
      "Iteration 33, loss = 0.99398237\n",
      "Iteration 34, loss = 0.99276854\n",
      "Iteration 35, loss = 0.99003444\n",
      "Iteration 36, loss = 0.98764007\n",
      "Iteration 37, loss = 0.99022477\n",
      "Iteration 38, loss = 0.98201956\n",
      "Iteration 39, loss = 0.97963855\n",
      "Iteration 40, loss = 0.98220944\n",
      "Iteration 41, loss = 0.98194452\n",
      "Iteration 42, loss = 0.98432619\n",
      "Iteration 43, loss = 0.97291118\n",
      "Iteration 44, loss = 0.97358807\n",
      "Iteration 45, loss = 0.97243180\n",
      "Iteration 46, loss = 0.96654962\n",
      "Iteration 47, loss = 0.96835569\n",
      "Iteration 48, loss = 0.96737409\n",
      "Iteration 49, loss = 0.96718571\n",
      "Iteration 50, loss = 0.96903639\n",
      "Iteration 51, loss = 0.96843788\n",
      "Iteration 52, loss = 0.96874491\n",
      "Iteration 53, loss = 0.96018811\n",
      "Iteration 54, loss = 0.95609602\n",
      "Iteration 55, loss = 0.95910557\n",
      "Iteration 56, loss = 0.95379879\n",
      "Iteration 57, loss = 0.95018985\n",
      "Iteration 58, loss = 0.94786830\n",
      "Iteration 59, loss = 0.94788372\n",
      "Iteration 60, loss = 0.94497950\n",
      "Iteration 61, loss = 0.94297540\n",
      "Iteration 62, loss = 0.94239577\n",
      "Iteration 63, loss = 0.93951311\n",
      "Iteration 64, loss = 0.93917956\n",
      "Iteration 65, loss = 0.93995241\n",
      "Iteration 66, loss = 0.93481990\n",
      "Iteration 67, loss = 0.93921150\n",
      "Iteration 68, loss = 0.93542302\n",
      "Iteration 69, loss = 0.93356180\n",
      "Iteration 70, loss = 0.93136091\n",
      "Iteration 71, loss = 0.92987513\n",
      "Iteration 72, loss = 0.93181745\n",
      "Iteration 73, loss = 0.93446607\n",
      "Iteration 74, loss = 0.93235245\n",
      "Iteration 75, loss = 0.93564186\n",
      "Iteration 76, loss = 0.92927643\n",
      "Iteration 77, loss = 0.92759774\n",
      "Iteration 78, loss = 0.92633615\n",
      "Iteration 79, loss = 0.92412533\n",
      "Iteration 80, loss = 0.92451260\n",
      "Iteration 81, loss = 0.92134469\n",
      "Iteration 82, loss = 0.91948574\n",
      "Iteration 83, loss = 0.92327852\n",
      "Iteration 84, loss = 0.92235840\n",
      "Iteration 85, loss = 0.91928488\n",
      "Iteration 86, loss = 0.91809254\n",
      "Iteration 87, loss = 0.91623708\n",
      "Iteration 88, loss = 0.92037504\n",
      "Iteration 89, loss = 0.91363474\n",
      "Iteration 90, loss = 0.91646982\n",
      "Iteration 91, loss = 0.91557863\n",
      "Iteration 92, loss = 0.91140550\n",
      "Iteration 93, loss = 0.90944691\n",
      "Iteration 94, loss = 0.90953458\n",
      "Iteration 95, loss = 0.90773884\n",
      "Iteration 96, loss = 0.90773991\n",
      "Iteration 97, loss = 0.91011321\n",
      "Iteration 98, loss = 0.91634365\n",
      "Iteration 99, loss = 0.91341610\n",
      "Iteration 100, loss = 0.90311083\n",
      "Iteration 1, loss = 1.35212483\n",
      "Iteration 2, loss = 1.20235876\n",
      "Iteration 3, loss = 1.19282643\n",
      "Iteration 4, loss = 1.16281149\n",
      "Iteration 5, loss = 1.13135987\n",
      "Iteration 6, loss = 1.10978376\n",
      "Iteration 7, loss = 1.09310208\n",
      "Iteration 8, loss = 1.07501313\n",
      "Iteration 9, loss = 1.06447565\n",
      "Iteration 10, loss = 1.05401894\n",
      "Iteration 11, loss = 1.04624623\n",
      "Iteration 12, loss = 1.04315294\n",
      "Iteration 13, loss = 1.03630339\n",
      "Iteration 14, loss = 1.03477197\n",
      "Iteration 15, loss = 1.03366373\n",
      "Iteration 16, loss = 1.03274858\n",
      "Iteration 17, loss = 1.02581615\n",
      "Iteration 18, loss = 1.02073147\n",
      "Iteration 19, loss = 1.01950315\n",
      "Iteration 20, loss = 1.01542007\n",
      "Iteration 21, loss = 1.01272852\n",
      "Iteration 22, loss = 1.01109953\n",
      "Iteration 23, loss = 1.00754484\n",
      "Iteration 24, loss = 1.00546155\n",
      "Iteration 25, loss = 1.00375072\n",
      "Iteration 26, loss = 1.00327256\n",
      "Iteration 27, loss = 1.01270810\n",
      "Iteration 28, loss = 1.00671587\n",
      "Iteration 29, loss = 0.99576961\n",
      "Iteration 30, loss = 1.00254216\n",
      "Iteration 31, loss = 0.99786210\n",
      "Iteration 32, loss = 0.99523307\n",
      "Iteration 33, loss = 0.99084706\n",
      "Iteration 34, loss = 0.98791897\n",
      "Iteration 35, loss = 0.98807156\n",
      "Iteration 36, loss = 0.98440213\n",
      "Iteration 37, loss = 0.98553030\n",
      "Iteration 38, loss = 0.98017560\n",
      "Iteration 39, loss = 0.98153883\n",
      "Iteration 40, loss = 0.97625452\n",
      "Iteration 41, loss = 0.97750596\n",
      "Iteration 42, loss = 0.97634592\n",
      "Iteration 43, loss = 0.97187939\n",
      "Iteration 44, loss = 0.97341656\n",
      "Iteration 45, loss = 0.97061740\n",
      "Iteration 46, loss = 0.96725039\n",
      "Iteration 47, loss = 0.96722803\n",
      "Iteration 48, loss = 0.96175221\n",
      "Iteration 49, loss = 0.96794426\n",
      "Iteration 50, loss = 0.96282765\n",
      "Iteration 51, loss = 0.96201456\n",
      "Iteration 52, loss = 0.96803514\n",
      "Iteration 53, loss = 0.95971894\n",
      "Iteration 54, loss = 0.95837992\n",
      "Iteration 55, loss = 0.95389601\n",
      "Iteration 56, loss = 0.96098548\n",
      "Iteration 57, loss = 0.95407136\n",
      "Iteration 58, loss = 0.95281232\n",
      "Iteration 59, loss = 0.94894422\n",
      "Iteration 60, loss = 0.95053421\n",
      "Iteration 61, loss = 0.94430849\n",
      "Iteration 62, loss = 0.94355727\n",
      "Iteration 63, loss = 0.94518831\n",
      "Iteration 64, loss = 0.94109489\n",
      "Iteration 65, loss = 0.94027445\n",
      "Iteration 66, loss = 0.93890888\n",
      "Iteration 67, loss = 0.93868536\n",
      "Iteration 68, loss = 0.93724608\n",
      "Iteration 69, loss = 0.93422343\n",
      "Iteration 70, loss = 0.93378840\n",
      "Iteration 71, loss = 0.93129875\n",
      "Iteration 72, loss = 0.93372801\n",
      "Iteration 73, loss = 0.93279722\n",
      "Iteration 74, loss = 0.92836948\n",
      "Iteration 75, loss = 0.92928461\n",
      "Iteration 76, loss = 0.93155710\n",
      "Iteration 77, loss = 0.92880859\n",
      "Iteration 78, loss = 0.92564269\n",
      "Iteration 79, loss = 0.92984379\n",
      "Iteration 80, loss = 0.92222440\n",
      "Iteration 81, loss = 0.92891302\n",
      "Iteration 82, loss = 0.92302527\n",
      "Iteration 83, loss = 0.91931136\n",
      "Iteration 84, loss = 0.92001399\n",
      "Iteration 85, loss = 0.91744096\n",
      "Iteration 86, loss = 0.91644000\n",
      "Iteration 87, loss = 0.92075472\n",
      "Iteration 88, loss = 0.91239468\n",
      "Iteration 89, loss = 0.91174463\n",
      "Iteration 90, loss = 0.91375352\n",
      "Iteration 91, loss = 0.91261209\n",
      "Iteration 92, loss = 0.91145155\n",
      "Iteration 93, loss = 0.91027768\n",
      "Iteration 94, loss = 0.90734124\n",
      "Iteration 95, loss = 0.91294071\n",
      "Iteration 96, loss = 0.90623585\n",
      "Iteration 97, loss = 0.90502250\n",
      "Iteration 98, loss = 0.90318049\n",
      "Iteration 99, loss = 0.91103298\n",
      "Iteration 100, loss = 0.90011030\n",
      "Iteration 1, loss = 1.35363575\n",
      "Iteration 2, loss = 1.20396004\n",
      "Iteration 3, loss = 1.18329236\n",
      "Iteration 4, loss = 1.16330954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 1.12945806\n",
      "Iteration 6, loss = 1.10882178\n",
      "Iteration 7, loss = 1.09489350\n",
      "Iteration 8, loss = 1.07733149\n",
      "Iteration 9, loss = 1.07149938\n",
      "Iteration 10, loss = 1.06048251\n",
      "Iteration 11, loss = 1.05232045\n",
      "Iteration 12, loss = 1.05081001\n",
      "Iteration 13, loss = 1.04579321\n",
      "Iteration 14, loss = 1.03739829\n",
      "Iteration 15, loss = 1.03388467\n",
      "Iteration 16, loss = 1.03360993\n",
      "Iteration 17, loss = 1.02714111\n",
      "Iteration 18, loss = 1.02508005\n",
      "Iteration 19, loss = 1.02159185\n",
      "Iteration 20, loss = 1.02473422\n",
      "Iteration 21, loss = 1.01909490\n",
      "Iteration 22, loss = 1.01584336\n",
      "Iteration 23, loss = 1.01741605\n",
      "Iteration 24, loss = 1.02203109\n",
      "Iteration 25, loss = 1.01717853\n",
      "Iteration 26, loss = 1.01344830\n",
      "Iteration 27, loss = 1.01976435\n",
      "Iteration 28, loss = 1.00698228\n",
      "Iteration 29, loss = 1.01241702\n",
      "Iteration 30, loss = 1.00967991\n",
      "Iteration 31, loss = 1.00679880\n",
      "Iteration 32, loss = 1.00997367\n",
      "Iteration 33, loss = 1.00243578\n",
      "Iteration 34, loss = 0.99987389\n",
      "Iteration 35, loss = 0.99965687\n",
      "Iteration 36, loss = 0.99647680\n",
      "Iteration 37, loss = 0.99454777\n",
      "Iteration 38, loss = 0.99452376\n",
      "Iteration 39, loss = 0.99089582\n",
      "Iteration 40, loss = 0.98806858\n",
      "Iteration 41, loss = 0.98816012\n",
      "Iteration 42, loss = 0.99257951\n",
      "Iteration 43, loss = 0.98674116\n",
      "Iteration 44, loss = 0.98431863\n",
      "Iteration 45, loss = 0.98442950\n",
      "Iteration 46, loss = 0.98136087\n",
      "Iteration 47, loss = 0.97545409\n",
      "Iteration 48, loss = 0.97623054\n",
      "Iteration 49, loss = 0.98225763\n",
      "Iteration 50, loss = 0.97313328\n",
      "Iteration 51, loss = 0.97242479\n",
      "Iteration 52, loss = 0.97346397\n",
      "Iteration 53, loss = 0.96850604\n",
      "Iteration 54, loss = 0.96257731\n",
      "Iteration 55, loss = 0.96168415\n",
      "Iteration 56, loss = 0.96176543\n",
      "Iteration 57, loss = 0.95630442\n",
      "Iteration 58, loss = 0.95614052\n",
      "Iteration 59, loss = 0.95646471\n",
      "Iteration 60, loss = 0.95074274\n",
      "Iteration 61, loss = 0.95174677\n",
      "Iteration 62, loss = 0.94967131\n",
      "Iteration 63, loss = 0.95132555\n",
      "Iteration 64, loss = 0.95507867\n",
      "Iteration 65, loss = 0.94304620\n",
      "Iteration 66, loss = 0.94577190\n",
      "Iteration 67, loss = 0.93902861\n",
      "Iteration 68, loss = 0.94156414\n",
      "Iteration 69, loss = 0.93537478\n",
      "Iteration 70, loss = 0.93752454\n",
      "Iteration 71, loss = 0.93603485\n",
      "Iteration 72, loss = 0.93170731\n",
      "Iteration 73, loss = 0.93029931\n",
      "Iteration 74, loss = 0.92990982\n",
      "Iteration 75, loss = 0.93168723\n",
      "Iteration 76, loss = 0.92445863\n",
      "Iteration 77, loss = 0.92848044\n",
      "Iteration 78, loss = 0.92046801\n",
      "Iteration 79, loss = 0.92125756\n",
      "Iteration 80, loss = 0.92377050\n",
      "Iteration 81, loss = 0.91686028\n",
      "Iteration 82, loss = 0.91393988\n",
      "Iteration 83, loss = 0.91231763\n",
      "Iteration 84, loss = 0.90953129\n",
      "Iteration 85, loss = 0.90928609\n",
      "Iteration 86, loss = 0.90873682\n",
      "Iteration 87, loss = 0.91013323\n",
      "Iteration 88, loss = 0.90981322\n",
      "Iteration 89, loss = 0.90393375\n",
      "Iteration 90, loss = 0.90302547\n",
      "Iteration 91, loss = 0.89857733\n",
      "Iteration 92, loss = 0.89738118\n",
      "Iteration 93, loss = 0.89789794\n",
      "Iteration 94, loss = 0.89473625\n",
      "Iteration 95, loss = 0.89481394\n",
      "Iteration 96, loss = 0.89536617\n",
      "Iteration 97, loss = 0.89508627\n",
      "Iteration 98, loss = 0.89288367\n",
      "Iteration 99, loss = 0.89233395\n",
      "Iteration 100, loss = 0.88673631\n",
      "Iteration 1, loss = 1.34084455\n",
      "Iteration 2, loss = 1.21795571\n",
      "Iteration 3, loss = 1.19756562\n",
      "Iteration 4, loss = 1.17383152\n",
      "Iteration 5, loss = 1.14606626\n",
      "Iteration 6, loss = 1.12916564\n",
      "Iteration 7, loss = 1.11392478\n",
      "Iteration 8, loss = 1.09482346\n",
      "Iteration 9, loss = 1.08828848\n",
      "Iteration 10, loss = 1.07639785\n",
      "Iteration 11, loss = 1.06928668\n",
      "Iteration 12, loss = 1.06476768\n",
      "Iteration 13, loss = 1.06299095\n",
      "Iteration 14, loss = 1.05694269\n",
      "Iteration 15, loss = 1.05640475\n",
      "Iteration 16, loss = 1.04880324\n",
      "Iteration 17, loss = 1.04447249\n",
      "Iteration 18, loss = 1.03902138\n",
      "Iteration 19, loss = 1.03686314\n",
      "Iteration 20, loss = 1.03417591\n",
      "Iteration 21, loss = 1.02939928\n",
      "Iteration 22, loss = 1.03041639\n",
      "Iteration 23, loss = 1.02542412\n",
      "Iteration 24, loss = 1.02170988\n",
      "Iteration 25, loss = 1.01781871\n",
      "Iteration 26, loss = 1.01462686\n",
      "Iteration 27, loss = 1.01324700\n",
      "Iteration 28, loss = 1.01032788\n",
      "Iteration 29, loss = 1.00903241\n",
      "Iteration 30, loss = 1.00975885\n",
      "Iteration 31, loss = 1.00241088\n",
      "Iteration 32, loss = 1.00830014\n",
      "Iteration 33, loss = 0.99849524\n",
      "Iteration 34, loss = 0.99714587\n",
      "Iteration 35, loss = 0.99558760\n",
      "Iteration 36, loss = 0.99438673\n",
      "Iteration 37, loss = 0.99032798\n",
      "Iteration 38, loss = 0.98823723\n",
      "Iteration 39, loss = 0.98914717\n",
      "Iteration 40, loss = 0.98933565\n",
      "Iteration 41, loss = 0.98169874\n",
      "Iteration 42, loss = 0.97993094\n",
      "Iteration 43, loss = 0.98533051\n",
      "Iteration 44, loss = 0.97657448\n",
      "Iteration 45, loss = 0.97716698\n",
      "Iteration 46, loss = 0.98025364\n",
      "Iteration 47, loss = 0.97091475\n",
      "Iteration 48, loss = 0.97058374\n",
      "Iteration 49, loss = 0.97078817\n",
      "Iteration 50, loss = 0.96973286\n",
      "Iteration 51, loss = 0.96636915\n",
      "Iteration 52, loss = 0.96271934\n",
      "Iteration 53, loss = 0.96458687\n",
      "Iteration 54, loss = 0.96046790\n",
      "Iteration 55, loss = 0.96519745\n",
      "Iteration 56, loss = 0.96013347\n",
      "Iteration 57, loss = 0.95931866\n",
      "Iteration 58, loss = 0.95611044\n",
      "Iteration 59, loss = 0.96062297\n",
      "Iteration 60, loss = 0.94809008\n",
      "Iteration 61, loss = 0.95332081\n",
      "Iteration 62, loss = 0.94650540\n",
      "Iteration 63, loss = 0.94747959\n",
      "Iteration 64, loss = 0.95053275\n",
      "Iteration 65, loss = 0.93907736\n",
      "Iteration 66, loss = 0.94044472\n",
      "Iteration 67, loss = 0.93929276\n",
      "Iteration 68, loss = 0.93634357\n",
      "Iteration 69, loss = 0.93969606\n",
      "Iteration 70, loss = 0.93508633\n",
      "Iteration 71, loss = 0.93185177\n",
      "Iteration 72, loss = 0.93032411\n",
      "Iteration 73, loss = 0.92923927\n",
      "Iteration 74, loss = 0.93173634\n",
      "Iteration 75, loss = 0.92990704\n",
      "Iteration 76, loss = 0.92985724\n",
      "Iteration 77, loss = 0.92325723\n",
      "Iteration 78, loss = 0.92335122\n",
      "Iteration 79, loss = 0.92505159\n",
      "Iteration 80, loss = 0.91957834\n",
      "Iteration 81, loss = 0.91730031\n",
      "Iteration 82, loss = 0.91461893\n",
      "Iteration 83, loss = 0.91547788\n",
      "Iteration 84, loss = 0.91132011\n",
      "Iteration 85, loss = 0.91279648\n",
      "Iteration 86, loss = 0.91424782\n",
      "Iteration 87, loss = 0.91768655\n",
      "Iteration 88, loss = 0.90954267\n",
      "Iteration 89, loss = 0.91228371\n",
      "Iteration 90, loss = 0.90788252\n",
      "Iteration 91, loss = 0.91050951\n",
      "Iteration 92, loss = 0.90546459\n",
      "Iteration 93, loss = 0.90490146\n",
      "Iteration 94, loss = 0.90709966\n",
      "Iteration 95, loss = 0.89816706\n",
      "Iteration 96, loss = 0.90669548\n",
      "Iteration 97, loss = 0.89809502\n",
      "Iteration 98, loss = 0.89396083\n",
      "Iteration 99, loss = 0.89860935\n",
      "Iteration 100, loss = 0.89666825\n",
      "Iteration 1, loss = 1.34613426\n",
      "Iteration 2, loss = 1.21026609\n",
      "Iteration 3, loss = 1.18018006\n",
      "Iteration 4, loss = 1.15379941\n",
      "Iteration 5, loss = 1.12088276\n",
      "Iteration 6, loss = 1.10121358\n",
      "Iteration 7, loss = 1.08107019\n",
      "Iteration 8, loss = 1.06636017\n",
      "Iteration 9, loss = 1.05149386\n",
      "Iteration 10, loss = 1.04277989\n",
      "Iteration 11, loss = 1.03544220\n",
      "Iteration 12, loss = 1.02869726\n",
      "Iteration 13, loss = 1.01941518\n",
      "Iteration 14, loss = 1.01807461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 1.01350518\n",
      "Iteration 16, loss = 1.01181808\n",
      "Iteration 17, loss = 1.00433843\n",
      "Iteration 18, loss = 0.99781887\n",
      "Iteration 19, loss = 0.99743084\n",
      "Iteration 20, loss = 0.99505105\n",
      "Iteration 21, loss = 0.99170361\n",
      "Iteration 22, loss = 0.99017007\n",
      "Iteration 23, loss = 0.98792305\n",
      "Iteration 24, loss = 0.98582776\n",
      "Iteration 25, loss = 0.98593584\n",
      "Iteration 26, loss = 0.98510214\n",
      "Iteration 27, loss = 0.97748977\n",
      "Iteration 28, loss = 0.97801406\n",
      "Iteration 29, loss = 0.97750777\n",
      "Iteration 30, loss = 0.97760500\n",
      "Iteration 31, loss = 0.97371066\n",
      "Iteration 32, loss = 0.96958327\n",
      "Iteration 33, loss = 0.97921288\n",
      "Iteration 34, loss = 0.96415351\n",
      "Iteration 35, loss = 0.96642234\n",
      "Iteration 36, loss = 0.96454386\n",
      "Iteration 37, loss = 0.96466299\n",
      "Iteration 38, loss = 0.96491283\n",
      "Iteration 39, loss = 0.96260801\n",
      "Iteration 40, loss = 0.95815022\n",
      "Iteration 41, loss = 0.95713513\n",
      "Iteration 42, loss = 0.95326790\n",
      "Iteration 43, loss = 0.95016863\n",
      "Iteration 44, loss = 0.94751129\n",
      "Iteration 45, loss = 0.94671636\n",
      "Iteration 46, loss = 0.94665450\n",
      "Iteration 47, loss = 0.94537060\n",
      "Iteration 48, loss = 0.94278799\n",
      "Iteration 49, loss = 0.94341215\n",
      "Iteration 50, loss = 0.94156041\n",
      "Iteration 51, loss = 0.93512871\n",
      "Iteration 52, loss = 0.94027446\n",
      "Iteration 53, loss = 0.93541857\n",
      "Iteration 54, loss = 0.93380385\n",
      "Iteration 55, loss = 0.93125789\n",
      "Iteration 56, loss = 0.93371485\n",
      "Iteration 57, loss = 0.93022019\n",
      "Iteration 58, loss = 0.92983773\n",
      "Iteration 59, loss = 0.92826913\n",
      "Iteration 60, loss = 0.92231809\n",
      "Iteration 61, loss = 0.93413494\n",
      "Iteration 62, loss = 0.93516988\n",
      "Iteration 63, loss = 0.91920744\n",
      "Iteration 64, loss = 0.92460072\n",
      "Iteration 65, loss = 0.91609268\n",
      "Iteration 66, loss = 0.91467631\n",
      "Iteration 67, loss = 0.91687615\n",
      "Iteration 68, loss = 0.91404115\n",
      "Iteration 69, loss = 0.90888689\n",
      "Iteration 70, loss = 0.91317194\n",
      "Iteration 71, loss = 0.91054184\n",
      "Iteration 72, loss = 0.90913481\n",
      "Iteration 73, loss = 0.90454721\n",
      "Iteration 74, loss = 0.90578166\n",
      "Iteration 75, loss = 0.90542214\n",
      "Iteration 76, loss = 0.90403546\n",
      "Iteration 77, loss = 0.90056419\n",
      "Iteration 78, loss = 0.89926134\n",
      "Iteration 79, loss = 0.89534166\n",
      "Iteration 80, loss = 0.89961888\n",
      "Iteration 81, loss = 0.89883572\n",
      "Iteration 82, loss = 0.89949612\n",
      "Iteration 83, loss = 0.89810410\n",
      "Iteration 84, loss = 0.89244044\n",
      "Iteration 85, loss = 0.89140278\n",
      "Iteration 86, loss = 0.88841452\n",
      "Iteration 87, loss = 0.88949120\n",
      "Iteration 88, loss = 0.88897299\n",
      "Iteration 89, loss = 0.88251057\n",
      "Iteration 90, loss = 0.88684368\n",
      "Iteration 91, loss = 0.88950239\n",
      "Iteration 92, loss = 0.89049724\n",
      "Iteration 93, loss = 0.89287199\n",
      "Iteration 94, loss = 0.88849261\n",
      "Iteration 95, loss = 0.87775470\n",
      "Iteration 96, loss = 0.88094986\n",
      "Iteration 97, loss = 0.87449043\n",
      "Iteration 98, loss = 0.87377619\n",
      "Iteration 99, loss = 0.87299893\n",
      "Iteration 100, loss = 0.86873175\n",
      "Iteration 1, loss = 7.26795660\n",
      "Iteration 2, loss = 4.98867965\n",
      "Iteration 3, loss = 3.80131411\n",
      "Iteration 4, loss = 3.56615521\n",
      "Iteration 5, loss = 3.14260714\n",
      "Iteration 6, loss = 2.69339959\n",
      "Iteration 7, loss = 1.82484091\n",
      "Iteration 8, loss = 1.62533306\n",
      "Iteration 9, loss = 1.61471421\n",
      "Iteration 10, loss = 1.46399798\n",
      "Iteration 11, loss = 1.32509780\n",
      "Iteration 12, loss = 1.36074235\n",
      "Iteration 13, loss = 1.29178754\n",
      "Iteration 14, loss = 1.23906307\n",
      "Iteration 15, loss = 1.22276527\n",
      "Iteration 16, loss = 1.24925333\n",
      "Iteration 17, loss = 1.21377302\n",
      "Iteration 18, loss = 1.22225591\n",
      "Iteration 19, loss = 1.16108291\n",
      "Iteration 20, loss = 1.19577720\n",
      "Iteration 21, loss = 1.19464268\n",
      "Iteration 22, loss = 1.15651693\n",
      "Iteration 23, loss = 1.16496359\n",
      "Iteration 24, loss = 1.18442787\n",
      "Iteration 25, loss = 1.17704331\n",
      "Iteration 26, loss = 1.18173119\n",
      "Iteration 27, loss = 1.20012727\n",
      "Iteration 28, loss = 1.17037092\n",
      "Iteration 29, loss = 1.18893823\n",
      "Iteration 30, loss = 1.16567233\n",
      "Iteration 31, loss = 1.23959940\n",
      "Iteration 32, loss = 1.22778303\n",
      "Iteration 33, loss = 1.17461589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Single-layer model (neurons=128): CV accuracy = 0.4606\n",
      "Iteration 1, loss = 1.35468373\n",
      "Iteration 2, loss = 1.30519838\n",
      "Iteration 3, loss = 1.27349788\n",
      "Iteration 4, loss = 1.24700391\n",
      "Iteration 5, loss = 1.22766013\n",
      "Iteration 6, loss = 1.21584599\n",
      "Iteration 7, loss = 1.20959215\n",
      "Iteration 8, loss = 1.20361991\n",
      "Iteration 9, loss = 1.19652581\n",
      "Iteration 10, loss = 1.19282039\n",
      "Iteration 11, loss = 1.18666413\n",
      "Iteration 12, loss = 1.18262830\n",
      "Iteration 13, loss = 1.17861930\n",
      "Iteration 14, loss = 1.17391512\n",
      "Iteration 15, loss = 1.17220381\n",
      "Iteration 16, loss = 1.16707159\n",
      "Iteration 17, loss = 1.16422879\n",
      "Iteration 18, loss = 1.16015830\n",
      "Iteration 19, loss = 1.15519221\n",
      "Iteration 20, loss = 1.15229842\n",
      "Iteration 21, loss = 1.14704351\n",
      "Iteration 22, loss = 1.14167383\n",
      "Iteration 23, loss = 1.13679267\n",
      "Iteration 24, loss = 1.13280421\n",
      "Iteration 25, loss = 1.13040142\n",
      "Iteration 26, loss = 1.12134331\n",
      "Iteration 27, loss = 1.11926375\n",
      "Iteration 28, loss = 1.11879941\n",
      "Iteration 29, loss = 1.11799947\n",
      "Iteration 30, loss = 1.10864324\n",
      "Iteration 31, loss = 1.10273434\n",
      "Iteration 32, loss = 1.09028523\n",
      "Iteration 33, loss = 1.08616621\n",
      "Iteration 34, loss = 1.08462966\n",
      "Iteration 35, loss = 1.07420980\n",
      "Iteration 36, loss = 1.07360477\n",
      "Iteration 37, loss = 1.06896943\n",
      "Iteration 38, loss = 1.06040208\n",
      "Iteration 39, loss = 1.05623226\n",
      "Iteration 40, loss = 1.05302879\n",
      "Iteration 41, loss = 1.05416694\n",
      "Iteration 42, loss = 1.04676136\n",
      "Iteration 43, loss = 1.04862174\n",
      "Iteration 44, loss = 1.03821098\n",
      "Iteration 45, loss = 1.04045599\n",
      "Iteration 46, loss = 1.03256376\n",
      "Iteration 47, loss = 1.02854479\n",
      "Iteration 48, loss = 1.02800158\n",
      "Iteration 49, loss = 1.02074198\n",
      "Iteration 50, loss = 1.01467881\n",
      "Iteration 51, loss = 1.01210869\n",
      "Iteration 52, loss = 1.01818107\n",
      "Iteration 53, loss = 1.01470127\n",
      "Iteration 54, loss = 1.00446275\n",
      "Iteration 55, loss = 1.00641116\n",
      "Iteration 56, loss = 1.00630184\n",
      "Iteration 57, loss = 1.00606851\n",
      "Iteration 58, loss = 0.99642975\n",
      "Iteration 59, loss = 0.99503044\n",
      "Iteration 60, loss = 0.99402191\n",
      "Iteration 61, loss = 0.98982553\n",
      "Iteration 62, loss = 0.99732225\n",
      "Iteration 63, loss = 0.98748539\n",
      "Iteration 64, loss = 0.99160345\n",
      "Iteration 65, loss = 0.99173902\n",
      "Iteration 66, loss = 0.98573237\n",
      "Iteration 67, loss = 0.98382701\n",
      "Iteration 68, loss = 0.98235995\n",
      "Iteration 69, loss = 0.98198608\n",
      "Iteration 70, loss = 0.98534701\n",
      "Iteration 71, loss = 0.98105482\n",
      "Iteration 72, loss = 0.97667729\n",
      "Iteration 73, loss = 0.97483484\n",
      "Iteration 74, loss = 0.98046543\n",
      "Iteration 75, loss = 0.97740347\n",
      "Iteration 76, loss = 0.97607959\n",
      "Iteration 77, loss = 0.99255024\n",
      "Iteration 78, loss = 0.98122019\n",
      "Iteration 79, loss = 0.98583617\n",
      "Iteration 80, loss = 0.97149927\n",
      "Iteration 81, loss = 0.97492987\n",
      "Iteration 82, loss = 0.96414525\n",
      "Iteration 83, loss = 0.96400527\n",
      "Iteration 84, loss = 0.96269049\n",
      "Iteration 85, loss = 0.96233690\n",
      "Iteration 86, loss = 0.96768537\n",
      "Iteration 87, loss = 0.96240391\n",
      "Iteration 88, loss = 0.96468976\n",
      "Iteration 89, loss = 0.96587025\n",
      "Iteration 90, loss = 0.95517613\n",
      "Iteration 91, loss = 0.96183605\n",
      "Iteration 92, loss = 0.95711544\n",
      "Iteration 93, loss = 0.95645938\n",
      "Iteration 94, loss = 0.95431227\n",
      "Iteration 95, loss = 0.95497211\n",
      "Iteration 96, loss = 0.96281940\n",
      "Iteration 97, loss = 0.95923517\n",
      "Iteration 98, loss = 0.94915106\n",
      "Iteration 99, loss = 0.95399381\n",
      "Iteration 100, loss = 0.96151841\n",
      "Iteration 1, loss = 1.35521803\n",
      "Iteration 2, loss = 1.29925881\n",
      "Iteration 3, loss = 1.26677161\n",
      "Iteration 4, loss = 1.24057581\n",
      "Iteration 5, loss = 1.21924858\n",
      "Iteration 6, loss = 1.20727158\n",
      "Iteration 7, loss = 1.19783490\n",
      "Iteration 8, loss = 1.18609579\n",
      "Iteration 9, loss = 1.17382025\n",
      "Iteration 10, loss = 1.16388206\n",
      "Iteration 11, loss = 1.15069692\n",
      "Iteration 12, loss = 1.13393913\n",
      "Iteration 13, loss = 1.12065948\n",
      "Iteration 14, loss = 1.10648273\n",
      "Iteration 15, loss = 1.09356255\n",
      "Iteration 16, loss = 1.08140223\n",
      "Iteration 17, loss = 1.06754202\n",
      "Iteration 18, loss = 1.06803369\n",
      "Iteration 19, loss = 1.06387167\n",
      "Iteration 20, loss = 1.04821152\n",
      "Iteration 21, loss = 1.03924092\n",
      "Iteration 22, loss = 1.03823450\n",
      "Iteration 23, loss = 1.02785343\n",
      "Iteration 24, loss = 1.02686270\n",
      "Iteration 25, loss = 1.02361380\n",
      "Iteration 26, loss = 1.02021614\n",
      "Iteration 27, loss = 1.01599598\n",
      "Iteration 28, loss = 1.01116227\n",
      "Iteration 29, loss = 1.00415691\n",
      "Iteration 30, loss = 1.00473644\n",
      "Iteration 31, loss = 0.99896972\n",
      "Iteration 32, loss = 0.99672292\n",
      "Iteration 33, loss = 0.99893138\n",
      "Iteration 34, loss = 0.99871042\n",
      "Iteration 35, loss = 0.99446644\n",
      "Iteration 36, loss = 0.98445824\n",
      "Iteration 37, loss = 0.98638593\n",
      "Iteration 38, loss = 0.98530979\n",
      "Iteration 39, loss = 0.98529359\n",
      "Iteration 40, loss = 0.98060745\n",
      "Iteration 41, loss = 0.98367822\n",
      "Iteration 42, loss = 0.97466417\n",
      "Iteration 43, loss = 0.97114797\n",
      "Iteration 44, loss = 0.97920159\n",
      "Iteration 45, loss = 0.97315376\n",
      "Iteration 46, loss = 0.97033214\n",
      "Iteration 47, loss = 0.96811889\n",
      "Iteration 48, loss = 0.96643040\n",
      "Iteration 49, loss = 0.96658518\n",
      "Iteration 50, loss = 0.96254714\n",
      "Iteration 51, loss = 0.96541282\n",
      "Iteration 52, loss = 0.97594456\n",
      "Iteration 53, loss = 0.97663602\n",
      "Iteration 54, loss = 0.96372385\n",
      "Iteration 55, loss = 0.95924409\n",
      "Iteration 56, loss = 0.96039848\n",
      "Iteration 57, loss = 0.95969141\n",
      "Iteration 58, loss = 0.95344034\n",
      "Iteration 59, loss = 0.95214123\n",
      "Iteration 60, loss = 0.95051189\n",
      "Iteration 61, loss = 0.95090221\n",
      "Iteration 62, loss = 0.95328820\n",
      "Iteration 63, loss = 0.94966021\n",
      "Iteration 64, loss = 0.96168408\n",
      "Iteration 65, loss = 0.96016651\n",
      "Iteration 66, loss = 0.94497306\n",
      "Iteration 67, loss = 0.94114766\n",
      "Iteration 68, loss = 0.94354912\n",
      "Iteration 69, loss = 0.94373122\n",
      "Iteration 70, loss = 0.94731103\n",
      "Iteration 71, loss = 0.94587148\n",
      "Iteration 72, loss = 0.93468668\n",
      "Iteration 73, loss = 0.93500633\n",
      "Iteration 74, loss = 0.93268878\n",
      "Iteration 75, loss = 0.93365272\n",
      "Iteration 76, loss = 0.93483731\n",
      "Iteration 77, loss = 0.94131877\n",
      "Iteration 78, loss = 0.93857109\n",
      "Iteration 79, loss = 0.93589348\n",
      "Iteration 80, loss = 0.93241469\n",
      "Iteration 81, loss = 0.92527662\n",
      "Iteration 82, loss = 0.93024535\n",
      "Iteration 83, loss = 0.92319609\n",
      "Iteration 84, loss = 0.92760441\n",
      "Iteration 85, loss = 0.92447078\n",
      "Iteration 86, loss = 0.92135684\n",
      "Iteration 87, loss = 0.93239724\n",
      "Iteration 88, loss = 0.92405407\n",
      "Iteration 89, loss = 0.93601300\n",
      "Iteration 90, loss = 0.91859618\n",
      "Iteration 91, loss = 0.91243879\n",
      "Iteration 92, loss = 0.91767914\n",
      "Iteration 93, loss = 0.91830522\n",
      "Iteration 94, loss = 0.92314008\n",
      "Iteration 95, loss = 0.92101773\n",
      "Iteration 96, loss = 0.92211988\n",
      "Iteration 97, loss = 0.91478829\n",
      "Iteration 98, loss = 0.90924511\n",
      "Iteration 99, loss = 0.91332938\n",
      "Iteration 100, loss = 0.90687817\n",
      "Iteration 1, loss = 1.35688454\n",
      "Iteration 2, loss = 1.29926556\n",
      "Iteration 3, loss = 1.27091012\n",
      "Iteration 4, loss = 1.24329203\n",
      "Iteration 5, loss = 1.22602985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 1.21648742\n",
      "Iteration 7, loss = 1.20853933\n",
      "Iteration 8, loss = 1.20248191\n",
      "Iteration 9, loss = 1.19874489\n",
      "Iteration 10, loss = 1.19301304\n",
      "Iteration 11, loss = 1.18872389\n",
      "Iteration 12, loss = 1.18405270\n",
      "Iteration 13, loss = 1.18142915\n",
      "Iteration 14, loss = 1.17717367\n",
      "Iteration 15, loss = 1.17406461\n",
      "Iteration 16, loss = 1.16984098\n",
      "Iteration 17, loss = 1.16871395\n",
      "Iteration 18, loss = 1.16468377\n",
      "Iteration 19, loss = 1.15904677\n",
      "Iteration 20, loss = 1.15711874\n",
      "Iteration 21, loss = 1.15269300\n",
      "Iteration 22, loss = 1.14785067\n",
      "Iteration 23, loss = 1.14385962\n",
      "Iteration 24, loss = 1.13931086\n",
      "Iteration 25, loss = 1.13525694\n",
      "Iteration 26, loss = 1.13180206\n",
      "Iteration 27, loss = 1.12567378\n",
      "Iteration 28, loss = 1.12917331\n",
      "Iteration 29, loss = 1.12808369\n",
      "Iteration 30, loss = 1.11595974\n",
      "Iteration 31, loss = 1.10958782\n",
      "Iteration 32, loss = 1.10962444\n",
      "Iteration 33, loss = 1.10115804\n",
      "Iteration 34, loss = 1.09756921\n",
      "Iteration 35, loss = 1.09108597\n",
      "Iteration 36, loss = 1.09220125\n",
      "Iteration 37, loss = 1.08608382\n",
      "Iteration 38, loss = 1.07910316\n",
      "Iteration 39, loss = 1.07475095\n",
      "Iteration 40, loss = 1.06755382\n",
      "Iteration 41, loss = 1.06536278\n",
      "Iteration 42, loss = 1.05876017\n",
      "Iteration 43, loss = 1.05664952\n",
      "Iteration 44, loss = 1.05357655\n",
      "Iteration 45, loss = 1.04647458\n",
      "Iteration 46, loss = 1.04403176\n",
      "Iteration 47, loss = 1.04088429\n",
      "Iteration 48, loss = 1.04204645\n",
      "Iteration 49, loss = 1.03537613\n",
      "Iteration 50, loss = 1.03140268\n",
      "Iteration 51, loss = 1.02699555\n",
      "Iteration 52, loss = 1.04059385\n",
      "Iteration 53, loss = 1.02665216\n",
      "Iteration 54, loss = 1.01772985\n",
      "Iteration 55, loss = 1.01638373\n",
      "Iteration 56, loss = 1.01491236\n",
      "Iteration 57, loss = 1.01115718\n",
      "Iteration 58, loss = 1.01237387\n",
      "Iteration 59, loss = 1.01785372\n",
      "Iteration 60, loss = 1.01450451\n",
      "Iteration 61, loss = 1.00732791\n",
      "Iteration 62, loss = 1.00173904\n",
      "Iteration 63, loss = 1.00266897\n",
      "Iteration 64, loss = 0.99822998\n",
      "Iteration 65, loss = 0.99810905\n",
      "Iteration 66, loss = 1.00359503\n",
      "Iteration 67, loss = 1.00877662\n",
      "Iteration 68, loss = 1.01632037\n",
      "Iteration 69, loss = 0.99893740\n",
      "Iteration 70, loss = 0.98773412\n",
      "Iteration 71, loss = 0.99102833\n",
      "Iteration 72, loss = 0.98691150\n",
      "Iteration 73, loss = 0.98671483\n",
      "Iteration 74, loss = 0.98895731\n",
      "Iteration 75, loss = 0.99098058\n",
      "Iteration 76, loss = 1.00550115\n",
      "Iteration 77, loss = 1.00917552\n",
      "Iteration 78, loss = 0.99962162\n",
      "Iteration 79, loss = 0.99126081\n",
      "Iteration 80, loss = 0.98294705\n",
      "Iteration 81, loss = 0.99351186\n",
      "Iteration 82, loss = 0.97490644\n",
      "Iteration 83, loss = 0.97402254\n",
      "Iteration 84, loss = 0.98214338\n",
      "Iteration 85, loss = 0.97455816\n",
      "Iteration 86, loss = 0.97225423\n",
      "Iteration 87, loss = 0.97052039\n",
      "Iteration 88, loss = 0.97245991\n",
      "Iteration 89, loss = 0.97652557\n",
      "Iteration 90, loss = 0.96787290\n",
      "Iteration 91, loss = 0.96473374\n",
      "Iteration 92, loss = 0.97051415\n",
      "Iteration 93, loss = 0.97811070\n",
      "Iteration 94, loss = 0.97254857\n",
      "Iteration 95, loss = 0.97704054\n",
      "Iteration 96, loss = 0.97045413\n",
      "Iteration 97, loss = 0.96427351\n",
      "Iteration 98, loss = 0.95970389\n",
      "Iteration 99, loss = 0.96137308\n",
      "Iteration 100, loss = 0.95940756\n",
      "Iteration 1, loss = 1.35272970\n",
      "Iteration 2, loss = 1.30070497\n",
      "Iteration 3, loss = 1.27120327\n",
      "Iteration 4, loss = 1.24334999\n",
      "Iteration 5, loss = 1.22400360\n",
      "Iteration 6, loss = 1.21697992\n",
      "Iteration 7, loss = 1.20572557\n",
      "Iteration 8, loss = 1.19804300\n",
      "Iteration 9, loss = 1.18875615\n",
      "Iteration 10, loss = 1.17468883\n",
      "Iteration 11, loss = 1.16425339\n",
      "Iteration 12, loss = 1.15067130\n",
      "Iteration 13, loss = 1.13829398\n",
      "Iteration 14, loss = 1.12496422\n",
      "Iteration 15, loss = 1.11242848\n",
      "Iteration 16, loss = 1.09877582\n",
      "Iteration 17, loss = 1.09508928\n",
      "Iteration 18, loss = 1.07981060\n",
      "Iteration 19, loss = 1.07430114\n",
      "Iteration 20, loss = 1.06570289\n",
      "Iteration 21, loss = 1.06736615\n",
      "Iteration 22, loss = 1.05239931\n",
      "Iteration 23, loss = 1.05440212\n",
      "Iteration 24, loss = 1.04556885\n",
      "Iteration 25, loss = 1.04131429\n",
      "Iteration 26, loss = 1.03390117\n",
      "Iteration 27, loss = 1.03652756\n",
      "Iteration 28, loss = 1.03924156\n",
      "Iteration 29, loss = 1.03802828\n",
      "Iteration 30, loss = 1.03781141\n",
      "Iteration 31, loss = 1.03046054\n",
      "Iteration 32, loss = 1.03166943\n",
      "Iteration 33, loss = 1.04710076\n",
      "Iteration 34, loss = 1.04517338\n",
      "Iteration 35, loss = 1.01062038\n",
      "Iteration 36, loss = 1.01307998\n",
      "Iteration 37, loss = 1.00981437\n",
      "Iteration 38, loss = 1.00448132\n",
      "Iteration 39, loss = 1.00129699\n",
      "Iteration 40, loss = 0.99555836\n",
      "Iteration 41, loss = 0.99341311\n",
      "Iteration 42, loss = 0.98943682\n",
      "Iteration 43, loss = 0.98781877\n",
      "Iteration 44, loss = 0.98520962\n",
      "Iteration 45, loss = 0.98800950\n",
      "Iteration 46, loss = 0.98283930\n",
      "Iteration 47, loss = 0.97957966\n",
      "Iteration 48, loss = 0.98266811\n",
      "Iteration 49, loss = 0.98448502\n",
      "Iteration 50, loss = 0.98662914\n",
      "Iteration 51, loss = 0.99754807\n",
      "Iteration 52, loss = 1.00296720\n",
      "Iteration 53, loss = 0.98704068\n",
      "Iteration 54, loss = 0.98905529\n",
      "Iteration 55, loss = 0.99480242\n",
      "Iteration 56, loss = 0.97751313\n",
      "Iteration 57, loss = 0.98093352\n",
      "Iteration 58, loss = 0.97248886\n",
      "Iteration 59, loss = 0.96623821\n",
      "Iteration 60, loss = 0.96372498\n",
      "Iteration 61, loss = 0.96335897\n",
      "Iteration 62, loss = 0.95988576\n",
      "Iteration 63, loss = 0.96040511\n",
      "Iteration 64, loss = 0.96349360\n",
      "Iteration 65, loss = 0.95924106\n",
      "Iteration 66, loss = 0.95973798\n",
      "Iteration 67, loss = 0.95222394\n",
      "Iteration 68, loss = 0.95132534\n",
      "Iteration 69, loss = 0.95343082\n",
      "Iteration 70, loss = 0.95519086\n",
      "Iteration 71, loss = 0.95165167\n",
      "Iteration 72, loss = 0.94449837\n",
      "Iteration 73, loss = 0.94393537\n",
      "Iteration 74, loss = 0.94515067\n",
      "Iteration 75, loss = 0.94989571\n",
      "Iteration 76, loss = 0.95336195\n",
      "Iteration 77, loss = 0.95611438\n",
      "Iteration 78, loss = 0.95269819\n",
      "Iteration 79, loss = 0.94677057\n",
      "Iteration 80, loss = 0.94661719\n",
      "Iteration 81, loss = 0.94493049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 82, loss = 0.94125700\n",
      "Iteration 83, loss = 0.93219825\n",
      "Iteration 84, loss = 0.93384257\n",
      "Iteration 85, loss = 0.93006919\n",
      "Iteration 86, loss = 0.92934737\n",
      "Iteration 87, loss = 0.93064611\n",
      "Iteration 88, loss = 0.93037292\n",
      "Iteration 89, loss = 0.93070587\n",
      "Iteration 90, loss = 0.93063994\n",
      "Iteration 91, loss = 0.92502509\n",
      "Iteration 92, loss = 0.91999521\n",
      "Iteration 93, loss = 0.92705922\n",
      "Iteration 94, loss = 0.92510796\n",
      "Iteration 95, loss = 0.93769647\n",
      "Iteration 96, loss = 0.94059003\n",
      "Iteration 97, loss = 0.93655303\n",
      "Iteration 98, loss = 0.92712521\n",
      "Iteration 99, loss = 0.92101068\n",
      "Iteration 100, loss = 0.91400598\n",
      "Iteration 1, loss = 1.35303214\n",
      "Iteration 2, loss = 1.29218405\n",
      "Iteration 3, loss = 1.25705084\n",
      "Iteration 4, loss = 1.23218801\n",
      "Iteration 5, loss = 1.21426438\n",
      "Iteration 6, loss = 1.20461226\n",
      "Iteration 7, loss = 1.19537788\n",
      "Iteration 8, loss = 1.18280425\n",
      "Iteration 9, loss = 1.16931267\n",
      "Iteration 10, loss = 1.15646111\n",
      "Iteration 11, loss = 1.14396787\n",
      "Iteration 12, loss = 1.12883957\n",
      "Iteration 13, loss = 1.11199083\n",
      "Iteration 14, loss = 1.10194685\n",
      "Iteration 15, loss = 1.08696844\n",
      "Iteration 16, loss = 1.07103065\n",
      "Iteration 17, loss = 1.05798409\n",
      "Iteration 18, loss = 1.05091916\n",
      "Iteration 19, loss = 1.04038829\n",
      "Iteration 20, loss = 1.03270182\n",
      "Iteration 21, loss = 1.03221759\n",
      "Iteration 22, loss = 1.01967489\n",
      "Iteration 23, loss = 1.02020987\n",
      "Iteration 24, loss = 1.01679733\n",
      "Iteration 25, loss = 1.00612069\n",
      "Iteration 26, loss = 1.00210683\n",
      "Iteration 27, loss = 0.99988910\n",
      "Iteration 28, loss = 0.99333914\n",
      "Iteration 29, loss = 0.99646167\n",
      "Iteration 30, loss = 0.98957719\n",
      "Iteration 31, loss = 1.00254647\n",
      "Iteration 32, loss = 0.99218896\n",
      "Iteration 33, loss = 0.99018313\n",
      "Iteration 34, loss = 0.97518906\n",
      "Iteration 35, loss = 0.96864133\n",
      "Iteration 36, loss = 0.96947350\n",
      "Iteration 37, loss = 0.96715398\n",
      "Iteration 38, loss = 0.96411560\n",
      "Iteration 39, loss = 0.95905315\n",
      "Iteration 40, loss = 0.95587057\n",
      "Iteration 41, loss = 0.95075509\n",
      "Iteration 42, loss = 0.95214822\n",
      "Iteration 43, loss = 0.94597034\n",
      "Iteration 44, loss = 0.94410050\n",
      "Iteration 45, loss = 0.94196369\n",
      "Iteration 46, loss = 0.94117471\n",
      "Iteration 47, loss = 0.93800794\n",
      "Iteration 48, loss = 0.93485779\n",
      "Iteration 49, loss = 0.93673291\n",
      "Iteration 50, loss = 0.93753141\n",
      "Iteration 51, loss = 0.93624926\n",
      "Iteration 52, loss = 0.93662865\n",
      "Iteration 53, loss = 0.92581880\n",
      "Iteration 54, loss = 0.92436743\n",
      "Iteration 55, loss = 0.92357563\n",
      "Iteration 56, loss = 0.92580383\n",
      "Iteration 57, loss = 0.92496187\n",
      "Iteration 58, loss = 0.91970759\n",
      "Iteration 59, loss = 0.92421091\n",
      "Iteration 60, loss = 0.91739342\n",
      "Iteration 61, loss = 0.91238269\n",
      "Iteration 62, loss = 0.91345412\n",
      "Iteration 63, loss = 0.90922404\n",
      "Iteration 64, loss = 0.91244988\n",
      "Iteration 65, loss = 0.90589243\n",
      "Iteration 66, loss = 0.90566502\n",
      "Iteration 67, loss = 0.90674934\n",
      "Iteration 68, loss = 0.90586076\n",
      "Iteration 69, loss = 0.90594621\n",
      "Iteration 70, loss = 0.90215419\n",
      "Iteration 71, loss = 0.90083695\n",
      "Iteration 72, loss = 0.90154144\n",
      "Iteration 73, loss = 0.90846893\n",
      "Iteration 74, loss = 0.89854946\n",
      "Iteration 75, loss = 0.91111316\n",
      "Iteration 76, loss = 0.91539482\n",
      "Iteration 77, loss = 0.91423019\n",
      "Iteration 78, loss = 0.89744899\n",
      "Iteration 79, loss = 0.89508063\n",
      "Iteration 80, loss = 0.89814351\n",
      "Iteration 81, loss = 0.89184806\n",
      "Iteration 82, loss = 0.89440278\n",
      "Iteration 83, loss = 0.88908182\n",
      "Iteration 84, loss = 0.89095034\n",
      "Iteration 85, loss = 0.89816434\n",
      "Iteration 86, loss = 0.91703146\n",
      "Iteration 87, loss = 0.90080009\n",
      "Iteration 88, loss = 0.89419980\n",
      "Iteration 89, loss = 0.88644287\n",
      "Iteration 90, loss = 0.88201901\n",
      "Iteration 91, loss = 0.87786519\n",
      "Iteration 92, loss = 0.87743242\n",
      "Iteration 93, loss = 0.87639295\n",
      "Iteration 94, loss = 0.87248152\n",
      "Iteration 95, loss = 0.87457164\n",
      "Iteration 96, loss = 0.87152299\n",
      "Iteration 97, loss = 0.87037072\n",
      "Iteration 98, loss = 0.87285958\n",
      "Iteration 99, loss = 0.86862666\n",
      "Iteration 100, loss = 0.87221513\n",
      "Iteration 1, loss = 2.25396911\n",
      "Iteration 2, loss = 1.38161419\n",
      "Iteration 3, loss = 1.36359154\n",
      "Iteration 4, loss = 1.34722939\n",
      "Iteration 5, loss = 1.33232341\n",
      "Iteration 6, loss = 1.31878886\n",
      "Iteration 7, loss = 1.30714391\n",
      "Iteration 8, loss = 1.29651722\n",
      "Iteration 9, loss = 1.28742218\n",
      "Iteration 10, loss = 1.27952268\n",
      "Iteration 11, loss = 1.27255161\n",
      "Iteration 12, loss = 1.26642942\n",
      "Iteration 13, loss = 1.26141131\n",
      "Iteration 14, loss = 1.25682229\n",
      "Iteration 15, loss = 1.25293974\n",
      "Iteration 16, loss = 1.24950068\n",
      "Iteration 17, loss = 1.24658110\n",
      "Iteration 18, loss = 1.24386268\n",
      "Iteration 19, loss = 1.24161783\n",
      "Iteration 20, loss = 1.23962150\n",
      "Iteration 21, loss = 1.23779850\n",
      "Iteration 22, loss = 1.23625540\n",
      "Iteration 23, loss = 1.23498003\n",
      "Iteration 24, loss = 1.23354228\n",
      "Iteration 25, loss = 1.23271360\n",
      "Iteration 26, loss = 1.23162619\n",
      "Iteration 27, loss = 1.23069943\n",
      "Iteration 28, loss = 1.22993948\n",
      "Iteration 29, loss = 1.22935194\n",
      "Iteration 30, loss = 1.22863642\n",
      "Iteration 31, loss = 1.22825352\n",
      "Iteration 32, loss = 1.22770457\n",
      "Iteration 33, loss = 1.22731304\n",
      "Iteration 34, loss = 1.22688794\n",
      "Iteration 35, loss = 1.22654138\n",
      "Iteration 36, loss = 1.22624010\n",
      "Iteration 37, loss = 1.22597723\n",
      "Iteration 38, loss = 1.22581380\n",
      "Iteration 39, loss = 1.22551315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 1.22530544\n",
      "Iteration 41, loss = 1.22520181\n",
      "Iteration 42, loss = 1.22498710\n",
      "Iteration 43, loss = 1.22496361\n",
      "Iteration 44, loss = 1.22471735\n",
      "Iteration 45, loss = 1.22464641\n",
      "Iteration 46, loss = 1.22455798\n",
      "Iteration 47, loss = 1.22442502\n",
      "Iteration 48, loss = 1.22436068\n",
      "Iteration 49, loss = 1.22430161\n",
      "Iteration 50, loss = 1.22425887\n",
      "Iteration 51, loss = 1.22418355\n",
      "Iteration 52, loss = 1.22416184\n",
      "Iteration 53, loss = 1.22413176\n",
      "Iteration 54, loss = 1.22406087\n",
      "Iteration 55, loss = 1.22398475\n",
      "Iteration 56, loss = 1.22396330\n",
      "Iteration 57, loss = 1.22391387\n",
      "Iteration 58, loss = 1.22388437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Two-layer model (128, 2): CV accuracy = 0.445\n",
      "Iteration 1, loss = 1.31710391\n",
      "Iteration 2, loss = 1.25116591\n",
      "Iteration 3, loss = 1.22910434\n",
      "Iteration 4, loss = 1.20989223\n",
      "Iteration 5, loss = 1.20592600\n",
      "Iteration 6, loss = 1.20173787\n",
      "Iteration 7, loss = 1.19568826\n",
      "Iteration 8, loss = 1.18981305\n",
      "Iteration 9, loss = 1.18529381\n",
      "Iteration 10, loss = 1.18260114\n",
      "Iteration 11, loss = 1.17540071\n",
      "Iteration 12, loss = 1.17158058\n",
      "Iteration 13, loss = 1.16691214\n",
      "Iteration 14, loss = 1.16301707\n",
      "Iteration 15, loss = 1.15795493\n",
      "Iteration 16, loss = 1.15530487\n",
      "Iteration 17, loss = 1.15086645\n",
      "Iteration 18, loss = 1.14598552\n",
      "Iteration 19, loss = 1.14069546\n",
      "Iteration 20, loss = 1.13560872\n",
      "Iteration 21, loss = 1.13429820\n",
      "Iteration 22, loss = 1.13215772\n",
      "Iteration 23, loss = 1.12271558\n",
      "Iteration 24, loss = 1.11876280\n",
      "Iteration 25, loss = 1.11063634\n",
      "Iteration 26, loss = 1.10654076\n",
      "Iteration 27, loss = 1.10194915\n",
      "Iteration 28, loss = 1.09768793\n",
      "Iteration 29, loss = 1.09259595\n",
      "Iteration 30, loss = 1.08803134\n",
      "Iteration 31, loss = 1.08374141\n",
      "Iteration 32, loss = 1.08087040\n",
      "Iteration 33, loss = 1.08135661\n",
      "Iteration 34, loss = 1.07237868\n",
      "Iteration 35, loss = 1.07969511\n",
      "Iteration 36, loss = 1.06761713\n",
      "Iteration 37, loss = 1.06560175\n",
      "Iteration 38, loss = 1.06086171\n",
      "Iteration 39, loss = 1.06305104\n",
      "Iteration 40, loss = 1.05781999\n",
      "Iteration 41, loss = 1.05441817\n",
      "Iteration 42, loss = 1.04952099\n",
      "Iteration 43, loss = 1.04674392\n",
      "Iteration 44, loss = 1.04999300\n",
      "Iteration 45, loss = 1.04161661\n",
      "Iteration 46, loss = 1.04245503\n",
      "Iteration 47, loss = 1.03793396\n",
      "Iteration 48, loss = 1.03374341\n",
      "Iteration 49, loss = 1.02977712\n",
      "Iteration 50, loss = 1.02814834\n",
      "Iteration 51, loss = 1.02663131\n",
      "Iteration 52, loss = 1.02239821\n",
      "Iteration 53, loss = 1.02179725\n",
      "Iteration 54, loss = 1.02203167\n",
      "Iteration 55, loss = 1.01482899\n",
      "Iteration 56, loss = 1.01341203\n",
      "Iteration 57, loss = 1.01192714\n",
      "Iteration 58, loss = 1.00778842\n",
      "Iteration 59, loss = 1.00683958\n",
      "Iteration 60, loss = 1.01214066\n",
      "Iteration 61, loss = 1.00752463\n",
      "Iteration 62, loss = 1.00641268\n",
      "Iteration 63, loss = 1.00311603\n",
      "Iteration 64, loss = 0.99983542\n",
      "Iteration 65, loss = 0.99527986\n",
      "Iteration 66, loss = 0.99516297\n",
      "Iteration 67, loss = 0.99027579\n",
      "Iteration 68, loss = 0.98965168\n",
      "Iteration 69, loss = 0.98520309\n",
      "Iteration 70, loss = 0.98505638\n",
      "Iteration 71, loss = 0.98954304\n",
      "Iteration 72, loss = 0.98422241\n",
      "Iteration 73, loss = 0.98778067\n",
      "Iteration 74, loss = 0.98102339\n",
      "Iteration 75, loss = 0.97596752\n",
      "Iteration 76, loss = 0.97718012\n",
      "Iteration 77, loss = 0.97698822\n",
      "Iteration 78, loss = 0.97522212\n",
      "Iteration 79, loss = 0.97269337\n",
      "Iteration 80, loss = 0.97334206\n",
      "Iteration 81, loss = 0.97294811\n",
      "Iteration 82, loss = 0.98754257\n",
      "Iteration 83, loss = 0.98820297\n",
      "Iteration 84, loss = 0.97302716\n",
      "Iteration 85, loss = 0.96066774\n",
      "Iteration 86, loss = 0.96675265\n",
      "Iteration 87, loss = 0.96522420\n",
      "Iteration 88, loss = 0.97266836\n",
      "Iteration 89, loss = 0.96524421\n",
      "Iteration 90, loss = 0.96014726\n",
      "Iteration 91, loss = 0.96181079\n",
      "Iteration 92, loss = 0.95841035\n",
      "Iteration 93, loss = 0.95983588\n",
      "Iteration 94, loss = 0.95417165\n",
      "Iteration 95, loss = 0.95826909\n",
      "Iteration 96, loss = 0.95712208\n",
      "Iteration 97, loss = 0.95717279\n",
      "Iteration 98, loss = 0.97145586\n",
      "Iteration 99, loss = 0.96138668\n",
      "Iteration 100, loss = 0.95512333\n",
      "Iteration 1, loss = 1.31890821\n",
      "Iteration 2, loss = 1.25172096\n",
      "Iteration 3, loss = 1.22348948\n",
      "Iteration 4, loss = 1.20786161\n",
      "Iteration 5, loss = 1.19942309\n",
      "Iteration 6, loss = 1.19508204\n",
      "Iteration 7, loss = 1.19089424\n",
      "Iteration 8, loss = 1.18494759\n",
      "Iteration 9, loss = 1.17830815\n",
      "Iteration 10, loss = 1.17719964\n",
      "Iteration 11, loss = 1.16894453\n",
      "Iteration 12, loss = 1.16642304\n",
      "Iteration 13, loss = 1.16191681\n",
      "Iteration 14, loss = 1.15760579\n",
      "Iteration 15, loss = 1.15545186\n",
      "Iteration 16, loss = 1.15066869\n",
      "Iteration 17, loss = 1.14527301\n",
      "Iteration 18, loss = 1.14169135\n",
      "Iteration 19, loss = 1.13472247\n",
      "Iteration 20, loss = 1.13366729\n",
      "Iteration 21, loss = 1.12455737\n",
      "Iteration 22, loss = 1.12044613\n",
      "Iteration 23, loss = 1.11641645\n",
      "Iteration 24, loss = 1.10946769\n",
      "Iteration 25, loss = 1.10377046\n",
      "Iteration 26, loss = 1.09888925\n",
      "Iteration 27, loss = 1.09549928\n",
      "Iteration 28, loss = 1.09115173\n",
      "Iteration 29, loss = 1.08584438\n",
      "Iteration 30, loss = 1.08078555\n",
      "Iteration 31, loss = 1.07663266\n",
      "Iteration 32, loss = 1.07394236\n",
      "Iteration 33, loss = 1.07183788\n",
      "Iteration 34, loss = 1.06508958\n",
      "Iteration 35, loss = 1.07064337\n",
      "Iteration 36, loss = 1.05790681\n",
      "Iteration 37, loss = 1.05791664\n",
      "Iteration 38, loss = 1.05174793\n",
      "Iteration 39, loss = 1.04956892\n",
      "Iteration 40, loss = 1.04900902\n",
      "Iteration 41, loss = 1.04518863\n",
      "Iteration 42, loss = 1.04422928\n",
      "Iteration 43, loss = 1.03856087\n",
      "Iteration 44, loss = 1.04859715\n",
      "Iteration 45, loss = 1.03438402\n",
      "Iteration 46, loss = 1.04101906\n",
      "Iteration 47, loss = 1.03556355\n",
      "Iteration 48, loss = 1.02750382\n",
      "Iteration 49, loss = 1.02689673\n",
      "Iteration 50, loss = 1.02741989\n",
      "Iteration 51, loss = 1.02918589\n",
      "Iteration 52, loss = 1.02477633\n",
      "Iteration 53, loss = 1.02090560\n",
      "Iteration 54, loss = 1.01684583\n",
      "Iteration 55, loss = 1.01483272\n",
      "Iteration 56, loss = 1.01359530\n",
      "Iteration 57, loss = 1.01296577\n",
      "Iteration 58, loss = 1.01102720\n",
      "Iteration 59, loss = 1.00988661\n",
      "Iteration 60, loss = 1.01354335\n",
      "Iteration 61, loss = 1.01369612\n",
      "Iteration 62, loss = 1.01136519\n",
      "Iteration 63, loss = 1.00620170\n",
      "Iteration 64, loss = 1.00661995\n",
      "Iteration 65, loss = 1.00814053\n",
      "Iteration 66, loss = 1.00135651\n",
      "Iteration 67, loss = 0.99853705\n",
      "Iteration 68, loss = 0.99770301\n",
      "Iteration 69, loss = 0.99885418\n",
      "Iteration 70, loss = 0.99669999\n",
      "Iteration 71, loss = 0.99814780\n",
      "Iteration 72, loss = 0.99607920\n",
      "Iteration 73, loss = 1.00106714\n",
      "Iteration 74, loss = 0.99513679\n",
      "Iteration 75, loss = 0.99326253\n",
      "Iteration 76, loss = 0.99277387\n",
      "Iteration 77, loss = 0.99198668\n",
      "Iteration 78, loss = 0.98913649\n",
      "Iteration 79, loss = 0.98818403\n",
      "Iteration 80, loss = 0.98683407\n",
      "Iteration 81, loss = 0.98775913\n",
      "Iteration 82, loss = 0.98770889\n",
      "Iteration 83, loss = 0.99168751\n",
      "Iteration 84, loss = 0.98597633\n",
      "Iteration 85, loss = 0.98485301\n",
      "Iteration 86, loss = 0.98208114\n",
      "Iteration 87, loss = 0.98173243\n",
      "Iteration 88, loss = 0.98018628\n",
      "Iteration 89, loss = 0.98779168\n",
      "Iteration 90, loss = 0.98111942\n",
      "Iteration 91, loss = 0.98101417\n",
      "Iteration 92, loss = 0.97846939\n",
      "Iteration 93, loss = 0.97622738\n",
      "Iteration 94, loss = 0.97597353\n",
      "Iteration 95, loss = 0.97705637\n",
      "Iteration 96, loss = 0.97707297\n",
      "Iteration 97, loss = 0.97853546\n",
      "Iteration 98, loss = 0.97716868\n",
      "Iteration 99, loss = 0.98201654\n",
      "Iteration 100, loss = 0.99858155\n",
      "Iteration 1, loss = 1.31843664\n",
      "Iteration 2, loss = 1.25036674\n",
      "Iteration 3, loss = 1.22553159\n",
      "Iteration 4, loss = 1.20902840\n",
      "Iteration 5, loss = 1.20318429\n",
      "Iteration 6, loss = 1.19791982\n",
      "Iteration 7, loss = 1.19386244\n",
      "Iteration 8, loss = 1.18823886\n",
      "Iteration 9, loss = 1.18280756\n",
      "Iteration 10, loss = 1.17885504\n",
      "Iteration 11, loss = 1.17507188\n",
      "Iteration 12, loss = 1.17187549\n",
      "Iteration 13, loss = 1.16960386\n",
      "Iteration 14, loss = 1.16494870\n",
      "Iteration 15, loss = 1.16131380\n",
      "Iteration 16, loss = 1.15631601\n",
      "Iteration 17, loss = 1.15329855\n",
      "Iteration 18, loss = 1.14857461\n",
      "Iteration 19, loss = 1.14413405\n",
      "Iteration 20, loss = 1.14140629\n",
      "Iteration 21, loss = 1.13510515\n",
      "Iteration 22, loss = 1.13139875\n",
      "Iteration 23, loss = 1.12746758\n",
      "Iteration 24, loss = 1.12046288\n",
      "Iteration 25, loss = 1.11633410\n",
      "Iteration 26, loss = 1.11396775\n",
      "Iteration 27, loss = 1.11195816\n",
      "Iteration 28, loss = 1.10942442\n",
      "Iteration 29, loss = 1.10564066\n",
      "Iteration 30, loss = 1.09630650\n",
      "Iteration 31, loss = 1.09364681\n",
      "Iteration 32, loss = 1.09236076\n",
      "Iteration 33, loss = 1.08521605\n",
      "Iteration 34, loss = 1.08344016\n",
      "Iteration 35, loss = 1.07885541\n",
      "Iteration 36, loss = 1.07476396\n",
      "Iteration 37, loss = 1.07420527\n",
      "Iteration 38, loss = 1.06919697\n",
      "Iteration 39, loss = 1.06646314\n",
      "Iteration 40, loss = 1.06343108\n",
      "Iteration 41, loss = 1.06109764\n",
      "Iteration 42, loss = 1.05798340\n",
      "Iteration 43, loss = 1.05801206\n",
      "Iteration 44, loss = 1.05554543\n",
      "Iteration 45, loss = 1.05373508\n",
      "Iteration 46, loss = 1.05840592\n",
      "Iteration 47, loss = 1.05367344\n",
      "Iteration 48, loss = 1.05122350\n",
      "Iteration 49, loss = 1.05099577\n",
      "Iteration 50, loss = 1.04970268\n",
      "Iteration 51, loss = 1.04382219\n",
      "Iteration 52, loss = 1.04225535\n",
      "Iteration 53, loss = 1.04031059\n",
      "Iteration 54, loss = 1.03788174\n",
      "Iteration 55, loss = 1.03487766\n",
      "Iteration 56, loss = 1.03762319\n",
      "Iteration 57, loss = 1.03357274\n",
      "Iteration 58, loss = 1.03177115\n",
      "Iteration 59, loss = 1.03179558\n",
      "Iteration 60, loss = 1.03819191\n",
      "Iteration 61, loss = 1.03035466\n",
      "Iteration 62, loss = 1.02578806\n",
      "Iteration 63, loss = 1.02102447\n",
      "Iteration 64, loss = 1.02516631\n",
      "Iteration 65, loss = 1.02496079\n",
      "Iteration 66, loss = 1.02269954\n",
      "Iteration 67, loss = 1.02051007\n",
      "Iteration 68, loss = 1.01501738\n",
      "Iteration 69, loss = 1.01191638\n",
      "Iteration 70, loss = 1.01264427\n",
      "Iteration 71, loss = 1.01009748\n",
      "Iteration 72, loss = 1.01160888\n",
      "Iteration 73, loss = 1.00657465\n",
      "Iteration 74, loss = 1.00712206\n",
      "Iteration 75, loss = 1.00373909\n",
      "Iteration 76, loss = 1.00408838\n",
      "Iteration 77, loss = 1.00349571\n",
      "Iteration 78, loss = 1.00495432\n",
      "Iteration 79, loss = 0.99754678\n",
      "Iteration 80, loss = 0.99527233\n",
      "Iteration 81, loss = 0.99606518\n",
      "Iteration 82, loss = 0.99285813\n",
      "Iteration 83, loss = 0.99681263\n",
      "Iteration 84, loss = 0.99570142\n",
      "Iteration 85, loss = 0.99779447\n",
      "Iteration 86, loss = 0.99033187\n",
      "Iteration 87, loss = 0.98819122\n",
      "Iteration 88, loss = 0.98503990\n",
      "Iteration 89, loss = 0.98877609\n",
      "Iteration 90, loss = 0.98313289\n",
      "Iteration 91, loss = 0.99345201\n",
      "Iteration 92, loss = 0.98550578\n",
      "Iteration 93, loss = 0.98204425\n",
      "Iteration 94, loss = 0.98332696\n",
      "Iteration 95, loss = 0.97994565\n",
      "Iteration 96, loss = 0.98335714\n",
      "Iteration 97, loss = 0.97998408\n",
      "Iteration 98, loss = 0.97896569\n",
      "Iteration 99, loss = 0.97521011\n",
      "Iteration 100, loss = 0.97524905\n",
      "Iteration 1, loss = 1.31763611\n",
      "Iteration 2, loss = 1.25387917\n",
      "Iteration 3, loss = 1.22759460\n",
      "Iteration 4, loss = 1.21042834\n",
      "Iteration 5, loss = 1.20189795\n",
      "Iteration 6, loss = 1.19829594\n",
      "Iteration 7, loss = 1.19328363\n",
      "Iteration 8, loss = 1.18912986\n",
      "Iteration 9, loss = 1.18338845\n",
      "Iteration 10, loss = 1.17815320\n",
      "Iteration 11, loss = 1.17427813\n",
      "Iteration 12, loss = 1.17091849\n",
      "Iteration 13, loss = 1.16667088\n",
      "Iteration 14, loss = 1.16355827\n",
      "Iteration 15, loss = 1.15897788\n",
      "Iteration 16, loss = 1.15382888\n",
      "Iteration 17, loss = 1.15033903\n",
      "Iteration 18, loss = 1.14510660\n",
      "Iteration 19, loss = 1.14166968\n",
      "Iteration 20, loss = 1.13723423\n",
      "Iteration 21, loss = 1.13058127\n",
      "Iteration 22, loss = 1.12619041\n",
      "Iteration 23, loss = 1.12097763\n",
      "Iteration 24, loss = 1.11629653\n",
      "Iteration 25, loss = 1.11188206\n",
      "Iteration 26, loss = 1.10776217\n",
      "Iteration 27, loss = 1.10453057\n",
      "Iteration 28, loss = 1.09913350\n",
      "Iteration 29, loss = 1.09744428\n",
      "Iteration 30, loss = 1.09341682\n",
      "Iteration 31, loss = 1.08797680\n",
      "Iteration 32, loss = 1.08492921\n",
      "Iteration 33, loss = 1.08386913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 1.07725573\n",
      "Iteration 35, loss = 1.07881060\n",
      "Iteration 36, loss = 1.07836249\n",
      "Iteration 37, loss = 1.08538033\n",
      "Iteration 38, loss = 1.08345129\n",
      "Iteration 39, loss = 1.07006593\n",
      "Iteration 40, loss = 1.07232492\n",
      "Iteration 41, loss = 1.06421921\n",
      "Iteration 42, loss = 1.06033206\n",
      "Iteration 43, loss = 1.06311114\n",
      "Iteration 44, loss = 1.05763789\n",
      "Iteration 45, loss = 1.05800817\n",
      "Iteration 46, loss = 1.06076941\n",
      "Iteration 47, loss = 1.05054327\n",
      "Iteration 48, loss = 1.05177453\n",
      "Iteration 49, loss = 1.05282380\n",
      "Iteration 50, loss = 1.04437426\n",
      "Iteration 51, loss = 1.04224709\n",
      "Iteration 52, loss = 1.04051392\n",
      "Iteration 53, loss = 1.03844186\n",
      "Iteration 54, loss = 1.03493884\n",
      "Iteration 55, loss = 1.03601234\n",
      "Iteration 56, loss = 1.03542317\n",
      "Iteration 57, loss = 1.03079567\n",
      "Iteration 58, loss = 1.02637001\n",
      "Iteration 59, loss = 1.02926080\n",
      "Iteration 60, loss = 1.03003025\n",
      "Iteration 61, loss = 1.03173119\n",
      "Iteration 62, loss = 1.03011285\n",
      "Iteration 63, loss = 1.01707601\n",
      "Iteration 64, loss = 1.01994099\n",
      "Iteration 65, loss = 1.01774298\n",
      "Iteration 66, loss = 1.01237717\n",
      "Iteration 67, loss = 1.01814393\n",
      "Iteration 68, loss = 1.01601330\n",
      "Iteration 69, loss = 1.01188031\n",
      "Iteration 70, loss = 1.01472487\n",
      "Iteration 71, loss = 1.01167407\n",
      "Iteration 72, loss = 1.00963462\n",
      "Iteration 73, loss = 1.00297815\n",
      "Iteration 74, loss = 1.00251061\n",
      "Iteration 75, loss = 1.00005533\n",
      "Iteration 76, loss = 0.99957876\n",
      "Iteration 77, loss = 1.00258167\n",
      "Iteration 78, loss = 1.00794184\n",
      "Iteration 79, loss = 0.99900107\n",
      "Iteration 80, loss = 0.99412085\n",
      "Iteration 81, loss = 0.99287817\n",
      "Iteration 82, loss = 0.99110616\n",
      "Iteration 83, loss = 0.99443328\n",
      "Iteration 84, loss = 0.98959946\n",
      "Iteration 85, loss = 0.99576622\n",
      "Iteration 86, loss = 0.99008464\n",
      "Iteration 87, loss = 0.98732204\n",
      "Iteration 88, loss = 0.98660567\n",
      "Iteration 89, loss = 0.98603568\n",
      "Iteration 90, loss = 0.98408028\n",
      "Iteration 91, loss = 0.98733245\n",
      "Iteration 92, loss = 0.99167939\n",
      "Iteration 93, loss = 0.98229372\n",
      "Iteration 94, loss = 0.98097874\n",
      "Iteration 95, loss = 0.98361891\n",
      "Iteration 96, loss = 0.98268143\n",
      "Iteration 97, loss = 0.97989997\n",
      "Iteration 98, loss = 0.97829679\n",
      "Iteration 99, loss = 0.98146803\n",
      "Iteration 100, loss = 0.98378753\n",
      "Iteration 1, loss = 1.31244164\n",
      "Iteration 2, loss = 1.24324589\n",
      "Iteration 3, loss = 1.19336084\n",
      "Iteration 4, loss = 1.16957034\n",
      "Iteration 5, loss = 1.15070938\n",
      "Iteration 6, loss = 1.13068183\n",
      "Iteration 7, loss = 1.11510419\n",
      "Iteration 8, loss = 1.10051707\n",
      "Iteration 9, loss = 1.08665534\n",
      "Iteration 10, loss = 1.07628239\n",
      "Iteration 11, loss = 1.06602374\n",
      "Iteration 12, loss = 1.05614802\n",
      "Iteration 13, loss = 1.05264657\n",
      "Iteration 14, loss = 1.04718724\n",
      "Iteration 15, loss = 1.04445543\n",
      "Iteration 16, loss = 1.04542552\n",
      "Iteration 17, loss = 1.04109513\n",
      "Iteration 18, loss = 1.05596643\n",
      "Iteration 19, loss = 1.04174682\n",
      "Iteration 20, loss = 1.03944552\n",
      "Iteration 21, loss = 1.03578097\n",
      "Iteration 22, loss = 1.02443826\n",
      "Iteration 23, loss = 1.02352351\n",
      "Iteration 24, loss = 1.02278372\n",
      "Iteration 25, loss = 1.01503134\n",
      "Iteration 26, loss = 1.01201481\n",
      "Iteration 27, loss = 1.01411887\n",
      "Iteration 28, loss = 1.00975883\n",
      "Iteration 29, loss = 1.00653223\n",
      "Iteration 30, loss = 1.00479984\n",
      "Iteration 31, loss = 1.00891514\n",
      "Iteration 32, loss = 1.00330706\n",
      "Iteration 33, loss = 0.99723432\n",
      "Iteration 34, loss = 0.99487175\n",
      "Iteration 35, loss = 0.99711202\n",
      "Iteration 36, loss = 0.99540964\n",
      "Iteration 37, loss = 0.98739973\n",
      "Iteration 38, loss = 0.99581630\n",
      "Iteration 39, loss = 0.99269920\n",
      "Iteration 40, loss = 0.98998895\n",
      "Iteration 41, loss = 0.98178918\n",
      "Iteration 42, loss = 0.98582430\n",
      "Iteration 43, loss = 0.98054485\n",
      "Iteration 44, loss = 0.97630278\n",
      "Iteration 45, loss = 0.97546947\n",
      "Iteration 46, loss = 0.97327974\n",
      "Iteration 47, loss = 0.97363126\n",
      "Iteration 48, loss = 0.96942461\n",
      "Iteration 49, loss = 0.97170509\n",
      "Iteration 50, loss = 0.96833247\n",
      "Iteration 51, loss = 0.97042359\n",
      "Iteration 52, loss = 0.96499155\n",
      "Iteration 53, loss = 0.96414088\n",
      "Iteration 54, loss = 0.96822966\n",
      "Iteration 55, loss = 0.96360079\n",
      "Iteration 56, loss = 0.95965604\n",
      "Iteration 57, loss = 0.95862526\n",
      "Iteration 58, loss = 0.96058348\n",
      "Iteration 59, loss = 0.95767334\n",
      "Iteration 60, loss = 0.95358741\n",
      "Iteration 61, loss = 0.95439925\n",
      "Iteration 62, loss = 0.95253334\n",
      "Iteration 63, loss = 0.95159864\n",
      "Iteration 64, loss = 0.95062369\n",
      "Iteration 65, loss = 0.95555896\n",
      "Iteration 66, loss = 0.95387348\n",
      "Iteration 67, loss = 0.95118764\n",
      "Iteration 68, loss = 0.94827849\n",
      "Iteration 69, loss = 0.94855565\n",
      "Iteration 70, loss = 0.94449686\n",
      "Iteration 71, loss = 0.94882537\n",
      "Iteration 72, loss = 0.94288528\n",
      "Iteration 73, loss = 0.94497475\n",
      "Iteration 74, loss = 0.94136648\n",
      "Iteration 75, loss = 0.94379884\n",
      "Iteration 76, loss = 0.94467319\n",
      "Iteration 77, loss = 0.93890434\n",
      "Iteration 78, loss = 0.93923512\n",
      "Iteration 79, loss = 0.93779001\n",
      "Iteration 80, loss = 0.94784272\n",
      "Iteration 81, loss = 0.94667167\n",
      "Iteration 82, loss = 0.93636970\n",
      "Iteration 83, loss = 0.93317110\n",
      "Iteration 84, loss = 0.93200740\n",
      "Iteration 85, loss = 0.93318910\n",
      "Iteration 86, loss = 0.93342114\n",
      "Iteration 87, loss = 0.93253028\n",
      "Iteration 88, loss = 0.92890639\n",
      "Iteration 89, loss = 0.93090760\n",
      "Iteration 90, loss = 0.94317968\n",
      "Iteration 91, loss = 0.94373350\n",
      "Iteration 92, loss = 0.94845240\n",
      "Iteration 93, loss = 0.94394308\n",
      "Iteration 94, loss = 0.94122764\n",
      "Iteration 95, loss = 0.94542760\n",
      "Iteration 96, loss = 0.93554277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 97, loss = 0.93347815\n",
      "Iteration 98, loss = 0.92124143\n",
      "Iteration 99, loss = 0.92119248\n",
      "Iteration 100, loss = 0.92032503\n",
      "Iteration 1, loss = 3.55528059\n",
      "Iteration 2, loss = 1.34554953\n",
      "Iteration 3, loss = 1.32913969\n",
      "Iteration 4, loss = 1.32724491\n",
      "Iteration 5, loss = 1.30290996\n",
      "Iteration 6, loss = 1.32193249\n",
      "Iteration 7, loss = 1.28271500\n",
      "Iteration 8, loss = 1.27373667\n",
      "Iteration 9, loss = 1.26613159\n",
      "Iteration 10, loss = 1.25936915\n",
      "Iteration 11, loss = 1.25396437\n",
      "Iteration 12, loss = 1.24890463\n",
      "Iteration 13, loss = 1.24449359\n",
      "Iteration 14, loss = 1.24102590\n",
      "Iteration 15, loss = 1.23827828\n",
      "Iteration 16, loss = 1.23600443\n",
      "Iteration 17, loss = 1.23387913\n",
      "Iteration 18, loss = 1.23257724\n",
      "Iteration 19, loss = 1.23130252\n",
      "Iteration 20, loss = 1.22998669\n",
      "Iteration 21, loss = 1.22915216\n",
      "Iteration 22, loss = 1.22840982\n",
      "Iteration 23, loss = 1.22783035\n",
      "Iteration 24, loss = 1.22724526\n",
      "Iteration 25, loss = 1.22685269\n",
      "Iteration 26, loss = 1.22641956\n",
      "Iteration 27, loss = 1.22611837\n",
      "Iteration 28, loss = 1.22581342\n",
      "Iteration 29, loss = 1.22549767\n",
      "Iteration 30, loss = 1.22531558\n",
      "Iteration 31, loss = 1.22506807\n",
      "Iteration 32, loss = 1.22496534\n",
      "Iteration 33, loss = 1.22480426\n",
      "Iteration 34, loss = 1.22466557\n",
      "Iteration 35, loss = 1.22451598\n",
      "Iteration 36, loss = 1.22444170\n",
      "Iteration 37, loss = 1.22433505\n",
      "Iteration 38, loss = 1.22429600\n",
      "Iteration 39, loss = 1.22417317\n",
      "Iteration 40, loss = 1.22411648\n",
      "Iteration 41, loss = 1.22404408\n",
      "Iteration 42, loss = 1.22409559\n",
      "Iteration 43, loss = 1.22394425\n",
      "Iteration 44, loss = 1.22395343\n",
      "Iteration 45, loss = 1.22391486\n",
      "Iteration 46, loss = 1.22387531\n",
      "Iteration 47, loss = 1.22382348\n",
      "Iteration 48, loss = 1.22383286\n",
      "Iteration 49, loss = 1.22388401\n",
      "Iteration 50, loss = 1.22378065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Two-layer model (128, 4): CV accuracy = 0.445\n",
      "Iteration 1, loss = 1.55802802\n",
      "Iteration 2, loss = 1.29492308\n",
      "Iteration 3, loss = 1.18366985\n",
      "Iteration 4, loss = 1.19390429\n",
      "Iteration 5, loss = 1.16157518\n",
      "Iteration 6, loss = 1.12510650\n",
      "Iteration 7, loss = 1.10784273\n",
      "Iteration 8, loss = 1.09909984\n",
      "Iteration 9, loss = 1.09103722\n",
      "Iteration 10, loss = 1.07640687\n",
      "Iteration 11, loss = 1.07006438\n",
      "Iteration 12, loss = 1.06331428\n",
      "Iteration 13, loss = 1.05774297\n",
      "Iteration 14, loss = 1.06496701\n",
      "Iteration 15, loss = 1.04758048\n",
      "Iteration 16, loss = 1.04321638\n",
      "Iteration 17, loss = 1.04161012\n",
      "Iteration 18, loss = 1.03763980\n",
      "Iteration 19, loss = 1.03689273\n",
      "Iteration 20, loss = 1.03305646\n",
      "Iteration 21, loss = 1.03502674\n",
      "Iteration 22, loss = 1.02727427\n",
      "Iteration 23, loss = 1.02169877\n",
      "Iteration 24, loss = 1.02738825\n",
      "Iteration 25, loss = 1.02093007\n",
      "Iteration 26, loss = 1.02030164\n",
      "Iteration 27, loss = 1.02439784\n",
      "Iteration 28, loss = 1.01946413\n",
      "Iteration 29, loss = 1.01764509\n",
      "Iteration 30, loss = 1.01582747\n",
      "Iteration 31, loss = 1.02128270\n",
      "Iteration 32, loss = 1.01929411\n",
      "Iteration 33, loss = 1.01725245\n",
      "Iteration 34, loss = 0.99936362\n",
      "Iteration 35, loss = 0.99748250\n",
      "Iteration 36, loss = 0.99542161\n",
      "Iteration 37, loss = 0.99215126\n",
      "Iteration 38, loss = 0.99068062\n",
      "Iteration 39, loss = 0.98729442\n",
      "Iteration 40, loss = 0.98992293\n",
      "Iteration 41, loss = 0.98409251\n",
      "Iteration 42, loss = 0.98842650\n",
      "Iteration 43, loss = 0.98296904\n",
      "Iteration 44, loss = 0.97441398\n",
      "Iteration 45, loss = 0.97265438\n",
      "Iteration 46, loss = 0.97537496\n",
      "Iteration 47, loss = 0.97618332\n",
      "Iteration 48, loss = 0.97081861\n",
      "Iteration 49, loss = 0.96698945\n",
      "Iteration 50, loss = 0.96403277\n",
      "Iteration 51, loss = 0.96203818\n",
      "Iteration 52, loss = 0.95932481\n",
      "Iteration 53, loss = 0.96760074\n",
      "Iteration 54, loss = 0.96119285\n",
      "Iteration 55, loss = 0.95436424\n",
      "Iteration 56, loss = 0.95617906\n",
      "Iteration 57, loss = 0.96118904\n",
      "Iteration 58, loss = 0.95903523\n",
      "Iteration 59, loss = 0.95904948\n",
      "Iteration 60, loss = 0.97822839\n",
      "Iteration 61, loss = 1.00497305\n",
      "Iteration 62, loss = 0.98738937\n",
      "Iteration 63, loss = 0.97199911\n",
      "Iteration 64, loss = 0.95339087\n",
      "Iteration 65, loss = 0.95254591\n",
      "Iteration 66, loss = 0.94793818\n",
      "Iteration 67, loss = 0.94726893\n",
      "Iteration 68, loss = 0.94388085\n",
      "Iteration 69, loss = 0.93710882\n",
      "Iteration 70, loss = 0.94453024\n",
      "Iteration 71, loss = 0.93719403\n",
      "Iteration 72, loss = 0.93251490\n",
      "Iteration 73, loss = 0.93520898\n",
      "Iteration 74, loss = 0.93480741\n",
      "Iteration 75, loss = 0.93673918\n",
      "Iteration 76, loss = 0.92556962\n",
      "Iteration 77, loss = 0.93034005\n",
      "Iteration 78, loss = 0.93417732\n",
      "Iteration 79, loss = 0.93210755\n",
      "Iteration 80, loss = 0.92522148\n",
      "Iteration 81, loss = 0.93441497\n",
      "Iteration 82, loss = 0.92240465\n",
      "Iteration 83, loss = 0.92170237\n",
      "Iteration 84, loss = 0.92193072\n",
      "Iteration 85, loss = 0.91657630\n",
      "Iteration 86, loss = 0.91873298\n",
      "Iteration 87, loss = 0.91772981\n",
      "Iteration 88, loss = 0.92152109\n",
      "Iteration 89, loss = 0.91949841\n",
      "Iteration 90, loss = 0.91904644\n",
      "Iteration 91, loss = 0.92137973\n",
      "Iteration 92, loss = 0.92015987\n",
      "Iteration 93, loss = 0.91807108\n",
      "Iteration 94, loss = 0.91141238\n",
      "Iteration 95, loss = 0.91438249\n",
      "Iteration 96, loss = 0.90784862\n",
      "Iteration 97, loss = 0.90890458\n",
      "Iteration 98, loss = 0.90845007\n",
      "Iteration 99, loss = 0.90255922\n",
      "Iteration 100, loss = 0.90452538\n",
      "Iteration 1, loss = 1.59227860\n",
      "Iteration 2, loss = 1.47050269\n",
      "Iteration 3, loss = 1.29374171\n",
      "Iteration 4, loss = 1.16890148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 1.14276759\n",
      "Iteration 6, loss = 1.12706533\n",
      "Iteration 7, loss = 1.10323895\n",
      "Iteration 8, loss = 1.07642749\n",
      "Iteration 9, loss = 1.07194852\n",
      "Iteration 10, loss = 1.06991621\n",
      "Iteration 11, loss = 1.06422128\n",
      "Iteration 12, loss = 1.05776712\n",
      "Iteration 13, loss = 1.05068169\n",
      "Iteration 14, loss = 1.05259718\n",
      "Iteration 15, loss = 1.04643315\n",
      "Iteration 16, loss = 1.04560367\n",
      "Iteration 17, loss = 1.04174415\n",
      "Iteration 18, loss = 1.03680272\n",
      "Iteration 19, loss = 1.03586346\n",
      "Iteration 20, loss = 1.03363660\n",
      "Iteration 21, loss = 1.03733992\n",
      "Iteration 22, loss = 1.02718709\n",
      "Iteration 23, loss = 1.02556345\n",
      "Iteration 24, loss = 1.02369159\n",
      "Iteration 25, loss = 1.02084607\n",
      "Iteration 26, loss = 1.02506035\n",
      "Iteration 27, loss = 1.02558505\n",
      "Iteration 28, loss = 1.02728625\n",
      "Iteration 29, loss = 1.03803347\n",
      "Iteration 30, loss = 1.02135065\n",
      "Iteration 31, loss = 1.02668145\n",
      "Iteration 32, loss = 1.01531564\n",
      "Iteration 33, loss = 1.00968958\n",
      "Iteration 34, loss = 1.00653356\n",
      "Iteration 35, loss = 1.00780612\n",
      "Iteration 36, loss = 1.00454396\n",
      "Iteration 37, loss = 1.00140336\n",
      "Iteration 38, loss = 0.99818406\n",
      "Iteration 39, loss = 0.99988960\n",
      "Iteration 40, loss = 0.99207985\n",
      "Iteration 41, loss = 0.99319593\n",
      "Iteration 42, loss = 0.99243858\n",
      "Iteration 43, loss = 0.99084410\n",
      "Iteration 44, loss = 0.98514486\n",
      "Iteration 45, loss = 0.98438875\n",
      "Iteration 46, loss = 0.98057075\n",
      "Iteration 47, loss = 0.98008204\n",
      "Iteration 48, loss = 0.97921878\n",
      "Iteration 49, loss = 0.97529213\n",
      "Iteration 50, loss = 0.97767681\n",
      "Iteration 51, loss = 0.98031123\n",
      "Iteration 52, loss = 0.97355651\n",
      "Iteration 53, loss = 0.97253761\n",
      "Iteration 54, loss = 0.97273498\n",
      "Iteration 55, loss = 0.96650808\n",
      "Iteration 56, loss = 0.96719362\n",
      "Iteration 57, loss = 0.96423344\n",
      "Iteration 58, loss = 0.96644671\n",
      "Iteration 59, loss = 0.97026708\n",
      "Iteration 60, loss = 1.00156011\n",
      "Iteration 61, loss = 1.00667152\n",
      "Iteration 62, loss = 0.98508322\n",
      "Iteration 63, loss = 0.97928178\n",
      "Iteration 64, loss = 0.96178030\n",
      "Iteration 65, loss = 0.96768104\n",
      "Iteration 66, loss = 0.96493800\n",
      "Iteration 67, loss = 0.95493786\n",
      "Iteration 68, loss = 0.95162561\n",
      "Iteration 69, loss = 0.94951747\n",
      "Iteration 70, loss = 0.94948945\n",
      "Iteration 71, loss = 0.94911412\n",
      "Iteration 72, loss = 0.94486448\n",
      "Iteration 73, loss = 0.94908363\n",
      "Iteration 74, loss = 0.94658439\n",
      "Iteration 75, loss = 0.94421521\n",
      "Iteration 76, loss = 0.94438979\n",
      "Iteration 77, loss = 0.94659256\n",
      "Iteration 78, loss = 0.93909059\n",
      "Iteration 79, loss = 0.94126115\n",
      "Iteration 80, loss = 0.94005809\n",
      "Iteration 81, loss = 0.93609625\n",
      "Iteration 82, loss = 0.93625569\n",
      "Iteration 83, loss = 0.93384783\n",
      "Iteration 84, loss = 0.93431164\n",
      "Iteration 85, loss = 0.93234271\n",
      "Iteration 86, loss = 0.93096439\n",
      "Iteration 87, loss = 0.93102185\n",
      "Iteration 88, loss = 0.93115376\n",
      "Iteration 89, loss = 0.92699891\n",
      "Iteration 90, loss = 0.92891151\n",
      "Iteration 91, loss = 0.92741627\n",
      "Iteration 92, loss = 0.92426560\n",
      "Iteration 93, loss = 0.92526237\n",
      "Iteration 94, loss = 0.92164509\n",
      "Iteration 95, loss = 0.92367937\n",
      "Iteration 96, loss = 0.91998221\n",
      "Iteration 97, loss = 0.92273804\n",
      "Iteration 98, loss = 0.92778536\n",
      "Iteration 99, loss = 0.91970310\n",
      "Iteration 100, loss = 0.92262851\n",
      "Iteration 1, loss = 1.53776696\n",
      "Iteration 2, loss = 1.26816474\n",
      "Iteration 3, loss = 1.17599188\n",
      "Iteration 4, loss = 1.18209771\n",
      "Iteration 5, loss = 1.15391646\n",
      "Iteration 6, loss = 1.10910264\n",
      "Iteration 7, loss = 1.09818466\n",
      "Iteration 8, loss = 1.08628244\n",
      "Iteration 9, loss = 1.08173746\n",
      "Iteration 10, loss = 1.06679707\n",
      "Iteration 11, loss = 1.06021271\n",
      "Iteration 12, loss = 1.05403901\n",
      "Iteration 13, loss = 1.04995972\n",
      "Iteration 14, loss = 1.04745583\n",
      "Iteration 15, loss = 1.04585633\n",
      "Iteration 16, loss = 1.03964007\n",
      "Iteration 17, loss = 1.03829297\n",
      "Iteration 18, loss = 1.04153095\n",
      "Iteration 19, loss = 1.02975588\n",
      "Iteration 20, loss = 1.03026313\n",
      "Iteration 21, loss = 1.02696907\n",
      "Iteration 22, loss = 1.02547230\n",
      "Iteration 23, loss = 1.02956848\n",
      "Iteration 24, loss = 1.02461017\n",
      "Iteration 25, loss = 1.02790270\n",
      "Iteration 26, loss = 1.03268777\n",
      "Iteration 27, loss = 1.03388106\n",
      "Iteration 28, loss = 1.02413498\n",
      "Iteration 29, loss = 1.02926379\n",
      "Iteration 30, loss = 1.01591673\n",
      "Iteration 31, loss = 1.00903010\n",
      "Iteration 32, loss = 1.01532032\n",
      "Iteration 33, loss = 1.01290900\n",
      "Iteration 34, loss = 1.00766947\n",
      "Iteration 35, loss = 1.00742823\n",
      "Iteration 36, loss = 1.00318967\n",
      "Iteration 37, loss = 1.00797714\n",
      "Iteration 38, loss = 0.99529446\n",
      "Iteration 39, loss = 0.99314347\n",
      "Iteration 40, loss = 0.99036449\n",
      "Iteration 41, loss = 0.99203106\n",
      "Iteration 42, loss = 0.98784422\n",
      "Iteration 43, loss = 0.98721954\n",
      "Iteration 44, loss = 0.98589107\n",
      "Iteration 45, loss = 0.98104281\n",
      "Iteration 46, loss = 0.98271980\n",
      "Iteration 47, loss = 0.98130626\n",
      "Iteration 48, loss = 0.97771823\n",
      "Iteration 49, loss = 0.98104257\n",
      "Iteration 50, loss = 0.97879682\n",
      "Iteration 51, loss = 0.98107493\n",
      "Iteration 52, loss = 0.97243757\n",
      "Iteration 53, loss = 0.97297077\n",
      "Iteration 54, loss = 0.97332517\n",
      "Iteration 55, loss = 0.96926996\n",
      "Iteration 56, loss = 0.96962374\n",
      "Iteration 57, loss = 0.96714056\n",
      "Iteration 58, loss = 0.96414872\n",
      "Iteration 59, loss = 0.96040988\n",
      "Iteration 60, loss = 0.97365692\n",
      "Iteration 61, loss = 0.99197856\n",
      "Iteration 62, loss = 0.97633321\n",
      "Iteration 63, loss = 0.97499557\n",
      "Iteration 64, loss = 0.96651440\n",
      "Iteration 65, loss = 0.95789450\n",
      "Iteration 66, loss = 0.95887195\n",
      "Iteration 67, loss = 0.95352255\n",
      "Iteration 68, loss = 0.95417660\n",
      "Iteration 69, loss = 0.95067651\n",
      "Iteration 70, loss = 0.94973352\n",
      "Iteration 71, loss = 0.94538134\n",
      "Iteration 72, loss = 0.94511468\n",
      "Iteration 73, loss = 0.94375362\n",
      "Iteration 74, loss = 0.94339365\n",
      "Iteration 75, loss = 0.94267236\n",
      "Iteration 76, loss = 0.94597882\n",
      "Iteration 77, loss = 0.94015239\n",
      "Iteration 78, loss = 0.93658137\n",
      "Iteration 79, loss = 0.94087837\n",
      "Iteration 80, loss = 0.93964684\n",
      "Iteration 81, loss = 0.93448900\n",
      "Iteration 82, loss = 0.93643978\n",
      "Iteration 83, loss = 0.93292471\n",
      "Iteration 84, loss = 0.94211681\n",
      "Iteration 85, loss = 0.93651447\n",
      "Iteration 86, loss = 0.93883185\n",
      "Iteration 87, loss = 0.94059610\n",
      "Iteration 88, loss = 0.93642721\n",
      "Iteration 89, loss = 0.92862803\n",
      "Iteration 90, loss = 0.92639057\n",
      "Iteration 91, loss = 0.92506155\n",
      "Iteration 92, loss = 0.92205617\n",
      "Iteration 93, loss = 0.92219385\n",
      "Iteration 94, loss = 0.92198262\n",
      "Iteration 95, loss = 0.91689025\n",
      "Iteration 96, loss = 0.91722905\n",
      "Iteration 97, loss = 0.91947574\n",
      "Iteration 98, loss = 0.92706869\n",
      "Iteration 99, loss = 0.92383225\n",
      "Iteration 100, loss = 0.91325618\n",
      "Iteration 1, loss = 1.54553830\n",
      "Iteration 2, loss = 1.27921665\n",
      "Iteration 3, loss = 1.20095085\n",
      "Iteration 4, loss = 1.20694714\n",
      "Iteration 5, loss = 1.17067555\n",
      "Iteration 6, loss = 1.13290636\n",
      "Iteration 7, loss = 1.11468925\n",
      "Iteration 8, loss = 1.11179855\n",
      "Iteration 9, loss = 1.09599851\n",
      "Iteration 10, loss = 1.08849493\n",
      "Iteration 11, loss = 1.07881490\n",
      "Iteration 12, loss = 1.07303174\n",
      "Iteration 13, loss = 1.06950537\n",
      "Iteration 14, loss = 1.06488582\n",
      "Iteration 15, loss = 1.06056430\n",
      "Iteration 16, loss = 1.05814132\n",
      "Iteration 17, loss = 1.05105171\n",
      "Iteration 18, loss = 1.05075849\n",
      "Iteration 19, loss = 1.04547332\n",
      "Iteration 20, loss = 1.04288823\n",
      "Iteration 21, loss = 1.03968770\n",
      "Iteration 22, loss = 1.03872450\n",
      "Iteration 23, loss = 1.03712792\n",
      "Iteration 24, loss = 1.03791569\n",
      "Iteration 25, loss = 1.03051951\n",
      "Iteration 26, loss = 1.03357320\n",
      "Iteration 27, loss = 1.04019887\n",
      "Iteration 28, loss = 1.02893456\n",
      "Iteration 29, loss = 1.04096773\n",
      "Iteration 30, loss = 1.02543537\n",
      "Iteration 31, loss = 1.01892333\n",
      "Iteration 32, loss = 1.02318566\n",
      "Iteration 33, loss = 1.01293171\n",
      "Iteration 34, loss = 1.00669163\n",
      "Iteration 35, loss = 1.00617599\n",
      "Iteration 36, loss = 1.00787480\n",
      "Iteration 37, loss = 1.00282086\n",
      "Iteration 38, loss = 0.99773679\n",
      "Iteration 39, loss = 0.99505850\n",
      "Iteration 40, loss = 0.99268036\n",
      "Iteration 41, loss = 0.99072721\n",
      "Iteration 42, loss = 0.99037437\n",
      "Iteration 43, loss = 0.98984446\n",
      "Iteration 44, loss = 0.98473128\n",
      "Iteration 45, loss = 0.98304808\n",
      "Iteration 46, loss = 0.98343429\n",
      "Iteration 47, loss = 0.98429304\n",
      "Iteration 48, loss = 0.97601536\n",
      "Iteration 49, loss = 0.97706445\n",
      "Iteration 50, loss = 0.98027594\n",
      "Iteration 51, loss = 0.97524579\n",
      "Iteration 52, loss = 0.97420290\n",
      "Iteration 53, loss = 0.97299565\n",
      "Iteration 54, loss = 0.97786725\n",
      "Iteration 55, loss = 0.98324693\n",
      "Iteration 56, loss = 0.97245540\n",
      "Iteration 57, loss = 0.97273163\n",
      "Iteration 58, loss = 0.96864698\n",
      "Iteration 59, loss = 0.95918071\n",
      "Iteration 60, loss = 0.98239008\n",
      "Iteration 61, loss = 0.96714350\n",
      "Iteration 62, loss = 0.96499560\n",
      "Iteration 63, loss = 0.97372855\n",
      "Iteration 64, loss = 0.95700668\n",
      "Iteration 65, loss = 0.95702830\n",
      "Iteration 66, loss = 0.96262910\n",
      "Iteration 67, loss = 0.95392366\n",
      "Iteration 68, loss = 0.95155285\n",
      "Iteration 69, loss = 0.94716306\n",
      "Iteration 70, loss = 0.95516679\n",
      "Iteration 71, loss = 0.95490579\n",
      "Iteration 72, loss = 0.95013615\n",
      "Iteration 73, loss = 0.95055105\n",
      "Iteration 74, loss = 0.94047810\n",
      "Iteration 75, loss = 0.94716698\n",
      "Iteration 76, loss = 0.95155137\n",
      "Iteration 77, loss = 0.94027908\n",
      "Iteration 78, loss = 0.94050600\n",
      "Iteration 79, loss = 0.94438743\n",
      "Iteration 80, loss = 0.93842280\n",
      "Iteration 81, loss = 0.93889972\n",
      "Iteration 82, loss = 0.93324355\n",
      "Iteration 83, loss = 0.93592515\n",
      "Iteration 84, loss = 0.93604424\n",
      "Iteration 85, loss = 0.93054142\n",
      "Iteration 86, loss = 0.93357415\n",
      "Iteration 87, loss = 0.93386125\n",
      "Iteration 88, loss = 0.93050184\n",
      "Iteration 89, loss = 0.92542664\n",
      "Iteration 90, loss = 0.92567246\n",
      "Iteration 91, loss = 0.92523114\n",
      "Iteration 92, loss = 0.92120909\n",
      "Iteration 93, loss = 0.92063686\n",
      "Iteration 94, loss = 0.91997276\n",
      "Iteration 95, loss = 0.92558158\n",
      "Iteration 96, loss = 0.92782893\n",
      "Iteration 97, loss = 0.92725199\n",
      "Iteration 98, loss = 0.92555401\n",
      "Iteration 99, loss = 0.92620398\n",
      "Iteration 100, loss = 0.91774819\n",
      "Iteration 1, loss = 1.54331195\n",
      "Iteration 2, loss = 1.27455481\n",
      "Iteration 3, loss = 1.17879180\n",
      "Iteration 4, loss = 1.18173873\n",
      "Iteration 5, loss = 1.13617648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 1.10037898\n",
      "Iteration 7, loss = 1.08844094\n",
      "Iteration 8, loss = 1.07090084\n",
      "Iteration 9, loss = 1.06297864\n",
      "Iteration 10, loss = 1.04657675\n",
      "Iteration 11, loss = 1.04132276\n",
      "Iteration 12, loss = 1.03434258\n",
      "Iteration 13, loss = 1.03181995\n",
      "Iteration 14, loss = 1.03319386\n",
      "Iteration 15, loss = 1.02771896\n",
      "Iteration 16, loss = 1.01935017\n",
      "Iteration 17, loss = 1.02065529\n",
      "Iteration 18, loss = 1.02087121\n",
      "Iteration 19, loss = 1.01278971\n",
      "Iteration 20, loss = 1.01090730\n",
      "Iteration 21, loss = 1.00667356\n",
      "Iteration 22, loss = 1.00285152\n",
      "Iteration 23, loss = 1.00121353\n",
      "Iteration 24, loss = 0.99674366\n",
      "Iteration 25, loss = 0.99657036\n",
      "Iteration 26, loss = 0.99474068\n",
      "Iteration 27, loss = 0.99943664\n",
      "Iteration 28, loss = 0.99936894\n",
      "Iteration 29, loss = 0.99516292\n",
      "Iteration 30, loss = 0.98971194\n",
      "Iteration 31, loss = 0.99831447\n",
      "Iteration 32, loss = 0.99172746\n",
      "Iteration 33, loss = 0.98104260\n",
      "Iteration 34, loss = 0.98127641\n",
      "Iteration 35, loss = 0.97685777\n",
      "Iteration 36, loss = 0.97981378\n",
      "Iteration 37, loss = 0.97919398\n",
      "Iteration 38, loss = 0.97504200\n",
      "Iteration 39, loss = 0.97315603\n",
      "Iteration 40, loss = 0.97047668\n",
      "Iteration 41, loss = 0.96872524\n",
      "Iteration 42, loss = 0.97182418\n",
      "Iteration 43, loss = 0.96672490\n",
      "Iteration 44, loss = 0.96648011\n",
      "Iteration 45, loss = 0.96304792\n",
      "Iteration 46, loss = 0.97595215\n",
      "Iteration 47, loss = 0.98636122\n",
      "Iteration 48, loss = 0.97212121\n",
      "Iteration 49, loss = 0.96937462\n",
      "Iteration 50, loss = 0.95508581\n",
      "Iteration 51, loss = 0.94948616\n",
      "Iteration 52, loss = 0.96205593\n",
      "Iteration 53, loss = 0.95419995\n",
      "Iteration 54, loss = 0.95059862\n",
      "Iteration 55, loss = 0.94350452\n",
      "Iteration 56, loss = 0.94284294\n",
      "Iteration 57, loss = 0.94011120\n",
      "Iteration 58, loss = 0.93894920\n",
      "Iteration 59, loss = 0.93940533\n",
      "Iteration 60, loss = 0.94145425\n",
      "Iteration 61, loss = 0.93365941\n",
      "Iteration 62, loss = 0.93747953\n",
      "Iteration 63, loss = 0.93428454\n",
      "Iteration 64, loss = 0.93381414\n",
      "Iteration 65, loss = 0.93412906\n",
      "Iteration 66, loss = 0.92931634\n",
      "Iteration 67, loss = 0.92804093\n",
      "Iteration 68, loss = 0.92283089\n",
      "Iteration 69, loss = 0.92597984\n",
      "Iteration 70, loss = 0.92558653\n",
      "Iteration 71, loss = 0.92651779\n",
      "Iteration 72, loss = 0.92681608\n",
      "Iteration 73, loss = 0.92552091\n",
      "Iteration 74, loss = 0.91778074\n",
      "Iteration 75, loss = 0.91829409\n",
      "Iteration 76, loss = 0.91848578\n",
      "Iteration 77, loss = 0.91615659\n",
      "Iteration 78, loss = 0.91061815\n",
      "Iteration 79, loss = 0.91317183\n",
      "Iteration 80, loss = 0.91876407\n",
      "Iteration 81, loss = 0.90341219\n",
      "Iteration 82, loss = 0.90717410\n",
      "Iteration 83, loss = 0.90559828\n",
      "Iteration 84, loss = 0.90560052\n",
      "Iteration 85, loss = 0.90445523\n",
      "Iteration 86, loss = 0.90534630\n",
      "Iteration 87, loss = 0.90413000\n",
      "Iteration 88, loss = 0.90766921\n",
      "Iteration 89, loss = 0.91565251\n",
      "Iteration 90, loss = 0.90970947\n",
      "Iteration 91, loss = 0.90925842\n",
      "Iteration 92, loss = 0.91362929\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.58200049\n",
      "Iteration 2, loss = 1.47542179\n",
      "Iteration 3, loss = 1.31734501\n",
      "Iteration 4, loss = 1.27951052\n",
      "Iteration 5, loss = 1.31545858\n",
      "Iteration 6, loss = 1.24990956\n",
      "Iteration 7, loss = 1.23209984\n",
      "Iteration 8, loss = 1.22355070\n",
      "Iteration 9, loss = 1.21806617\n",
      "Iteration 10, loss = 1.21960388\n",
      "Iteration 11, loss = 1.21218400\n",
      "Iteration 12, loss = 1.21361994\n",
      "Iteration 13, loss = 1.21640633\n",
      "Iteration 14, loss = 1.21321573\n",
      "Iteration 15, loss = 1.21072605\n",
      "Iteration 16, loss = 1.21082675\n",
      "Iteration 17, loss = 1.21175919\n",
      "Iteration 18, loss = 1.20895973\n",
      "Iteration 19, loss = 1.20849798\n",
      "Iteration 20, loss = 1.20928788\n",
      "Iteration 21, loss = 1.21181051\n",
      "Iteration 22, loss = 1.20678841\n",
      "Iteration 23, loss = 1.20830802\n",
      "Iteration 24, loss = 1.20573117\n",
      "Iteration 25, loss = 1.20607725\n",
      "Iteration 26, loss = 1.21134717\n",
      "Iteration 27, loss = 1.20627375\n",
      "Iteration 28, loss = 1.20816573\n",
      "Iteration 29, loss = 1.20945547\n",
      "Iteration 30, loss = 1.20203853\n",
      "Iteration 31, loss = 1.21005576\n",
      "Iteration 32, loss = 1.20307909\n",
      "Iteration 33, loss = 1.20359673\n",
      "Iteration 34, loss = 1.20350671\n",
      "Iteration 35, loss = 1.21183625\n",
      "Iteration 36, loss = 1.20497429\n",
      "Iteration 37, loss = 1.20414549\n",
      "Iteration 38, loss = 1.21000099\n",
      "Iteration 39, loss = 1.20056009\n",
      "Iteration 40, loss = 1.20153183\n",
      "Iteration 41, loss = 1.19747419\n",
      "Iteration 42, loss = 1.19816552\n",
      "Iteration 43, loss = 1.19780135\n",
      "Iteration 44, loss = 1.19801547\n",
      "Iteration 45, loss = 1.20470853\n",
      "Iteration 46, loss = 1.20347217\n",
      "Iteration 47, loss = 1.20041697\n",
      "Iteration 48, loss = 1.20232588\n",
      "Iteration 49, loss = 1.20439433\n",
      "Iteration 50, loss = 1.20372434\n",
      "Iteration 51, loss = 1.22494813\n",
      "Iteration 52, loss = 1.20457033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Two-layer model (128, 8): CV accuracy = 0.445\n",
      "Iteration 1, loss = 1.43668256\n",
      "Iteration 2, loss = 1.21486450\n",
      "Iteration 3, loss = 1.23504662\n",
      "Iteration 4, loss = 1.18639209\n",
      "Iteration 5, loss = 1.14398015\n",
      "Iteration 6, loss = 1.12959297\n",
      "Iteration 7, loss = 1.11400219\n",
      "Iteration 8, loss = 1.09706603\n",
      "Iteration 9, loss = 1.08805203\n",
      "Iteration 10, loss = 1.07532693\n",
      "Iteration 11, loss = 1.07469601\n",
      "Iteration 12, loss = 1.06652630\n",
      "Iteration 13, loss = 1.06101032\n",
      "Iteration 14, loss = 1.04955330\n",
      "Iteration 15, loss = 1.04734553\n",
      "Iteration 16, loss = 1.04300555\n",
      "Iteration 17, loss = 1.03796380\n",
      "Iteration 18, loss = 1.03553386\n",
      "Iteration 19, loss = 1.03170309\n",
      "Iteration 20, loss = 1.03696499\n",
      "Iteration 21, loss = 1.03336592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 1.02730674\n",
      "Iteration 23, loss = 1.02734170\n",
      "Iteration 24, loss = 1.02314347\n",
      "Iteration 25, loss = 1.01433140\n",
      "Iteration 26, loss = 1.01297975\n",
      "Iteration 27, loss = 1.01017654\n",
      "Iteration 28, loss = 1.00887931\n",
      "Iteration 29, loss = 1.00747188\n",
      "Iteration 30, loss = 1.00125284\n",
      "Iteration 31, loss = 0.99789086\n",
      "Iteration 32, loss = 0.99619183\n",
      "Iteration 33, loss = 0.99170418\n",
      "Iteration 34, loss = 0.99107198\n",
      "Iteration 35, loss = 0.99123652\n",
      "Iteration 36, loss = 0.98608161\n",
      "Iteration 37, loss = 0.99174993\n",
      "Iteration 38, loss = 0.99043968\n",
      "Iteration 39, loss = 0.98181726\n",
      "Iteration 40, loss = 0.97648061\n",
      "Iteration 41, loss = 0.97887087\n",
      "Iteration 42, loss = 0.97377573\n",
      "Iteration 43, loss = 0.97281663\n",
      "Iteration 44, loss = 0.97826811\n",
      "Iteration 45, loss = 0.97936255\n",
      "Iteration 46, loss = 0.98270426\n",
      "Iteration 47, loss = 0.97225300\n",
      "Iteration 48, loss = 0.96358390\n",
      "Iteration 49, loss = 0.96537621\n",
      "Iteration 50, loss = 0.96125159\n",
      "Iteration 51, loss = 0.96075035\n",
      "Iteration 52, loss = 0.96093278\n",
      "Iteration 53, loss = 0.96194137\n",
      "Iteration 54, loss = 0.95615880\n",
      "Iteration 55, loss = 0.96792487\n",
      "Iteration 56, loss = 0.96262426\n",
      "Iteration 57, loss = 0.95924088\n",
      "Iteration 58, loss = 0.95573361\n",
      "Iteration 59, loss = 0.94935211\n",
      "Iteration 60, loss = 0.95473146\n",
      "Iteration 61, loss = 0.94931496\n",
      "Iteration 62, loss = 0.94558502\n",
      "Iteration 63, loss = 0.94225723\n",
      "Iteration 64, loss = 0.94259344\n",
      "Iteration 65, loss = 0.94318487\n",
      "Iteration 66, loss = 0.95034085\n",
      "Iteration 67, loss = 0.94527869\n",
      "Iteration 68, loss = 0.94621255\n",
      "Iteration 69, loss = 0.94279770\n",
      "Iteration 70, loss = 0.94352299\n",
      "Iteration 71, loss = 0.93183217\n",
      "Iteration 72, loss = 0.93310510\n",
      "Iteration 73, loss = 0.92456889\n",
      "Iteration 74, loss = 0.93015044\n",
      "Iteration 75, loss = 0.92313040\n",
      "Iteration 76, loss = 0.92201071\n",
      "Iteration 77, loss = 0.91982486\n",
      "Iteration 78, loss = 0.92361828\n",
      "Iteration 79, loss = 0.92152051\n",
      "Iteration 80, loss = 0.91995226\n",
      "Iteration 81, loss = 0.91638093\n",
      "Iteration 82, loss = 0.92185251\n",
      "Iteration 83, loss = 0.91495901\n",
      "Iteration 84, loss = 0.93019572\n",
      "Iteration 85, loss = 0.91857521\n",
      "Iteration 86, loss = 0.93082007\n",
      "Iteration 87, loss = 0.91341548\n",
      "Iteration 88, loss = 0.90992672\n",
      "Iteration 89, loss = 0.91565108\n",
      "Iteration 90, loss = 0.90998422\n",
      "Iteration 91, loss = 0.90602481\n",
      "Iteration 92, loss = 0.90037003\n",
      "Iteration 93, loss = 0.89959473\n",
      "Iteration 94, loss = 0.90098086\n",
      "Iteration 95, loss = 0.91057033\n",
      "Iteration 96, loss = 0.90904645\n",
      "Iteration 97, loss = 0.89239509\n",
      "Iteration 98, loss = 0.90004042\n",
      "Iteration 99, loss = 0.89288851\n",
      "Iteration 100, loss = 0.89411959\n",
      "Iteration 1, loss = 1.43813609\n",
      "Iteration 2, loss = 1.20029728\n",
      "Iteration 3, loss = 1.21629035\n",
      "Iteration 4, loss = 1.17717712\n",
      "Iteration 5, loss = 1.12586029\n",
      "Iteration 6, loss = 1.10357549\n",
      "Iteration 7, loss = 1.08853305\n",
      "Iteration 8, loss = 1.07423744\n",
      "Iteration 9, loss = 1.06348052\n",
      "Iteration 10, loss = 1.05380808\n",
      "Iteration 11, loss = 1.05167347\n",
      "Iteration 12, loss = 1.04793115\n",
      "Iteration 13, loss = 1.05375603\n",
      "Iteration 14, loss = 1.04305890\n",
      "Iteration 15, loss = 1.03084408\n",
      "Iteration 16, loss = 1.03010199\n",
      "Iteration 17, loss = 1.02732312\n",
      "Iteration 18, loss = 1.02789711\n",
      "Iteration 19, loss = 1.01902398\n",
      "Iteration 20, loss = 1.01997045\n",
      "Iteration 21, loss = 1.01449480\n",
      "Iteration 22, loss = 1.01415560\n",
      "Iteration 23, loss = 1.01679098\n",
      "Iteration 24, loss = 1.01385473\n",
      "Iteration 25, loss = 1.00734915\n",
      "Iteration 26, loss = 1.00265260\n",
      "Iteration 27, loss = 0.99948486\n",
      "Iteration 28, loss = 0.99680798\n",
      "Iteration 29, loss = 0.99791285\n",
      "Iteration 30, loss = 0.99450158\n",
      "Iteration 31, loss = 0.99493132\n",
      "Iteration 32, loss = 0.99694493\n",
      "Iteration 33, loss = 0.99648238\n",
      "Iteration 34, loss = 0.99340139\n",
      "Iteration 35, loss = 0.98930238\n",
      "Iteration 36, loss = 0.98249293\n",
      "Iteration 37, loss = 0.98287091\n",
      "Iteration 38, loss = 0.98125652\n",
      "Iteration 39, loss = 0.97960700\n",
      "Iteration 40, loss = 0.97821908\n",
      "Iteration 41, loss = 0.97910529\n",
      "Iteration 42, loss = 0.97979446\n",
      "Iteration 43, loss = 0.97395381\n",
      "Iteration 44, loss = 0.97370318\n",
      "Iteration 45, loss = 0.96462455\n",
      "Iteration 46, loss = 0.97432330\n",
      "Iteration 47, loss = 0.96525378\n",
      "Iteration 48, loss = 0.96266267\n",
      "Iteration 49, loss = 0.96137260\n",
      "Iteration 50, loss = 0.95816823\n",
      "Iteration 51, loss = 0.95525505\n",
      "Iteration 52, loss = 0.95237822\n",
      "Iteration 53, loss = 0.94842088\n",
      "Iteration 54, loss = 0.94843787\n",
      "Iteration 55, loss = 0.94929481\n",
      "Iteration 56, loss = 0.94463624\n",
      "Iteration 57, loss = 0.94621544\n",
      "Iteration 58, loss = 0.94513951\n",
      "Iteration 59, loss = 0.94411930\n",
      "Iteration 60, loss = 0.94031503\n",
      "Iteration 61, loss = 0.93609367\n",
      "Iteration 62, loss = 0.93842388\n",
      "Iteration 63, loss = 0.93713794\n",
      "Iteration 64, loss = 0.93051577\n",
      "Iteration 65, loss = 0.92960263\n",
      "Iteration 66, loss = 0.92745491\n",
      "Iteration 67, loss = 0.92707906\n",
      "Iteration 68, loss = 0.92784070\n",
      "Iteration 69, loss = 0.91802949\n",
      "Iteration 70, loss = 0.91798134\n",
      "Iteration 71, loss = 0.91706328\n",
      "Iteration 72, loss = 0.91801437\n",
      "Iteration 73, loss = 0.91437209\n",
      "Iteration 74, loss = 0.90979022\n",
      "Iteration 75, loss = 0.90817365\n",
      "Iteration 76, loss = 0.91066259\n",
      "Iteration 77, loss = 0.90183957\n",
      "Iteration 78, loss = 0.90450509\n",
      "Iteration 79, loss = 0.90988172\n",
      "Iteration 80, loss = 0.90667174\n",
      "Iteration 81, loss = 0.89997123\n",
      "Iteration 82, loss = 0.90125654\n",
      "Iteration 83, loss = 0.89565511\n",
      "Iteration 84, loss = 0.90829535\n",
      "Iteration 85, loss = 0.89196452\n",
      "Iteration 86, loss = 0.89791682\n",
      "Iteration 87, loss = 0.89251139\n",
      "Iteration 88, loss = 0.88561563\n",
      "Iteration 89, loss = 0.88565640\n",
      "Iteration 90, loss = 0.88608109\n",
      "Iteration 91, loss = 0.88043610\n",
      "Iteration 92, loss = 0.88893778\n",
      "Iteration 93, loss = 0.87593883\n",
      "Iteration 94, loss = 0.87503852\n",
      "Iteration 95, loss = 0.88091049\n",
      "Iteration 96, loss = 0.88369615\n",
      "Iteration 97, loss = 0.88093356\n",
      "Iteration 98, loss = 0.87463454\n",
      "Iteration 99, loss = 0.89235318\n",
      "Iteration 100, loss = 0.88595350\n",
      "Iteration 1, loss = 1.43980117\n",
      "Iteration 2, loss = 1.20116218\n",
      "Iteration 3, loss = 1.20250029\n",
      "Iteration 4, loss = 1.15935193\n",
      "Iteration 5, loss = 1.12322191\n",
      "Iteration 6, loss = 1.09768490\n",
      "Iteration 7, loss = 1.08345108\n",
      "Iteration 8, loss = 1.07339207\n",
      "Iteration 9, loss = 1.06417184\n",
      "Iteration 10, loss = 1.05526495\n",
      "Iteration 11, loss = 1.04649102\n",
      "Iteration 12, loss = 1.04718479\n",
      "Iteration 13, loss = 1.04170000\n",
      "Iteration 14, loss = 1.03844489\n",
      "Iteration 15, loss = 1.03384446\n",
      "Iteration 16, loss = 1.03932734\n",
      "Iteration 17, loss = 1.03277131\n",
      "Iteration 18, loss = 1.02971686\n",
      "Iteration 19, loss = 1.02220257\n",
      "Iteration 20, loss = 1.01498429\n",
      "Iteration 21, loss = 1.01239965\n",
      "Iteration 22, loss = 1.00986147\n",
      "Iteration 23, loss = 1.00531881\n",
      "Iteration 24, loss = 1.00258083\n",
      "Iteration 25, loss = 0.99973137\n",
      "Iteration 26, loss = 0.99661392\n",
      "Iteration 27, loss = 0.99378625\n",
      "Iteration 28, loss = 0.99193689\n",
      "Iteration 29, loss = 0.99002608\n",
      "Iteration 30, loss = 0.98424333\n",
      "Iteration 31, loss = 0.98329185\n",
      "Iteration 32, loss = 0.98120692\n",
      "Iteration 33, loss = 0.97806979\n",
      "Iteration 34, loss = 0.97928313\n",
      "Iteration 35, loss = 0.97935027\n",
      "Iteration 36, loss = 0.98018462\n",
      "Iteration 37, loss = 0.96737601\n",
      "Iteration 38, loss = 0.97202678\n",
      "Iteration 39, loss = 0.97002732\n",
      "Iteration 40, loss = 0.96598017\n",
      "Iteration 41, loss = 0.95986679\n",
      "Iteration 42, loss = 0.96051727\n",
      "Iteration 43, loss = 0.96416399\n",
      "Iteration 44, loss = 0.96678076\n",
      "Iteration 45, loss = 0.95287057\n",
      "Iteration 46, loss = 0.95861012\n",
      "Iteration 47, loss = 0.95073658\n",
      "Iteration 48, loss = 0.95990483\n",
      "Iteration 49, loss = 0.96019659\n",
      "Iteration 50, loss = 0.93952205\n",
      "Iteration 51, loss = 0.94872517\n",
      "Iteration 52, loss = 0.94645765\n",
      "Iteration 53, loss = 0.94283679\n",
      "Iteration 54, loss = 0.93519898\n",
      "Iteration 55, loss = 0.93469891\n",
      "Iteration 56, loss = 0.92617779\n",
      "Iteration 57, loss = 0.93499654\n",
      "Iteration 58, loss = 0.93149786\n",
      "Iteration 59, loss = 0.92235348\n",
      "Iteration 60, loss = 0.91984943\n",
      "Iteration 61, loss = 0.92236147\n",
      "Iteration 62, loss = 0.92696702\n",
      "Iteration 63, loss = 0.91985774\n",
      "Iteration 64, loss = 0.91420006\n",
      "Iteration 65, loss = 0.92571713\n",
      "Iteration 66, loss = 0.91672853\n",
      "Iteration 67, loss = 0.91267180\n",
      "Iteration 68, loss = 0.90479998\n",
      "Iteration 69, loss = 0.91795211\n",
      "Iteration 70, loss = 0.91110872\n",
      "Iteration 71, loss = 0.90129577\n",
      "Iteration 72, loss = 0.90908235\n",
      "Iteration 73, loss = 0.90393864\n",
      "Iteration 74, loss = 0.89797931\n",
      "Iteration 75, loss = 0.89019711\n",
      "Iteration 76, loss = 0.89910938\n",
      "Iteration 77, loss = 0.88962058\n",
      "Iteration 78, loss = 0.90852298\n",
      "Iteration 79, loss = 0.90278548\n",
      "Iteration 80, loss = 0.90037157\n",
      "Iteration 81, loss = 0.90587103\n",
      "Iteration 82, loss = 0.89907430\n",
      "Iteration 83, loss = 0.91293356\n",
      "Iteration 84, loss = 0.89082658\n",
      "Iteration 85, loss = 0.88623821\n",
      "Iteration 86, loss = 0.88235113\n",
      "Iteration 87, loss = 0.88843906\n",
      "Iteration 88, loss = 0.88445317\n",
      "Iteration 89, loss = 0.87679246\n",
      "Iteration 90, loss = 0.87015560\n",
      "Iteration 91, loss = 0.87025159\n",
      "Iteration 92, loss = 0.86959263\n",
      "Iteration 93, loss = 0.87514469\n",
      "Iteration 94, loss = 0.87842137\n",
      "Iteration 95, loss = 0.85990870\n",
      "Iteration 96, loss = 0.87427132\n",
      "Iteration 97, loss = 0.86934301\n",
      "Iteration 98, loss = 0.85893760\n",
      "Iteration 99, loss = 0.86204299\n",
      "Iteration 100, loss = 0.86306107\n",
      "Iteration 1, loss = 1.44477941\n",
      "Iteration 2, loss = 1.21250925\n",
      "Iteration 3, loss = 1.21424792\n",
      "Iteration 4, loss = 1.17919974\n",
      "Iteration 5, loss = 1.13327117\n",
      "Iteration 6, loss = 1.11786047\n",
      "Iteration 7, loss = 1.10314161\n",
      "Iteration 8, loss = 1.08961380\n",
      "Iteration 9, loss = 1.07977104\n",
      "Iteration 10, loss = 1.08285152\n",
      "Iteration 11, loss = 1.07650909\n",
      "Iteration 12, loss = 1.06276892\n",
      "Iteration 13, loss = 1.05760257\n",
      "Iteration 14, loss = 1.05426220\n",
      "Iteration 15, loss = 1.05562722\n",
      "Iteration 16, loss = 1.05219626\n",
      "Iteration 17, loss = 1.04660136\n",
      "Iteration 18, loss = 1.03923270\n",
      "Iteration 19, loss = 1.03769518\n",
      "Iteration 20, loss = 1.02929878\n",
      "Iteration 21, loss = 1.02964325\n",
      "Iteration 22, loss = 1.02441517\n",
      "Iteration 23, loss = 1.02238647\n",
      "Iteration 24, loss = 1.01758631\n",
      "Iteration 25, loss = 1.01349903\n",
      "Iteration 26, loss = 1.00969081\n",
      "Iteration 27, loss = 1.00896509\n",
      "Iteration 28, loss = 1.01232673\n",
      "Iteration 29, loss = 1.01495034\n",
      "Iteration 30, loss = 1.00355741\n",
      "Iteration 31, loss = 1.01813906\n",
      "Iteration 32, loss = 1.01402291\n",
      "Iteration 33, loss = 1.00678120\n",
      "Iteration 34, loss = 1.00016580\n",
      "Iteration 35, loss = 0.99333785\n",
      "Iteration 36, loss = 1.00200488\n",
      "Iteration 37, loss = 0.97546460\n",
      "Iteration 38, loss = 0.98823341\n",
      "Iteration 39, loss = 0.97978711\n",
      "Iteration 40, loss = 0.97406301\n",
      "Iteration 41, loss = 0.97819500\n",
      "Iteration 42, loss = 0.97591138\n",
      "Iteration 43, loss = 0.97083980\n",
      "Iteration 44, loss = 0.96661621\n",
      "Iteration 45, loss = 0.96465826\n",
      "Iteration 46, loss = 0.96147924\n",
      "Iteration 47, loss = 0.95707109\n",
      "Iteration 48, loss = 0.95852106\n",
      "Iteration 49, loss = 0.95965040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.95594376\n",
      "Iteration 51, loss = 0.95217628\n",
      "Iteration 52, loss = 0.95463857\n",
      "Iteration 53, loss = 0.94768747\n",
      "Iteration 54, loss = 0.94349436\n",
      "Iteration 55, loss = 0.94587080\n",
      "Iteration 56, loss = 0.94149811\n",
      "Iteration 57, loss = 0.93927572\n",
      "Iteration 58, loss = 0.93664931\n",
      "Iteration 59, loss = 0.93570173\n",
      "Iteration 60, loss = 0.93300521\n",
      "Iteration 61, loss = 0.93223353\n",
      "Iteration 62, loss = 0.93250854\n",
      "Iteration 63, loss = 0.93328731\n",
      "Iteration 64, loss = 0.93078933\n",
      "Iteration 65, loss = 0.91945226\n",
      "Iteration 66, loss = 0.92657296\n",
      "Iteration 67, loss = 0.91872301\n",
      "Iteration 68, loss = 0.91566941\n",
      "Iteration 69, loss = 0.91907340\n",
      "Iteration 70, loss = 0.91015525\n",
      "Iteration 71, loss = 0.90985371\n",
      "Iteration 72, loss = 0.91022643\n",
      "Iteration 73, loss = 0.90938041\n",
      "Iteration 74, loss = 0.90559698\n",
      "Iteration 75, loss = 0.90665257\n",
      "Iteration 76, loss = 0.90137269\n",
      "Iteration 77, loss = 0.89911720\n",
      "Iteration 78, loss = 0.91902789\n",
      "Iteration 79, loss = 0.92512455\n",
      "Iteration 80, loss = 0.91425804\n",
      "Iteration 81, loss = 0.89409553\n",
      "Iteration 82, loss = 0.90513791\n",
      "Iteration 83, loss = 0.94533769\n",
      "Iteration 84, loss = 0.91990394\n",
      "Iteration 85, loss = 0.90724769\n",
      "Iteration 86, loss = 0.89290854\n",
      "Iteration 87, loss = 0.89510019\n",
      "Iteration 88, loss = 0.88703203\n",
      "Iteration 89, loss = 0.89391796\n",
      "Iteration 90, loss = 0.88395672\n",
      "Iteration 91, loss = 0.88210813\n",
      "Iteration 92, loss = 0.87935718\n",
      "Iteration 93, loss = 0.87607750\n",
      "Iteration 94, loss = 0.87153607\n",
      "Iteration 95, loss = 0.87554761\n",
      "Iteration 96, loss = 0.87919780\n",
      "Iteration 97, loss = 0.87974170\n",
      "Iteration 98, loss = 0.87519153\n",
      "Iteration 99, loss = 0.87220753\n",
      "Iteration 100, loss = 0.87629052\n",
      "Iteration 1, loss = 1.43814877\n",
      "Iteration 2, loss = 1.19268851\n",
      "Iteration 3, loss = 1.16359934\n",
      "Iteration 4, loss = 1.11456436\n",
      "Iteration 5, loss = 1.07570738\n",
      "Iteration 6, loss = 1.05331334\n",
      "Iteration 7, loss = 1.04060344\n",
      "Iteration 8, loss = 1.02910200\n",
      "Iteration 9, loss = 1.02955033\n",
      "Iteration 10, loss = 1.03289467\n",
      "Iteration 11, loss = 1.02474221\n",
      "Iteration 12, loss = 1.01502308\n",
      "Iteration 13, loss = 1.01550279\n",
      "Iteration 14, loss = 1.01345433\n",
      "Iteration 15, loss = 1.00514768\n",
      "Iteration 16, loss = 1.00178567\n",
      "Iteration 17, loss = 1.00361686\n",
      "Iteration 18, loss = 0.99930976\n",
      "Iteration 19, loss = 0.99386525\n",
      "Iteration 20, loss = 0.99175242\n",
      "Iteration 21, loss = 0.98833973\n",
      "Iteration 22, loss = 0.98861792\n",
      "Iteration 23, loss = 0.98108804\n",
      "Iteration 24, loss = 0.98015659\n",
      "Iteration 25, loss = 0.97925074\n",
      "Iteration 26, loss = 0.99374941\n",
      "Iteration 27, loss = 0.99723340\n",
      "Iteration 28, loss = 0.97859929\n",
      "Iteration 29, loss = 0.97413231\n",
      "Iteration 30, loss = 0.96925739\n",
      "Iteration 31, loss = 0.97424211\n",
      "Iteration 32, loss = 0.96560069\n",
      "Iteration 33, loss = 0.96166077\n",
      "Iteration 34, loss = 0.95886210\n",
      "Iteration 35, loss = 0.95476247\n",
      "Iteration 36, loss = 0.94690087\n",
      "Iteration 37, loss = 0.95017858\n",
      "Iteration 38, loss = 0.94850085\n",
      "Iteration 39, loss = 0.95178208\n",
      "Iteration 40, loss = 0.94515320\n",
      "Iteration 41, loss = 0.94590244\n",
      "Iteration 42, loss = 0.93214150\n",
      "Iteration 43, loss = 0.93533563\n",
      "Iteration 44, loss = 0.93983263\n",
      "Iteration 45, loss = 0.93435304\n",
      "Iteration 46, loss = 0.92652881\n",
      "Iteration 47, loss = 0.93060945\n",
      "Iteration 48, loss = 0.92068180\n",
      "Iteration 49, loss = 0.92014581\n",
      "Iteration 50, loss = 0.92256165\n",
      "Iteration 51, loss = 0.91171769\n",
      "Iteration 52, loss = 0.91113740\n",
      "Iteration 53, loss = 0.91127079\n",
      "Iteration 54, loss = 0.90986370\n",
      "Iteration 55, loss = 0.90703281\n",
      "Iteration 56, loss = 0.89826414\n",
      "Iteration 57, loss = 0.89915781\n",
      "Iteration 58, loss = 0.89588160\n",
      "Iteration 59, loss = 0.90440179\n",
      "Iteration 60, loss = 0.89524247\n",
      "Iteration 61, loss = 0.89398883\n",
      "Iteration 62, loss = 0.88858937\n",
      "Iteration 63, loss = 0.88161195\n",
      "Iteration 64, loss = 0.88101633\n",
      "Iteration 65, loss = 0.88518128\n",
      "Iteration 66, loss = 0.87695775\n",
      "Iteration 67, loss = 0.88694098\n",
      "Iteration 68, loss = 0.88471513\n",
      "Iteration 69, loss = 0.89176973\n",
      "Iteration 70, loss = 0.87961355\n",
      "Iteration 71, loss = 0.86682297\n",
      "Iteration 72, loss = 0.87654726\n",
      "Iteration 73, loss = 0.85995248\n",
      "Iteration 74, loss = 0.85568392\n",
      "Iteration 75, loss = 0.85186528\n",
      "Iteration 76, loss = 0.85397700\n",
      "Iteration 77, loss = 0.85009013\n",
      "Iteration 78, loss = 0.84742016\n",
      "Iteration 79, loss = 0.84431882\n",
      "Iteration 80, loss = 0.85023736\n",
      "Iteration 81, loss = 0.84538885\n",
      "Iteration 82, loss = 0.83924217\n",
      "Iteration 83, loss = 0.83632554\n",
      "Iteration 84, loss = 0.83366597\n",
      "Iteration 85, loss = 0.83560404\n",
      "Iteration 86, loss = 0.83994903\n",
      "Iteration 87, loss = 0.84185930\n",
      "Iteration 88, loss = 0.83568556\n",
      "Iteration 89, loss = 0.82879214\n",
      "Iteration 90, loss = 0.81359384\n",
      "Iteration 91, loss = 0.81684871\n",
      "Iteration 92, loss = 0.81353988\n",
      "Iteration 93, loss = 0.81538441\n",
      "Iteration 94, loss = 0.81723686\n",
      "Iteration 95, loss = 0.82807668\n",
      "Iteration 96, loss = 0.81299300\n",
      "Iteration 97, loss = 0.81517184\n",
      "Iteration 98, loss = 0.83428475\n",
      "Iteration 99, loss = 0.82377903\n",
      "Iteration 100, loss = 0.81327831\n",
      "Iteration 1, loss = 11.28211885\n",
      "Iteration 2, loss = 8.50274031\n",
      "Iteration 3, loss = 4.89335311\n",
      "Iteration 4, loss = 2.57724139\n",
      "Iteration 5, loss = 1.99815458\n",
      "Iteration 6, loss = 1.53145906\n",
      "Iteration 7, loss = 1.35929297\n",
      "Iteration 8, loss = 1.28011502\n",
      "Iteration 9, loss = 1.23825590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 1.20652843\n",
      "Iteration 11, loss = 1.19785783\n",
      "Iteration 12, loss = 1.20672560\n",
      "Iteration 13, loss = 1.19102470\n",
      "Iteration 14, loss = 1.18922705\n",
      "Iteration 15, loss = 1.20204524\n",
      "Iteration 16, loss = 1.20698414\n",
      "Iteration 17, loss = 1.20674981\n",
      "Iteration 18, loss = 1.20270249\n",
      "Iteration 19, loss = 1.19401316\n",
      "Iteration 20, loss = 1.18004566\n",
      "Iteration 21, loss = 1.18101651\n",
      "Iteration 22, loss = 1.16914916\n",
      "Iteration 23, loss = 1.16653693\n",
      "Iteration 24, loss = 1.16449112\n",
      "Iteration 25, loss = 1.16068495\n",
      "Iteration 26, loss = 1.16213065\n",
      "Iteration 27, loss = 1.15472278\n",
      "Iteration 28, loss = 1.17883561\n",
      "Iteration 29, loss = 1.15387070\n",
      "Iteration 30, loss = 1.15101172\n",
      "Iteration 31, loss = 1.17695534\n",
      "Iteration 32, loss = 1.17389804\n",
      "Iteration 33, loss = 1.18033695\n",
      "Iteration 34, loss = 1.14882046\n",
      "Iteration 35, loss = 1.14831140\n",
      "Iteration 36, loss = 1.17443811\n",
      "Iteration 37, loss = 1.15309577\n",
      "Iteration 38, loss = 1.17728211\n",
      "Iteration 39, loss = 1.18061747\n",
      "Iteration 40, loss = 1.19072349\n",
      "Iteration 41, loss = 1.17401851\n",
      "Iteration 42, loss = 1.21596380\n",
      "Iteration 43, loss = 1.22886105\n",
      "Iteration 44, loss = 1.21801352\n",
      "Iteration 45, loss = 1.17926498\n",
      "Iteration 46, loss = 1.17462787\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Two-layer model (128, 16): CV accuracy = 0.445\n",
      "Iteration 1, loss = 1.42798009\n",
      "Iteration 2, loss = 1.25916137\n",
      "Iteration 3, loss = 1.23523012\n",
      "Iteration 4, loss = 1.21675445\n",
      "Iteration 5, loss = 1.18559188\n",
      "Iteration 6, loss = 1.17821098\n",
      "Iteration 7, loss = 1.14280694\n",
      "Iteration 8, loss = 1.12056700\n",
      "Iteration 9, loss = 1.10153713\n",
      "Iteration 10, loss = 1.07855961\n",
      "Iteration 11, loss = 1.06662871\n",
      "Iteration 12, loss = 1.05809703\n",
      "Iteration 13, loss = 1.04646049\n",
      "Iteration 14, loss = 1.04146184\n",
      "Iteration 15, loss = 1.03342667\n",
      "Iteration 16, loss = 1.03215289\n",
      "Iteration 17, loss = 1.02309145\n",
      "Iteration 18, loss = 1.02046868\n",
      "Iteration 19, loss = 1.01897412\n",
      "Iteration 20, loss = 1.01217908\n",
      "Iteration 21, loss = 1.02217618\n",
      "Iteration 22, loss = 1.00861231\n",
      "Iteration 23, loss = 1.01256977\n",
      "Iteration 24, loss = 1.00565102\n",
      "Iteration 25, loss = 0.99723360\n",
      "Iteration 26, loss = 0.99007113\n",
      "Iteration 27, loss = 0.98943692\n",
      "Iteration 28, loss = 0.99823312\n",
      "Iteration 29, loss = 0.98910175\n",
      "Iteration 30, loss = 0.98519824\n",
      "Iteration 31, loss = 0.98839338\n",
      "Iteration 32, loss = 0.98742048\n",
      "Iteration 33, loss = 0.98454819\n",
      "Iteration 34, loss = 0.97314906\n",
      "Iteration 35, loss = 0.97001608\n",
      "Iteration 36, loss = 0.96533564\n",
      "Iteration 37, loss = 0.96458148\n",
      "Iteration 38, loss = 0.97220314\n",
      "Iteration 39, loss = 0.97817322\n",
      "Iteration 40, loss = 0.97089692\n",
      "Iteration 41, loss = 0.96592788\n",
      "Iteration 42, loss = 0.97204805\n",
      "Iteration 43, loss = 0.95783739\n",
      "Iteration 44, loss = 0.95105618\n",
      "Iteration 45, loss = 0.94665066\n",
      "Iteration 46, loss = 0.94613901\n",
      "Iteration 47, loss = 0.94594582\n",
      "Iteration 48, loss = 0.94043977\n",
      "Iteration 49, loss = 0.94195803\n",
      "Iteration 50, loss = 0.94299354\n",
      "Iteration 51, loss = 0.94262187\n",
      "Iteration 52, loss = 0.93246612\n",
      "Iteration 53, loss = 0.93030710\n",
      "Iteration 54, loss = 0.92762513\n",
      "Iteration 55, loss = 0.92885912\n",
      "Iteration 56, loss = 0.93016093\n",
      "Iteration 57, loss = 0.93064380\n",
      "Iteration 58, loss = 0.93181607\n",
      "Iteration 59, loss = 0.93318058\n",
      "Iteration 60, loss = 0.93753423\n",
      "Iteration 61, loss = 0.92426305\n",
      "Iteration 62, loss = 0.93335839\n",
      "Iteration 63, loss = 0.92050059\n",
      "Iteration 64, loss = 0.93129851\n",
      "Iteration 65, loss = 0.91588716\n",
      "Iteration 66, loss = 0.91894851\n",
      "Iteration 67, loss = 0.90888838\n",
      "Iteration 68, loss = 0.90530479\n",
      "Iteration 69, loss = 0.90872209\n",
      "Iteration 70, loss = 0.89660120\n",
      "Iteration 71, loss = 0.90844290\n",
      "Iteration 72, loss = 0.90557795\n",
      "Iteration 73, loss = 0.89927332\n",
      "Iteration 74, loss = 0.89190615\n",
      "Iteration 75, loss = 0.89621385\n",
      "Iteration 76, loss = 0.89554189\n",
      "Iteration 77, loss = 0.89672937\n",
      "Iteration 78, loss = 0.89659496\n",
      "Iteration 79, loss = 0.90589936\n",
      "Iteration 80, loss = 0.89026948\n",
      "Iteration 81, loss = 0.89724939\n",
      "Iteration 82, loss = 0.89362154\n",
      "Iteration 83, loss = 0.89513840\n",
      "Iteration 84, loss = 0.89360818\n",
      "Iteration 85, loss = 0.88630461\n",
      "Iteration 86, loss = 0.89088532\n",
      "Iteration 87, loss = 0.88934892\n",
      "Iteration 88, loss = 0.88358862\n",
      "Iteration 89, loss = 0.87106830\n",
      "Iteration 90, loss = 0.88416586\n",
      "Iteration 91, loss = 0.89668910\n",
      "Iteration 92, loss = 0.87866717\n",
      "Iteration 93, loss = 0.87855787\n",
      "Iteration 94, loss = 0.87363500\n",
      "Iteration 95, loss = 0.87017028\n",
      "Iteration 96, loss = 0.87025496\n",
      "Iteration 97, loss = 0.87006706\n",
      "Iteration 98, loss = 0.86323630\n",
      "Iteration 99, loss = 0.85852127\n",
      "Iteration 100, loss = 0.86131298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.42007238\n",
      "Iteration 2, loss = 1.24789525\n",
      "Iteration 3, loss = 1.22002496\n",
      "Iteration 4, loss = 1.19804456\n",
      "Iteration 5, loss = 1.16981201\n",
      "Iteration 6, loss = 1.14686419\n",
      "Iteration 7, loss = 1.11566659\n",
      "Iteration 8, loss = 1.09273356\n",
      "Iteration 9, loss = 1.06709490\n",
      "Iteration 10, loss = 1.05313360\n",
      "Iteration 11, loss = 1.04192298\n",
      "Iteration 12, loss = 1.03962274\n",
      "Iteration 13, loss = 1.03113513\n",
      "Iteration 14, loss = 1.03123419\n",
      "Iteration 15, loss = 1.02533251\n",
      "Iteration 16, loss = 1.02339546\n",
      "Iteration 17, loss = 1.02405921\n",
      "Iteration 18, loss = 1.02117989\n",
      "Iteration 19, loss = 1.01523677\n",
      "Iteration 20, loss = 1.01190357\n",
      "Iteration 21, loss = 1.00467219\n",
      "Iteration 22, loss = 1.00337862\n",
      "Iteration 23, loss = 1.00195421\n",
      "Iteration 24, loss = 1.00242886\n",
      "Iteration 25, loss = 0.99875655\n",
      "Iteration 26, loss = 0.99629878\n",
      "Iteration 27, loss = 0.99344910\n",
      "Iteration 28, loss = 1.00496874\n",
      "Iteration 29, loss = 1.00008480\n",
      "Iteration 30, loss = 0.99626174\n",
      "Iteration 31, loss = 0.98571309\n",
      "Iteration 32, loss = 0.98474784\n",
      "Iteration 33, loss = 0.97925686\n",
      "Iteration 34, loss = 0.97631678\n",
      "Iteration 35, loss = 0.97120071\n",
      "Iteration 36, loss = 0.96597499\n",
      "Iteration 37, loss = 0.97474540\n",
      "Iteration 38, loss = 0.96618303\n",
      "Iteration 39, loss = 0.96150216\n",
      "Iteration 40, loss = 0.95985936\n",
      "Iteration 41, loss = 0.95893115\n",
      "Iteration 42, loss = 0.95557371\n",
      "Iteration 43, loss = 0.95903629\n",
      "Iteration 44, loss = 0.94973342\n",
      "Iteration 45, loss = 0.95517207\n",
      "Iteration 46, loss = 0.95386188\n",
      "Iteration 47, loss = 0.95331106\n",
      "Iteration 48, loss = 0.94719870\n",
      "Iteration 49, loss = 0.94545523\n",
      "Iteration 50, loss = 0.93809697\n",
      "Iteration 51, loss = 0.94638982\n",
      "Iteration 52, loss = 0.93338031\n",
      "Iteration 53, loss = 0.93377747\n",
      "Iteration 54, loss = 0.93338870\n",
      "Iteration 55, loss = 0.93121333\n",
      "Iteration 56, loss = 0.93014220\n",
      "Iteration 57, loss = 0.94362668\n",
      "Iteration 58, loss = 0.94302589\n",
      "Iteration 59, loss = 0.93687693\n",
      "Iteration 60, loss = 0.93644588\n",
      "Iteration 61, loss = 0.93056045\n",
      "Iteration 62, loss = 0.94005561\n",
      "Iteration 63, loss = 0.92445621\n",
      "Iteration 64, loss = 0.92579849\n",
      "Iteration 65, loss = 0.92003359\n",
      "Iteration 66, loss = 0.91188633\n",
      "Iteration 67, loss = 0.91150073\n",
      "Iteration 68, loss = 0.91814925\n",
      "Iteration 69, loss = 0.91784005\n",
      "Iteration 70, loss = 0.90587202\n",
      "Iteration 71, loss = 0.92351320\n",
      "Iteration 72, loss = 0.91278863\n",
      "Iteration 73, loss = 0.90917387\n",
      "Iteration 74, loss = 0.90213044\n",
      "Iteration 75, loss = 0.89707786\n",
      "Iteration 76, loss = 0.89842956\n",
      "Iteration 77, loss = 0.90223358\n",
      "Iteration 78, loss = 0.89268867\n",
      "Iteration 79, loss = 0.89420775\n",
      "Iteration 80, loss = 0.89155657\n",
      "Iteration 81, loss = 0.89299254\n",
      "Iteration 82, loss = 0.88493559\n",
      "Iteration 83, loss = 0.88691984\n",
      "Iteration 84, loss = 0.88553386\n",
      "Iteration 85, loss = 0.88051482\n",
      "Iteration 86, loss = 0.87833083\n",
      "Iteration 87, loss = 0.88490941\n",
      "Iteration 88, loss = 0.87973275\n",
      "Iteration 89, loss = 0.87418335\n",
      "Iteration 90, loss = 0.87088480\n",
      "Iteration 91, loss = 0.88047667\n",
      "Iteration 92, loss = 0.86585727\n",
      "Iteration 93, loss = 0.87254872\n",
      "Iteration 94, loss = 0.86786481\n",
      "Iteration 95, loss = 0.87686908\n",
      "Iteration 96, loss = 0.87685421\n",
      "Iteration 97, loss = 0.87791405\n",
      "Iteration 98, loss = 0.88359747\n",
      "Iteration 99, loss = 0.88271263\n",
      "Iteration 100, loss = 0.87357624\n",
      "Iteration 1, loss = 1.41737161\n",
      "Iteration 2, loss = 1.24471489\n",
      "Iteration 3, loss = 1.21848238\n",
      "Iteration 4, loss = 1.18778201\n",
      "Iteration 5, loss = 1.16048630\n",
      "Iteration 6, loss = 1.13606185\n",
      "Iteration 7, loss = 1.10377903\n",
      "Iteration 8, loss = 1.08344553\n",
      "Iteration 9, loss = 1.06716178\n",
      "Iteration 10, loss = 1.05425579\n",
      "Iteration 11, loss = 1.04413249\n",
      "Iteration 12, loss = 1.04769184\n",
      "Iteration 13, loss = 1.03876031\n",
      "Iteration 14, loss = 1.03402782\n",
      "Iteration 15, loss = 1.03579660\n",
      "Iteration 16, loss = 1.03443706\n",
      "Iteration 17, loss = 1.02116725\n",
      "Iteration 18, loss = 1.02847387\n",
      "Iteration 19, loss = 1.02511573\n",
      "Iteration 20, loss = 1.01436075\n",
      "Iteration 21, loss = 1.01681562\n",
      "Iteration 22, loss = 1.01027553\n",
      "Iteration 23, loss = 1.00538885\n",
      "Iteration 24, loss = 1.00371529\n",
      "Iteration 25, loss = 0.99856642\n",
      "Iteration 26, loss = 0.99814246\n",
      "Iteration 27, loss = 0.99950080\n",
      "Iteration 28, loss = 0.98829253\n",
      "Iteration 29, loss = 0.98986404\n",
      "Iteration 30, loss = 0.98870412\n",
      "Iteration 31, loss = 0.98428627\n",
      "Iteration 32, loss = 0.98392255\n",
      "Iteration 33, loss = 0.98048741\n",
      "Iteration 34, loss = 0.98093085\n",
      "Iteration 35, loss = 0.99167264\n",
      "Iteration 36, loss = 0.97565466\n",
      "Iteration 37, loss = 0.96937239\n",
      "Iteration 38, loss = 0.96131245\n",
      "Iteration 39, loss = 0.95961740\n",
      "Iteration 40, loss = 0.95574887\n",
      "Iteration 41, loss = 0.95526156\n",
      "Iteration 42, loss = 0.95768384\n",
      "Iteration 43, loss = 0.95956604\n",
      "Iteration 44, loss = 0.94892344\n",
      "Iteration 45, loss = 0.94935606\n",
      "Iteration 46, loss = 0.94301045\n",
      "Iteration 47, loss = 0.94339147\n",
      "Iteration 48, loss = 0.94658598\n",
      "Iteration 49, loss = 0.93917784\n",
      "Iteration 50, loss = 0.93771697\n",
      "Iteration 51, loss = 0.93215880\n",
      "Iteration 52, loss = 0.93534949\n",
      "Iteration 53, loss = 0.93277194\n",
      "Iteration 54, loss = 0.93647874\n",
      "Iteration 55, loss = 0.92771536\n",
      "Iteration 56, loss = 0.92986428\n",
      "Iteration 57, loss = 0.92439257\n",
      "Iteration 58, loss = 0.92206113\n",
      "Iteration 59, loss = 0.92207324\n",
      "Iteration 60, loss = 0.92971398\n",
      "Iteration 61, loss = 0.92762500\n",
      "Iteration 62, loss = 0.92955073\n",
      "Iteration 63, loss = 0.91956015\n",
      "Iteration 64, loss = 0.90289487\n",
      "Iteration 65, loss = 0.90453869\n",
      "Iteration 66, loss = 0.90207761\n",
      "Iteration 67, loss = 0.90223762\n",
      "Iteration 68, loss = 0.90221240\n",
      "Iteration 69, loss = 0.89399754\n",
      "Iteration 70, loss = 0.89583287\n",
      "Iteration 71, loss = 0.90029646\n",
      "Iteration 72, loss = 0.89552733\n",
      "Iteration 73, loss = 0.89618327\n",
      "Iteration 74, loss = 0.88132183\n",
      "Iteration 75, loss = 0.88352262\n",
      "Iteration 76, loss = 0.87904850\n",
      "Iteration 77, loss = 0.88334465\n",
      "Iteration 78, loss = 0.88221547\n",
      "Iteration 79, loss = 0.87860425\n",
      "Iteration 80, loss = 0.87058245\n",
      "Iteration 81, loss = 0.88154053\n",
      "Iteration 82, loss = 0.87222416\n",
      "Iteration 83, loss = 0.86370600\n",
      "Iteration 84, loss = 0.86550612\n",
      "Iteration 85, loss = 0.86669862\n",
      "Iteration 86, loss = 0.85929971\n",
      "Iteration 87, loss = 0.87230607\n",
      "Iteration 88, loss = 0.86902436\n",
      "Iteration 89, loss = 0.86289758\n",
      "Iteration 90, loss = 0.85718825\n",
      "Iteration 91, loss = 0.85168028\n",
      "Iteration 92, loss = 0.85560199\n",
      "Iteration 93, loss = 0.84793864\n",
      "Iteration 94, loss = 0.84887037\n",
      "Iteration 95, loss = 0.85433546\n",
      "Iteration 96, loss = 0.84801656\n",
      "Iteration 97, loss = 0.84707579\n",
      "Iteration 98, loss = 0.84328479\n",
      "Iteration 99, loss = 0.85154391\n",
      "Iteration 100, loss = 0.85153186\n",
      "Iteration 1, loss = 1.42955433\n",
      "Iteration 2, loss = 1.25441729\n",
      "Iteration 3, loss = 1.23004870\n",
      "Iteration 4, loss = 1.22898526\n",
      "Iteration 5, loss = 1.20292753\n",
      "Iteration 6, loss = 1.18169925\n",
      "Iteration 7, loss = 1.15769526\n",
      "Iteration 8, loss = 1.13083382\n",
      "Iteration 9, loss = 1.11134439\n",
      "Iteration 10, loss = 1.09402228\n",
      "Iteration 11, loss = 1.07920452\n",
      "Iteration 12, loss = 1.07717136\n",
      "Iteration 13, loss = 1.06291931\n",
      "Iteration 14, loss = 1.06666539\n",
      "Iteration 15, loss = 1.05482832\n",
      "Iteration 16, loss = 1.04814586\n",
      "Iteration 17, loss = 1.03952838\n",
      "Iteration 18, loss = 1.03861513\n",
      "Iteration 19, loss = 1.03209595\n",
      "Iteration 20, loss = 1.03180536\n",
      "Iteration 21, loss = 1.02715702\n",
      "Iteration 22, loss = 1.02617850\n",
      "Iteration 23, loss = 1.02024587\n",
      "Iteration 24, loss = 1.02179183\n",
      "Iteration 25, loss = 1.01405411\n",
      "Iteration 26, loss = 1.01048711\n",
      "Iteration 27, loss = 1.00814442\n",
      "Iteration 28, loss = 1.00799562\n",
      "Iteration 29, loss = 1.00519113\n",
      "Iteration 30, loss = 1.00212780\n",
      "Iteration 31, loss = 1.00469332\n",
      "Iteration 32, loss = 1.00033434\n",
      "Iteration 33, loss = 0.99830450\n",
      "Iteration 34, loss = 0.99509493\n",
      "Iteration 35, loss = 0.99909504\n",
      "Iteration 36, loss = 0.99212678\n",
      "Iteration 37, loss = 0.98545538\n",
      "Iteration 38, loss = 0.97459538\n",
      "Iteration 39, loss = 0.97594892\n",
      "Iteration 40, loss = 0.97118848\n",
      "Iteration 41, loss = 0.96890349\n",
      "Iteration 42, loss = 0.96588350\n",
      "Iteration 43, loss = 0.96166871\n",
      "Iteration 44, loss = 0.95446816\n",
      "Iteration 45, loss = 0.96270252\n",
      "Iteration 46, loss = 0.95466741\n",
      "Iteration 47, loss = 0.95185174\n",
      "Iteration 48, loss = 0.95317428\n",
      "Iteration 49, loss = 0.95076015\n",
      "Iteration 50, loss = 0.95085571\n",
      "Iteration 51, loss = 0.95380950\n",
      "Iteration 52, loss = 0.95705536\n",
      "Iteration 53, loss = 0.94499657\n",
      "Iteration 54, loss = 0.94319740\n",
      "Iteration 55, loss = 0.93188195\n",
      "Iteration 56, loss = 0.92725794\n",
      "Iteration 57, loss = 0.92597259\n",
      "Iteration 58, loss = 0.93022181\n",
      "Iteration 59, loss = 0.92770516\n",
      "Iteration 60, loss = 0.92836473\n",
      "Iteration 61, loss = 0.91355735\n",
      "Iteration 62, loss = 0.91826531\n",
      "Iteration 63, loss = 0.91424066\n",
      "Iteration 64, loss = 0.90974468\n",
      "Iteration 65, loss = 0.90207630\n",
      "Iteration 66, loss = 0.90620934\n",
      "Iteration 67, loss = 0.90666548\n",
      "Iteration 68, loss = 0.90264363\n",
      "Iteration 69, loss = 0.89796415\n",
      "Iteration 70, loss = 0.90689153\n",
      "Iteration 71, loss = 0.90263095\n",
      "Iteration 72, loss = 0.90405589\n",
      "Iteration 73, loss = 0.89482713\n",
      "Iteration 74, loss = 0.88929455\n",
      "Iteration 75, loss = 0.89200876\n",
      "Iteration 76, loss = 0.89130895\n",
      "Iteration 77, loss = 0.89720399\n",
      "Iteration 78, loss = 0.88758460\n",
      "Iteration 79, loss = 0.88555904\n",
      "Iteration 80, loss = 0.88788562\n",
      "Iteration 81, loss = 0.87856229\n",
      "Iteration 82, loss = 0.88692841\n",
      "Iteration 83, loss = 0.87607665\n",
      "Iteration 84, loss = 0.86746094\n",
      "Iteration 85, loss = 0.86541665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 86, loss = 0.87017699\n",
      "Iteration 87, loss = 0.86238111\n",
      "Iteration 88, loss = 0.86510362\n",
      "Iteration 89, loss = 0.85490834\n",
      "Iteration 90, loss = 0.85881864\n",
      "Iteration 91, loss = 0.85203208\n",
      "Iteration 92, loss = 0.85297756\n",
      "Iteration 93, loss = 0.85506365\n",
      "Iteration 94, loss = 0.84944356\n",
      "Iteration 95, loss = 0.85722410\n",
      "Iteration 96, loss = 0.85109142\n",
      "Iteration 97, loss = 0.86034225\n",
      "Iteration 98, loss = 0.86263074\n",
      "Iteration 99, loss = 0.85681723\n",
      "Iteration 100, loss = 0.84248385\n",
      "Iteration 1, loss = 1.41886803\n",
      "Iteration 2, loss = 1.24595237\n",
      "Iteration 3, loss = 1.22732779\n",
      "Iteration 4, loss = 1.19766709\n",
      "Iteration 5, loss = 1.17038030\n",
      "Iteration 6, loss = 1.14987915\n",
      "Iteration 7, loss = 1.10842480\n",
      "Iteration 8, loss = 1.07895027\n",
      "Iteration 9, loss = 1.05268447\n",
      "Iteration 10, loss = 1.03797549\n",
      "Iteration 11, loss = 1.03075747\n",
      "Iteration 12, loss = 1.02175901\n",
      "Iteration 13, loss = 1.01354252\n",
      "Iteration 14, loss = 1.01031451\n",
      "Iteration 15, loss = 1.00592065\n",
      "Iteration 16, loss = 1.00280107\n",
      "Iteration 17, loss = 1.01159898\n",
      "Iteration 18, loss = 1.02014718\n",
      "Iteration 19, loss = 1.00193491\n",
      "Iteration 20, loss = 1.00355598\n",
      "Iteration 21, loss = 0.99161522\n",
      "Iteration 22, loss = 0.98898626\n",
      "Iteration 23, loss = 0.98503240\n",
      "Iteration 24, loss = 0.98081644\n",
      "Iteration 25, loss = 0.97563090\n",
      "Iteration 26, loss = 0.97373557\n",
      "Iteration 27, loss = 0.97664529\n",
      "Iteration 28, loss = 0.98117231\n",
      "Iteration 29, loss = 0.97020604\n",
      "Iteration 30, loss = 0.96351447\n",
      "Iteration 31, loss = 0.95687874\n",
      "Iteration 32, loss = 0.96412050\n",
      "Iteration 33, loss = 0.95491631\n",
      "Iteration 34, loss = 0.95419426\n",
      "Iteration 35, loss = 0.96002514\n",
      "Iteration 36, loss = 0.95253838\n",
      "Iteration 37, loss = 0.95573927\n",
      "Iteration 38, loss = 0.95162002\n",
      "Iteration 39, loss = 0.94444265\n",
      "Iteration 40, loss = 0.94038553\n",
      "Iteration 41, loss = 0.93953971\n",
      "Iteration 42, loss = 0.93776177\n",
      "Iteration 43, loss = 0.93635079\n",
      "Iteration 44, loss = 0.92461173\n",
      "Iteration 45, loss = 0.92449103\n",
      "Iteration 46, loss = 0.92596713\n",
      "Iteration 47, loss = 0.91801625\n",
      "Iteration 48, loss = 0.91402439\n",
      "Iteration 49, loss = 0.91879087\n",
      "Iteration 50, loss = 0.91192774\n",
      "Iteration 51, loss = 0.90965799\n",
      "Iteration 52, loss = 0.90578118\n",
      "Iteration 53, loss = 0.90331541\n",
      "Iteration 54, loss = 0.90291436\n",
      "Iteration 55, loss = 0.90294613\n",
      "Iteration 56, loss = 0.90108497\n",
      "Iteration 57, loss = 0.89535526\n",
      "Iteration 58, loss = 0.90096391\n",
      "Iteration 59, loss = 0.89422065\n",
      "Iteration 60, loss = 0.89457692\n",
      "Iteration 61, loss = 0.89578152\n",
      "Iteration 62, loss = 0.88892524\n",
      "Iteration 63, loss = 0.88319148\n",
      "Iteration 64, loss = 0.87769154\n",
      "Iteration 65, loss = 0.87456902\n",
      "Iteration 66, loss = 0.88202825\n",
      "Iteration 67, loss = 0.87142965\n",
      "Iteration 68, loss = 0.87365308\n",
      "Iteration 69, loss = 0.87360509\n",
      "Iteration 70, loss = 0.86303757\n",
      "Iteration 71, loss = 0.86131363\n",
      "Iteration 72, loss = 0.85980211\n",
      "Iteration 73, loss = 0.85162121\n",
      "Iteration 74, loss = 0.85134732\n",
      "Iteration 75, loss = 0.84848270\n",
      "Iteration 76, loss = 0.84286381\n",
      "Iteration 77, loss = 0.86358483\n",
      "Iteration 78, loss = 0.85819175\n",
      "Iteration 79, loss = 0.85564528\n",
      "Iteration 80, loss = 0.85603386\n",
      "Iteration 81, loss = 0.83643693\n",
      "Iteration 82, loss = 0.84155799\n",
      "Iteration 83, loss = 0.83961675\n",
      "Iteration 84, loss = 0.83030061\n",
      "Iteration 85, loss = 0.81904302\n",
      "Iteration 86, loss = 0.83065487\n",
      "Iteration 87, loss = 0.82329301\n",
      "Iteration 88, loss = 0.80822108\n",
      "Iteration 89, loss = 0.81624749\n",
      "Iteration 90, loss = 0.81265382\n",
      "Iteration 91, loss = 0.81590689\n",
      "Iteration 92, loss = 0.80440394\n",
      "Iteration 93, loss = 0.79485135\n",
      "Iteration 94, loss = 0.80119835\n",
      "Iteration 95, loss = 0.80612501\n",
      "Iteration 96, loss = 0.79398785\n",
      "Iteration 97, loss = 0.78825256\n",
      "Iteration 98, loss = 0.78984257\n",
      "Iteration 99, loss = 0.78737219\n",
      "Iteration 100, loss = 0.81052378\n",
      "Iteration 1, loss = 8.42244443\n",
      "Iteration 2, loss = 2.01138368\n",
      "Iteration 3, loss = 1.61709445\n",
      "Iteration 4, loss = 1.34188093\n",
      "Iteration 5, loss = 1.29829656\n",
      "Iteration 6, loss = 1.28128822\n",
      "Iteration 7, loss = 1.27168610\n",
      "Iteration 8, loss = 1.25202821\n",
      "Iteration 9, loss = 1.24243999\n",
      "Iteration 10, loss = 1.23260770\n",
      "Iteration 11, loss = 1.22687803\n",
      "Iteration 12, loss = 1.22250005\n",
      "Iteration 13, loss = 1.22516086\n",
      "Iteration 14, loss = 1.22227723\n",
      "Iteration 15, loss = 1.21740643\n",
      "Iteration 16, loss = 1.21723350\n",
      "Iteration 17, loss = 1.21394093\n",
      "Iteration 18, loss = 1.21686946\n",
      "Iteration 19, loss = 1.21712246\n",
      "Iteration 20, loss = 1.22542141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 1.23638658\n",
      "Iteration 22, loss = 1.22299509\n",
      "Iteration 23, loss = 1.21955878\n",
      "Iteration 24, loss = 1.21645544\n",
      "Iteration 25, loss = 1.22200305\n",
      "Iteration 26, loss = 1.21553253\n",
      "Iteration 27, loss = 1.21887101\n",
      "Iteration 28, loss = 1.21346995\n",
      "Iteration 29, loss = 1.21165914\n",
      "Iteration 30, loss = 1.21484769\n",
      "Iteration 31, loss = 1.21421134\n",
      "Iteration 32, loss = 1.20754302\n",
      "Iteration 33, loss = 1.20619746\n",
      "Iteration 34, loss = 1.20870477\n",
      "Iteration 35, loss = 1.21102625\n",
      "Iteration 36, loss = 1.20731645\n",
      "Iteration 37, loss = 1.20442479\n",
      "Iteration 38, loss = 1.20431390\n",
      "Iteration 39, loss = 1.20986236\n",
      "Iteration 40, loss = 1.22950925\n",
      "Iteration 41, loss = 1.21333829\n",
      "Iteration 42, loss = 1.21143493\n",
      "Iteration 43, loss = 1.20318494\n",
      "Iteration 44, loss = 1.20491269\n",
      "Iteration 45, loss = 1.20675862\n",
      "Iteration 46, loss = 1.20693655\n",
      "Iteration 47, loss = 1.20752378\n",
      "Iteration 48, loss = 1.22734186\n",
      "Iteration 49, loss = 1.24745674\n",
      "Iteration 50, loss = 1.24151940\n",
      "Iteration 51, loss = 1.22418646\n",
      "Iteration 52, loss = 1.20568552\n",
      "Iteration 53, loss = 1.20251592\n",
      "Iteration 54, loss = 1.19708674\n",
      "Iteration 55, loss = 1.20140201\n",
      "Iteration 56, loss = 1.19725165\n",
      "Iteration 57, loss = 1.19769460\n",
      "Iteration 58, loss = 1.19582436\n",
      "Iteration 59, loss = 1.20046593\n",
      "Iteration 60, loss = 1.19368988\n",
      "Iteration 61, loss = 1.19486792\n",
      "Iteration 62, loss = 1.19464593\n",
      "Iteration 63, loss = 1.19316861\n",
      "Iteration 64, loss = 1.19685278\n",
      "Iteration 65, loss = 1.19368810\n",
      "Iteration 66, loss = 1.19712993\n",
      "Iteration 67, loss = 1.19363876\n",
      "Iteration 68, loss = 1.18894423\n",
      "Iteration 69, loss = 1.18762095\n",
      "Iteration 70, loss = 1.19719739\n",
      "Iteration 71, loss = 1.18896651\n",
      "Iteration 72, loss = 1.19410053\n",
      "Iteration 73, loss = 1.19257551\n",
      "Iteration 74, loss = 1.19003987\n",
      "Iteration 75, loss = 1.19135015\n",
      "Iteration 76, loss = 1.18701514\n",
      "Iteration 77, loss = 1.19163524\n",
      "Iteration 78, loss = 1.18799226\n",
      "Iteration 79, loss = 1.18444085\n",
      "Iteration 80, loss = 1.18518336\n",
      "Iteration 81, loss = 1.18380458\n",
      "Iteration 82, loss = 1.18480831\n",
      "Iteration 83, loss = 1.18662371\n",
      "Iteration 84, loss = 1.18883837\n",
      "Iteration 85, loss = 1.18395161\n",
      "Iteration 86, loss = 1.18774236\n",
      "Iteration 87, loss = 1.18879204\n",
      "Iteration 88, loss = 1.18691264\n",
      "Iteration 89, loss = 1.18139615\n",
      "Iteration 90, loss = 1.18555565\n",
      "Iteration 91, loss = 1.18033602\n",
      "Iteration 92, loss = 1.18215288\n",
      "Iteration 93, loss = 1.17930316\n",
      "Iteration 94, loss = 1.18748726\n",
      "Iteration 95, loss = 1.19094403\n",
      "Iteration 96, loss = 1.18921385\n",
      "Iteration 97, loss = 1.19028724\n",
      "Iteration 98, loss = 1.18261916\n",
      "Iteration 99, loss = 1.17955394\n",
      "Iteration 100, loss = 1.18747194\n",
      "Two-layer model (128, 32): CV accuracy = 0.445\n",
      "Iteration 1, loss = 1.47649836\n",
      "Iteration 2, loss = 1.22514600\n",
      "Iteration 3, loss = 1.19026220\n",
      "Iteration 4, loss = 1.13934992\n",
      "Iteration 5, loss = 1.12117365\n",
      "Iteration 6, loss = 1.09788417\n",
      "Iteration 7, loss = 1.07538620\n",
      "Iteration 8, loss = 1.08021128\n",
      "Iteration 9, loss = 1.06221958\n",
      "Iteration 10, loss = 1.04839915\n",
      "Iteration 11, loss = 1.04300516\n",
      "Iteration 12, loss = 1.03809431\n",
      "Iteration 13, loss = 1.03124068\n",
      "Iteration 14, loss = 1.02945949\n",
      "Iteration 15, loss = 1.01966726\n",
      "Iteration 16, loss = 1.01457901\n",
      "Iteration 17, loss = 1.01536479\n",
      "Iteration 18, loss = 1.00659766\n",
      "Iteration 19, loss = 1.00343778\n",
      "Iteration 20, loss = 0.99528212\n",
      "Iteration 21, loss = 1.00101149\n",
      "Iteration 22, loss = 1.00308542\n",
      "Iteration 23, loss = 0.99157162\n",
      "Iteration 24, loss = 1.00056565\n",
      "Iteration 25, loss = 0.97950107\n",
      "Iteration 26, loss = 0.97936192\n",
      "Iteration 27, loss = 0.98052481\n",
      "Iteration 28, loss = 0.96710086\n",
      "Iteration 29, loss = 0.96332591\n",
      "Iteration 30, loss = 0.97070475\n",
      "Iteration 31, loss = 0.96067769\n",
      "Iteration 32, loss = 0.95766458\n",
      "Iteration 33, loss = 0.95792414\n",
      "Iteration 34, loss = 0.96137722\n",
      "Iteration 35, loss = 0.96074844\n",
      "Iteration 36, loss = 0.95029691\n",
      "Iteration 37, loss = 0.94513397\n",
      "Iteration 38, loss = 0.93877214\n",
      "Iteration 39, loss = 0.95043387\n",
      "Iteration 40, loss = 0.94806214\n",
      "Iteration 41, loss = 0.94370570\n",
      "Iteration 42, loss = 0.93732680\n",
      "Iteration 43, loss = 0.94195085\n",
      "Iteration 44, loss = 0.95381686\n",
      "Iteration 45, loss = 0.94187051\n",
      "Iteration 46, loss = 0.94165678\n",
      "Iteration 47, loss = 0.93667665\n",
      "Iteration 48, loss = 0.92761198\n",
      "Iteration 49, loss = 0.92190643\n",
      "Iteration 50, loss = 0.92340298\n",
      "Iteration 51, loss = 0.92225537\n",
      "Iteration 52, loss = 0.90841734\n",
      "Iteration 53, loss = 0.91380737\n",
      "Iteration 54, loss = 0.89957638\n",
      "Iteration 55, loss = 0.90813610\n",
      "Iteration 56, loss = 0.91184230\n",
      "Iteration 57, loss = 0.89667075\n",
      "Iteration 58, loss = 0.90164927\n",
      "Iteration 59, loss = 0.89382696\n",
      "Iteration 60, loss = 0.89304163\n",
      "Iteration 61, loss = 0.90420788\n",
      "Iteration 62, loss = 0.89389312\n",
      "Iteration 63, loss = 0.89634347\n",
      "Iteration 64, loss = 0.91159790\n",
      "Iteration 65, loss = 0.89820034\n",
      "Iteration 66, loss = 0.88143610\n",
      "Iteration 67, loss = 0.88273414\n",
      "Iteration 68, loss = 0.87988295\n",
      "Iteration 69, loss = 0.87375266\n",
      "Iteration 70, loss = 0.87110535\n",
      "Iteration 71, loss = 0.86888807\n",
      "Iteration 72, loss = 0.86499152\n",
      "Iteration 73, loss = 0.86231998\n",
      "Iteration 74, loss = 0.86387664\n",
      "Iteration 75, loss = 0.85895212\n",
      "Iteration 76, loss = 0.86133501\n",
      "Iteration 77, loss = 0.86329975\n",
      "Iteration 78, loss = 0.85696464\n",
      "Iteration 79, loss = 0.85179663\n",
      "Iteration 80, loss = 0.85638530\n",
      "Iteration 81, loss = 0.85934075\n",
      "Iteration 82, loss = 0.85601198\n",
      "Iteration 83, loss = 0.85388061\n",
      "Iteration 84, loss = 0.85552755\n",
      "Iteration 85, loss = 0.85680495\n",
      "Iteration 86, loss = 0.86229138\n",
      "Iteration 87, loss = 0.86377547\n",
      "Iteration 88, loss = 0.85006113\n",
      "Iteration 89, loss = 0.83501549\n",
      "Iteration 90, loss = 0.82926093\n",
      "Iteration 91, loss = 0.84003133\n",
      "Iteration 92, loss = 0.82708480\n",
      "Iteration 93, loss = 0.82737069\n",
      "Iteration 94, loss = 0.82723081\n",
      "Iteration 95, loss = 0.83083393\n",
      "Iteration 96, loss = 0.82446577\n",
      "Iteration 97, loss = 0.81653432\n",
      "Iteration 98, loss = 0.81790583\n",
      "Iteration 99, loss = 0.82700366\n",
      "Iteration 100, loss = 0.86119714\n",
      "Iteration 1, loss = 1.46610702\n",
      "Iteration 2, loss = 1.23766570\n",
      "Iteration 3, loss = 1.18689897\n",
      "Iteration 4, loss = 1.12852536\n",
      "Iteration 5, loss = 1.11993996\n",
      "Iteration 6, loss = 1.08813497\n",
      "Iteration 7, loss = 1.06489167\n",
      "Iteration 8, loss = 1.06959333\n",
      "Iteration 9, loss = 1.05066775\n",
      "Iteration 10, loss = 1.04617942\n",
      "Iteration 11, loss = 1.03541249\n",
      "Iteration 12, loss = 1.03321330\n",
      "Iteration 13, loss = 1.02229124\n",
      "Iteration 14, loss = 1.02160706\n",
      "Iteration 15, loss = 1.02107146\n",
      "Iteration 16, loss = 1.01185490\n",
      "Iteration 17, loss = 1.00891060\n",
      "Iteration 18, loss = 1.00475476\n",
      "Iteration 19, loss = 1.00101425\n",
      "Iteration 20, loss = 1.01002505\n",
      "Iteration 21, loss = 1.00768595\n",
      "Iteration 22, loss = 0.99494400\n",
      "Iteration 23, loss = 0.99033814\n",
      "Iteration 24, loss = 0.99236847\n",
      "Iteration 25, loss = 0.98486660\n",
      "Iteration 26, loss = 0.99024309\n",
      "Iteration 27, loss = 0.97466980\n",
      "Iteration 28, loss = 0.97431806\n",
      "Iteration 29, loss = 0.97601377\n",
      "Iteration 30, loss = 0.97227739\n",
      "Iteration 31, loss = 0.97431086\n",
      "Iteration 32, loss = 0.97363656\n",
      "Iteration 33, loss = 0.96483916\n",
      "Iteration 34, loss = 0.94981911\n",
      "Iteration 35, loss = 0.95883417\n",
      "Iteration 36, loss = 0.95989623\n",
      "Iteration 37, loss = 0.94448189\n",
      "Iteration 38, loss = 0.94934218\n",
      "Iteration 39, loss = 0.94652580\n",
      "Iteration 40, loss = 0.94052390\n",
      "Iteration 41, loss = 0.92899822\n",
      "Iteration 42, loss = 0.94666718\n",
      "Iteration 43, loss = 0.93942084\n",
      "Iteration 44, loss = 0.94153218\n",
      "Iteration 45, loss = 0.93644235\n",
      "Iteration 46, loss = 0.92925256\n",
      "Iteration 47, loss = 0.91868633\n",
      "Iteration 48, loss = 0.91255329\n",
      "Iteration 49, loss = 0.91453174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.91620044\n",
      "Iteration 51, loss = 0.91397803\n",
      "Iteration 52, loss = 0.90937289\n",
      "Iteration 53, loss = 0.90491594\n",
      "Iteration 54, loss = 0.89716825\n",
      "Iteration 55, loss = 0.91253943\n",
      "Iteration 56, loss = 0.92390815\n",
      "Iteration 57, loss = 0.90776227\n",
      "Iteration 58, loss = 0.91823136\n",
      "Iteration 59, loss = 0.90568894\n",
      "Iteration 60, loss = 0.90265113\n",
      "Iteration 61, loss = 0.90531979\n",
      "Iteration 62, loss = 0.90646857\n",
      "Iteration 63, loss = 0.90757226\n",
      "Iteration 64, loss = 0.90031527\n",
      "Iteration 65, loss = 0.88696089\n",
      "Iteration 66, loss = 0.87736406\n",
      "Iteration 67, loss = 0.87864188\n",
      "Iteration 68, loss = 0.87381752\n",
      "Iteration 69, loss = 0.87498552\n",
      "Iteration 70, loss = 0.87381825\n",
      "Iteration 71, loss = 0.86747656\n",
      "Iteration 72, loss = 0.86720633\n",
      "Iteration 73, loss = 0.86070551\n",
      "Iteration 74, loss = 0.85842134\n",
      "Iteration 75, loss = 0.85324951\n",
      "Iteration 76, loss = 0.85306662\n",
      "Iteration 77, loss = 0.86512285\n",
      "Iteration 78, loss = 0.84578487\n",
      "Iteration 79, loss = 0.84029032\n",
      "Iteration 80, loss = 0.84593681\n",
      "Iteration 81, loss = 0.83496229\n",
      "Iteration 82, loss = 0.83101946\n",
      "Iteration 83, loss = 0.83447435\n",
      "Iteration 84, loss = 0.83396602\n",
      "Iteration 85, loss = 0.82776453\n",
      "Iteration 86, loss = 0.83164590\n",
      "Iteration 87, loss = 0.84095852\n",
      "Iteration 88, loss = 0.82304728\n",
      "Iteration 89, loss = 0.83062452\n",
      "Iteration 90, loss = 0.81335204\n",
      "Iteration 91, loss = 0.82174271\n",
      "Iteration 92, loss = 0.80743350\n",
      "Iteration 93, loss = 0.82870709\n",
      "Iteration 94, loss = 0.81027101\n",
      "Iteration 95, loss = 0.81617065\n",
      "Iteration 96, loss = 0.81354599\n",
      "Iteration 97, loss = 0.80492395\n",
      "Iteration 98, loss = 0.79172184\n",
      "Iteration 99, loss = 0.79671239\n",
      "Iteration 100, loss = 0.80357015\n",
      "Iteration 1, loss = 1.46672838\n",
      "Iteration 2, loss = 1.23338336\n",
      "Iteration 3, loss = 1.18171126\n",
      "Iteration 4, loss = 1.12448277\n",
      "Iteration 5, loss = 1.11544780\n",
      "Iteration 6, loss = 1.08879410\n",
      "Iteration 7, loss = 1.07394858\n",
      "Iteration 8, loss = 1.06204806\n",
      "Iteration 9, loss = 1.06719073\n",
      "Iteration 10, loss = 1.05054908\n",
      "Iteration 11, loss = 1.04075737\n",
      "Iteration 12, loss = 1.03472602\n",
      "Iteration 13, loss = 1.03013380\n",
      "Iteration 14, loss = 1.03170590\n",
      "Iteration 15, loss = 1.02892769\n",
      "Iteration 16, loss = 1.01921920\n",
      "Iteration 17, loss = 1.01511683\n",
      "Iteration 18, loss = 1.01500214\n",
      "Iteration 19, loss = 1.01707703\n",
      "Iteration 20, loss = 1.03270287\n",
      "Iteration 21, loss = 1.00728579\n",
      "Iteration 22, loss = 1.00175430\n",
      "Iteration 23, loss = 0.99811081\n",
      "Iteration 24, loss = 0.99482351\n",
      "Iteration 25, loss = 0.99761913\n",
      "Iteration 26, loss = 0.98566930\n",
      "Iteration 27, loss = 0.97741339\n",
      "Iteration 28, loss = 0.97447002\n",
      "Iteration 29, loss = 0.97265756\n",
      "Iteration 30, loss = 0.97790359\n",
      "Iteration 31, loss = 0.98204309\n",
      "Iteration 32, loss = 0.97607462\n",
      "Iteration 33, loss = 0.97441647\n",
      "Iteration 34, loss = 0.96514871\n",
      "Iteration 35, loss = 0.95929001\n",
      "Iteration 36, loss = 0.95962277\n",
      "Iteration 37, loss = 0.94891757\n",
      "Iteration 38, loss = 0.95642949\n",
      "Iteration 39, loss = 0.94817080\n",
      "Iteration 40, loss = 0.94260833\n",
      "Iteration 41, loss = 0.93392437\n",
      "Iteration 42, loss = 0.93456585\n",
      "Iteration 43, loss = 0.93553818\n",
      "Iteration 44, loss = 0.93102299\n",
      "Iteration 45, loss = 0.92473900\n",
      "Iteration 46, loss = 0.91731212\n",
      "Iteration 47, loss = 0.91564892\n",
      "Iteration 48, loss = 0.91328825\n",
      "Iteration 49, loss = 0.90597528\n",
      "Iteration 50, loss = 0.90619713\n",
      "Iteration 51, loss = 0.90960665\n",
      "Iteration 52, loss = 0.90499848\n",
      "Iteration 53, loss = 0.90250833\n",
      "Iteration 54, loss = 0.91387549\n",
      "Iteration 55, loss = 0.91591320\n",
      "Iteration 56, loss = 0.91593194\n",
      "Iteration 57, loss = 0.90095747\n",
      "Iteration 58, loss = 0.89642627\n",
      "Iteration 59, loss = 0.89805819\n",
      "Iteration 60, loss = 0.87907175\n",
      "Iteration 61, loss = 0.87959367\n",
      "Iteration 62, loss = 0.87868407\n",
      "Iteration 63, loss = 0.88016402\n",
      "Iteration 64, loss = 0.89686680\n",
      "Iteration 65, loss = 0.88298995\n",
      "Iteration 66, loss = 0.86367237\n",
      "Iteration 67, loss = 0.85804355\n",
      "Iteration 68, loss = 0.85636114\n",
      "Iteration 69, loss = 0.85338354\n",
      "Iteration 70, loss = 0.84671043\n",
      "Iteration 71, loss = 0.84426544\n",
      "Iteration 72, loss = 0.84184218\n",
      "Iteration 73, loss = 0.84768108\n",
      "Iteration 74, loss = 0.84542655\n",
      "Iteration 75, loss = 0.83801101\n",
      "Iteration 76, loss = 0.83451057\n",
      "Iteration 77, loss = 0.82813593\n",
      "Iteration 78, loss = 0.85498825\n",
      "Iteration 79, loss = 0.83617966\n",
      "Iteration 80, loss = 0.83506107\n",
      "Iteration 81, loss = 0.83114727\n",
      "Iteration 82, loss = 0.82247067\n",
      "Iteration 83, loss = 0.82927144\n",
      "Iteration 84, loss = 0.83197165\n",
      "Iteration 85, loss = 0.84814334\n",
      "Iteration 86, loss = 0.82866883\n",
      "Iteration 87, loss = 0.82175548\n",
      "Iteration 88, loss = 0.81282476\n",
      "Iteration 89, loss = 0.81630544\n",
      "Iteration 90, loss = 0.81758265\n",
      "Iteration 91, loss = 0.80825761\n",
      "Iteration 92, loss = 0.81328106\n",
      "Iteration 93, loss = 0.80537709\n",
      "Iteration 94, loss = 0.80548228\n",
      "Iteration 95, loss = 0.80290222\n",
      "Iteration 96, loss = 0.81498367\n",
      "Iteration 97, loss = 0.80553367\n",
      "Iteration 98, loss = 0.80024436\n",
      "Iteration 99, loss = 0.80795917\n",
      "Iteration 100, loss = 0.79465792\n",
      "Iteration 1, loss = 1.47374542\n",
      "Iteration 2, loss = 1.24681699\n",
      "Iteration 3, loss = 1.21243141\n",
      "Iteration 4, loss = 1.15474559\n",
      "Iteration 5, loss = 1.14939088\n",
      "Iteration 6, loss = 1.11660000\n",
      "Iteration 7, loss = 1.10028313\n",
      "Iteration 8, loss = 1.08191977\n",
      "Iteration 9, loss = 1.09122474\n",
      "Iteration 10, loss = 1.06512739\n",
      "Iteration 11, loss = 1.06243049\n",
      "Iteration 12, loss = 1.05646458\n",
      "Iteration 13, loss = 1.05646130\n",
      "Iteration 14, loss = 1.05112509\n",
      "Iteration 15, loss = 1.04697453\n",
      "Iteration 16, loss = 1.04469541\n",
      "Iteration 17, loss = 1.04184549\n",
      "Iteration 18, loss = 1.04613231\n",
      "Iteration 19, loss = 1.04442901\n",
      "Iteration 20, loss = 1.03286683\n",
      "Iteration 21, loss = 1.01913523\n",
      "Iteration 22, loss = 1.01576705\n",
      "Iteration 23, loss = 1.01170329\n",
      "Iteration 24, loss = 1.00564847\n",
      "Iteration 25, loss = 0.99863816\n",
      "Iteration 26, loss = 0.99615919\n",
      "Iteration 27, loss = 0.99306259\n",
      "Iteration 28, loss = 0.99186267\n",
      "Iteration 29, loss = 0.98283855\n",
      "Iteration 30, loss = 0.98615838\n",
      "Iteration 31, loss = 0.97418600\n",
      "Iteration 32, loss = 0.97025263\n",
      "Iteration 33, loss = 0.97104778\n",
      "Iteration 34, loss = 0.97152379\n",
      "Iteration 35, loss = 0.97374745\n",
      "Iteration 36, loss = 0.98519293\n",
      "Iteration 37, loss = 0.97700882\n",
      "Iteration 38, loss = 0.97474843\n",
      "Iteration 39, loss = 0.95680644\n",
      "Iteration 40, loss = 0.95868649\n",
      "Iteration 41, loss = 0.96085960\n",
      "Iteration 42, loss = 0.95749536\n",
      "Iteration 43, loss = 0.93914982\n",
      "Iteration 44, loss = 0.94251546\n",
      "Iteration 45, loss = 0.93582403\n",
      "Iteration 46, loss = 0.94030010\n",
      "Iteration 47, loss = 0.93176543\n",
      "Iteration 48, loss = 0.93241244\n",
      "Iteration 49, loss = 0.91662196\n",
      "Iteration 50, loss = 0.92362441\n",
      "Iteration 51, loss = 0.91509370\n",
      "Iteration 52, loss = 0.92463278\n",
      "Iteration 53, loss = 0.92438932\n",
      "Iteration 54, loss = 0.93458437\n",
      "Iteration 55, loss = 0.94182958\n",
      "Iteration 56, loss = 0.92340283\n",
      "Iteration 57, loss = 0.91950756\n",
      "Iteration 58, loss = 0.89392000\n",
      "Iteration 59, loss = 0.89704120\n",
      "Iteration 60, loss = 0.89770002\n",
      "Iteration 61, loss = 0.89429701\n",
      "Iteration 62, loss = 0.89546426\n",
      "Iteration 63, loss = 0.89049776\n",
      "Iteration 64, loss = 0.88639277\n",
      "Iteration 65, loss = 0.87804411\n",
      "Iteration 66, loss = 0.88050100\n",
      "Iteration 67, loss = 0.88031712\n",
      "Iteration 68, loss = 0.87859622\n",
      "Iteration 69, loss = 0.87015611\n",
      "Iteration 70, loss = 0.88208387\n",
      "Iteration 71, loss = 0.87875078\n",
      "Iteration 72, loss = 0.86614218\n",
      "Iteration 73, loss = 0.85929276\n",
      "Iteration 74, loss = 0.85422994\n",
      "Iteration 75, loss = 0.84953220\n",
      "Iteration 76, loss = 0.85468553\n",
      "Iteration 77, loss = 0.85166049\n",
      "Iteration 78, loss = 0.84583054\n",
      "Iteration 79, loss = 0.85536771\n",
      "Iteration 80, loss = 0.83838605\n",
      "Iteration 81, loss = 0.83318172\n",
      "Iteration 82, loss = 0.83043469\n",
      "Iteration 83, loss = 0.82765900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 84, loss = 0.82868433\n",
      "Iteration 85, loss = 0.83620491\n",
      "Iteration 86, loss = 0.83285354\n",
      "Iteration 87, loss = 0.81757006\n",
      "Iteration 88, loss = 0.83256792\n",
      "Iteration 89, loss = 0.82429866\n",
      "Iteration 90, loss = 0.80610629\n",
      "Iteration 91, loss = 0.80457809\n",
      "Iteration 92, loss = 0.80317723\n",
      "Iteration 93, loss = 0.80066696\n",
      "Iteration 94, loss = 0.79735635\n",
      "Iteration 95, loss = 0.79500586\n",
      "Iteration 96, loss = 0.79230020\n",
      "Iteration 97, loss = 0.81493388\n",
      "Iteration 98, loss = 0.79646476\n",
      "Iteration 99, loss = 0.80151737\n",
      "Iteration 100, loss = 0.79154243\n",
      "Iteration 1, loss = 1.47042893\n",
      "Iteration 2, loss = 1.22416140\n",
      "Iteration 3, loss = 1.18205226\n",
      "Iteration 4, loss = 1.11662666\n",
      "Iteration 5, loss = 1.09774657\n",
      "Iteration 6, loss = 1.07045861\n",
      "Iteration 7, loss = 1.06590483\n",
      "Iteration 8, loss = 1.04479884\n",
      "Iteration 9, loss = 1.03009442\n",
      "Iteration 10, loss = 1.02893194\n",
      "Iteration 11, loss = 1.02429011\n",
      "Iteration 12, loss = 1.01402074\n",
      "Iteration 13, loss = 1.00319474\n",
      "Iteration 14, loss = 1.00578219\n",
      "Iteration 15, loss = 1.00014801\n",
      "Iteration 16, loss = 0.99656386\n",
      "Iteration 17, loss = 0.99457374\n",
      "Iteration 18, loss = 0.98613846\n",
      "Iteration 19, loss = 0.98545515\n",
      "Iteration 20, loss = 0.98474387\n",
      "Iteration 21, loss = 0.98941675\n",
      "Iteration 22, loss = 0.98950614\n",
      "Iteration 23, loss = 0.98070065\n",
      "Iteration 24, loss = 0.96939777\n",
      "Iteration 25, loss = 0.96729856\n",
      "Iteration 26, loss = 0.96522477\n",
      "Iteration 27, loss = 0.95798323\n",
      "Iteration 28, loss = 0.96470992\n",
      "Iteration 29, loss = 0.96428885\n",
      "Iteration 30, loss = 0.97294268\n",
      "Iteration 31, loss = 0.95150723\n",
      "Iteration 32, loss = 0.94194964\n",
      "Iteration 33, loss = 0.94301518\n",
      "Iteration 34, loss = 0.93354374\n",
      "Iteration 35, loss = 0.94232940\n",
      "Iteration 36, loss = 0.93212155\n",
      "Iteration 37, loss = 0.92220867\n",
      "Iteration 38, loss = 0.92951786\n",
      "Iteration 39, loss = 0.92541532\n",
      "Iteration 40, loss = 0.92037574\n",
      "Iteration 41, loss = 0.92179443\n",
      "Iteration 42, loss = 0.92490655\n",
      "Iteration 43, loss = 0.92285022\n",
      "Iteration 44, loss = 0.90779199\n",
      "Iteration 45, loss = 0.91178256\n",
      "Iteration 46, loss = 0.90258951\n",
      "Iteration 47, loss = 0.90173318\n",
      "Iteration 48, loss = 0.89768316\n",
      "Iteration 49, loss = 0.89755981\n",
      "Iteration 50, loss = 0.88348863\n",
      "Iteration 51, loss = 0.89045817\n",
      "Iteration 52, loss = 0.88678116\n",
      "Iteration 53, loss = 0.89608401\n",
      "Iteration 54, loss = 0.90243174\n",
      "Iteration 55, loss = 0.88663274\n",
      "Iteration 56, loss = 0.90353808\n",
      "Iteration 57, loss = 0.89277359\n",
      "Iteration 58, loss = 0.86640933\n",
      "Iteration 59, loss = 0.86807200\n",
      "Iteration 60, loss = 0.86433127\n",
      "Iteration 61, loss = 0.87677178\n",
      "Iteration 62, loss = 0.87518371\n",
      "Iteration 63, loss = 0.86947221\n",
      "Iteration 64, loss = 0.85485836\n",
      "Iteration 65, loss = 0.85580968\n",
      "Iteration 66, loss = 0.85631267\n",
      "Iteration 67, loss = 0.87891224\n",
      "Iteration 68, loss = 0.87009949\n",
      "Iteration 69, loss = 0.86457527\n",
      "Iteration 70, loss = 0.84242933\n",
      "Iteration 71, loss = 0.84018671\n",
      "Iteration 72, loss = 0.83545983\n",
      "Iteration 73, loss = 0.83681387\n",
      "Iteration 74, loss = 0.83914306\n",
      "Iteration 75, loss = 0.84096947\n",
      "Iteration 76, loss = 0.85594933\n",
      "Iteration 77, loss = 0.85100820\n",
      "Iteration 78, loss = 0.84604665\n",
      "Iteration 79, loss = 0.82987947\n",
      "Iteration 80, loss = 0.84355481\n",
      "Iteration 81, loss = 0.84718984\n",
      "Iteration 82, loss = 0.82689898\n",
      "Iteration 83, loss = 0.82613563\n",
      "Iteration 84, loss = 0.81362492\n",
      "Iteration 85, loss = 0.81462558\n",
      "Iteration 86, loss = 0.81375103\n",
      "Iteration 87, loss = 0.80270405\n",
      "Iteration 88, loss = 0.79373202\n",
      "Iteration 89, loss = 0.80011055\n",
      "Iteration 90, loss = 0.80332947\n",
      "Iteration 91, loss = 0.79644968\n",
      "Iteration 92, loss = 0.79433840\n",
      "Iteration 93, loss = 0.78719929\n",
      "Iteration 94, loss = 0.79303107\n",
      "Iteration 95, loss = 0.77851418\n",
      "Iteration 96, loss = 0.77324503\n",
      "Iteration 97, loss = 0.76663454\n",
      "Iteration 98, loss = 0.77030685\n",
      "Iteration 99, loss = 0.76964044\n",
      "Iteration 100, loss = 0.77279914\n",
      "Iteration 1, loss = 12.13587067\n",
      "Iteration 2, loss = 10.53202297\n",
      "Iteration 3, loss = 4.86494736\n",
      "Iteration 4, loss = 3.03243768\n",
      "Iteration 5, loss = 2.20118163\n",
      "Iteration 6, loss = 1.68789467\n",
      "Iteration 7, loss = 1.42795882\n",
      "Iteration 8, loss = 1.29950901\n",
      "Iteration 9, loss = 1.24946119\n",
      "Iteration 10, loss = 1.22615612\n",
      "Iteration 11, loss = 1.21745378\n",
      "Iteration 12, loss = 1.18794679\n",
      "Iteration 13, loss = 1.17427429\n",
      "Iteration 14, loss = 1.17852283\n",
      "Iteration 15, loss = 1.17300055\n",
      "Iteration 16, loss = 1.19446344\n",
      "Iteration 17, loss = 1.16582900\n",
      "Iteration 18, loss = 1.16283891\n",
      "Iteration 19, loss = 1.15642600\n",
      "Iteration 20, loss = 1.17621286\n",
      "Iteration 21, loss = 1.20048805\n",
      "Iteration 22, loss = 1.17217492\n",
      "Iteration 23, loss = 1.17712519\n",
      "Iteration 24, loss = 1.15271786\n",
      "Iteration 25, loss = 1.15458873\n",
      "Iteration 26, loss = 1.15565346\n",
      "Iteration 27, loss = 1.13505288\n",
      "Iteration 28, loss = 1.12862681\n",
      "Iteration 29, loss = 1.12456368\n",
      "Iteration 30, loss = 1.12370980\n",
      "Iteration 31, loss = 1.17131421\n",
      "Iteration 32, loss = 1.18014849\n",
      "Iteration 33, loss = 1.18159666\n",
      "Iteration 34, loss = 1.19658282\n",
      "Iteration 35, loss = 1.17711581\n",
      "Iteration 36, loss = 1.13897591\n",
      "Iteration 37, loss = 1.12086100\n",
      "Iteration 38, loss = 1.12295821\n",
      "Iteration 39, loss = 1.10719428\n",
      "Iteration 40, loss = 1.11924373\n",
      "Iteration 41, loss = 1.13030294\n",
      "Iteration 42, loss = 1.10871195\n",
      "Iteration 43, loss = 1.11165613\n",
      "Iteration 44, loss = 1.10719084\n",
      "Iteration 45, loss = 1.10098999\n",
      "Iteration 46, loss = 1.08849320\n",
      "Iteration 47, loss = 1.08415167\n",
      "Iteration 48, loss = 1.08641094\n",
      "Iteration 49, loss = 1.09400015\n",
      "Iteration 50, loss = 1.08488694\n",
      "Iteration 51, loss = 1.08722009\n",
      "Iteration 52, loss = 1.09077206\n",
      "Iteration 53, loss = 1.10761385\n",
      "Iteration 54, loss = 1.08027607\n",
      "Iteration 55, loss = 1.07322743\n",
      "Iteration 56, loss = 1.08872136\n",
      "Iteration 57, loss = 1.10403628\n",
      "Iteration 58, loss = 1.08239543\n",
      "Iteration 59, loss = 1.06807382\n",
      "Iteration 60, loss = 1.13311382\n",
      "Iteration 61, loss = 1.12168427\n",
      "Iteration 62, loss = 1.12983235\n",
      "Iteration 63, loss = 1.09798258\n",
      "Iteration 64, loss = 1.11232498\n",
      "Iteration 65, loss = 1.10339680\n",
      "Iteration 66, loss = 1.08371683\n",
      "Iteration 67, loss = 1.10353415\n",
      "Iteration 68, loss = 1.15553042\n",
      "Iteration 69, loss = 1.14876681\n",
      "Iteration 70, loss = 1.09894049\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Two-layer model (128, 64): CV accuracy = 0.106\n",
      "Iteration 1, loss = 1.40809426\n",
      "Iteration 2, loss = 1.18208711\n",
      "Iteration 3, loss = 1.13961608\n",
      "Iteration 4, loss = 1.12215738\n",
      "Iteration 5, loss = 1.09561345\n",
      "Iteration 6, loss = 1.07225138\n",
      "Iteration 7, loss = 1.07620433\n",
      "Iteration 8, loss = 1.04111023\n",
      "Iteration 9, loss = 1.05358471\n",
      "Iteration 10, loss = 1.02933751\n",
      "Iteration 11, loss = 1.02124456\n",
      "Iteration 12, loss = 1.01188913\n",
      "Iteration 13, loss = 1.00981220\n",
      "Iteration 14, loss = 1.01004939\n",
      "Iteration 15, loss = 1.00690490\n",
      "Iteration 16, loss = 0.99044146\n",
      "Iteration 17, loss = 0.98815991\n",
      "Iteration 18, loss = 0.99413424\n",
      "Iteration 19, loss = 0.98068595\n",
      "Iteration 20, loss = 0.97501916\n",
      "Iteration 21, loss = 0.97476427\n",
      "Iteration 22, loss = 0.96519517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.96370724\n",
      "Iteration 24, loss = 0.95234631\n",
      "Iteration 25, loss = 0.94807941\n",
      "Iteration 26, loss = 0.93989484\n",
      "Iteration 27, loss = 0.94294871\n",
      "Iteration 28, loss = 0.93559722\n",
      "Iteration 29, loss = 0.92089716\n",
      "Iteration 30, loss = 0.92228841\n",
      "Iteration 31, loss = 0.92087952\n",
      "Iteration 32, loss = 0.94004722\n",
      "Iteration 33, loss = 0.92929953\n",
      "Iteration 34, loss = 0.90907483\n",
      "Iteration 35, loss = 0.90212322\n",
      "Iteration 36, loss = 0.89474888\n",
      "Iteration 37, loss = 0.89401135\n",
      "Iteration 38, loss = 0.90035472\n",
      "Iteration 39, loss = 0.88889106\n",
      "Iteration 40, loss = 0.88581215\n",
      "Iteration 41, loss = 0.87615269\n",
      "Iteration 42, loss = 0.87480642\n",
      "Iteration 43, loss = 0.86869607\n",
      "Iteration 44, loss = 0.86434503\n",
      "Iteration 45, loss = 0.86769862\n",
      "Iteration 46, loss = 0.85663902\n",
      "Iteration 47, loss = 0.85282917\n",
      "Iteration 48, loss = 0.84987782\n",
      "Iteration 49, loss = 0.84462133\n",
      "Iteration 50, loss = 0.84467058\n",
      "Iteration 51, loss = 0.84497077\n",
      "Iteration 52, loss = 0.84170285\n",
      "Iteration 53, loss = 0.85551365\n",
      "Iteration 54, loss = 0.84182029\n",
      "Iteration 55, loss = 0.86012670\n",
      "Iteration 56, loss = 0.83972621\n",
      "Iteration 57, loss = 0.84215230\n",
      "Iteration 58, loss = 0.82952313\n",
      "Iteration 59, loss = 0.82018991\n",
      "Iteration 60, loss = 0.80121008\n",
      "Iteration 61, loss = 0.80622290\n",
      "Iteration 62, loss = 0.79375236\n",
      "Iteration 63, loss = 0.80399440\n",
      "Iteration 64, loss = 0.81586428\n",
      "Iteration 65, loss = 0.79781062\n",
      "Iteration 66, loss = 0.79534409\n",
      "Iteration 67, loss = 0.80147857\n",
      "Iteration 68, loss = 0.79462075\n",
      "Iteration 69, loss = 0.80162990\n",
      "Iteration 70, loss = 0.79726310\n",
      "Iteration 71, loss = 0.78201059\n",
      "Iteration 72, loss = 0.78875951\n",
      "Iteration 73, loss = 0.76513876\n",
      "Iteration 74, loss = 0.76212654\n",
      "Iteration 75, loss = 0.75470268\n",
      "Iteration 76, loss = 0.75240662\n",
      "Iteration 77, loss = 0.76414963\n",
      "Iteration 78, loss = 0.76005620\n",
      "Iteration 79, loss = 0.77160138\n",
      "Iteration 80, loss = 0.74380947\n",
      "Iteration 81, loss = 0.74230316\n",
      "Iteration 82, loss = 0.72957799\n",
      "Iteration 83, loss = 0.72854383\n",
      "Iteration 84, loss = 0.73257687\n",
      "Iteration 85, loss = 0.72175316\n",
      "Iteration 86, loss = 0.72053347\n",
      "Iteration 87, loss = 0.70861408\n",
      "Iteration 88, loss = 0.70249039\n",
      "Iteration 89, loss = 0.69837784\n",
      "Iteration 90, loss = 0.69335242\n",
      "Iteration 91, loss = 0.69111238\n",
      "Iteration 92, loss = 0.68418721\n",
      "Iteration 93, loss = 0.68497673\n",
      "Iteration 94, loss = 0.68395798\n",
      "Iteration 95, loss = 0.67917112\n",
      "Iteration 96, loss = 0.70774553\n",
      "Iteration 97, loss = 0.70439557\n",
      "Iteration 98, loss = 0.67649798\n",
      "Iteration 99, loss = 0.68333182\n",
      "Iteration 100, loss = 0.68766710\n",
      "Iteration 1, loss = 1.41492079\n",
      "Iteration 2, loss = 1.18520641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 1.13349566\n",
      "Iteration 4, loss = 1.09773681\n",
      "Iteration 5, loss = 1.10599008\n",
      "Iteration 6, loss = 1.08158401\n",
      "Iteration 7, loss = 1.05785383\n",
      "Iteration 8, loss = 1.04529719\n",
      "Iteration 9, loss = 1.03837260\n",
      "Iteration 10, loss = 1.02524156\n",
      "Iteration 11, loss = 1.01843047\n",
      "Iteration 12, loss = 1.01250602\n",
      "Iteration 13, loss = 1.00354271\n",
      "Iteration 14, loss = 0.99754113\n",
      "Iteration 15, loss = 1.00582025\n",
      "Iteration 16, loss = 0.99912972\n",
      "Iteration 17, loss = 0.98266812\n",
      "Iteration 18, loss = 0.98803380\n",
      "Iteration 19, loss = 0.98281111\n",
      "Iteration 20, loss = 0.97535574\n",
      "Iteration 21, loss = 0.96075223\n",
      "Iteration 22, loss = 0.96413195\n",
      "Iteration 23, loss = 0.97220809\n",
      "Iteration 24, loss = 0.95758420\n",
      "Iteration 25, loss = 0.94721004\n",
      "Iteration 26, loss = 0.95356946\n",
      "Iteration 27, loss = 0.98069804\n",
      "Iteration 28, loss = 0.95453406\n",
      "Iteration 29, loss = 0.96222192\n",
      "Iteration 30, loss = 0.95761730\n",
      "Iteration 31, loss = 0.94663357\n",
      "Iteration 32, loss = 0.93194419\n",
      "Iteration 33, loss = 0.93015929\n",
      "Iteration 34, loss = 0.91366903\n",
      "Iteration 35, loss = 0.91077291\n",
      "Iteration 36, loss = 0.90768366\n",
      "Iteration 37, loss = 0.89981507\n",
      "Iteration 38, loss = 0.89920124\n",
      "Iteration 39, loss = 0.89186824\n",
      "Iteration 40, loss = 0.88484396\n",
      "Iteration 41, loss = 0.87845698\n",
      "Iteration 42, loss = 0.87795669\n",
      "Iteration 43, loss = 0.87307505\n",
      "Iteration 44, loss = 0.88836865\n",
      "Iteration 45, loss = 0.85809496\n",
      "Iteration 46, loss = 0.86987851\n",
      "Iteration 47, loss = 0.85409607\n",
      "Iteration 48, loss = 0.85218329\n",
      "Iteration 49, loss = 0.84122046\n",
      "Iteration 50, loss = 0.83857593\n",
      "Iteration 51, loss = 0.83588204\n",
      "Iteration 52, loss = 0.83962855\n",
      "Iteration 53, loss = 0.84051800\n",
      "Iteration 54, loss = 0.83788637\n",
      "Iteration 55, loss = 0.83039145\n",
      "Iteration 56, loss = 0.83562328\n",
      "Iteration 57, loss = 0.80883302\n",
      "Iteration 58, loss = 0.81095162\n",
      "Iteration 59, loss = 0.81974271\n",
      "Iteration 60, loss = 0.79897149\n",
      "Iteration 61, loss = 0.79814834\n",
      "Iteration 62, loss = 0.80069358\n",
      "Iteration 63, loss = 0.79325616\n",
      "Iteration 64, loss = 0.81259767\n",
      "Iteration 65, loss = 0.81259561\n",
      "Iteration 66, loss = 0.81244687\n",
      "Iteration 67, loss = 0.78452641\n",
      "Iteration 68, loss = 0.77922195\n",
      "Iteration 69, loss = 0.77901246\n",
      "Iteration 70, loss = 0.77758462\n",
      "Iteration 71, loss = 0.76938690\n",
      "Iteration 72, loss = 0.77862568\n",
      "Iteration 73, loss = 0.75743255\n",
      "Iteration 74, loss = 0.74331862\n",
      "Iteration 75, loss = 0.73615290\n",
      "Iteration 76, loss = 0.72527018\n",
      "Iteration 77, loss = 0.74341117\n",
      "Iteration 78, loss = 0.74552280\n",
      "Iteration 79, loss = 0.73436453\n",
      "Iteration 80, loss = 0.71725813\n",
      "Iteration 81, loss = 0.73374734\n",
      "Iteration 82, loss = 0.71475358\n",
      "Iteration 83, loss = 0.69541391\n",
      "Iteration 84, loss = 0.69414533\n",
      "Iteration 85, loss = 0.68534071\n",
      "Iteration 86, loss = 0.68667806\n",
      "Iteration 87, loss = 0.67480322\n",
      "Iteration 88, loss = 0.67350815\n",
      "Iteration 89, loss = 0.67482863\n",
      "Iteration 90, loss = 0.67214781\n",
      "Iteration 91, loss = 0.65938098\n",
      "Iteration 92, loss = 0.66024802\n",
      "Iteration 93, loss = 0.67022416\n",
      "Iteration 94, loss = 0.64231455\n",
      "Iteration 95, loss = 0.62897506\n",
      "Iteration 96, loss = 0.64251236\n",
      "Iteration 97, loss = 0.64358975\n",
      "Iteration 98, loss = 0.62958160\n",
      "Iteration 99, loss = 0.62907374\n",
      "Iteration 100, loss = 0.65468453\n",
      "Iteration 1, loss = 1.40834727\n",
      "Iteration 2, loss = 1.17837334\n",
      "Iteration 3, loss = 1.15835107\n",
      "Iteration 4, loss = 1.12077201\n",
      "Iteration 5, loss = 1.10059567\n",
      "Iteration 6, loss = 1.09491406\n",
      "Iteration 7, loss = 1.07329575\n",
      "Iteration 8, loss = 1.04897384\n",
      "Iteration 9, loss = 1.04950089\n",
      "Iteration 10, loss = 1.04349369\n",
      "Iteration 11, loss = 1.04073272\n",
      "Iteration 12, loss = 1.01991574\n",
      "Iteration 13, loss = 1.01901859\n",
      "Iteration 14, loss = 1.00810520\n",
      "Iteration 15, loss = 1.00820452\n",
      "Iteration 16, loss = 1.00165654\n",
      "Iteration 17, loss = 1.00241924\n",
      "Iteration 18, loss = 0.99405941\n",
      "Iteration 19, loss = 0.98869834\n",
      "Iteration 20, loss = 0.97852491\n",
      "Iteration 21, loss = 0.97328097\n",
      "Iteration 22, loss = 0.97608782\n",
      "Iteration 23, loss = 0.97687852\n",
      "Iteration 24, loss = 0.96555488\n",
      "Iteration 25, loss = 0.95523823\n",
      "Iteration 26, loss = 0.95289466\n",
      "Iteration 27, loss = 0.96578388\n",
      "Iteration 28, loss = 0.95472612\n",
      "Iteration 29, loss = 0.94701659\n",
      "Iteration 30, loss = 0.94229570\n",
      "Iteration 31, loss = 0.93577613\n",
      "Iteration 32, loss = 0.92751828\n",
      "Iteration 33, loss = 0.93158231\n",
      "Iteration 34, loss = 0.94341611\n",
      "Iteration 35, loss = 0.92454470\n",
      "Iteration 36, loss = 0.91557671\n",
      "Iteration 37, loss = 0.90852532\n",
      "Iteration 38, loss = 0.91596825\n",
      "Iteration 39, loss = 0.89949295\n",
      "Iteration 40, loss = 0.90132492\n",
      "Iteration 41, loss = 0.89566060\n",
      "Iteration 42, loss = 0.89101470\n",
      "Iteration 43, loss = 0.87651658\n",
      "Iteration 44, loss = 0.86660666\n",
      "Iteration 45, loss = 0.86746933\n",
      "Iteration 46, loss = 0.86528199\n",
      "Iteration 47, loss = 0.84895581\n",
      "Iteration 48, loss = 0.84346094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49, loss = 0.85543238\n",
      "Iteration 50, loss = 0.84039145\n",
      "Iteration 51, loss = 0.83226662\n",
      "Iteration 52, loss = 0.83478790\n",
      "Iteration 53, loss = 0.83334217\n",
      "Iteration 54, loss = 0.81892673\n",
      "Iteration 55, loss = 0.83532694\n",
      "Iteration 56, loss = 0.81118549\n",
      "Iteration 57, loss = 0.80661464\n",
      "Iteration 58, loss = 0.81416398\n",
      "Iteration 59, loss = 0.81398175\n",
      "Iteration 60, loss = 0.82043667\n",
      "Iteration 61, loss = 0.80773623\n",
      "Iteration 62, loss = 0.79240017\n",
      "Iteration 63, loss = 0.79491869\n",
      "Iteration 64, loss = 0.79162272\n",
      "Iteration 65, loss = 0.78775477\n",
      "Iteration 66, loss = 0.78484993\n",
      "Iteration 67, loss = 0.77221274\n",
      "Iteration 68, loss = 0.76182156\n",
      "Iteration 69, loss = 0.75659119\n",
      "Iteration 70, loss = 0.77554310\n",
      "Iteration 71, loss = 0.78796096\n",
      "Iteration 72, loss = 0.77356886\n",
      "Iteration 73, loss = 0.77355020\n",
      "Iteration 74, loss = 0.75432397\n",
      "Iteration 75, loss = 0.73137827\n",
      "Iteration 76, loss = 0.73425964\n",
      "Iteration 77, loss = 0.73287234\n",
      "Iteration 78, loss = 0.76036748\n",
      "Iteration 79, loss = 0.72579461\n",
      "Iteration 80, loss = 0.71409162\n",
      "Iteration 81, loss = 0.70790760\n",
      "Iteration 82, loss = 0.71316101\n",
      "Iteration 83, loss = 0.72064739\n",
      "Iteration 84, loss = 0.70140915\n",
      "Iteration 85, loss = 0.70471513\n",
      "Iteration 86, loss = 0.71558542\n",
      "Iteration 87, loss = 0.72054902\n",
      "Iteration 88, loss = 0.72004700\n",
      "Iteration 89, loss = 0.75471023\n",
      "Iteration 90, loss = 0.74814317\n",
      "Iteration 91, loss = 0.69750067\n",
      "Iteration 92, loss = 0.69666200\n",
      "Iteration 93, loss = 0.69351896\n",
      "Iteration 94, loss = 0.66963240\n",
      "Iteration 95, loss = 0.66867335\n",
      "Iteration 96, loss = 0.67048319\n",
      "Iteration 97, loss = 0.66577352\n",
      "Iteration 98, loss = 0.65217310\n",
      "Iteration 99, loss = 0.67358222\n",
      "Iteration 100, loss = 0.66985050\n",
      "Iteration 1, loss = 1.40005494\n",
      "Iteration 2, loss = 1.19874821\n",
      "Iteration 3, loss = 1.16315384\n",
      "Iteration 4, loss = 1.12874652\n",
      "Iteration 5, loss = 1.10475891\n",
      "Iteration 6, loss = 1.09908823\n",
      "Iteration 7, loss = 1.07908122\n",
      "Iteration 8, loss = 1.05835645\n",
      "Iteration 9, loss = 1.05564979\n",
      "Iteration 10, loss = 1.04554107\n",
      "Iteration 11, loss = 1.03925730\n",
      "Iteration 12, loss = 1.03645590\n",
      "Iteration 13, loss = 1.02283582\n",
      "Iteration 14, loss = 1.01527079\n",
      "Iteration 15, loss = 1.00595728\n",
      "Iteration 16, loss = 1.00628959\n",
      "Iteration 17, loss = 0.99229844\n",
      "Iteration 18, loss = 1.00974603\n",
      "Iteration 19, loss = 0.99845744\n",
      "Iteration 20, loss = 0.99180368\n",
      "Iteration 21, loss = 0.98627109\n",
      "Iteration 22, loss = 0.96685630\n",
      "Iteration 23, loss = 0.97546965\n",
      "Iteration 24, loss = 0.95441403\n",
      "Iteration 25, loss = 0.95550716\n",
      "Iteration 26, loss = 0.94645389\n",
      "Iteration 27, loss = 0.95159415\n",
      "Iteration 28, loss = 0.95168677\n",
      "Iteration 29, loss = 0.95215238\n",
      "Iteration 30, loss = 0.93430673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.92676995\n",
      "Iteration 32, loss = 0.93151040\n",
      "Iteration 33, loss = 0.94436949\n",
      "Iteration 34, loss = 0.93386253\n",
      "Iteration 35, loss = 0.93040270\n",
      "Iteration 36, loss = 0.91198105\n",
      "Iteration 37, loss = 0.91370865\n",
      "Iteration 38, loss = 0.89085614\n",
      "Iteration 39, loss = 0.88523426\n",
      "Iteration 40, loss = 0.89404158\n",
      "Iteration 41, loss = 0.89431251\n",
      "Iteration 42, loss = 0.89489392\n",
      "Iteration 43, loss = 0.88703440\n",
      "Iteration 44, loss = 0.87140145\n",
      "Iteration 45, loss = 0.86629467\n",
      "Iteration 46, loss = 0.86362727\n",
      "Iteration 47, loss = 0.86313987\n",
      "Iteration 48, loss = 0.86977915\n",
      "Iteration 49, loss = 0.85002230\n",
      "Iteration 50, loss = 0.85605616\n",
      "Iteration 51, loss = 0.84155133\n",
      "Iteration 52, loss = 0.83803506\n",
      "Iteration 53, loss = 0.84334232\n",
      "Iteration 54, loss = 0.84190200\n",
      "Iteration 55, loss = 0.83606101\n",
      "Iteration 56, loss = 0.82603258\n",
      "Iteration 57, loss = 0.82229030\n",
      "Iteration 58, loss = 0.83372779\n",
      "Iteration 59, loss = 0.83416468\n",
      "Iteration 60, loss = 0.81697935\n",
      "Iteration 61, loss = 0.81256252\n",
      "Iteration 62, loss = 0.80824923\n",
      "Iteration 63, loss = 0.79479621\n",
      "Iteration 64, loss = 0.79892413\n",
      "Iteration 65, loss = 0.81179376\n",
      "Iteration 66, loss = 0.81019812\n",
      "Iteration 67, loss = 0.79449829\n",
      "Iteration 68, loss = 0.80705031\n",
      "Iteration 69, loss = 0.79124765\n",
      "Iteration 70, loss = 0.79949715\n",
      "Iteration 71, loss = 0.83794719\n",
      "Iteration 72, loss = 0.81179994\n",
      "Iteration 73, loss = 0.79710216\n",
      "Iteration 74, loss = 0.78510344\n",
      "Iteration 75, loss = 0.77414635\n",
      "Iteration 76, loss = 0.76021799\n",
      "Iteration 77, loss = 0.75525637\n",
      "Iteration 78, loss = 0.75811500\n",
      "Iteration 79, loss = 0.75237235\n",
      "Iteration 80, loss = 0.74690110\n",
      "Iteration 81, loss = 0.74361886\n",
      "Iteration 82, loss = 0.74759338\n",
      "Iteration 83, loss = 0.73330008\n",
      "Iteration 84, loss = 0.72986816\n",
      "Iteration 85, loss = 0.71884938\n",
      "Iteration 86, loss = 0.73191144\n",
      "Iteration 87, loss = 0.73203329\n",
      "Iteration 88, loss = 0.71975303\n",
      "Iteration 89, loss = 0.71223684\n",
      "Iteration 90, loss = 0.71044866\n",
      "Iteration 91, loss = 0.70605338\n",
      "Iteration 92, loss = 0.70705085\n",
      "Iteration 93, loss = 0.69875114\n",
      "Iteration 94, loss = 0.67623746\n",
      "Iteration 95, loss = 0.67597869\n",
      "Iteration 96, loss = 0.67847945\n",
      "Iteration 97, loss = 0.66345525\n",
      "Iteration 98, loss = 0.66730331\n",
      "Iteration 99, loss = 0.67328580\n",
      "Iteration 100, loss = 0.65342253\n",
      "Iteration 1, loss = 1.38389701\n",
      "Iteration 2, loss = 1.17670107\n",
      "Iteration 3, loss = 1.12307499\n",
      "Iteration 4, loss = 1.08351235\n",
      "Iteration 5, loss = 1.06284870\n",
      "Iteration 6, loss = 1.04179067\n",
      "Iteration 7, loss = 1.03079926\n",
      "Iteration 8, loss = 1.04015684\n",
      "Iteration 9, loss = 1.02150038\n",
      "Iteration 10, loss = 1.02180082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.99901115\n",
      "Iteration 12, loss = 1.00841581\n",
      "Iteration 13, loss = 1.00392194\n",
      "Iteration 14, loss = 0.99226811\n",
      "Iteration 15, loss = 0.97638754\n",
      "Iteration 16, loss = 0.97093836\n",
      "Iteration 17, loss = 0.96531918\n",
      "Iteration 18, loss = 0.96222682\n",
      "Iteration 19, loss = 0.96742877\n",
      "Iteration 20, loss = 0.94582202\n",
      "Iteration 21, loss = 0.93863095\n",
      "Iteration 22, loss = 0.94123604\n",
      "Iteration 23, loss = 0.93868324\n",
      "Iteration 24, loss = 0.93531780\n",
      "Iteration 25, loss = 0.92737481\n",
      "Iteration 26, loss = 0.92368246\n",
      "Iteration 27, loss = 0.91881897\n",
      "Iteration 28, loss = 0.92513552\n",
      "Iteration 29, loss = 0.91753314\n",
      "Iteration 30, loss = 0.91966472\n",
      "Iteration 31, loss = 0.92185860\n",
      "Iteration 32, loss = 0.92843269\n",
      "Iteration 33, loss = 0.91084234\n",
      "Iteration 34, loss = 0.92591627\n",
      "Iteration 35, loss = 0.91079932\n",
      "Iteration 36, loss = 0.91292572\n",
      "Iteration 37, loss = 0.90050303\n",
      "Iteration 38, loss = 0.90842532\n",
      "Iteration 39, loss = 0.90027869\n",
      "Iteration 40, loss = 0.90195802\n",
      "Iteration 41, loss = 0.88530867\n",
      "Iteration 42, loss = 0.87977617\n",
      "Iteration 43, loss = 0.90333215\n",
      "Iteration 44, loss = 0.88021593\n",
      "Iteration 45, loss = 0.86507804\n",
      "Iteration 46, loss = 0.87006547\n",
      "Iteration 47, loss = 0.85270132\n",
      "Iteration 48, loss = 0.85256323\n",
      "Iteration 49, loss = 0.84338845\n",
      "Iteration 50, loss = 0.83877681\n",
      "Iteration 51, loss = 0.83551299\n",
      "Iteration 52, loss = 0.83940630\n",
      "Iteration 53, loss = 0.82548941\n",
      "Iteration 54, loss = 0.82306005\n",
      "Iteration 55, loss = 0.82345764\n",
      "Iteration 56, loss = 0.81862640\n",
      "Iteration 57, loss = 0.82419822\n",
      "Iteration 58, loss = 0.81804612\n",
      "Iteration 59, loss = 0.80343218\n",
      "Iteration 60, loss = 0.80203025\n",
      "Iteration 61, loss = 0.82271796\n",
      "Iteration 62, loss = 0.81007070\n",
      "Iteration 63, loss = 0.80386947\n",
      "Iteration 64, loss = 0.78161835\n",
      "Iteration 65, loss = 0.78789557\n",
      "Iteration 66, loss = 0.79535062\n",
      "Iteration 67, loss = 0.80604042\n",
      "Iteration 68, loss = 0.77571432\n",
      "Iteration 69, loss = 0.77410714\n",
      "Iteration 70, loss = 0.80773489\n",
      "Iteration 71, loss = 0.78987674\n",
      "Iteration 72, loss = 0.80620143\n",
      "Iteration 73, loss = 0.76194055\n",
      "Iteration 74, loss = 0.75151602\n",
      "Iteration 75, loss = 0.74391844\n",
      "Iteration 76, loss = 0.74374669\n",
      "Iteration 77, loss = 0.75997266\n",
      "Iteration 78, loss = 0.74357995\n",
      "Iteration 79, loss = 0.72491312\n",
      "Iteration 80, loss = 0.73165128\n",
      "Iteration 81, loss = 0.72805066\n",
      "Iteration 82, loss = 0.76621717\n",
      "Iteration 83, loss = 0.74292197\n",
      "Iteration 84, loss = 0.72295943\n",
      "Iteration 85, loss = 0.72246462\n",
      "Iteration 86, loss = 0.69820696\n",
      "Iteration 87, loss = 0.69159813\n",
      "Iteration 88, loss = 0.69563579\n",
      "Iteration 89, loss = 0.68977937\n",
      "Iteration 90, loss = 0.69310729\n",
      "Iteration 91, loss = 0.70543192\n",
      "Iteration 92, loss = 0.67886360\n",
      "Iteration 93, loss = 0.67358738\n",
      "Iteration 94, loss = 0.66948479\n",
      "Iteration 95, loss = 0.66721892\n",
      "Iteration 96, loss = 0.68207039\n",
      "Iteration 97, loss = 0.68083339\n",
      "Iteration 98, loss = 0.68839442\n",
      "Iteration 99, loss = 0.66807316\n",
      "Iteration 100, loss = 0.68146439\n",
      "Iteration 1, loss = 14.13361108\n",
      "Iteration 2, loss = 9.89684940\n",
      "Iteration 3, loss = 4.55737317\n",
      "Iteration 4, loss = 2.32453931\n",
      "Iteration 5, loss = 1.61926817\n",
      "Iteration 6, loss = 1.35885790\n",
      "Iteration 7, loss = 1.28356608\n",
      "Iteration 8, loss = 1.26340193\n",
      "Iteration 9, loss = 1.19201715\n",
      "Iteration 10, loss = 1.19782941\n",
      "Iteration 11, loss = 1.18027679\n",
      "Iteration 12, loss = 1.17664389\n",
      "Iteration 13, loss = 1.18058375\n",
      "Iteration 14, loss = 1.15896977\n",
      "Iteration 15, loss = 1.18360582\n",
      "Iteration 16, loss = 1.15529821\n",
      "Iteration 17, loss = 1.15054636\n",
      "Iteration 18, loss = 1.16631464\n",
      "Iteration 19, loss = 1.14539378\n",
      "Iteration 20, loss = 1.16241645\n",
      "Iteration 21, loss = 1.16570092\n",
      "Iteration 22, loss = 1.14179391\n",
      "Iteration 23, loss = 1.13047229\n",
      "Iteration 24, loss = 1.12402985\n",
      "Iteration 25, loss = 1.13465642\n",
      "Iteration 26, loss = 1.14251614\n",
      "Iteration 27, loss = 1.10846616\n",
      "Iteration 28, loss = 1.09903702\n",
      "Iteration 29, loss = 1.12393160\n",
      "Iteration 30, loss = 1.10245808\n",
      "Iteration 31, loss = 1.09848371\n",
      "Iteration 32, loss = 1.09464882\n",
      "Iteration 33, loss = 1.12668255\n",
      "Iteration 34, loss = 1.10734782\n",
      "Iteration 35, loss = 1.11961050\n",
      "Iteration 36, loss = 1.10182440\n",
      "Iteration 37, loss = 1.07978018\n",
      "Iteration 38, loss = 1.08856269\n",
      "Iteration 39, loss = 1.08876379\n",
      "Iteration 40, loss = 1.09055244\n",
      "Iteration 41, loss = 1.08025610\n",
      "Iteration 42, loss = 1.07920244\n",
      "Iteration 43, loss = 1.05878579\n",
      "Iteration 44, loss = 1.05857417\n",
      "Iteration 45, loss = 1.04360224\n",
      "Iteration 46, loss = 1.06634392\n",
      "Iteration 47, loss = 1.09068881\n",
      "Iteration 48, loss = 1.08741724\n",
      "Iteration 49, loss = 1.07322825\n",
      "Iteration 50, loss = 1.06008574\n",
      "Iteration 51, loss = 1.10822480\n",
      "Iteration 52, loss = 1.06814708\n",
      "Iteration 53, loss = 1.03950943\n",
      "Iteration 54, loss = 1.03464145\n",
      "Iteration 55, loss = 1.03692708\n",
      "Iteration 56, loss = 1.04169849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57, loss = 1.05068637\n",
      "Iteration 58, loss = 1.04741886\n",
      "Iteration 59, loss = 1.02910853\n",
      "Iteration 60, loss = 1.04108936\n",
      "Iteration 61, loss = 1.05564265\n",
      "Iteration 62, loss = 1.02237743\n",
      "Iteration 63, loss = 1.04001455\n",
      "Iteration 64, loss = 1.08426042\n",
      "Iteration 65, loss = 1.09084433\n",
      "Iteration 66, loss = 1.05296662\n",
      "Iteration 67, loss = 1.10059098\n",
      "Iteration 68, loss = 1.06803367\n",
      "Iteration 69, loss = 1.04162778\n",
      "Iteration 70, loss = 1.05432161\n",
      "Iteration 71, loss = 1.05867020\n",
      "Iteration 72, loss = 1.04087435\n",
      "Iteration 73, loss = 1.01562922\n",
      "Iteration 74, loss = 1.02242354\n",
      "Iteration 75, loss = 1.03982520\n",
      "Iteration 76, loss = 1.04775031\n",
      "Iteration 77, loss = 1.01958508\n",
      "Iteration 78, loss = 1.01323952\n",
      "Iteration 79, loss = 1.02225936\n",
      "Iteration 80, loss = 1.01380456\n",
      "Iteration 81, loss = 1.02057373\n",
      "Iteration 82, loss = 1.02039861\n",
      "Iteration 83, loss = 1.06171472\n",
      "Iteration 84, loss = 1.07074647\n",
      "Iteration 85, loss = 1.08907221\n",
      "Iteration 86, loss = 1.04245543\n",
      "Iteration 87, loss = 1.02991046\n",
      "Iteration 88, loss = 1.05669549\n",
      "Iteration 89, loss = 1.06049180\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Two-layer model (128, 128): CV accuracy = 0.446\n",
      "Iteration 1, loss = 1.35947669\n",
      "Iteration 2, loss = 1.19343973\n",
      "Iteration 3, loss = 1.11615163\n",
      "Iteration 4, loss = 1.11312895\n",
      "Iteration 5, loss = 1.06985173\n",
      "Iteration 6, loss = 1.07761738\n",
      "Iteration 7, loss = 1.06636871\n",
      "Iteration 8, loss = 1.05701686\n",
      "Iteration 9, loss = 1.02962738\n",
      "Iteration 10, loss = 1.02121467\n",
      "Iteration 11, loss = 1.01138741\n",
      "Iteration 12, loss = 0.99838166\n",
      "Iteration 13, loss = 0.99431464\n",
      "Iteration 14, loss = 0.99153699\n",
      "Iteration 15, loss = 0.98663889\n",
      "Iteration 16, loss = 0.97256944\n",
      "Iteration 17, loss = 0.97046608\n",
      "Iteration 18, loss = 0.97430152\n",
      "Iteration 19, loss = 0.96351943\n",
      "Iteration 20, loss = 0.96749216\n",
      "Iteration 21, loss = 0.96866680\n",
      "Iteration 22, loss = 0.95999780\n",
      "Iteration 23, loss = 0.96156162\n",
      "Iteration 24, loss = 0.97887883\n",
      "Iteration 25, loss = 0.95105450\n",
      "Iteration 26, loss = 0.94625894\n",
      "Iteration 27, loss = 0.93203108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.94651441\n",
      "Iteration 29, loss = 0.92251643\n",
      "Iteration 30, loss = 0.91699039\n",
      "Iteration 31, loss = 0.90412965\n",
      "Iteration 32, loss = 0.90159133\n",
      "Iteration 33, loss = 0.89673526\n",
      "Iteration 34, loss = 0.89225603\n",
      "Iteration 35, loss = 0.88814722\n",
      "Iteration 36, loss = 0.88316637\n",
      "Iteration 37, loss = 0.87996099\n",
      "Iteration 38, loss = 0.87529686\n",
      "Iteration 39, loss = 0.86807874\n",
      "Iteration 40, loss = 0.87055442\n",
      "Iteration 41, loss = 0.88891361\n",
      "Iteration 42, loss = 0.86929482\n",
      "Iteration 43, loss = 0.87347283\n",
      "Iteration 44, loss = 0.86792804\n",
      "Iteration 45, loss = 0.86960532\n",
      "Iteration 46, loss = 0.84860955\n",
      "Iteration 47, loss = 0.83573379\n",
      "Iteration 48, loss = 0.83345572\n",
      "Iteration 49, loss = 0.83479986\n",
      "Iteration 50, loss = 0.82485217\n",
      "Iteration 51, loss = 0.81802637\n",
      "Iteration 52, loss = 0.82066583\n",
      "Iteration 53, loss = 0.81352430\n",
      "Iteration 54, loss = 0.81210171\n",
      "Iteration 55, loss = 0.81810680\n",
      "Iteration 56, loss = 0.81215579\n",
      "Iteration 57, loss = 0.80503526\n",
      "Iteration 58, loss = 0.78778142\n",
      "Iteration 59, loss = 0.79566062\n",
      "Iteration 60, loss = 0.79609674\n",
      "Iteration 61, loss = 0.80793415\n",
      "Iteration 62, loss = 0.78673103\n",
      "Iteration 63, loss = 0.76495174\n",
      "Iteration 64, loss = 0.75850940\n",
      "Iteration 65, loss = 0.76626723\n",
      "Iteration 66, loss = 0.76351857\n",
      "Iteration 67, loss = 0.74349088\n",
      "Iteration 68, loss = 0.75615663\n",
      "Iteration 69, loss = 0.74052773\n",
      "Iteration 70, loss = 0.72950846\n",
      "Iteration 71, loss = 0.72745023\n",
      "Iteration 72, loss = 0.71807706\n",
      "Iteration 73, loss = 0.72743808\n",
      "Iteration 74, loss = 0.73549625\n",
      "Iteration 75, loss = 0.72079477\n",
      "Iteration 76, loss = 0.73950765\n",
      "Iteration 77, loss = 0.75802296\n",
      "Iteration 78, loss = 0.70967338\n",
      "Iteration 79, loss = 0.69992155\n",
      "Iteration 80, loss = 0.68954469\n",
      "Iteration 81, loss = 0.68305733\n",
      "Iteration 82, loss = 0.69721397\n",
      "Iteration 83, loss = 0.69297636\n",
      "Iteration 84, loss = 0.68204394\n",
      "Iteration 85, loss = 0.68369713\n",
      "Iteration 86, loss = 0.66332713\n",
      "Iteration 87, loss = 0.66831339\n",
      "Iteration 88, loss = 0.65115780\n",
      "Iteration 89, loss = 0.65156280\n",
      "Iteration 90, loss = 0.66626214\n",
      "Iteration 91, loss = 0.65758602\n",
      "Iteration 92, loss = 0.64000116\n",
      "Iteration 93, loss = 0.62485947\n",
      "Iteration 94, loss = 0.61640804\n",
      "Iteration 95, loss = 0.61707502\n",
      "Iteration 96, loss = 0.61851827\n",
      "Iteration 97, loss = 0.61270481\n",
      "Iteration 98, loss = 0.63039453\n",
      "Iteration 99, loss = 0.61584380\n",
      "Iteration 100, loss = 0.59512151\n",
      "Iteration 1, loss = 1.34545498\n",
      "Iteration 2, loss = 1.16684313\n",
      "Iteration 3, loss = 1.10417196\n",
      "Iteration 4, loss = 1.08623768\n",
      "Iteration 5, loss = 1.05798782\n",
      "Iteration 6, loss = 1.05666450\n",
      "Iteration 7, loss = 1.06216568\n",
      "Iteration 8, loss = 1.03554414\n",
      "Iteration 9, loss = 1.01469570\n",
      "Iteration 10, loss = 1.01707175\n",
      "Iteration 11, loss = 1.00603132\n",
      "Iteration 12, loss = 0.99740500\n",
      "Iteration 13, loss = 0.98668414\n",
      "Iteration 14, loss = 0.99115267\n",
      "Iteration 15, loss = 0.97254163\n",
      "Iteration 16, loss = 0.97672546\n",
      "Iteration 17, loss = 0.97339744\n",
      "Iteration 18, loss = 0.96206946\n",
      "Iteration 19, loss = 0.95058988\n",
      "Iteration 20, loss = 0.95438069\n",
      "Iteration 21, loss = 0.94847748\n",
      "Iteration 22, loss = 0.94245033\n",
      "Iteration 23, loss = 0.92973672\n",
      "Iteration 24, loss = 0.93228594\n",
      "Iteration 25, loss = 0.94035714\n",
      "Iteration 26, loss = 0.95175066\n",
      "Iteration 27, loss = 0.93625792\n",
      "Iteration 28, loss = 0.93078767\n",
      "Iteration 29, loss = 0.93562763\n",
      "Iteration 30, loss = 0.93432030\n",
      "Iteration 31, loss = 0.91279815\n",
      "Iteration 32, loss = 0.90030220\n",
      "Iteration 33, loss = 0.89597748\n",
      "Iteration 34, loss = 0.88440238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 0.89307416\n",
      "Iteration 36, loss = 0.87230253\n",
      "Iteration 37, loss = 0.88714285\n",
      "Iteration 38, loss = 0.88091830\n",
      "Iteration 39, loss = 0.86641522\n",
      "Iteration 40, loss = 0.86396871\n",
      "Iteration 41, loss = 0.84725608\n",
      "Iteration 42, loss = 0.85604005\n",
      "Iteration 43, loss = 0.84515485\n",
      "Iteration 44, loss = 0.83067156\n",
      "Iteration 45, loss = 0.83296289\n",
      "Iteration 46, loss = 0.83488315\n",
      "Iteration 47, loss = 0.84107961\n",
      "Iteration 48, loss = 0.83011344\n",
      "Iteration 49, loss = 0.82410928\n",
      "Iteration 50, loss = 0.81335492\n",
      "Iteration 51, loss = 0.80066730\n",
      "Iteration 52, loss = 0.79145962\n",
      "Iteration 53, loss = 0.79284053\n",
      "Iteration 54, loss = 0.79570524\n",
      "Iteration 55, loss = 0.78283659\n",
      "Iteration 56, loss = 0.77746587\n",
      "Iteration 57, loss = 0.77171328\n",
      "Iteration 58, loss = 0.76523252\n",
      "Iteration 59, loss = 0.76515268\n",
      "Iteration 60, loss = 0.76510513\n",
      "Iteration 61, loss = 0.74168016\n",
      "Iteration 62, loss = 0.73582630\n",
      "Iteration 63, loss = 0.74970501\n",
      "Iteration 64, loss = 0.73722884\n",
      "Iteration 65, loss = 0.75462320\n",
      "Iteration 66, loss = 0.74165856\n",
      "Iteration 67, loss = 0.72384706\n",
      "Iteration 68, loss = 0.73505148\n",
      "Iteration 69, loss = 0.72025434\n",
      "Iteration 70, loss = 0.71958781\n",
      "Iteration 71, loss = 0.70081008\n",
      "Iteration 72, loss = 0.69532707\n",
      "Iteration 73, loss = 0.68872734\n",
      "Iteration 74, loss = 0.68711302\n",
      "Iteration 75, loss = 0.68307120\n",
      "Iteration 76, loss = 0.68081521\n",
      "Iteration 77, loss = 0.67419081\n",
      "Iteration 78, loss = 0.69154896\n",
      "Iteration 79, loss = 0.67812043\n",
      "Iteration 80, loss = 0.66872620\n",
      "Iteration 81, loss = 0.65208537\n",
      "Iteration 82, loss = 0.63635626\n",
      "Iteration 83, loss = 0.62198435\n",
      "Iteration 84, loss = 0.62655652\n",
      "Iteration 85, loss = 0.64759870\n",
      "Iteration 86, loss = 0.64274388\n",
      "Iteration 87, loss = 0.64103566\n",
      "Iteration 88, loss = 0.60112566\n",
      "Iteration 89, loss = 0.59529824\n",
      "Iteration 90, loss = 0.57867507\n",
      "Iteration 91, loss = 0.57161649\n",
      "Iteration 92, loss = 0.56593521\n",
      "Iteration 93, loss = 0.57963257\n",
      "Iteration 94, loss = 0.57810460\n",
      "Iteration 95, loss = 0.56857095\n",
      "Iteration 96, loss = 0.56247541\n",
      "Iteration 97, loss = 0.54861878\n",
      "Iteration 98, loss = 0.54132711\n",
      "Iteration 99, loss = 0.52834210\n",
      "Iteration 100, loss = 0.52718956\n",
      "Iteration 1, loss = 1.32629389\n",
      "Iteration 2, loss = 1.18134445\n",
      "Iteration 3, loss = 1.11327758\n",
      "Iteration 4, loss = 1.09127726\n",
      "Iteration 5, loss = 1.06487827\n",
      "Iteration 6, loss = 1.06746591\n",
      "Iteration 7, loss = 1.06005945\n",
      "Iteration 8, loss = 1.05158514\n",
      "Iteration 9, loss = 1.04140722\n",
      "Iteration 10, loss = 1.04117948\n",
      "Iteration 11, loss = 1.03152140\n",
      "Iteration 12, loss = 1.02200806\n",
      "Iteration 13, loss = 1.01213340\n",
      "Iteration 14, loss = 1.01203103\n",
      "Iteration 15, loss = 1.00536018\n",
      "Iteration 16, loss = 0.99235147\n",
      "Iteration 17, loss = 0.98149734\n",
      "Iteration 18, loss = 0.97910910\n",
      "Iteration 19, loss = 0.96981065\n",
      "Iteration 20, loss = 0.96904929\n",
      "Iteration 21, loss = 0.96091834\n",
      "Iteration 22, loss = 0.95356410\n",
      "Iteration 23, loss = 0.94666680\n",
      "Iteration 24, loss = 0.94825651\n",
      "Iteration 25, loss = 0.93958620\n",
      "Iteration 26, loss = 0.93669969\n",
      "Iteration 27, loss = 0.93978569\n",
      "Iteration 28, loss = 0.92498739\n",
      "Iteration 29, loss = 0.92458602\n",
      "Iteration 30, loss = 0.91571742\n",
      "Iteration 31, loss = 0.90913053\n",
      "Iteration 32, loss = 0.90105525\n",
      "Iteration 33, loss = 0.91146159\n",
      "Iteration 34, loss = 0.89508085\n",
      "Iteration 35, loss = 0.90171919\n",
      "Iteration 36, loss = 0.89044105\n",
      "Iteration 37, loss = 0.90113350\n",
      "Iteration 38, loss = 0.89028860\n",
      "Iteration 39, loss = 0.87952098\n",
      "Iteration 40, loss = 0.86812053\n",
      "Iteration 41, loss = 0.85722553\n",
      "Iteration 42, loss = 0.86183895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.85528104\n",
      "Iteration 44, loss = 0.84846562\n",
      "Iteration 45, loss = 0.84158025\n",
      "Iteration 46, loss = 0.84844374\n",
      "Iteration 47, loss = 0.84502908\n",
      "Iteration 48, loss = 0.85704824\n",
      "Iteration 49, loss = 0.84791834\n",
      "Iteration 50, loss = 0.82697171\n",
      "Iteration 51, loss = 0.81855842\n",
      "Iteration 52, loss = 0.82238690\n",
      "Iteration 53, loss = 0.80886634\n",
      "Iteration 54, loss = 0.82586300\n",
      "Iteration 55, loss = 0.81408857\n",
      "Iteration 56, loss = 0.81963680\n",
      "Iteration 57, loss = 0.81749188\n",
      "Iteration 58, loss = 0.79370729\n",
      "Iteration 59, loss = 0.78578541\n",
      "Iteration 60, loss = 0.77425798\n",
      "Iteration 61, loss = 0.76099686\n",
      "Iteration 62, loss = 0.75107077\n",
      "Iteration 63, loss = 0.76274579\n",
      "Iteration 64, loss = 0.77068756\n",
      "Iteration 65, loss = 0.77238826\n",
      "Iteration 66, loss = 0.74618816\n",
      "Iteration 67, loss = 0.72993294\n",
      "Iteration 68, loss = 0.72649566\n",
      "Iteration 69, loss = 0.72070961\n",
      "Iteration 70, loss = 0.71910642\n",
      "Iteration 71, loss = 0.69949854\n",
      "Iteration 72, loss = 0.71086067\n",
      "Iteration 73, loss = 0.70833491\n",
      "Iteration 74, loss = 0.73019071\n",
      "Iteration 75, loss = 0.70606733\n",
      "Iteration 76, loss = 0.71303530\n",
      "Iteration 77, loss = 0.68726635\n",
      "Iteration 78, loss = 0.68384273\n",
      "Iteration 79, loss = 0.68266088\n",
      "Iteration 80, loss = 0.67834308\n",
      "Iteration 81, loss = 0.69441912\n",
      "Iteration 82, loss = 0.66771013\n",
      "Iteration 83, loss = 0.65979606\n",
      "Iteration 84, loss = 0.65600507\n",
      "Iteration 85, loss = 0.66601504\n",
      "Iteration 86, loss = 0.69138211\n",
      "Iteration 87, loss = 0.72327918\n",
      "Iteration 88, loss = 0.66462281\n",
      "Iteration 89, loss = 0.66531309\n",
      "Iteration 90, loss = 0.62031111\n",
      "Iteration 91, loss = 0.60179304\n",
      "Iteration 92, loss = 0.60896649\n",
      "Iteration 93, loss = 0.61217749\n",
      "Iteration 94, loss = 0.61082901\n",
      "Iteration 95, loss = 0.61335071\n",
      "Iteration 96, loss = 0.62855796\n",
      "Iteration 97, loss = 0.62242645\n",
      "Iteration 98, loss = 0.62398732\n",
      "Iteration 99, loss = 0.60461895\n",
      "Iteration 100, loss = 0.61279267\n",
      "Iteration 1, loss = 1.34255961\n",
      "Iteration 2, loss = 1.18932058\n",
      "Iteration 3, loss = 1.13672220\n",
      "Iteration 4, loss = 1.11310083\n",
      "Iteration 5, loss = 1.09353859\n",
      "Iteration 6, loss = 1.08142341\n",
      "Iteration 7, loss = 1.06882367\n",
      "Iteration 8, loss = 1.05179295\n",
      "Iteration 9, loss = 1.03486847\n",
      "Iteration 10, loss = 1.03622527\n",
      "Iteration 11, loss = 1.02329096\n",
      "Iteration 12, loss = 1.01678018\n",
      "Iteration 13, loss = 1.00518655\n",
      "Iteration 14, loss = 0.99594463\n",
      "Iteration 15, loss = 0.99221248\n",
      "Iteration 16, loss = 0.98450038\n",
      "Iteration 17, loss = 0.97895228\n",
      "Iteration 18, loss = 0.97916492\n",
      "Iteration 19, loss = 0.97694040\n",
      "Iteration 20, loss = 0.98068817\n",
      "Iteration 21, loss = 0.97050454\n",
      "Iteration 22, loss = 0.95492338\n",
      "Iteration 23, loss = 0.94277828\n",
      "Iteration 24, loss = 0.94020286\n",
      "Iteration 25, loss = 0.93694825\n",
      "Iteration 26, loss = 0.93131944\n",
      "Iteration 27, loss = 0.92872064\n",
      "Iteration 28, loss = 0.92251495\n",
      "Iteration 29, loss = 0.93000139\n",
      "Iteration 30, loss = 0.92937796\n",
      "Iteration 31, loss = 0.91202512\n",
      "Iteration 32, loss = 0.91738734\n",
      "Iteration 33, loss = 0.92161622\n",
      "Iteration 34, loss = 0.89888476\n",
      "Iteration 35, loss = 0.91237661\n",
      "Iteration 36, loss = 0.89582475\n",
      "Iteration 37, loss = 0.89394653\n",
      "Iteration 38, loss = 0.88611827\n",
      "Iteration 39, loss = 0.88178104\n",
      "Iteration 40, loss = 0.86910439\n",
      "Iteration 41, loss = 0.85780295\n",
      "Iteration 42, loss = 0.85842200\n",
      "Iteration 43, loss = 0.84846318\n",
      "Iteration 44, loss = 0.84494416\n",
      "Iteration 45, loss = 0.84697090\n",
      "Iteration 46, loss = 0.83006493\n",
      "Iteration 47, loss = 0.83383581\n",
      "Iteration 48, loss = 0.83922556\n",
      "Iteration 49, loss = 0.82475072\n",
      "Iteration 50, loss = 0.82342365\n",
      "Iteration 51, loss = 0.81754923\n",
      "Iteration 52, loss = 0.80251008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53, loss = 0.80644979\n",
      "Iteration 54, loss = 0.80951668\n",
      "Iteration 55, loss = 0.78408459\n",
      "Iteration 56, loss = 0.78771017\n",
      "Iteration 57, loss = 0.78330381\n",
      "Iteration 58, loss = 0.78188231\n",
      "Iteration 59, loss = 0.77095669\n",
      "Iteration 60, loss = 0.77351225\n",
      "Iteration 61, loss = 0.77271237\n",
      "Iteration 62, loss = 0.77477093\n",
      "Iteration 63, loss = 0.76860825\n",
      "Iteration 64, loss = 0.76894626\n",
      "Iteration 65, loss = 0.77412739\n",
      "Iteration 66, loss = 0.76879423\n",
      "Iteration 67, loss = 0.72626576\n",
      "Iteration 68, loss = 0.73249597\n",
      "Iteration 69, loss = 0.71460758\n",
      "Iteration 70, loss = 0.71980394\n",
      "Iteration 71, loss = 0.71417558\n",
      "Iteration 72, loss = 0.70333779\n",
      "Iteration 73, loss = 0.70450887\n",
      "Iteration 74, loss = 0.69219004\n",
      "Iteration 75, loss = 0.68635251\n",
      "Iteration 76, loss = 0.70847343\n",
      "Iteration 77, loss = 0.69268455\n",
      "Iteration 78, loss = 0.68748125\n",
      "Iteration 79, loss = 0.67180673\n",
      "Iteration 80, loss = 0.68125308\n",
      "Iteration 81, loss = 0.68023852\n",
      "Iteration 82, loss = 0.66308161\n",
      "Iteration 83, loss = 0.64843650\n",
      "Iteration 84, loss = 0.64931171\n",
      "Iteration 85, loss = 0.67059409\n",
      "Iteration 86, loss = 0.66708470\n",
      "Iteration 87, loss = 0.70969682\n",
      "Iteration 88, loss = 0.67292695\n",
      "Iteration 89, loss = 0.68979460\n",
      "Iteration 90, loss = 0.65438079\n",
      "Iteration 91, loss = 0.63842658\n",
      "Iteration 92, loss = 0.62242957\n",
      "Iteration 93, loss = 0.62833701\n",
      "Iteration 94, loss = 0.61872321\n",
      "Iteration 95, loss = 0.63173595\n",
      "Iteration 96, loss = 0.63676148\n",
      "Iteration 97, loss = 0.63392573\n",
      "Iteration 98, loss = 0.62751700\n",
      "Iteration 99, loss = 0.61958229\n",
      "Iteration 100, loss = 0.59669576\n",
      "Iteration 1, loss = 1.33011700\n",
      "Iteration 2, loss = 1.17441558\n",
      "Iteration 3, loss = 1.10396466\n",
      "Iteration 4, loss = 1.08384891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 1.04881956\n",
      "Iteration 6, loss = 1.02767667\n",
      "Iteration 7, loss = 1.01138087\n",
      "Iteration 8, loss = 1.00190248\n",
      "Iteration 9, loss = 0.99657114\n",
      "Iteration 10, loss = 0.99670797\n",
      "Iteration 11, loss = 0.98506602\n",
      "Iteration 12, loss = 0.97536729\n",
      "Iteration 13, loss = 0.97620858\n",
      "Iteration 14, loss = 0.98302998\n",
      "Iteration 15, loss = 0.98157613\n",
      "Iteration 16, loss = 0.96382124\n",
      "Iteration 17, loss = 0.95061584\n",
      "Iteration 18, loss = 0.94464165\n",
      "Iteration 19, loss = 0.94865342\n",
      "Iteration 20, loss = 0.94093209\n",
      "Iteration 21, loss = 0.94094557\n",
      "Iteration 22, loss = 0.92725093\n",
      "Iteration 23, loss = 0.92636498\n",
      "Iteration 24, loss = 0.91833121\n",
      "Iteration 25, loss = 0.91850530\n",
      "Iteration 26, loss = 0.91843252\n",
      "Iteration 27, loss = 0.90323188\n",
      "Iteration 28, loss = 0.89809338\n",
      "Iteration 29, loss = 0.89034016\n",
      "Iteration 30, loss = 0.88899023\n",
      "Iteration 31, loss = 0.88163486\n",
      "Iteration 32, loss = 0.88625796\n",
      "Iteration 33, loss = 0.86904234\n",
      "Iteration 34, loss = 0.87682897\n",
      "Iteration 35, loss = 0.87260303\n",
      "Iteration 36, loss = 0.86466477\n",
      "Iteration 37, loss = 0.85716344\n",
      "Iteration 38, loss = 0.84160711\n",
      "Iteration 39, loss = 0.85343350\n",
      "Iteration 40, loss = 0.84100909\n",
      "Iteration 41, loss = 0.83288929\n",
      "Iteration 42, loss = 0.82723534\n",
      "Iteration 43, loss = 0.84230407\n",
      "Iteration 44, loss = 0.83033756\n",
      "Iteration 45, loss = 0.82863492\n",
      "Iteration 46, loss = 0.82503882\n",
      "Iteration 47, loss = 0.84475895\n",
      "Iteration 48, loss = 0.80935047\n",
      "Iteration 49, loss = 0.80584763\n",
      "Iteration 50, loss = 0.78862778\n",
      "Iteration 51, loss = 0.79003342\n",
      "Iteration 52, loss = 0.78451163\n",
      "Iteration 53, loss = 0.79043208\n",
      "Iteration 54, loss = 0.79004220\n",
      "Iteration 55, loss = 0.77456883\n",
      "Iteration 56, loss = 0.77561100\n",
      "Iteration 57, loss = 0.76056596\n",
      "Iteration 58, loss = 0.76540143\n",
      "Iteration 59, loss = 0.75692017\n",
      "Iteration 60, loss = 0.75776321\n",
      "Iteration 61, loss = 0.76191104\n",
      "Iteration 62, loss = 0.77775907\n",
      "Iteration 63, loss = 0.75754312\n",
      "Iteration 64, loss = 0.76911115\n",
      "Iteration 65, loss = 0.75827386\n",
      "Iteration 66, loss = 0.74088853\n",
      "Iteration 67, loss = 0.72970990\n",
      "Iteration 68, loss = 0.74484791\n",
      "Iteration 69, loss = 0.73276823\n",
      "Iteration 70, loss = 0.71424509\n",
      "Iteration 71, loss = 0.73722401\n",
      "Iteration 72, loss = 0.73218711\n",
      "Iteration 73, loss = 0.71652820\n",
      "Iteration 74, loss = 0.71927932\n",
      "Iteration 75, loss = 0.68799497\n",
      "Iteration 76, loss = 0.69045560\n",
      "Iteration 77, loss = 0.68415965\n",
      "Iteration 78, loss = 0.67910849\n",
      "Iteration 79, loss = 0.67742498\n",
      "Iteration 80, loss = 0.67486014\n",
      "Iteration 81, loss = 0.66158597\n",
      "Iteration 82, loss = 0.66705419\n",
      "Iteration 83, loss = 0.66431402\n",
      "Iteration 84, loss = 0.63932040\n",
      "Iteration 85, loss = 0.63993441\n",
      "Iteration 86, loss = 0.65286875\n",
      "Iteration 87, loss = 0.63142894\n",
      "Iteration 88, loss = 0.63784934\n",
      "Iteration 89, loss = 0.62343126\n",
      "Iteration 90, loss = 0.61642980\n",
      "Iteration 91, loss = 0.62544819\n",
      "Iteration 92, loss = 0.60927211\n",
      "Iteration 93, loss = 0.59908545\n",
      "Iteration 94, loss = 0.61857175\n",
      "Iteration 95, loss = 0.60958531\n",
      "Iteration 96, loss = 0.61761104\n",
      "Iteration 97, loss = 0.61160440\n",
      "Iteration 98, loss = 0.59144092\n",
      "Iteration 99, loss = 0.58422646\n",
      "Iteration 100, loss = 0.60846045\n",
      "Iteration 1, loss = 16.56048686\n",
      "Iteration 2, loss = 10.81530688\n",
      "Iteration 3, loss = 8.10997594\n",
      "Iteration 4, loss = 4.23686963\n",
      "Iteration 5, loss = 1.80318208\n",
      "Iteration 6, loss = 1.33211476\n",
      "Iteration 7, loss = 1.23852108\n",
      "Iteration 8, loss = 1.20254633\n",
      "Iteration 9, loss = 1.19300448\n",
      "Iteration 10, loss = 1.17919321\n",
      "Iteration 11, loss = 1.16866692\n",
      "Iteration 12, loss = 1.16886711\n",
      "Iteration 13, loss = 1.16178215\n",
      "Iteration 14, loss = 1.15240513\n",
      "Iteration 15, loss = 1.14589790\n",
      "Iteration 16, loss = 1.14509628\n",
      "Iteration 17, loss = 1.14890125\n",
      "Iteration 18, loss = 1.15162659\n",
      "Iteration 19, loss = 1.12514084\n",
      "Iteration 20, loss = 1.12297395\n",
      "Iteration 21, loss = 1.12736009\n",
      "Iteration 22, loss = 1.11620494\n",
      "Iteration 23, loss = 1.11632040\n",
      "Iteration 24, loss = 1.10920687\n",
      "Iteration 25, loss = 1.10721431\n",
      "Iteration 26, loss = 1.11815453\n",
      "Iteration 27, loss = 1.15196384\n",
      "Iteration 28, loss = 1.11713787\n",
      "Iteration 29, loss = 1.10842961\n",
      "Iteration 30, loss = 1.09995751\n",
      "Iteration 31, loss = 1.08438803\n",
      "Iteration 32, loss = 1.10604187\n",
      "Iteration 33, loss = 1.09788260\n",
      "Iteration 34, loss = 1.08303472\n",
      "Iteration 35, loss = 1.07138173\n",
      "Iteration 36, loss = 1.07761714\n",
      "Iteration 37, loss = 1.06752778\n",
      "Iteration 38, loss = 1.07224649\n",
      "Iteration 39, loss = 1.08536204\n",
      "Iteration 40, loss = 1.05920800\n",
      "Iteration 41, loss = 1.05211267\n",
      "Iteration 42, loss = 1.06398400\n",
      "Iteration 43, loss = 1.04769037\n",
      "Iteration 44, loss = 1.05057456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 1.04527908\n",
      "Iteration 46, loss = 1.05606991\n",
      "Iteration 47, loss = 1.07109954\n",
      "Iteration 48, loss = 1.07133304\n",
      "Iteration 49, loss = 1.03522544\n",
      "Iteration 50, loss = 1.03021535\n",
      "Iteration 51, loss = 1.03266948\n",
      "Iteration 52, loss = 1.02392342\n",
      "Iteration 53, loss = 1.05194062\n",
      "Iteration 54, loss = 1.05222120\n",
      "Iteration 55, loss = 1.03517112\n",
      "Iteration 56, loss = 1.03949365\n",
      "Iteration 57, loss = 1.04402173\n",
      "Iteration 58, loss = 1.02979594\n",
      "Iteration 59, loss = 1.01616381\n",
      "Iteration 60, loss = 1.02417395\n",
      "Iteration 61, loss = 1.01063796\n",
      "Iteration 62, loss = 1.04314510\n",
      "Iteration 63, loss = 1.05870573\n",
      "Iteration 64, loss = 1.03536587\n",
      "Iteration 65, loss = 1.03218915\n",
      "Iteration 66, loss = 1.03942795\n",
      "Iteration 67, loss = 1.04235839\n",
      "Iteration 68, loss = 1.03953939\n",
      "Iteration 69, loss = 1.02297674\n",
      "Iteration 70, loss = 0.99914556\n",
      "Iteration 71, loss = 0.99327878\n",
      "Iteration 72, loss = 1.01469775\n",
      "Iteration 73, loss = 1.05133410\n",
      "Iteration 74, loss = 1.02157142\n",
      "Iteration 75, loss = 1.00649762\n",
      "Iteration 76, loss = 1.02867525\n",
      "Iteration 77, loss = 1.00532958\n",
      "Iteration 78, loss = 1.01489373\n",
      "Iteration 79, loss = 1.02514028\n",
      "Iteration 80, loss = 0.99228096\n",
      "Iteration 81, loss = 0.99580831\n",
      "Iteration 82, loss = 0.97972519\n",
      "Iteration 83, loss = 0.99836364\n",
      "Iteration 84, loss = 0.98924879\n",
      "Iteration 85, loss = 1.00865319\n",
      "Iteration 86, loss = 1.01866217\n",
      "Iteration 87, loss = 1.02085319\n",
      "Iteration 88, loss = 1.00533135\n",
      "Iteration 89, loss = 0.98478893\n",
      "Iteration 90, loss = 0.99870218\n",
      "Iteration 91, loss = 0.98422962\n",
      "Iteration 92, loss = 0.99579083\n",
      "Iteration 93, loss = 1.00581297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Two-layer model (128, 256): CV accuracy = 0.431\n",
      "Iteration 1, loss = 1.36931836\n",
      "Iteration 2, loss = 1.17468279\n",
      "Iteration 3, loss = 1.13692018\n",
      "Iteration 4, loss = 1.08509486\n",
      "Iteration 5, loss = 1.07066590\n",
      "Iteration 6, loss = 1.06620721\n",
      "Iteration 7, loss = 1.05493400\n",
      "Iteration 8, loss = 1.03272403\n",
      "Iteration 9, loss = 1.01782093\n",
      "Iteration 10, loss = 1.00743697\n",
      "Iteration 11, loss = 1.02553577\n",
      "Iteration 12, loss = 0.99853632\n",
      "Iteration 13, loss = 0.98344961\n",
      "Iteration 14, loss = 0.98132624\n",
      "Iteration 15, loss = 0.97305975\n",
      "Iteration 16, loss = 0.96438092\n",
      "Iteration 17, loss = 0.97573702\n",
      "Iteration 18, loss = 0.95576907\n",
      "Iteration 19, loss = 0.94583592\n",
      "Iteration 20, loss = 0.94352055\n",
      "Iteration 21, loss = 0.94054661\n",
      "Iteration 22, loss = 0.92815373\n",
      "Iteration 23, loss = 0.93694810\n",
      "Iteration 24, loss = 0.92214537\n",
      "Iteration 25, loss = 0.93639238\n",
      "Iteration 26, loss = 0.90449076\n",
      "Iteration 27, loss = 0.92059411\n",
      "Iteration 28, loss = 0.93007195\n",
      "Iteration 29, loss = 0.93707969\n",
      "Iteration 30, loss = 0.92698943\n",
      "Iteration 31, loss = 0.90469943\n",
      "Iteration 32, loss = 0.89441240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.89422119\n",
      "Iteration 34, loss = 0.87883878\n",
      "Iteration 35, loss = 0.88989832\n",
      "Iteration 36, loss = 0.88909944\n",
      "Iteration 37, loss = 0.86482080\n",
      "Iteration 38, loss = 0.86902105\n",
      "Iteration 39, loss = 0.86646571\n",
      "Iteration 40, loss = 0.85969509\n",
      "Iteration 41, loss = 0.84774395\n",
      "Iteration 42, loss = 0.84947340\n",
      "Iteration 43, loss = 0.84027926\n",
      "Iteration 44, loss = 0.82561917\n",
      "Iteration 45, loss = 0.82620567\n",
      "Iteration 46, loss = 0.82771676\n",
      "Iteration 47, loss = 0.80732344\n",
      "Iteration 48, loss = 0.80584939\n",
      "Iteration 49, loss = 0.80153667\n",
      "Iteration 50, loss = 0.78664332\n",
      "Iteration 51, loss = 0.78013176\n",
      "Iteration 52, loss = 0.78578907\n",
      "Iteration 53, loss = 0.77546807\n",
      "Iteration 54, loss = 0.78720178\n",
      "Iteration 55, loss = 0.76373308\n",
      "Iteration 56, loss = 0.75982982\n",
      "Iteration 57, loss = 0.73878593\n",
      "Iteration 58, loss = 0.75044773\n",
      "Iteration 59, loss = 0.77351876\n",
      "Iteration 60, loss = 0.75698384\n",
      "Iteration 61, loss = 0.74101025\n",
      "Iteration 62, loss = 0.75677297\n",
      "Iteration 63, loss = 0.75828742\n",
      "Iteration 64, loss = 0.74124302\n",
      "Iteration 65, loss = 0.74786396\n",
      "Iteration 66, loss = 0.72780344\n",
      "Iteration 67, loss = 0.72075603\n",
      "Iteration 68, loss = 0.70520768\n",
      "Iteration 69, loss = 0.69361606\n",
      "Iteration 70, loss = 0.66945318\n",
      "Iteration 71, loss = 0.69179825\n",
      "Iteration 72, loss = 0.67314528\n",
      "Iteration 73, loss = 0.66784630\n",
      "Iteration 74, loss = 0.65674938\n",
      "Iteration 75, loss = 0.63875067\n",
      "Iteration 76, loss = 0.63676916\n",
      "Iteration 77, loss = 0.63717563\n",
      "Iteration 78, loss = 0.63205031\n",
      "Iteration 79, loss = 0.63007627\n",
      "Iteration 80, loss = 0.63733059\n",
      "Iteration 81, loss = 0.61629232\n",
      "Iteration 82, loss = 0.61024545\n",
      "Iteration 83, loss = 0.60254195\n",
      "Iteration 84, loss = 0.59153343\n",
      "Iteration 85, loss = 0.57384441\n",
      "Iteration 86, loss = 0.56331819\n",
      "Iteration 87, loss = 0.56727397\n",
      "Iteration 88, loss = 0.57215398\n",
      "Iteration 89, loss = 0.56555168\n",
      "Iteration 90, loss = 0.57213588\n",
      "Iteration 91, loss = 0.56502735\n",
      "Iteration 92, loss = 0.58081221\n",
      "Iteration 93, loss = 0.55385480\n",
      "Iteration 94, loss = 0.52507238\n",
      "Iteration 95, loss = 0.52243056\n",
      "Iteration 96, loss = 0.55466940\n",
      "Iteration 97, loss = 0.55684656\n",
      "Iteration 98, loss = 0.53202740\n",
      "Iteration 99, loss = 0.52239152\n",
      "Iteration 100, loss = 0.50196826\n",
      "Iteration 1, loss = 1.35513168\n",
      "Iteration 2, loss = 1.17006413\n",
      "Iteration 3, loss = 1.09548873\n",
      "Iteration 4, loss = 1.05763703\n",
      "Iteration 5, loss = 1.06555595\n",
      "Iteration 6, loss = 1.04708092\n",
      "Iteration 7, loss = 1.04925123\n",
      "Iteration 8, loss = 1.03875124\n",
      "Iteration 9, loss = 1.01641953\n",
      "Iteration 10, loss = 1.00554468\n",
      "Iteration 11, loss = 1.02706327\n",
      "Iteration 12, loss = 0.99449611\n",
      "Iteration 13, loss = 0.98218138\n",
      "Iteration 14, loss = 0.98341236\n",
      "Iteration 15, loss = 0.97694243\n",
      "Iteration 16, loss = 0.96611286\n",
      "Iteration 17, loss = 0.97284989\n",
      "Iteration 18, loss = 0.97072741\n",
      "Iteration 19, loss = 0.96014339\n",
      "Iteration 20, loss = 0.94395928\n",
      "Iteration 21, loss = 0.95502245\n",
      "Iteration 22, loss = 0.94535269\n",
      "Iteration 23, loss = 0.97756137\n",
      "Iteration 24, loss = 0.95504660\n",
      "Iteration 25, loss = 0.92712718\n",
      "Iteration 26, loss = 0.95411473\n",
      "Iteration 27, loss = 0.91670548\n",
      "Iteration 28, loss = 0.90001155\n",
      "Iteration 29, loss = 0.89637946\n",
      "Iteration 30, loss = 0.88568423\n",
      "Iteration 31, loss = 0.87372343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.86734824\n",
      "Iteration 33, loss = 0.86125890\n",
      "Iteration 34, loss = 0.85562089\n",
      "Iteration 35, loss = 0.85094193\n",
      "Iteration 36, loss = 0.84332322\n",
      "Iteration 37, loss = 0.82973779\n",
      "Iteration 38, loss = 0.84243385\n",
      "Iteration 39, loss = 0.84922049\n",
      "Iteration 40, loss = 0.86964448\n",
      "Iteration 41, loss = 0.86147961\n",
      "Iteration 42, loss = 0.84781596\n",
      "Iteration 43, loss = 0.83625521\n",
      "Iteration 44, loss = 0.81951268\n",
      "Iteration 45, loss = 0.81395625\n",
      "Iteration 46, loss = 0.80804447\n",
      "Iteration 47, loss = 0.80199959\n",
      "Iteration 48, loss = 0.78817987\n",
      "Iteration 49, loss = 0.77957245\n",
      "Iteration 50, loss = 0.76857544\n",
      "Iteration 51, loss = 0.77339073\n",
      "Iteration 52, loss = 0.76696347\n",
      "Iteration 53, loss = 0.75574913\n",
      "Iteration 54, loss = 0.78931505\n",
      "Iteration 55, loss = 0.74958845\n",
      "Iteration 56, loss = 0.73064381\n",
      "Iteration 57, loss = 0.72074565\n",
      "Iteration 58, loss = 0.71479650\n",
      "Iteration 59, loss = 0.71384962\n",
      "Iteration 60, loss = 0.70776618\n",
      "Iteration 61, loss = 0.70862462\n",
      "Iteration 62, loss = 0.71875686\n",
      "Iteration 63, loss = 0.71374239\n",
      "Iteration 64, loss = 0.69398221\n",
      "Iteration 65, loss = 0.68920558\n",
      "Iteration 66, loss = 0.67956432\n",
      "Iteration 67, loss = 0.68060661\n",
      "Iteration 68, loss = 0.69191291\n",
      "Iteration 69, loss = 0.66128088\n",
      "Iteration 70, loss = 0.66685992\n",
      "Iteration 71, loss = 0.65241168\n",
      "Iteration 72, loss = 0.64606926\n",
      "Iteration 73, loss = 0.64885886\n",
      "Iteration 74, loss = 0.64007196\n",
      "Iteration 75, loss = 0.62855466\n",
      "Iteration 76, loss = 0.60694994\n",
      "Iteration 77, loss = 0.60463428\n",
      "Iteration 78, loss = 0.59546274\n",
      "Iteration 79, loss = 0.59987310\n",
      "Iteration 80, loss = 0.60056390\n",
      "Iteration 81, loss = 0.59231616\n",
      "Iteration 82, loss = 0.56492315\n",
      "Iteration 83, loss = 0.55559136\n",
      "Iteration 84, loss = 0.53254779\n",
      "Iteration 85, loss = 0.52859110\n",
      "Iteration 86, loss = 0.53427736\n",
      "Iteration 87, loss = 0.52684058\n",
      "Iteration 88, loss = 0.51651399\n",
      "Iteration 89, loss = 0.50839473\n",
      "Iteration 90, loss = 0.50829431\n",
      "Iteration 91, loss = 0.51031494\n",
      "Iteration 92, loss = 0.48188168\n",
      "Iteration 93, loss = 0.47876514\n",
      "Iteration 94, loss = 0.49414102\n",
      "Iteration 95, loss = 0.51369878\n",
      "Iteration 96, loss = 0.52631497\n",
      "Iteration 97, loss = 0.49194352\n",
      "Iteration 98, loss = 0.49705494\n",
      "Iteration 99, loss = 0.47671446\n",
      "Iteration 100, loss = 0.47025900\n",
      "Iteration 1, loss = 1.33981385\n",
      "Iteration 2, loss = 1.16973537\n",
      "Iteration 3, loss = 1.10366853\n",
      "Iteration 4, loss = 1.09103839\n",
      "Iteration 5, loss = 1.08506043\n",
      "Iteration 6, loss = 1.06270008\n",
      "Iteration 7, loss = 1.03279019\n",
      "Iteration 8, loss = 1.03063177\n",
      "Iteration 9, loss = 1.02303034\n",
      "Iteration 10, loss = 1.00888510\n",
      "Iteration 11, loss = 1.02140055\n",
      "Iteration 12, loss = 1.01121471\n",
      "Iteration 13, loss = 0.99522560\n",
      "Iteration 14, loss = 0.98912984\n",
      "Iteration 15, loss = 0.97432913\n",
      "Iteration 16, loss = 0.95897780\n",
      "Iteration 17, loss = 0.95917625\n",
      "Iteration 18, loss = 0.94341341\n",
      "Iteration 19, loss = 0.94084224\n",
      "Iteration 20, loss = 0.93736887\n",
      "Iteration 21, loss = 0.92729014\n",
      "Iteration 22, loss = 0.91867285\n",
      "Iteration 23, loss = 0.91828627\n",
      "Iteration 24, loss = 0.91081631\n",
      "Iteration 25, loss = 0.89883656\n",
      "Iteration 26, loss = 0.89943490\n",
      "Iteration 27, loss = 0.91483551\n",
      "Iteration 28, loss = 0.90712159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.89080784\n",
      "Iteration 30, loss = 0.88044347\n",
      "Iteration 31, loss = 0.88334930\n",
      "Iteration 32, loss = 0.86503399\n",
      "Iteration 33, loss = 0.84910335\n",
      "Iteration 34, loss = 0.84706389\n",
      "Iteration 35, loss = 0.86188437\n",
      "Iteration 36, loss = 0.84162470\n",
      "Iteration 37, loss = 0.83779093\n",
      "Iteration 38, loss = 0.83503584\n",
      "Iteration 39, loss = 0.85258368\n",
      "Iteration 40, loss = 0.83045030\n",
      "Iteration 41, loss = 0.81538986\n",
      "Iteration 42, loss = 0.80045522\n",
      "Iteration 43, loss = 0.79300781\n",
      "Iteration 44, loss = 0.79058741\n",
      "Iteration 45, loss = 0.81456121\n",
      "Iteration 46, loss = 0.79771777\n",
      "Iteration 47, loss = 0.78624698\n",
      "Iteration 48, loss = 0.76634480\n",
      "Iteration 49, loss = 0.75995550\n",
      "Iteration 50, loss = 0.74980429\n",
      "Iteration 51, loss = 0.74592164\n",
      "Iteration 52, loss = 0.75118051\n",
      "Iteration 53, loss = 0.73889911\n",
      "Iteration 54, loss = 0.73384291\n",
      "Iteration 55, loss = 0.71283187\n",
      "Iteration 56, loss = 0.73920544\n",
      "Iteration 57, loss = 0.70358036\n",
      "Iteration 58, loss = 0.70732233\n",
      "Iteration 59, loss = 0.69989790\n",
      "Iteration 60, loss = 0.68125836\n",
      "Iteration 61, loss = 0.67939129\n",
      "Iteration 62, loss = 0.67026061\n",
      "Iteration 63, loss = 0.67397084\n",
      "Iteration 64, loss = 0.66831879\n",
      "Iteration 65, loss = 0.66304439\n",
      "Iteration 66, loss = 0.66033455\n",
      "Iteration 67, loss = 0.63661417\n",
      "Iteration 68, loss = 0.62214290\n",
      "Iteration 69, loss = 0.62457747\n",
      "Iteration 70, loss = 0.64588560\n",
      "Iteration 71, loss = 0.60903848\n",
      "Iteration 72, loss = 0.60179295\n",
      "Iteration 73, loss = 0.61687128\n",
      "Iteration 74, loss = 0.61492843\n",
      "Iteration 75, loss = 0.59266194\n",
      "Iteration 76, loss = 0.57834620\n",
      "Iteration 77, loss = 0.58589666\n",
      "Iteration 78, loss = 0.60022951\n",
      "Iteration 79, loss = 0.61832339\n",
      "Iteration 80, loss = 0.58525132\n",
      "Iteration 81, loss = 0.56951287\n",
      "Iteration 82, loss = 0.54267614\n",
      "Iteration 83, loss = 0.54434815\n",
      "Iteration 84, loss = 0.54315829\n",
      "Iteration 85, loss = 0.52638149\n",
      "Iteration 86, loss = 0.49855919\n",
      "Iteration 87, loss = 0.48670494\n",
      "Iteration 88, loss = 0.48771298\n",
      "Iteration 89, loss = 0.47962611\n",
      "Iteration 90, loss = 0.47450852\n",
      "Iteration 91, loss = 0.46279042\n",
      "Iteration 92, loss = 0.46568172\n",
      "Iteration 93, loss = 0.46670503\n",
      "Iteration 94, loss = 0.47008087\n",
      "Iteration 95, loss = 0.45588325\n",
      "Iteration 96, loss = 0.43760643\n",
      "Iteration 97, loss = 0.42668482\n",
      "Iteration 98, loss = 0.46280245\n",
      "Iteration 99, loss = 0.43781092\n",
      "Iteration 100, loss = 0.41964340\n",
      "Iteration 1, loss = 1.32586473\n",
      "Iteration 2, loss = 1.17676993\n",
      "Iteration 3, loss = 1.10839441\n",
      "Iteration 4, loss = 1.09274875\n",
      "Iteration 5, loss = 1.08075093\n",
      "Iteration 6, loss = 1.04700828\n",
      "Iteration 7, loss = 1.04275733\n",
      "Iteration 8, loss = 1.02190680\n",
      "Iteration 9, loss = 1.02762520\n",
      "Iteration 10, loss = 1.01472314\n",
      "Iteration 11, loss = 1.01611617\n",
      "Iteration 12, loss = 0.99702142\n",
      "Iteration 13, loss = 0.98941889\n",
      "Iteration 14, loss = 0.98325351\n",
      "Iteration 15, loss = 0.99294052\n",
      "Iteration 16, loss = 0.97837190\n",
      "Iteration 17, loss = 0.96473737\n",
      "Iteration 18, loss = 0.97349943\n",
      "Iteration 19, loss = 0.97567202\n",
      "Iteration 20, loss = 0.96407143\n",
      "Iteration 21, loss = 0.95059824\n",
      "Iteration 22, loss = 0.93573997\n",
      "Iteration 23, loss = 0.93335546\n",
      "Iteration 24, loss = 0.93616354\n",
      "Iteration 25, loss = 0.91361784\n",
      "Iteration 26, loss = 0.92347473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.90856003\n",
      "Iteration 28, loss = 0.89414513\n",
      "Iteration 29, loss = 0.88713147\n",
      "Iteration 30, loss = 0.88127193\n",
      "Iteration 31, loss = 0.88365244\n",
      "Iteration 32, loss = 0.87762335\n",
      "Iteration 33, loss = 0.86771579\n",
      "Iteration 34, loss = 0.85388713\n",
      "Iteration 35, loss = 0.84390046\n",
      "Iteration 36, loss = 0.84025354\n",
      "Iteration 37, loss = 0.83653254\n",
      "Iteration 38, loss = 0.83361469\n",
      "Iteration 39, loss = 0.83891694\n",
      "Iteration 40, loss = 0.82844866\n",
      "Iteration 41, loss = 0.82444645\n",
      "Iteration 42, loss = 0.82227450\n",
      "Iteration 43, loss = 0.82037173\n",
      "Iteration 44, loss = 0.80040917\n",
      "Iteration 45, loss = 0.82336322\n",
      "Iteration 46, loss = 0.78978312\n",
      "Iteration 47, loss = 0.77024692\n",
      "Iteration 48, loss = 0.78515352\n",
      "Iteration 49, loss = 0.78667960\n",
      "Iteration 50, loss = 0.78585012\n",
      "Iteration 51, loss = 0.79495338\n",
      "Iteration 52, loss = 0.78298411\n",
      "Iteration 53, loss = 0.76134615\n",
      "Iteration 54, loss = 0.75227524\n",
      "Iteration 55, loss = 0.74734549\n",
      "Iteration 56, loss = 0.76332620\n",
      "Iteration 57, loss = 0.72848708\n",
      "Iteration 58, loss = 0.73701459\n",
      "Iteration 59, loss = 0.73804353\n",
      "Iteration 60, loss = 0.73853203\n",
      "Iteration 61, loss = 0.74764221\n",
      "Iteration 62, loss = 0.75446321\n",
      "Iteration 63, loss = 0.72299384\n",
      "Iteration 64, loss = 0.71904352\n",
      "Iteration 65, loss = 0.69870535\n",
      "Iteration 66, loss = 0.68113381\n",
      "Iteration 67, loss = 0.66847532\n",
      "Iteration 68, loss = 0.66341422\n",
      "Iteration 69, loss = 0.64731800\n",
      "Iteration 70, loss = 0.63622058\n",
      "Iteration 71, loss = 0.64436487\n",
      "Iteration 72, loss = 0.66066551\n",
      "Iteration 73, loss = 0.63726014\n",
      "Iteration 74, loss = 0.63036940\n",
      "Iteration 75, loss = 0.62492412\n",
      "Iteration 76, loss = 0.61495573\n",
      "Iteration 77, loss = 0.61413994\n",
      "Iteration 78, loss = 0.61433215\n",
      "Iteration 79, loss = 0.62866098\n",
      "Iteration 80, loss = 0.61770613\n",
      "Iteration 81, loss = 0.62255332\n",
      "Iteration 82, loss = 0.59984818\n",
      "Iteration 83, loss = 0.58074684\n",
      "Iteration 84, loss = 0.57691283\n",
      "Iteration 85, loss = 0.56647578\n",
      "Iteration 86, loss = 0.54198128\n",
      "Iteration 87, loss = 0.54509028\n",
      "Iteration 88, loss = 0.52183704\n",
      "Iteration 89, loss = 0.52391670\n",
      "Iteration 90, loss = 0.51894860\n",
      "Iteration 91, loss = 0.55679833\n",
      "Iteration 92, loss = 0.57755066\n",
      "Iteration 93, loss = 0.59567672\n",
      "Iteration 94, loss = 0.56779019\n",
      "Iteration 95, loss = 0.53362519\n",
      "Iteration 96, loss = 0.51626115\n",
      "Iteration 97, loss = 0.50869339\n",
      "Iteration 98, loss = 0.48999608\n",
      "Iteration 99, loss = 0.48584476\n",
      "Iteration 100, loss = 0.49349752\n",
      "Iteration 1, loss = 1.30075860\n",
      "Iteration 2, loss = 1.14292911\n",
      "Iteration 3, loss = 1.07545634\n",
      "Iteration 4, loss = 1.07482547\n",
      "Iteration 5, loss = 1.01718072\n",
      "Iteration 6, loss = 1.01977263\n",
      "Iteration 7, loss = 0.99641964\n",
      "Iteration 8, loss = 0.98503343\n",
      "Iteration 9, loss = 0.97233314\n",
      "Iteration 10, loss = 0.97791168\n",
      "Iteration 11, loss = 0.95510475\n",
      "Iteration 12, loss = 0.97665046\n",
      "Iteration 13, loss = 0.96602819\n",
      "Iteration 14, loss = 0.98244258\n",
      "Iteration 15, loss = 0.96010947\n",
      "Iteration 16, loss = 0.94521612\n",
      "Iteration 17, loss = 0.93845485\n",
      "Iteration 18, loss = 0.93273210\n",
      "Iteration 19, loss = 0.92217741\n",
      "Iteration 20, loss = 0.92044516\n",
      "Iteration 21, loss = 0.90025674\n",
      "Iteration 22, loss = 0.90583261\n",
      "Iteration 23, loss = 0.89358691\n",
      "Iteration 24, loss = 0.88576901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.88587691\n",
      "Iteration 26, loss = 0.88138251\n",
      "Iteration 27, loss = 0.86524009\n",
      "Iteration 28, loss = 0.86435946\n",
      "Iteration 29, loss = 0.86322686\n",
      "Iteration 30, loss = 0.85645697\n",
      "Iteration 31, loss = 0.84488672\n",
      "Iteration 32, loss = 0.84831978\n",
      "Iteration 33, loss = 0.82428081\n",
      "Iteration 34, loss = 0.83965617\n",
      "Iteration 35, loss = 0.84758910\n",
      "Iteration 36, loss = 0.83193304\n",
      "Iteration 37, loss = 0.82632255\n",
      "Iteration 38, loss = 0.83895916\n",
      "Iteration 39, loss = 0.82724444\n",
      "Iteration 40, loss = 0.79683158\n",
      "Iteration 41, loss = 0.77918679\n",
      "Iteration 42, loss = 0.80617070\n",
      "Iteration 43, loss = 0.80333486\n",
      "Iteration 44, loss = 0.80130057\n",
      "Iteration 45, loss = 0.76603193\n",
      "Iteration 46, loss = 0.78818453\n",
      "Iteration 47, loss = 0.75464936\n",
      "Iteration 48, loss = 0.74572460\n",
      "Iteration 49, loss = 0.73576907\n",
      "Iteration 50, loss = 0.72412934\n",
      "Iteration 51, loss = 0.71674541\n",
      "Iteration 52, loss = 0.71487746\n",
      "Iteration 53, loss = 0.73116212\n",
      "Iteration 54, loss = 0.71658035\n",
      "Iteration 55, loss = 0.70031592\n",
      "Iteration 56, loss = 0.74013830\n",
      "Iteration 57, loss = 0.73530086\n",
      "Iteration 58, loss = 0.70153690\n",
      "Iteration 59, loss = 0.68807957\n",
      "Iteration 60, loss = 0.69740931\n",
      "Iteration 61, loss = 0.67872027\n",
      "Iteration 62, loss = 0.65691430\n",
      "Iteration 63, loss = 0.64686790\n",
      "Iteration 64, loss = 0.63880636\n",
      "Iteration 65, loss = 0.61319366\n",
      "Iteration 66, loss = 0.60878481\n",
      "Iteration 67, loss = 0.63969183\n",
      "Iteration 68, loss = 0.63447077\n",
      "Iteration 69, loss = 0.64805721\n",
      "Iteration 70, loss = 0.62040423\n",
      "Iteration 71, loss = 0.60373191\n",
      "Iteration 72, loss = 0.60000526\n",
      "Iteration 73, loss = 0.60522892\n",
      "Iteration 74, loss = 0.60604828\n",
      "Iteration 75, loss = 0.59107736\n",
      "Iteration 76, loss = 0.56927624\n",
      "Iteration 77, loss = 0.55706641\n",
      "Iteration 78, loss = 0.55220359\n",
      "Iteration 79, loss = 0.55943682\n",
      "Iteration 80, loss = 0.55535901\n",
      "Iteration 81, loss = 0.53969309\n",
      "Iteration 82, loss = 0.51507800\n",
      "Iteration 83, loss = 0.52372909\n",
      "Iteration 84, loss = 0.53080393\n",
      "Iteration 85, loss = 0.53162452\n",
      "Iteration 86, loss = 0.53512941\n",
      "Iteration 87, loss = 0.54248343\n",
      "Iteration 88, loss = 0.52759721\n",
      "Iteration 89, loss = 0.54212049\n",
      "Iteration 90, loss = 0.52408106\n",
      "Iteration 91, loss = 0.53277231\n",
      "Iteration 92, loss = 0.54190029\n",
      "Iteration 93, loss = 0.50768497\n",
      "Iteration 94, loss = 0.47565313\n",
      "Iteration 95, loss = 0.46103256\n",
      "Iteration 96, loss = 0.43833374\n",
      "Iteration 97, loss = 0.46135650\n",
      "Iteration 98, loss = 0.45792691\n",
      "Iteration 99, loss = 0.44461847\n",
      "Iteration 100, loss = 0.44142190\n",
      "Iteration 1, loss = 15.70110898\n",
      "Iteration 2, loss = 9.93025163\n",
      "Iteration 3, loss = 3.45819517\n",
      "Iteration 4, loss = 1.70934374\n",
      "Iteration 5, loss = 1.29376708\n",
      "Iteration 6, loss = 1.22992010\n",
      "Iteration 7, loss = 1.20670795\n",
      "Iteration 8, loss = 1.19685889\n",
      "Iteration 9, loss = 1.19122677\n",
      "Iteration 10, loss = 1.18713370\n",
      "Iteration 11, loss = 1.17106754\n",
      "Iteration 12, loss = 1.16556407\n",
      "Iteration 13, loss = 1.15353141\n",
      "Iteration 14, loss = 1.14533199\n",
      "Iteration 15, loss = 1.13105171\n",
      "Iteration 16, loss = 1.12977061\n",
      "Iteration 17, loss = 1.13205258\n",
      "Iteration 18, loss = 1.12173778\n",
      "Iteration 19, loss = 1.11541527\n",
      "Iteration 20, loss = 1.09972147\n",
      "Iteration 21, loss = 1.12025705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 1.11842163\n",
      "Iteration 23, loss = 1.11244671\n",
      "Iteration 24, loss = 1.09756019\n",
      "Iteration 25, loss = 1.10243052\n",
      "Iteration 26, loss = 1.08793726\n",
      "Iteration 27, loss = 1.08662747\n",
      "Iteration 28, loss = 1.06973434\n",
      "Iteration 29, loss = 1.06598883\n",
      "Iteration 30, loss = 1.08957353\n",
      "Iteration 31, loss = 1.08227622\n",
      "Iteration 32, loss = 1.10044220\n",
      "Iteration 33, loss = 1.10520473\n",
      "Iteration 34, loss = 1.08978571\n",
      "Iteration 35, loss = 1.08300829\n",
      "Iteration 36, loss = 1.06691289\n",
      "Iteration 37, loss = 1.04660347\n",
      "Iteration 38, loss = 1.06394752\n",
      "Iteration 39, loss = 1.04255856\n",
      "Iteration 40, loss = 1.04354847\n",
      "Iteration 41, loss = 1.06096648\n",
      "Iteration 42, loss = 1.03491849\n",
      "Iteration 43, loss = 1.04828093\n",
      "Iteration 44, loss = 1.02985566\n",
      "Iteration 45, loss = 1.03863004\n",
      "Iteration 46, loss = 1.05928301\n",
      "Iteration 47, loss = 1.04291721\n",
      "Iteration 48, loss = 1.06689019\n",
      "Iteration 49, loss = 1.05660406\n",
      "Iteration 50, loss = 1.03498456\n",
      "Iteration 51, loss = 1.02777174\n",
      "Iteration 52, loss = 1.02611854\n",
      "Iteration 53, loss = 1.04226771\n",
      "Iteration 54, loss = 1.02465024\n",
      "Iteration 55, loss = 1.01962476\n",
      "Iteration 56, loss = 1.01934669\n",
      "Iteration 57, loss = 1.00453733\n",
      "Iteration 58, loss = 1.01057176\n",
      "Iteration 59, loss = 1.00946925\n",
      "Iteration 60, loss = 0.99934826\n",
      "Iteration 61, loss = 1.00375340\n",
      "Iteration 62, loss = 1.00360481\n",
      "Iteration 63, loss = 0.99604759\n",
      "Iteration 64, loss = 0.98882896\n",
      "Iteration 65, loss = 0.99503077\n",
      "Iteration 66, loss = 0.98976230\n",
      "Iteration 67, loss = 0.98716447\n",
      "Iteration 68, loss = 1.01705644\n",
      "Iteration 69, loss = 1.02088526\n",
      "Iteration 70, loss = 1.03409311\n",
      "Iteration 71, loss = 1.06796078\n",
      "Iteration 72, loss = 1.02879392\n",
      "Iteration 73, loss = 1.02718226\n",
      "Iteration 74, loss = 1.01598895\n",
      "Iteration 75, loss = 0.99076005\n",
      "Iteration 76, loss = 0.99603541\n",
      "Iteration 77, loss = 1.00872870\n",
      "Iteration 78, loss = 0.97716846\n",
      "Iteration 79, loss = 0.98960591\n",
      "Iteration 80, loss = 0.98567941\n",
      "Iteration 81, loss = 0.97841114\n",
      "Iteration 82, loss = 0.96165377\n",
      "Iteration 83, loss = 0.97250401\n",
      "Iteration 84, loss = 0.95758947\n",
      "Iteration 85, loss = 0.95947868\n",
      "Iteration 86, loss = 0.95255161\n",
      "Iteration 87, loss = 0.96660083\n",
      "Iteration 88, loss = 0.95697254\n",
      "Iteration 89, loss = 0.95770723\n",
      "Iteration 90, loss = 0.95549249\n",
      "Iteration 91, loss = 0.96139084\n",
      "Iteration 92, loss = 0.95413209\n",
      "Iteration 93, loss = 0.96279758\n",
      "Iteration 94, loss = 0.97190478\n",
      "Iteration 95, loss = 0.95357054\n",
      "Iteration 96, loss = 0.95625571\n",
      "Iteration 97, loss = 0.95242067\n",
      "Iteration 98, loss = 0.98432572\n",
      "Iteration 99, loss = 0.94748824\n",
      "Iteration 100, loss = 0.94539695\n",
      "Two-layer model (128, 512): CV accuracy = 0.447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#looping over different type of activator with num_mneurons = 128\n",
    "\n",
    "num_neurons = 128 \n",
    "activator = 'relu'\n",
    "\n",
    "second_layer_neurons = [2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "\n",
    "#Variables for 1st layer\n",
    "cv_mean = []\n",
    "cv_std  = []\n",
    "test_scores = []\n",
    "\n",
    "#variables to comapare results after adding 2nd layer\n",
    "cv_mean_2nd = []\n",
    "cv_std_2nd = []\n",
    "test_scores_2nd = []\n",
    "\n",
    "#MLPClassifier for 1 layer\n",
    "mlp1 = MLPClassifier(hidden_layer_sizes=(num_neurons), # hidden layer with num_neurons number of neurons\n",
    "                        activation = activator,  # taking different activator types as input\n",
    "                        # solver='sgd',  # default is Adam\n",
    "                        alpha=0.01,  # regularization parameter, default is 0.0001 (increase up to 1.0 for stronger regularization)\n",
    "                        learning_rate_init=.01 ,  # initial step-size for updating the weights, default is 0.001\n",
    "                        max_iter=100,  # number of epochs, default=200\n",
    "                        random_state=seed,\n",
    "                        verbose=1, \n",
    "                        )\n",
    "scores = cross_val_score(mlp1, X_train_scaled, y_train, cv=5, scoring=scoring_method)\n",
    "cv_mean.append(scores.mean())\n",
    "cv_std.append(scores.std())\n",
    "\n",
    "#train the classifier and tesing it\n",
    "mlp1.fit(X_train, y_train)\n",
    "test_acc = accuracy_score(y_test, mlp1.predict(X_test_scaled))\n",
    "test_scores.append(test_acc)\n",
    "\n",
    "#Printing results for 1 layer\n",
    "print(f\"Single-layer model (neurons={num_neurons}): CV accuracy = {test_acc:.4f}\")\n",
    "\n",
    "for num_layers in second_layer_neurons:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(num_neurons, num_layers), # hidden layer with num_neurons number of neurons\n",
    "                        activation = activator,  # taking different activator types as input\n",
    "                        # solver='sgd',  # default is Adam\n",
    "                        alpha=0.01,  # regularization parameter, default is 0.0001 (increase up to 1.0 for stronger regularization)\n",
    "                        learning_rate_init=.01 ,  # initial step-size for updating the weights, default is 0.001\n",
    "                        max_iter=100,  # number of epochs, default=200\n",
    "                        random_state=seed,\n",
    "                        verbose=1, \n",
    "                        )\n",
    "    scores = cross_val_score(mlp, X_train_scaled, y_train, cv=5, scoring=scoring_method)\n",
    "    cv_mean_2nd.append(scores.mean())\n",
    "    cv_std_2nd.append(scores.std())\n",
    "    \n",
    "    # in case the training won't converge we catch the warning and ignore it\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "        \n",
    "        #train the classifier and tesing it\n",
    "        mlp.fit(X_train, y_train)\n",
    "    test_acc = accuracy_score(y_test, mlp.predict(X_test_scaled))\n",
    "    test_scores_2nd.append(test_acc)\n",
    "    print(f\"Two-layer model ({num_neurons}, {num_layers}): CV accuracy = {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4eec9104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-layer model (neurons=128): CV accuracy = 0.447\n",
      "Two-layer model (128, 2): CV accuracy = 0.541\n",
      "Two-layer model (128, 4): CV accuracy = 0.537\n",
      "Two-layer model (128, 8): CV accuracy = 0.565\n",
      "Two-layer model (128, 16): CV accuracy = 0.544\n",
      "Two-layer model (128, 32): CV accuracy = 0.553\n",
      "Two-layer model (128, 64): CV accuracy = 0.552\n",
      "Two-layer model (128, 128): CV accuracy = 0.536\n",
      "Two-layer model (128, 256): CV accuracy = 0.522\n",
      "Two-layer model (128, 512): CV accuracy = 0.542\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAHACAYAAABAsrtkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB4SklEQVR4nO3dB1iVZRsH8L9sGSKKAi5QURC3uLXcs9SyzManpqmZmaVmaeVsWKlpppkNM7NSG2Zmabn33igoDkAc4GDLPt91P3hOoIAc1ln/33UdOe97Xg4P5wW5z/3ez/2U0Wg0GhARERERmSkrQw+AiIiIiKgkMeAlIiIiIrPGgJeIiIiIzBoDXiIiIiIyawx4iYiIiMisMeAlIiIiIrPGgJeIiIiIzBoDXiIiIiIyazaGHoAxyszMxJUrV+Di4oIyZcoYejhEREREdA9ZOy0+Ph5VqlSBlVX+OVwGvLmQYLd69eqGHgYRERERPUBERASqVauW7zEMeHMhmV3tC1iuXDlDD4eIiIiI7hEXF6cSlNq4LT8MeHOhLWOQYJcBLxEREZHxKkj5KSetEREREZFZY8BLRERERGaNAS8RERERmTXW8BIREVGxt4tKT09HRkaGoYdCJsza2ho2NjbF0iKWAS8REREVm9TUVFy9ehVJSUmGHgqZAUdHR3h5ecHOzq5Iz8OAl4iIiIpt4aaLFy+qzJwsBiBBChdwosJeJZA3T9HR0epnqk6dOg9cXCI/DHiJiIioWEiAIkGv9EaVzBxRUZQtWxa2trYICwtTP1sODg6Ffi5OWiMiIqJiVZRMHFFJ/CzxJ5KIiIiIzBoDXiOWkanB3vM3sfZYpPoo20RERGRepk+fjiZNmhh6GGaNAa+R2nDqKtp/tAXPfLUPr648pj7KtuwnIiIyd6WZ9JGJdfndJCC1ZGXuvg779u3LsT8lJQUVK1ZUj23bti3H8b///nuuzyXHZX9tPTw88MQTT+DChQsl+j1w0poRkqD2pRVHcO+v9rXYZLV/8f+aoWcDLwONjoiIqOT/Ds5YdxpXY5N1+7xcHTCtT0CJ/P2TNmpaq1atwtSpUxESEqLb5+zsDEvoipCRkaH63uZGJiJ+++23aN26tW7fmjVr1Gtz69Ytvb+evL4uLi44d+4cRo4ciT59+uDEiROqw0dJYIbXyMg7WPklz+19rHafPM7yBiIiMuekT/ZgN3vSpySudHp6eupurq6uKvMo96VLQNWqVREcHKyOkw4UFSpUyBH0rVixQgWDWidPnkTnzp3V50r2U4K5hIQEvcZz8OBBdOvWDe7u7mo8HTp0wJEjR3SPDxs2DI8++miOz0lLS0PlypXxzTff6MY6a9Ys1KxZU42lcePG+OWXX+7LtP79998IDAyEvb09du3aleeYhgwZgpUrV+LOnTu6fUuXLlX7C0PGKv11H374YfUG4/Tp0wgNDUVJYcBrZA5cvHXfL3l2EubK43IcERGRKWQOk1LTC3SLT07DtD+C8k36TP/jtDquIM8nX7soJNiU2lrt5XoJZiVIPHr0qC6I3b59uwpIRWJiInr06AE3NzcVtP7888/YtGkTxowZo9fXjY+PV4GkBKBSRiA9aHv37q32i+HDh2PDhg05MtN//vmnWuxj4MCBaluC3eXLl+OLL75AUFAQxo0bh//9739qvNlNmjQJH374Ic6cOYNGjRrlOSYJin18fPDrr7+q7fDwcOzYsQODBg1CUUlALqT1WElhSYORiYpPLtbjiIiIDOlOWgYCpm4slueS8PVaXDIaTv+nQMefntkDjnZFC3U6duyoAt7XX39dfZTMq2R8JRjt2bOn2vfGG2+oY3/88UckJyerQNPJyUntW7hwobpc/9FHH6l61YKQDHF2X375JcqXL6+CVcnstm3bFn5+fvj+++91X1vKDQYMGKBKDKS29oMPPlDBdps2bdTjtWrVUmNesmSJLkAXM2fOVN9TQUhmWbK6EjgvW7ZMBeGVKlVCUUjQPmfOHJVJl++ppDDDa2QquzgU63FERERUeBIcSqAo9a0ScEoArA2Cr1y5oi7Dy7aQLKmUDmiDXdGuXTtVXqCtCZaAVHsbNWpUrl/z+vXrGDFihMrsSpa5XLlyKqMsWVUtyfJKkKs9XkoTJCAVMibJ9kogm/3rLV++HOfPn8/xtZo3b17g10IC3b1796oJZhLwar9eYVSrVk29TrIin2TGJXNc1OWD88MMr5FpWbOCKsyXWqXcLsTIAo2erg7qOCIiImNX1tZaZVoLQsr1nv/24AOPWza0RYH+DsrXLiqpMZVSAqmhlUv4kjmV+l4pA5DgVgI2CUwL6tixY7r7EsjmRsoZbt68iU8//RTe3t6qvlYytdkv+Q8ePFiVI0gAumfPHlWr+9BDD6nHtOUW69evV5nT7OS5sssenD+I1CRLhvmFF15QmexevXrpyiz0tXPnTvX9Sy2vTF4raQx4jYy1VRk1C1UK8++lXY1cHpfjiIiIjJ3UvBa0rOChOpUKlPSR40rr76CUEkhtq5QmyDK3/v7+KkiTWlmpm81eHlCvXj2V+ZSMpTaQ3L17t1otTHu53tfX94FfUz7n888/VyUDIiIiAjdu3Lgv+HzsscdUlleC3qFDh+oeCwgIUIGtZISzj684SFZXxvXmm28WqaOCBOjy2pYWBrxGSFquSOuxyb+dxO2kNN1+zxJsyUJERGRMSR8JZzVGkvSRkoXPPvsMTz75pNqWTg0S3EoLs0WLFumOe+655zBt2jSVoZXevdHR0XjllVfUxK6C1u8KyRhLfa6UG8TFxWHixIm6iV3ZSVmDZFyl3CJ7twTJmErNsUxUk3KK9u3bIzY2VgXSklUtbGcFIXXL8n3llZ3WunjxYo5stvb7MhTW8BopCWpnPd5Qtz1nQCPserMzg10iIrKIpI8kebKTbUP1oZcsqQSV2lpdIffv3efo6IiNGzeqvrQtWrRQAXKXLl1Udlgf0lrs9u3baNasmQqWx44dq7LK9+ratatq7SWdIaS0Irt3330XU6ZMUd0a6tWrpwJVKXGQzGpRM/bSLu1B9bbjx49H06ZNc9yku4WhlNEUtWeHGZJ3U1IkLu+GHvQOpiT9dfIqRv+QVdqwZnRbNK3hZrCxEBERPYjUdUpmT4IqB4eiTa6WfvNS0ytdiWSittTsspwvJ6nVlRpdKWvo378/LO1nKk6PeI0lDUYsLSNTdz8xJcOgYyEiIipNEty2qV3R0MMwSlKmIDW9c+fOVXWwffv2NfSQjB4DXiOWnvFf8j0h5b9aXiIiIrJcMhlNMp7S2ksmyeW1HDD9h6+QEUvP/C/DG5+cbtCxEBERkXGQFc9YkaofTlozYmnZMryJKQx4iYiIiAqDAa8RS89Ww5vAgJeIiIioUBjwGrH0zOw1vJy0RkREpoGX28nYfpYY8JpISQMnrRERkbGTlchEUlKSoYdCZiLp7s+S9mersDhpzVRKGjhpjYiIjJwsNSttsqKionQLMchCBUSFyexKsCs/S/IzVZRljAUDXiOWxpIGIiIyMZ6enuqjNuglKgoJdrU/U0XBgNdkJq2xpIGIiIyfZHRluVtZCjctjX+7qPCkjKGomV0tBrwmMmmNK60REZEpkUCluIIVoqLipDUTWVqYbcmIiIiICocBr8ksLcyAl4iIiKgwGPCayNLC7NJAREREVDgMeE2kD++dtIwck9iIiIiIqGAY8BqxewPcxFROXCMiIiLSFwNeE+nDK1jHS0RERKQ/BrymlOFlwEtERESkNwa8JtKlQcRz4hoRERGR3hjwGjGWNBAREREVHQNeI8aSBiIiIqKiY8BrQiUN7MVLREREpD8GvEYs9W6Gt6xt1lrkLGkgIiIi0h8DXhNYac3N0VZ9ZMBLREREpD8GvCZQ0uDqaKc+MuAlIiIi0h8DXiOWdrekgRleIiIiIhMOeBctWgQfHx84ODigVatWOHDgQJ7HLlu2DGXKlMlxk8+715kzZ9C3b1+4urrCyckJLVq0QHh4OExN+t22ZOW1AS8nrRERERGZVsC7atUqjB8/HtOmTcORI0fQuHFj9OjRA1FRUXl+Trly5XD16lXdLSwsLMfj58+fR/v27eHv749t27bhxIkTmDJlSq6BsamUNJS/W9LAtmRERERE+rOBAX3yyScYMWIEhg4dqra/+OILrF+/HkuXLsWkSZNy/RzJ6np6eub5nG+//TZ69+6Njz/+WLevdu3aMIeShngGvERERESmk+FNTU3F4cOH0bVr1/8GY2Wltvfu3Zvn5yUkJMDb2xvVq1dHv379EBQUpHssMzNTBcx169ZVmeLKlSurMonff/8dpkhX0lD27qQ1ljQQERERmU7Ae+PGDWRkZMDDwyPHftm+du1arp/j5+ensr9r167FihUrVIDbtm1bXL58WT0upRASEH/44Yfo2bMn/vnnHzz++OPo378/tm/fnudYUlJSEBcXl+NmTBlebQ1vYioDXiIiIiKTKmnQV5s2bdRNS4LdevXqYcmSJXj33XdVACwk8ztu3Dh1v0mTJtizZ48ql+jQoUOuzztr1izMmDEDxl7DywwvERERkQlleN3d3WFtbY3r16/n2C/b+dXoZmdra4umTZsiNDRU95w2NjYICAjIcZwExfl1aZg8eTJiY2N1t4iICBjTwhMVnNiWjIiIiMjkAl47OzsEBgZi8+bNun2SoZXt7Fnc/EhJxMmTJ+Hl5aV7TmlBFhISkuO4s2fPqrrfvNjb26vuD9lvhqbRaJB2T4Y3JT0TqelZQTARERERmUBJg7QkGzJkCJo3b46WLVti/vz5SExM1HVtGDx4MKpWrapKDsTMmTPRunVr+Pr6IiYmBrNnz1ZtyYYPH657zokTJ2LgwIF4+OGH0alTJ2zYsAHr1q1TLcpMScbdCWvCtWxWhlfbmszOJisAJiIiIiIjD3glMI2OjsbUqVPVRDWpt5UAVTuRTcoQpHOD1u3bt1UbMznWzc1NZYilPjd7CYNMUpN6XQmSx44dqya6/frrr6o3ryl2aBAOttZwsLVCclqmKmtwc2LAS0RERFRQZTRy7ZxykC4Nskqb1PMaqrwhPjkNDaf/o+4Hv9sT7T/aihsJKfj71YdQz8vwJRdEREREphKvGXxpYcq/Q4OwtbaCi0NWMp6rrRERERHphwGvkUq726GhTBnA2qoMnOyt1TZXWyMiIiLSDwNeI8/w2t6tYXa2Z4aXiIiIqDAY8Bp5wGtjXSZHwMvFJ4iIiIj0w4DXyEsabKzuCXiZ4SUiIiLSCwNeYy9psM46RU4MeImIiIgKhQGvkUrLyMxZ0nC3SwNLGoiIiIj0w4DXyBeesLk7ac1FO2ktlQEvERERkT4Y8Bqp9LsZXtu7GV5tSUM8M7xEREREemHAa6TSdF0acrYlYw0vERERkX4Y8Bqp9Dy6NLAPLxEREZF+GPCaSJcG7aQ1ljQQERER6YcBr6l0aeCkNSIiIqJCYcBr5F0a7l1amG3JiIiIiPTDgNfU+vCyhpeIiIhILwx4TaRLg7YtmexPSc8w6NiIiIiITAkDXmPvw3u3S4OTXVbAK1jWQERERFRwDHiNVJp2pbW7JQ3WVmXgaGet7iemMMNLREREVFAMeI08w6stacg+cS0+Jc1g4yIiIiIyNQx4jb0P792ShhwT11jSQERERFRgDHiNVFpm3hle9uIlIiIiKjgGvCay0lqOkgZmeImIiIgKjAGvsXdpuDtpLXtrMk5aIyIiIio4BrzG3qXh7kprwkW72honrREREREVGANeE8rw/rfaGjO8RERERAXFgNfoV1q7v6SBXRqIiIiICo4Br5FK13ZpsLp/0hpLGoiIiIgKjgGv0XdpyFbSwElrRERERHpjwGv0JQ25rbTGkgYiIiKigmLAa/QlDbm1JWPAS0RERFRQDHhNaOEJFy4tTERERKQ3BrxGKu1uW7LsXRr+m7TGgJeIiIiooBjwGqn0uwtP2Gbr0qBrS8aAl4iIiKjAGPCaUIZXV9KQkg6NJisgJiIiIqL8MeA18hre7F0atBnejEwNUtKzAmIiIiIiyh8DXiPv0mCbrUuDo601ytzdjOfENSIiIqICYcBrQn14razKwNmOdbxERERE+mDAa+x9eLPV8Ar24iUiIiLSDwNeY+/Dm61Lg3C+O3GNJQ1EREREBcOA14S6NAhmeImIiIj0w4DX2Pvw3hPwurAXLxEREZFeGPAae1uye0sa7ga88Qx4iYiIiAqEAa+RSmVJAxEREZFhAt6tW7cWz1emfKXfDXhts7Uly7HaWjFOWpOFLPaev4m1xyLVR9kmIiIiMhdZ0ZMeevbsiWrVqmHo0KEYMmQIqlevXjIjs3D/lTTcm+G1LtYa3g2nrmLGutO4Gpus2+fl6oBpfQLQs4FXsXwNIiIiIpPK8EZGRmLMmDH45ZdfUKtWLfTo0QOrV69GampqyYzQQqVl5p7hdba3LbaAV4Ldl1YcyRHsimuxyWq/PE5ERERkcQGvu7s7xo0bh2PHjmH//v2oW7cuRo8ejSpVqmDs2LE4fvx4yYzUUjO899TwOmszvEUsaZCyBcns5la8oN0nj7O8gYiIiCx60lqzZs0wefJklfFNSEjA0qVLERgYiIceeghBQUHFN0oLo9FodG3J7uvScLeGNzG1aAHvgYu37svs5hgDoB7fEny9SF+HiIiIyCQD3rS0NFXS0Lt3b3h7e2Pjxo1YuHAhrl+/jtDQULVvwIABxT9aC6ENdoVdHiUNRV1pLSo+72A3uxHLD6P7vO14a81J/H40EpdvJxXp6xIREREZ/aS1V155BT/99JPKQg4aNAgff/wxGjRooHvcyckJc+bMUSUOVLRyhtzbklkXS1uyyi4OBT727PUEdftxf7jaruLqgOY+FdDCx0199PNwgdU9k+uIiIiITDbgPX36ND777DP0798f9vb2edb5sn1Z0Ses5RbwuhTTpLWWNSuobgwyQS23Kl35qp6uDvj95XY4Gh6DQ5du4WDYbZyKjMWV2GT8cfyKuqkxOdiguXdW8CvP27CqKxxsswJzIiIiIpMLeDdv3vzgJ7WxQYcOHQo7JouXPcNre08Nb3G1JbO2KqNaj0k3hntpQ2x53KOcA3o28FQ3kZSajmPhMTh46TYOhd3C4bDbqrxia0i0umnLMBpVc70bALshsEYFuDpmBeoFIRPlpMZYyi4kEy1BtIyXiIiIqFQC3lmzZsHDwwPDhg3LsV8mrEVHR+PNN98s1EDo/kUnJMa7t1RAO2lNAl4pKylTpvCBoPTZXfy/Zhjz49EcdcOe+fThdbSzQVtfd3XTjvXM1XgclAywut3GjYQUHAqTgPg2vtgOyBCl7KG5jxta+FRQgXDV8mVzHRP7AhMREZHBA94lS5bgxx9/vG9//fr18fTTTzPgLQZp2g4N90xYy17SoNFItjVDt9RwYbWp7a4Ldt99rD58K7nolVGVMTas5qpuw9rXVEF42M0kXQB86NJtXLiRiOBr8eq2Yl9WHbAEvBIAqyywTwXUqeyMf05fUxnne0sstH2BJThn0EtERET60jtaunbtGry87g86KlWqhKtXuVBBsS4rnEvQ6WBrpTK/EqPKxLWiBrxHwm6rjz4VHTGotQ+KSjLOPu5O6jagedYqfCrjezf7Kx9PXYlDZMwdRB67g7XH7tYB21sjNUOTZ19geSUk89stwJPlDURERKQXvaMlWUp49+7dqFmzZo79so+dGYpHWkbeGV4JKJ3tbRCXnI74lHRULuLXkiyskFKDkuLubK8ys9rsrATqxyJidBngI+G3EZ+Ske9zaPsCS21vm9oVS2ysREREZH70DnhHjBiB1157TfXi7dy5s24i2xtvvIEJEyaUxBgtTrpuWeHcM5nagLeorcmEBJwlHfDeS7LS7Xzd1U2kZWTii+3nMfefs8XWP5iIiIio0AHvxIkTcfPmTbWccGpqqtrn4OCgandl1TUqxmWF7+nQkGPiWmzRlxdOSc/Ascsx6r7U0xqKrbUVmntXKPb+wURERESFCnjlkvpHH32EKVOm4MyZMyhbtizq1KmTZ09e0p9kPHPrwaulrduVkoaiOHk5FqnpmXB3tkNNdycYUkH7AstxRERERCW+tLBwdnZGixYt1CprDHaLl7ZrgmQ+8yppEEUtaThwt35XsqtFaW9WHLR9gUVeI5HHOWGNiIiI9FWoKf6HDh3C6tWrER4eritr0Prtt98K85SUW4Y3j+BOVjYrjsUndPW7RpI11fYFvrcPr5jQvS5bkhEREVHpZHhXrlyJtm3bqnKGNWvWqMlrQUFB2LJlC1xdXQs3Csq9hjePDK+TXdED3sxMjWoRJloYsH73XhLU7nqzM34a0RqfPt0Enfwqqf07zt5QPX6JiIiISjzg/eCDDzBv3jysW7cOdnZ2+PTTTxEcHIynnnoKNWrU0HsAVIguDdoMbxEmrZ2NiledHhztrBHgVQ7GRMoWpPVYvyZV8UH/hrCzsVLlF9vOZi1dTERERFSiAe/58+fxyCOPqPsS8CYmJqr6z3HjxuHLL7/U9+kovz68eZQ0aGt4i5LhlUUgRLMabnlmko2Bl2tZDGnjre5/vCFEZaaJiIiI9KF3pOPm5ob4+Hh1v2rVqjh16pS6HxMTg6SkJH2fjgpR0lAsAe/FuxPWjKicIS+jO/rCxd4GZ67G4c+TXM2PiIiISjjgffjhh/Hvv/+q+wMGDMCrr76qFqN45pln0KVLF32fjgxU0vBf/a5xTFjLj5uTHUY+XEvdn/tPiG5SHxEREVGJdGlYuHAhkpOzZtC//fbbsLW1xZ49e/DEE0/gnXfe0ffpKN+Shge0JUstXMAbGXMHV2KTVa1s0xrlYQqGta+J7/ZeQtjNJKw6GIH/tc4qcyAiIiIq1oA3PT0df/75J3r06KG2raysMGnSJH2eggogPePBSwsXJcOrLWdoUKUcHO92fDB2stjGmE6+mL7uNBZsPocnmlVDWTtrQw+LiIiIzK2kwcbGBqNGjdJleKlkpGXmn+Et6kprB02onCG7Z1rVQNXyZREVn4Jley4ZejhERERkrjW8LVu2xLFjx0pmNJQjw5vX0sJFXWlNG/A2N7GA197GGuO71VX3F28LRWxSmqGHRERERCZA7+vZo0ePxvjx4xEREYHAwEA4OTnleLxRo0bFOT6LpJ2U9aClhQtT0hCTlIqz1xNMpkPDvR5rWhVLdpxX34N8fKOnv6GHREREROaW4X366adx8eJFjB07Fu3atUOTJk3QtGlT3cfCWLRoEXx8fODg4IBWrVrhwIEDeR67bNky1fc3+00+L7vnn3/+vmN69uwJs+nDe7dLQ2Jqht59aQ+HZfXfrVXJCe7O9jA1MtHu9e5+6v7S3RcRFcfyGiIiIirmDK8Eu8Vp1apVKmP8xRdfqGB3/vz5alJcSEgIKleunOvnlCtXTj2uJQHtvSTA/fbbb3Xb9vb2ZteHV9upwcXBtsDPLSuWiRbeplXOkF23AA/VXeJoeAwWbDmH9x5raOghERERkTkFvN7exdsO6pNPPlF9fIcOHaq2JfBdv349li5dmmcHCAlwPT09831eCXAfdIyp9uG1t7FS2d/0TI1afEKfgPfQ3RXWWtQ03YBXzv+bPf3x9Jf7sPJABEY8VAveFXOW1hAREREVOuBdvnx5vo8PHjy4wM+VmpqKw4cPY/Lkybp90uqsa9eu2Lt3b56fl5CQoALvzMxMNGvWDB988AHq16+f45ht27apDLGsDNe5c2e89957qFixYq7Pl5KSom5acXFxMOY+vBLwSVlDTFKaXhPXktMycOJyjLrfwgTrd7NrXasiHq5bCTvORuOTf8/i06cLV05DRERE5k/vgFdWVssuLS1NLSlsZ2cHR0dHvQLeGzduICMjAx4eHjn2y3ZwcHCun+Pn56eyvzI5LjY2FnPmzEHbtm0RFBSEatWq6coZ+vfvj5o1a+L8+fN466230KtXLxVEW1vf37t11qxZmDFjBoyuD69N7hle4WSXFfDG6zFx7XhEjAqmK7nYo0YFR5i6N3r4qYB37bErePHh2gioUs7QQyIiIiJzmLR2+/btHDfJtko9bfv27fHTTz+hpLVp00YF1TJJrkOHDvjtt99QqVIlLFmyJMfEur59+6Jhw4Z47LHH1GIZBw8eVFnf3EiGWYJn7U06UBiSlCoI2zwyvMJFO3EtJaPAz3vo7oS1lj4Vcq17NjUNqrri0UZe6v6cf/6r6SYiIiIqUsCbmzp16uDDDz+8L/v7IO7u7irjev369Rz7Zbug9beytLF0hwgNDc3zmFq1aqmvldcxUu8rE+Gy34yhLVlefXhztCZLKXgv2gN3V1gzxXZkeZnQ3U91btgSHKX7/oiIiIiKPeDVrsJ25coVvT5HyiCkl+/mzZt1+6QuV7Ylk1sQUhJx8uRJeHllZfpyc/nyZdy8eTPfY4yxS0NefXizr7aWUMAMb0amBkfuZnhNbYW1/NR0d8JTzaur+x9vCIZGo1+bNiIiIjJ/etfw/vHHHzm2JcC4evUqFi5cqPry6ktakg0ZMgTNmzdXq7hJW7LExERd1wYpX6hataqqsxUzZ85E69at4evri5iYGMyePRthYWEYPny4elxKLKQe94knnlBZYqnhfeONN9Tx0u7MFKTd7dKQVx/e7L14E5ILluENvhanliKWzLC/pwvMyatd6uC3I5dVycbWkCh09s9ZE05ERESWTe+AV2pis5NaUKmhlU4Ic+fO1XsAAwcORHR0NKZOnYpr166p2twNGzboJrKFh4erzg1aUjcsbczkWOnAIBniPXv2ICAgQD0uJRInTpzAd999pwLiKlWqoHv37nj33XdNphfvg/rwCmc7bYY3Xa92ZNK/Nr/nNUWerg54vq0Pluy4gI83hKBj3cqwyufNAhEREVkWvQNeKTkobmPGjFG33Nw70WzevHnqlpeyZcti48aNMGUP6sObI8NbwJKGg3cXnJAJa+bopY618eOBcARfi8e6E1fQr0lVQw+JiIiIjIR5pfrMxIP68Oo7aU3KTrQBb3MzDXjLO9phVIfa6v7cf84iNb3435gRERGRhQS8Uhv70Ucf3bf/448/xoABA4prXBYtXY8uDQVpS3b59h1cj0tRGeMm1cvDXA1t5wN3Z3uE30rCqoPhhh4OERERmWrAu2PHDvTu3fu+/bKwgzxGxdiHtwAlDQVZeELbrkv61pa1u3/hDXPhaGeDsV181f0FW0KRlFrwRTmIiIjIfOkd8EoXBGknlls/XEMvyWsudH14rQrSluzBJQ2Hwm6ZXTuyvDzdogaqVyiL6PgUfLv7kqGHQ0RERKYY8MrqZatWrbpv/8qVK3WdEqi4+vDmneF10aOk4eDdDg2WEPDa2VhhfLe66v4X288jJinV0EMiIiIiU+vSMGXKFPTv31/1t5VWZEIWipBlhX/++eeSGKPFSbtb0lCwDG/+l+1vJaYiNCpB3W/ubT4rrOWnb+OqWLL9gurY8MX2C5jUy9/QQyIiIiJTyvD26dMHv//+u1qmd/To0ZgwYYJayWzTpk339eilkp+09qCA99Dd7gx1KjvDzen+UhRzJEsNv97dT93/dvdFXItNNvSQiIiIyJQyvOKRRx5RNzLc0sIuupXW8g94zb0dWV661KuMQG83HA67jQVbzuGDxxsaekhERERkKhnegwcPYv/+/fftl32HDh0qrnFZtIIsLawtabiTlqHLCOdfv2sZ5QzZVwB8s2dWKcOqgxG4eCPR0EMiIiIiUwl4X375ZURERNy3PzIyUj1GpbO0sJP9f+3FElNzn7h2JzUDpyJjLWbC2r1a1qyAjn6VkJGpwSf/njX0cIiIiMhUAt7Tp0+jWbNm9+1v2rSpeoyKTpuxza9Lg72NNezuBsR51fEejbitevp6lnNANbeysEQTe2TV8q47fkUX/BMREZFl0Tvgtbe3x/Xr1+/bf/XqVdjYFKokmArRpSH74hOJeQS8h+6WMzT3cVOX+C1R/Squ6Nu4iro/558QQw+HiIiITCHg7d69OyZPnozY2P+yZTExMXjrrbfQrVu34h6fRSpIhjd7p4a8VlvTTliTS/uWTPrySj30tpBo7Ltw09DDISIiImMPeOfMmaNqeL29vdGpUyd1q1mzJq5du4a5c+eWzCgtTEFqeB/Ui1eC5iNhdzO83pYd8Pq4O2Fgi+rq/scbgqHRZL2+REREZBn0DnirVq2KEydO4OOPP1YrqwUGBuLTTz/FyZMnUb16VlBBJd+lIedqa/cHvLLogkxmk/Zlfp4usHRju9SBg60VjoTHYPOZKEMPh4iIiEpRoYpunZycMHLkyBz7zpw5g2+++UZlgKnk+/Bm79SQWy/eAxezyhmkF60sxGDpPMo54Pm2NdVyw7M3hqCTf2W+LkRERBZC7wxvdomJiSrIbdu2LerXr48NGzYU38gslFxul84KD1ppTTg72OZZ0nAo7JbFtiPLy0sdaqOcgw1Crsfjj+ORhh4OERERGXPAu3v3bgwbNgweHh4q0ysBr7QkO3XqVPGP0MKk3c3uCtsHdWnQZnjvCXglaP5vwQkGvFqujrYY1bG2uj/3n7NITc97wQ4iIiKywIA3KipK1e36+/vjySefRPny5bFt2zZYWVmp4Ff2U9Gl363fLVCGN49Ja2E3kxAdn6L69Daq5lpCIzVNQ9vWRCUXe1y+fQc/HQg39HCIiIjImAJe6cogE9NkgpqsqvbJJ5+gefPmJTs6S8/wPqCG19k+95IGbTsyCXYdbP9bkY2AsnbWagKb+GxLaJ49jImIiMhCA95du3Zhx44dOHuWy7SWdA/egvThzWvSmjbgbc5yhlwNbF4dNSo44kZCCr7dfdHQwyEiIiJjCXiDg4OxYsUKtaJaixYtVDuyefPmqccsdRWvkqCdsCYdBB70ukrLMXFvllK7wloLH7cSG6cps7OxwoTuddX9Jdsv4HZiqqGHRERERMYyaa1du3ZYunSpCnpHjRqFn3/+GRkZGRg9ejS++uorREdHl9xILURaRsF68GZfeCI+W8ArWcsLNxLVfUtfcCI/fRpVgb+ni3rtpFUZERERma9CdWlwdnbGiBEjsGfPHgQFBals7zvvvIMqVaoU/wgtTEF78GaftJY9w3vobjmDn4eL6kpAubOyKoM3evqp+8v2XMLV2DuGHhIREREZYx9eUa9ePbXYhExkW7VqVfGMyoJpuzQ8qEND9pKG7JPWdO3IarKc4UE6+VVWZR8p6ZlYsPmcoYdDRERExhrwatnY2KB///7F9XSw9C4NNg/owZu9pCH7pDXthDX2330wqZF+o2dWO73Vhy7jQnSCoYdERERExhzwUnGXNDw4w3tvH14pbQi6Eqfus0NDwcgbg87+lZGRqcHcf9l9hIiIyBwx4DUyaXqUNGgDXrkkL5PdjkXEqMCtavmy6kYFM7GHH6QhxvoTV3EqMtbQwyEiIqJixoDXWDO8epQ0aLO7By5q+++yflcf9bzKoV/jrAmXH28MMfRwiIiIqJgx4DXShScKkuGVTg4OtlmnMD45HYfCWL9bWOO61VWt4Hacjcae8zcMPRwiIiIyZMCbmJiIKVOmoG3btvD19UWtWrVy3Kho0jILPmkte1lD7J00HAmLUfcZ8OrPu6ITnmlZQ93/eEMINJr/lngmIiIi0/bfNfECGj58OLZv345BgwbBy8uLq6yVUIa3IJPWtAHvjYRU7L94C3fSMuBa1hZ1KjuX8CjN0yudffHL4cuqFvrf09fRvb6noYdEREREhgh4//77b6xfv16tukYl2JasAAtPZK/j3RYSpT4293ZTiyqQ/iqXc8DQdj74fNt5zN4Ygi71PNQSz0RERGRhJQ1ubm6oUIGXzEt84YkCBlrakob9F7QT1nhuiuLFDrVVlvxcVAJ+Pxpp6OEQERGRIQLed999F1OnTkVSUlJxfH0qwtLC2QPe1LulEC25wlqRSLD7Usfa6v4n/55FSnqGoYdEREREpV3SMHfuXJw/fx4eHh7w8fGBra1tjsePHDlS1DFZNOmnW9AuDcL57vLCws7GCg2qupbY2CzFkDY+WLrrIiJj7uDH/eEY2q6moYdEREREpRnwPvbYY0X5evQA6Xp2aXC0s9bdr1XRqcCfR3kra2eNV7vWwdtrTmHhllAMaF5dl0knIiIi06P3X/Fp06aVzEhI7y4NG05dxR/Hrui2g6/Ho/1HWzCtTwB6NvAq0XGau6eaV8dXOy7g0s0kle0d26WOoYdEREREhVTodODhw4exYsUKdTt69Ghhn4YK2aVBgt2XVhxBYmrOGtNrsclqvzxOhSc11OO7+6n7X+64gFuJqYYeEhEREZVWwBsVFYXOnTujRYsWGDt2rLoFBgaiS5cuiI6OLuw46J4uDbb5dGnIyNRgxrrTyG1pBO0+eVyOo8J7tKEXArzKISElHYu3hRp6OERERFRaAe8rr7yC+Ph4BAUF4datW+p26tQpxMXFqeCXiivDm3fAe+DiLVyNTc7zcXkGeVyOo8KTfsYTe2Zleb/bG4YrMXcMPSQiIiIqjYB3w4YN+Pzzz1GvXj3dvoCAACxatEgtSkHF05Ysv5KGqPi8g93CHEd561i3ElrWrIDU9Ex8uumcoYdDREREpRHwZmZm3teKTMg+eYxKvqShsotDgZ6roMdR3mTp7DfvZnl/PhyB0KgEQw+JiIiISjrglfrdV199FVeu/NcdIDIyEuPGjVN1vFTyk9Yk4+jl6oC8QmLZL4/LcVR0gd4V0LVeZUhJ9Cf/hhh6OERERFTSAe/ChQtVva4sOlG7dm11q1mzptr32Wef6ft0VIiFJ6ytyqjWY+Leo7Tb8rgcR8Xj9R5+KFMG+OvkNZy4HGPo4RAREVFJ9uGtXr26Wk1t06ZNCA4OVvuknrdr1676PhXl14f3AQtISJ/dxf9rproxZJ/A5unqwD68JcDfsxweb1IVvx2NxOyNIfj+hVaGHhIREREVkE1h6xq7deumblS80u62EpM+sA8iQW23AE/VjUEmqEnNrpQxMLNbMsZ1q4t1J65g57kb2B16A+183Q09JCIiIiqugHfBggUYOXIkHBwc1P38sDVZ8WR48ytpyE6C2za1K5bwqEhUr+CIZ1vWUC3KPt4Ygt9rV1Rv/oiIiMgMAt558+bhueeeUwGv3M+L/PFnwFs8bckKsrQwlb4xnevg58OXcTwiBhuDrqNnA09DD4mIiIiKI+C9ePFirvep5EoabB5Qw0uGUcnFHsPa1cTCraGY80+I6t7woGWgiYiIyLD0/ks9c+ZMJCUl3bf/zp076jEqpklrzPAarZEdaqG8o63qySuT2IiIiMjMAt4ZM2YgIeH+5vsSBMtjVPJ9eMmwyjnYYnTH2uq+rL6WnJZh6CERERFRPvSOqjQaTa4TdY4fP44KFbjQQXGttGbDTgtGbXAbH3iWc0BkzB38sD/c0MMhIiIyqIxMDfaev4m1xyLVR9k2ybZkbm5uKtCVW926dXMEvRkZGSrrO2rUqJIapwVOWmOG15g52Frj1a51MPm3k1i0NRQDW1SHs32huvwRERGZtA2nrt63LoCXka0LUOC/0PPnz1fZ3WHDhqnSBVdXV91jdnZ2auW1Nm3alNQ4LUZBVloj4zAgsBq+3HEBF28k4uudF/Ba17qGHhIREVGpB7svrTiCe/O512KT1X5ZJMsYgt4CB7xDhgxRH2UZ4bZt28LW1rYkx2Wx0tmlwWRInfWE7nUx5sej+GrHBQxq7Y2KzvaGHhYREVGpyMjUqMxubsULsk9Sd/K4LJJl6EWx9I6qOnTooAt2k5OTERcXl+NGRcMuDaaldwMvNKhaDompGfh823lDD4eIiKjUHLh4K0cZQ25Brzwuxxma3gGvdGMYM2YMKleuDCcnJ1Xbm/1GRcMuDabFyqoMJvbwV/e/3xumJrERERFZgqj45GI9riTpHVVNnDgRW7ZsweLFi2Fvb4+vv/5a1fRWqVIFy5cvL5lRWmCXBlt2aTAZD9dxR+taFZCakYn5/5419HCIiIhKpZzh5OXYAh1b2cUBJhfwrlu3Dp9//jmeeOIJ2NjY4KGHHsI777yDDz74AD/88EPJjNICuzQww2s6pGPJGz2zsry/HrmM0Kh4Qw+JiIioxBwOu43HFu3G17vyX323zN1uDS1rGr5trd5R1a1bt1CrVi11v1y5cmpbtG/fHjt27Cj+EVqYNG0fXtbwmpRmNdzQLcADMudwzkZmeYmIyPxExSdj/OpjeGLxHpyMjIWLg43qWCQRy71Ri3ZbWpMZesJaoQJeCXYvXsyK6P39/bF69Wpd5rd8+fLFP0ILa9Sclq4taWCG19RM7OEHaU+9IegajkXEGHo4RERExdYy9eudF9B5znb8diRS7XuqeTVsfb0jZg9orFqPebrmLFuQbWNpSSb07pQ/dOhQtaqadGuYNGkS+vTpg4ULFyItLQ2ffPJJyYzSgho1a98EMcNreup6uKB/02qqrGH2xmD8MLy1oYdERERUJLvO3cD0dUEIjUpQ242ruWJGvwZoUv2/JKcEtdJ6TLoxSBZYanaljMEYMrtaZTSymkQRhIWF4fDhw/D19UWjRo1gDqS9miysERsbq8o2SrNRM7JdAhjarmaJfG0qORG3ktB57jbVbWPFC63Qvo67oYdERERUqL9n768/o65aiopOdnijpx8GBFZXHYpMLV4r8lqo3t7e6kbF06hZa/G28xjcxseo3h3Rg1Wv4IjnWnlj2Z5L+HhjMNr5tsuxDDcREZExS07LwBfbz6s4JCU9U8UhsrDSuG514VrWdBcdK1DAu2DBggI/4dixY4syHovwoEbNIio+RR3XpnbFUhsXFY8xnX2x+lAETlyOxYZT19CroXHULxEREeVFLvhvDLqO99afxuXbWT3lpeXmjL4N4OfpAlNXoIB33rx5Obajo6PVAhTaSWoxMTFwdHRUi1Ew4DWvRs2kP3dnewxvXxMLtoRi9j8hqnsD28wREZGxCo1KwIx1Qdh57oballZibz9SD4809DKbq5QF+issXRm0t/fffx9NmjTBmTNnVEsyucn9Zs2a4d133y35EZuBgjZgNoZGzVQ4wx+uBTdHW1yITlST2IiIiIxNfHIa3l9/Gj3n71DBrp21FcZ08sXmCR3waKMqZhPsFmrSWu3atfHLL7+gadOmOfbLxLUnn3xS17LMlJX0pDWp4W3/0RZci03Os47Xo5w99kzqwhpeEyYtXN5bf0a9U5bWLQ621oYeEhERETIzNVhzNBIfbghGdHyK2te1XmVMeTQA3hWdYI7xmt7XWa9evYr09PT79mdkZOD69ev6Pp1FkiBWujCIvMLZt3rXY7Br4v7X2lsFu1KvvWJfmKGHQ0REhFORsXjyiz2Y8PNxFezWdHfCt0Nb4OshLUwq2NWX3gFvly5d8OKLL+LIkSM5srsvvfQSunbtWqhBLFq0CD4+PnBwcECrVq1w4MCBPI9dtmyZSrFnv8nn5WXUqFHqmPnz58OYSM+63Bo1a/UykkbNVHiS0X2tax11f9HWUHXpiIiIyBBuJ6birTUn0WfhLhwJj4GjnTXe7OmPDa89hE5+lWHu9A54ly5dCk9PTzRv3hz29vbq1rJlS3h4eODrr7/WewCrVq3C+PHjMW3aNBVEN27cGD169EBUVFSenyNpa8k0a2/SCzg3a9aswb59+1ClShUYIwl6d73ZGc+2rK5r5qxly4UnzMITzaqhViUn3E5Kw1c7Tb/ch4iITIuUUX6/9xI6ztmGH/eHQwpZ+zWpgi0TOuKljrVhb2MZ5XZ6B7yVKlXCX3/9heDgYPz888/qJpPWZJ90adCXrM42YsQItYJbQEAAvvjiC9XxQQLrvEjGVoJu7U2C7XtFRkbilVdewQ8//ABbW+PtGydlC42qZXW7SM/Mqui1scrKXJPpk+4Mr3f309X03kjIqpUiIiIqadLe9NHPdmHK2iDE3kmDv6cLVo1sjU+fbprnFWZzVeiFJ+rWratuRZGamqrKISZPnqzbZ2VlpUoj9u7dm+fnJSQkqMUuMjMzVXeIDz74APXr19c9LvsHDRqEiRMn5tifl5SUFHXLXgRdmrSNnG8mpKqPXFbYvPRq4ImGVV1xMjJWlTZM6/Pgn0kiIqLCkknxs/4+g7XHrujijAnd6+LZljUstk1mgQJeKTmQlmNOTk7q/oMytgV148YNNdnt3gytbEsGOTd+fn4q+yvLGMusvDlz5qBt27YICgpCtWrV1DEfffQRbGxsCtwTeNasWZgxYwYMRRfwJmYF3bZWlvnDaK4kWy/LMQ765gB+2BeOYe1qqhXZiIiIilNKegaW7rqEz7acQ1JqBuRi8dMtamBiDz9UcLKDJStQwHv06FGkpaXp7uelNC7Dt2nTRt20JNitV68elixZooJyyRh/+umnqh64oOORDHP2QF4yvNWrZ9XVloZydwPetIy7JQ3M8Jqd9r7uaFu7Ivacv4n5m85h7lONDT0kIiIyI9tCojBj3WlcvJGotpvVKK9WSWuYbX6QJStQwLt169Zc7xeVu7s7rK2t72tnJttSm1sQUp8rPYFDQ0PV9s6dO9WEtxo1auiOkSzyhAkTVKeGS5cu3fcc2sl3hnLv2tSWernB/LO8/nhs0W6sOXoZL3aohboepr9UIxERGVb4zSTM/PM0Np25rlvtc3IvfzzetCqs2N5Ux6CRlZ2dHQIDA7F58+Yc9beynT2Lmx8JZk+ePAkvr6w2XlK7e+LECRw7dkx3ky4NUs+7ceNGGCNXx5wBry1/QM1Sk+rl0aO+B2Ru4pyNIYYeDhERmbA7qRmY+08Ius7broJdmfA+4qGa2Pp6BzwRWI3BbmEyvP3790dB/fbbb9CHlBIMGTJEtTmT9maShU1MTFRdG8TgwYNRtWpVVWcrZs6cidatW8PX1xcxMTGYPXu2aks2fPhw9XjFihXV7d4ssGSMpf7XGDnb2UB+Lu82aWCG14xJx4Z/T1/HP6ev40j4bTSr4WboIRERkQmRBXL/OnlNLQl8JTZZ7Xuojrta0Mq3Mq8cFinglWXbSsrAgQMRHR2NqVOn4tq1a2jSpAk2bNigm8gWHh6uOjdo3b59W7Uxk2Pd3NxUhnjPnj2qpZmpkndhUscbk5RVJ80aXvNVx8NF9eb9+fBlfLwhGD+NaM0WdEREVCBnr8dj2tog7L1wU21XLV9WLQcsVw/5tyR/ZTTyVoEKvTZzcekweyvCbiap+34eLtg47uFS+bpU+iJj7qDT7G1IzcjE8mEt8XDdSoYeEhERGTHpoTt/01ks3xumFpKwt7HCqA611a2snWUsHFHUeK3QfXip5CauMcNr3uQd+f9ae2Pp7ov4eGOw6uDAWisiIrpXZqYGv8gVwY3BuHG3V79kc995JIDtLfVUqID3l19+werVq1W5gSwekZ20A6OiBrys4TV3L3eqjVUHw3EqMg5/n7qGRxplTbokIiISxyJiMO2PIByPiFHbtSs5YXrf+nioDq8KFobekdWCBQvUhDKpsZWevDLRTCaJXbhwAb169SrUIOi/XryCXRrMX0Vnewx/qJa6P+efEKRlZBp6SEREZARkCfo3fjmu2lhKsOtsb4O3e9fD368+zGC3NAPezz//HF9++SU+++wz1VbsjTfewL///qtWNZMaCiocljRYnuEP1VQr30iTcLlkRURElis9IxPf7r6ITnO2YfWhrL8J/ZtVxZYJHTDi4Vqws+HV36LQ+9WTMgZZ3UyULVsW8fHxuv63P/30U5EGY8myB7y2LGmwCC4Othjdsba6/+mmc0hOyzD0kIiIyAD2nr+JRxbsUiulxSeno36Vcvj1pTb45KkmqFzOwdDDMwt6R1bSz/bWrVvqvqxmtm/fPnX/4sWLqjccFUOGlyUNFkMmr1VxdcC1uGQs33v/KoBERGS+rsTcwcs/HsEzX+1DyPV4uDna4v3HG+CPMe0R6F3B0MOz7IC3c+fO+OOPP9R9qeUdN24cunXrpvrpPv744yUxRotQnpPWLJKDrTVe61ZX3f9823nEJWf1YiYiIvMlV/QWbjmHLnO3Y/2Jq2rxqUGtvbH19Y54rpU3rJn4MlyXhj///BO9e/dW9buy/K94+eWX1YQ1Wfihb9++ePHFF4t/hBZZ0sAfdEvSv2lVLNl+HuejE/HVjguY0N04VwQkIqKi23zmuipdCL+V1Xu/hY8bZvRtgIAqpdP331IVOOB97LHHVGeG559/HsOGDUPt2lm1h08//bS6UXGWNDDDa0kkoz+xhx9GrTiCb3ZdxOA2PqjkYm/oYRERUTGSCcoz1wVha0i02vYoZ4+3etdD38ZVuEpaKShwZCU1upLBXblyJerWrYsOHTrg+++/x507d0p2hBbYloxdGixPj/qeaFzNFUmpGVi0NdTQwyEiomKSmJKOjzYEo8e8HSrYlau4skLa5gkd0a9JVQa7xhbwVq9eHVOnTsX58+exadMm+Pj44KWXXoKXlxdGjRqFgwcPluxILamkgRleiyP/4b3R01/d/2F/GCLuXuoiIiLTJBP51x6LVHW6i7edV8vJd/SrhI2vPYxJvfxVf10qPYWKrDp16oTvvvsOV69exezZs3Hy5Em0bt0ajRs3Lv4RWghXR2Z4LV07X3e1zHBahgbz/j1r6OEQEVEhnb4Sh4Ff7sOrK4+pLjw1Kjji68HN8e3zLVCrkrOhh2eRivT2wsXFBV26dEFYWBiCg4Nx+vTp4huZhXG2s1GzNDM17MNryaSWd1foDaw5FokXO9SGn6eLoYdEREQFFJOUik/+PYsV+8LU33MHWyu83NFXLRwhXXnIcAoVWUnd7vLly9GxY0fUqVNH1fWOHz8ely6xj2hhSQfjsnZZvwxRccnIkN8UsjiNq5dHrwaekJbWszeGGHo4RERUAPI3+8f94WqVtOV7s4LdRxp6qTrdV7rUYbBrahleWWRi6dKlWL16NVJTU9G/f39VzyslDlR4G05dVS1KElOyVtr669Q1HP1oC6b1CUDPBl6GHh6VMmlLtjHoGjaduY7DYbfYfJyIyIgdDruN6X8E4WRkrNqu6+GM6X3qo62vu6GHRtmU0RRwebSAgACEhISgadOmeOGFF/Dss8/C1dUV5iguLk59b7GxsShXrlyJB7svrTiiMrzZaat4F/+vGYNeC/TmLyew6lAEWtasgFUjW3MWLxGRkYmKT8aHfwfjtyORatvFwQbjutbFoDbeLE00wnitwBnerl274qeffsoxMW337t1o3rw57O3ZM7Swl0Aks5vbOw7ZJyGOPN4twJOrrliYV7vWUXW8By7ewvaz0ejoV9nQQyIiIgBpGZn4bs8lzN90Dgkp6WrfU82rqU477s6Mh4xVgd+CLFiw4L4uDL169UJkZNY7G9KfBDNXY5PzfFyCXnlcjiPLUqV8WQxu7a3uf7whBJms6SYiMrhd526g16c78d76MyrYlf7pa0a3xcdPNmawa85dGgpYDUH5XA4pzuPIvIzu5IuVByNw+moc1p+8ij6Nqxh6SEREFkl6o7+//gw2BF1T2xWd7PBGTz8MCKwOK16BNQnsemxAlV0civU4Mi8VnOww4qFamLfpLOb+E4KeDTxZF0ZEVIqS0zLwxfbzauGIlPRMVV44qLU3xnWrm2PBKDLzgHfJkiXw8PAovtFYGJmQ5OXqgGuxybnW8cp7Rk9XB3UcWaYXHqqJ5Xsv4dLNJKw+FIHnWmWVORARUcmRK9gbg67jvfWncfn2HbWvda0KmNG3Afujm6gipYukU0NGRgZ+//13nDlzpvhGZSHknaK0HhP3XhDRbsvjnLBmuWTpyZc7+ar7Czafw53UrNZ1RERUMkKjEjB46QGMWnFYBbuSmFr4bFP8NKI1g11LCnifeuopLFy4ULcAhXRpkH2NGjXCr7/+WhJjNGvSckxaj0kmNzvZZksyEs+1roGq5cvielwKZv4ZpNZm33v+JhcnISIqRvHJaXh//Wn0nL8DO8/dgJ21FcZ08sXmCR3waKMqbA9paSUNO3bswNtvv63ur1mzRqX9Y2Ji8N133+G9997DE088URLjNGsS1ErrMenGIBPUpGZXyhiY2SVhb2ONrvUq47u9YfjpQIS6Cck6cHESIqKikS44a45G4sMNwYiOT1H75P/cKY8GwLuik6GHR4YKeKW5b4UKWTWlGzZsUAGuo6MjHnnkEUycOLG4xmVxJLhtU7uioYdBRkgWJ5GlKu8ltd+yaAmvBBARFc6pyFhMXXsKR8Jj1HZNdydM7ROATux9bnb0DnirV6+OvXv3qqBXAt6VK1eq/bdv34aDA7sJEBUnLk5CRFT8biemYvY/IfjpQDikw6qjnTXGdPbFC+1rqqtqZH70Dnhfe+01PPfcc3B2doa3tzc6duyoK3Vo2LBhSYyRyGLpszgJrxAQET04ifDj/jDM+ecsYu+kqX19G1fBW73r3TeXhiw84B09ejRatmyJiIgIdOvWDVZWWfPeatWqpWp4iaj4cHESIqLiIYmBaX8E4czVOLXt7+mCGX3ro1UtJgssQaH68EpnBrkJaUt28uRJtG3bFm5ubsU9PiKLxsVJiIiKRuY7zPr7DNYeu6K2ZcGICd3r4tmWNWDDxXwshlVhShq++eYbXbDboUMHNGvWTNX2btu2rSTGSARLX5zkQdW5605EIi0js5RGRURk/FLSM9QKaZ3nblPBrnQVe6ZlDWx9vSMGt/FhsGth9D7bv/zyCxo3bqzur1u3DhcvXkRwcDDGjRuna1dGRKW3OIn4cX8EBn9zQE3EICKydNtCotBz/k58tCEYSakZaFajPP54uT1m9W+olm0ny1NGI4109SCdGEJDQ1GtWjWMHDlStSSbP3++CnwlEI6Ly6qNMWXyPbi6uqoWbOXKlTP0cIhUazLpxpB9Apu2D6+1lRVeW3kUiakZqFHBEV8PaY66HlwNiIgsT/jNJMz88zQ2nbmutt2d7TG5lz8eb1oVVuxkY3b0idf0ruH18PDA6dOn4eXlpdqSLV68WO1PSkqCtTVbeRAZYnGS30a3wwvfHUT4rST0/3wPPn26CbrU8zD0sImISoUsu/75tlAs2XEBqemZsLEqg6HtfDC2Sx24ONgaenhkBPQOeIcOHaqWEpaAV5bZ69q1q9q/f/9++Pv7l8QYiegBi5PI+u5/jGmPl1Ycxv6LtzB8+SG80cMfozrU4nKYRGS25CL1XyevqSWBr9y9Atbe1x3T+wbAtzKvdFERShq0dbzSlmzAgAGqtEHI0sLly5dHv379YOpY0kCmSjIb09cF4cf94WpbLuNJzZqDLa++EJF5OXs9HtPWBmHvhZtqu2r5spjyaD30qO/JN/oWQp94rVABr7ljwEumTH6lv98Xpmp+pcl64+rl8dWgQFQux9ZlRGT6ZMGI+ZvOqiXX5f84exsrjOpQW93K2vHNvSWJ0yNeK1RPju3bt6NPnz7w9fVVt759+2Lnzp2FHS8RFSPJbEjLneXDWqp+k8cjYtBn4S6cuJy1VjwRkSnKzNRg9cEIdJm7Dd/uvqSC3R71PbBpfAeM61aXwS4Vb8C7YsUKVbcr3RnGjh2rbmXLlkWXLl3w448/6vt0RFRC2vm6Y+3L7eBb2RnX41Iw4Iu9+ON4VuN1IiJTciwiBo8v3oM3fj2BGwmpqF3JCd+/0BJLBjVH9QqOhh4emQC9Sxrq1aun2pFJ393sPvnkE3z11Vc4c+YMTB1LGsicxCWn4dWfjmJrSLTafrlTbUzo5scWPURk9G4kpODjDcFYfeiy2nays8arXevg+bY1YWfDhSMsXVxJ1vDa29sjKChIlTJkJ715GzRogOTk//qEmioGvGRu5NKf/NGQlj2iW4AH5g1sAmf7Qq0uTkRUotIzMtVchE/+PYv45HS1r3/TqpjUy5/zEah0anhlCeHNmzfft3/Tpk3qMSIyzpZmk3vXw9wBjWFnbYV/T1/HE5/vQcStJEMPjYgoh73nb+KRBbvUxFsJdutXKYdfX2qDTwY2YbBLhaZ3emfChAmqbvfYsWNo27at2rd7924sW7YMn376aeFHQkQl7onAaqhZyQkjlx9GyPV49Fu0G58/1wyta+Xe35eIqLRcibmD9/86g/UnrqptN0dbvN7DD0+3qKFbZIeosArVlmzNmjWYO3eurl5X6nonTpxoFj14BUsayNxdjb2DEcsP4VRknFqRaGa/Bni2VQ1DD4uILFByWga+3nkBi7aex520DEhs+1wrb0zoXhflHe0MPTyyxBre9PR0fPDBBxg2bJhuwQlzxICXLGUpztd/Oa7Lpgxp440pjwbAxpoTQYiodGw+c12VLsiy6KKFjxum962P+lVcDT00svRJa87Ozjh16hR8fHxgrhjwkqWQX/+FW0Ix99+zarudb0UserYZsypEVKIu3kjEzHVBuu4xHuXs8VbveujbuApXSSPjmLQm/XZl4QkiMn3yh+WVLnXwxf8C4Whnjd2hN/HYot0IjYo39NCIyAwlpqTjow3B6DFvhwp2ba3LqBXSNk/oiH5NqjLYJeOZtNarVy9MmjQJJ0+eRGBgIJycnHI8LquuEZFp6dnAEzUqtFV1vZduJuHxRXuw4Jmm6ORf2dBDIyIzuZokC9/M+isY1+Ky2pd2qFsJ0/oEoFYlZ0MPjyyA3iUNVlZ5J4XlnVlGRgZMHUsayJKbvI9ecQQHLt2CJFom9/LHiIdqMetCRIV2+kocpq8LwoGLt9R2jQqOar5A13qV+X8LGW8NryVgwEuWLDU9E1PXnsLKgxFq+4lm1fD+4w3gYMt16omo4GKSUtXCESv2hSFTAzjYWuHljr4Y8XAt/n9CpR6vcZklIspBluuc1b8h/Dxd8O6fp/Hrkcu4cCMBSwYForILm74T0YNXdlx1MAKzNwbjdlKa2vdIQy+89Ug9VC1f1tDDIwtV4ElrW7ZsQUBAgIqm7yWRdf369bFjx47iHh8RGYBcZhzaria+G9YS5RxscDQ8Bv0W7sapyFhDD42IjNjhsNtq4utba06qYLeuhzN+HN4Ki55rxmCXTCPgnT9/PkaMGJFryljSyS+++CLmzZtX3OMjIgN6qE4l/P5yO9Sq5ISrscl48os9ur69RERaUfHJGL/6GJ5YvAcnI2PhYm+DqY8GYP3Yh9DW193QwyMqeMB7/Phx9OzZM8/Hu3fvjsOHDxfXuIjISMgM6jWj2+HhupWQnJaJl388ouryMqUoj4gsWlpGJr7acQGd52zHb0ci1b6nmlfD1okdMax9TdhyIRsyEgWu4b1+/TpsbW3zfiIbG0RHZzWQJiLz4lrWFkuHNMeHfwfj610XsWDzOZy9Fo9PBjaGox2nAhBZop3nojH9jyCcj05U242ruapV0prWcDP00IjuU+C/VFWrVlUrrPn6+ub6+IkTJ+Dl5VXQpyMiEyNLDr/zaADqerrgnTWnsCHoGsIWJ+GrwYGo5uZo6OERUSmJuJWE99efUf8HiIpOdnijpx8GBFaHlRXbjJFxKvC1ht69e2PKlClITs5qGJ3dnTt3MG3aNDz66KPFPT4iMjJPNa+On0a2gruzHc5cjVOT2Q5eyuqvSUTmKzktA/M3nUXXT7arYNfaqgyeb+uDLa93xMAWNRjsklErcB9eKWlo1qwZrK2tMWbMGPj5+an9wcHBWLRokVpw4siRI/Dw8ICpYx9eogeLjLmDEd8dwumrcWp50Pcea6D+6BGReZEwYWPQdby3/jQu376j9rWuVUGVL/h78m8kmeHCE2FhYXjppZewceNG9QugnqBMGfTo0UMFvTVr1oQ5YMBLVDBJqemYsPo4/j6VdWlzWLuaeKu3vyp/ICLTFxqVgBnrgrDz3A217eXqgLcfqaf66nKVNDL7ldZu376N0NBQFfTWqVMHbm7mVaDOgJeo4KRbw4It5zB/0zm1/VAddyx8tpma6EZEpik+OU1NTv129yWkZ2pgZ22FkQ/XwuhOtTlRlYwGlxYuIga8RPr76+RV1YdTWpfVcnfCV0Oao3YlZ0MPi4j0fAO75mgkPtwQjOj4FLWva73KmPJoALwrOhl6eEQ5MOAtIga8RIUjK7GNXH4IV2KT4eJgg0XPNlP9e4nINH5/p649hSPhMWrbp6IjpvWpj07+lQ09NKJcMeAtIga8RIUnWaFRKw6rJUZl0vbbjwRgWDsf1vsRGalbiamYvTEEKw+GQyICRztrjOnsixfa14S9jbWhh0eUJwa8RcSAl6hoUtIzVK/enw9f1q289O5jDfjHk8iIZGRq8OP+MMz55yxi76SpfX0bV8FbvevB09XB0MMjKtZ4jZXnRFTsJLD9+MlG8PN0wQd/ncHqQ5dxIToRXwwKhLuzvaGHR2TxDly8hWl/BKle2sLf0wUz+tZHq1oVDT00ohLBDG8umOElKj7bQqLwyo9HEZ+Sjqrly+LLwYGoX8XV0MMiskjXYpMx6+8zWHvsitou52CD13v44dmWNdhOkEwOSxqKiAEvUfH38hyx/BAu3khEWVtrzBvYGD0bcClyotIsM1q66xI+23IOSakZkJL6p1vUwMQefqjgZGfo4REVCgPeImLAS1T8YpPS8PKPR7ArNKuB/biudTG2iy8nsxGVwlWWGetOqzecolmN8pjRtwEaVuOVFjJtDHiLiAEvUclIz8jEe+vPYNmeS2pbVmuaM6AxytpxMhtRcQu/mYSZf57GpjPX1bbUz0/u5Y/Hm1aFlbRQITJxnLRGREZJagSn962vJshMWXsK609exaWbifhqcHNUKV/W0MMjMgt3UjPw+bZQLNlxAanpmbCxKoPn2/rg1a514OLAFRDJMjHDmwtmeIlKZ5a49OuVHqCSeVoyKBCB3ua1TDlRaZI/53+dvIb3159Wi7+I9r7umN43AL6VXQw9PCKDxmtGMSVz0aJF8PHxgYODA1q1aoUDBw7keeyyZctUzV/2m3xedtOnT4e/vz+cnJzg5uaGrl27Yv/+/aXwnRBRQbWsWQFrX26nsr03ElLwzJf78Mvdvr1EpJ+z1+Px7Ff7VZ28BLvSEeWL/zXD9y+0ZLBLZAwB76pVqzB+/HhMmzYNR44cQePGjdGjRw9ERUXl+TkSxV+9elV3CwsLy/F43bp1sXDhQpw8eRK7du1SwXT37t0RHR1dCt8RERVU9QqO+PWltuge4IHUjEy8/vNxlZ2ShvhE9GCyYMSMdUHo9elO7L1wE/Y2Vni1Sx1sGt9BdULhpFAiIylpkIxuixYtVIAqMjMzUb16dbzyyiuYNGlSrhne1157DTExWWt965Py3rRpE7p06VLg41nSQFQ6MjM1mLfpLD7bEqq2O/pVwoJnmqIc6w2J8vydkSsiH28Mxo2EVLWvR30PvPNIgHojSWQJ4kylpCE1NRWHDx9WJQe6AVlZqe29e/fm+XkJCQnw9vZWgXG/fv0QFBSU79f48ssv1Qsi2ePcpKSkqBct+42ISo/MGJ/Q3Q+fPdNUZai2hUTj8UW7dW2UiOg/xyJi8PjiPXjj1xMq2K1VyQnLh7XEkkHNGewSGWPAe+PGDWRkZMDDwyPHftm+du1arp/j5+eHpUuXYu3atVixYoXKCLdt2xaXL+es/fvzzz/h7Oys6nvnzZuHf//9F+7u7rk+56xZs1RArL1JIE1Epa9P4yr4ZVRbeJZzwPnoRDy2aDd2ncvq20tk6aTW/Y1fjqvfi+MRMXCys8Zbvf2x4dWH8XDdSoYeHpFRM2hJw5UrV1C1alXs2bMHbdq00e1/4403sH379gJNNEtLS0O9evXwzDPP4N1339XtT0xMVPW9ElR/9dVX2LJli3q+ypUr55rhlZuWZHgl6GVJA5FhRMUlY+T3h1Umy9qqDKY8Ug9D2vqwHpEstn/19/vC8Mm/ZxGfnK729W9aFZN6+aNyuZyTtoksSZyplDRIxtXa2hrXr2c1xdaSbU9PzwI9h62tLZo2bYrQ0KzaPy3p0ODr64vWrVvjm2++gY2NjfqYG3t7e/VCZb8RkeHIH/GVI1urP+oygW36utN4a81J1VOUyJLsPX8TjyzYpVZKk2C3fpVy+PWlNvhkYBMGu0R6MGjAa2dnh8DAQGzevFm3T0oUZDt7xjc/UhIh3Ri8vLzyPU6eN3sWl4iMm4OtNeY+1VitDCWJ3Z8OROB/3+zHzQT+HpP5uxJzR7UYe+arfQi5Ho/yjrZ4//EG+GNMewR6VzD08IhMjsFXWpOWZEOGDEHz5s3RsmVLzJ8/X5UjDB06VD0+ePBgVfYgdbZi5syZKmsr2Vvp1DB79mzVlmz48OHqcfnc999/H3379lVBsJQ0SJ/fyMhIDBgwwKDfKxHpR0oYXuxQG3U8nDH2p2NqsYp+i3arldnqefFKDJmf5LQMfL3zAhZtPY87aRmQFYCfa+WNCd3roryjnaGHR2SyDB7wDhw4UPXHnTp1qpqo1qRJE2zYsEE3kS08PFx1btC6ffs2RowYoY6VRSUkQyw1wAEBAepxKZEIDg7Gd999p4LdihUrqrZnO3fuRP369Q32fRJR4XX298Ca0W0xfPkhhN1MwhOL92D+wCboXr9gpU9Exk6m02w+E4WZf55G+K0kta+Fj5tairt+FVdDD4/I5Bm8D68xYh9eIuN0OzFVXebdc/6m2n69e1283MmXk9nIpEn7PVk8QtrxCY9y9nirdz30bVyFP9tExRSvMeDNBQNeIuOVlpGJd/88jeV7w3StzGY/2UjV/BKZksSUdCzcGopvdl5UKw3aWpfBC+1rYUxnXzjbG/wCLJFZxWv8jSIik2JrbYWZ/RrAz9MF09YGYd3xK7h0I1HV9Xq6ctY6GT/JM/1x/Apm/RWMa3HJal+HupUwtU8AaldyNvTwiMwSM7y5YIaXyHRaNo3+4TBuJ6Whkos9vhwUiKY13Aw9LKI8nb4Sh+nrgtQETFGjgiOmPBqArvUqs3yBSE8saSgiBrxEpiP8ZhJGLD+kWjfZ2Vjhoyca4vGm1Qw9LKIcYpJS1cIRK/aFIVMjbfes8HJHX4x4uBbLcYgKiQFvETHgJTItCSnpeG3lUWw6E6W2X+xQC2/08FertBEZkiycsupgBGZvDFZXIsQjDb3w1iP1ULV8WUMPj8ikMeAtIga8RKYnM1ODOf+E4PNt59V2Z//K+PTpJnBxsDX00MhCHQ67jel/BOFkZKzaruvhjOl96qOtr7uhh0ZkFhjwFhEDXiLTtfZYJCb+ckItQ1ynsjO+HtIc3hWdDD0ssiBR8cn48O9g/HYkUm272NtgXLe6GNTGW026JKLiwYC3iBjwEpm24xExqq43Kj5FLcn6+XPN0LY2s2pU8i3zlu2+hE83n1NlNuKp5tXwRk9/uDvbG3p4RGaHAW8RMeAlMn3X45IxcvkhHL8cCxurMpjWtz4GtfY29LDITO08F63KF85HJ6rtxtVc1Spp7BpCVHIY8BYRA14i85CcloE3fz2BtceuqO3/ta6BaX3q87IyFZuIW0l4f/0ZbAi6prYrOtnhjZ5+GBBYHVacNElUorjwBBERpPWTNeYPbKIWqZi9MQQr9oXjfFSiKnFwc7Iz9PDIxN9MfbH9PBZvO4+U9EzVEUSuIEitrmtZTpQkMjbM8OaCGV4i87Pp9HW8uvIoElMzVLN/mcxW18PF0MMiEyN/MjcGXcd760/j8u07al/rWhVU+YK/J/9eEJUmljQUEQNeIvMUci0ew5cfRMStO3C2t1Fty7rU8zD0sMhEhEYlYMa6IOw8d0Nte7k64O1H6qm+ulwljaj0MeAtIga8RObrVmIqXlpxGPsv3oLEKLJAxagOtRiwUJ7ik9OwYPM5fLv7EtIzNbCztsLIh2thdKfacLRjZSCRoTDgLSIGvETmTXr0Tl8XhB/3h6vtx5pUwYdPNOISr3TfYiZrjkbiww3BiI5PUfu6+FfGlEcD4OPO3s5EhsZJa0RE+bCzscIHjzdEPU8XTF93Gr8fu4KLN5Pw5aBAeJRzMPTwyAiciozF1LWncCQ8Rm37VHRUHT46+Vc29NCIqBCY4c0FM7xElmN36A2M/uEIYu+kwaOcPb4a3ByNqpU39LDIgCUv0tFj5cFwyF9HRztrjOnsixfa14S9Da8AEBkTljQUEQNeIssSdjMRL3x3SE1KsrexwsdPNkK/JlUNPSwqRRmZGvy4Pwxz/jmr3vyIvo2r4K3e9eDpyqw/kTFiwFtEDHiJLHNi0qsrj2FLcJTafrlTbUzo5sfFAyzAgYu3MO2PIJy5Gqe2/T1dMKNvfbSqVdHQQyOifDDgLSIGvESWm+X7eGMwlmy/oLa7BXhg3sAmqoUZmZ9rscmY9fcZ3Up85Rxs8HoPPzzbsgZsuBofkdFjwFtEDHiJLNtvRy5j0q8nkZqRCT8PF7VIRfUKjoYeFhWTlPQMLN11CZ9tOYek1AzVnu7pFjUwsYcfKnAFPiKTwYC3iBjwEtGR8Nt48fvDqh2Vm6MtFv8vEK15idvkbQuJwox1p3HxRqLablqjPGb2bYCG1VwNPTQi0hMD3iJiwEtE4mrsHYxYfginIuNgY1UGM/s1wLOtahh6WFQI4TeTMPPP09h05rradne2x6Re/ujftCrrtIlMFAPeImLAS0Rad1IzMPGX4/jzxFW1PaSNt1p4gDWepnP+Pt8WiiU7LqgFR+SNy/NtfTC2ax2Uc7A19PCIqAi48AQRUTEpa2eNz55pqmp55/57Ft/tDUNodAIWPdsM5R1Z72msJJfz18lreH/9aVyJTVb72vu6Y3rfAPhWdjH08IiolDHDmwtmeIkoNxtOXcP41cfURCdZeUsmszF4Mj5nr8dj2tog7L1wU21XLV8WUx6thx71PVFGZqgRkVlgSUMRMeAlorxIr9bh3x1CZMwduNjbYMEzTbncrJGQBSPmbzqL5XvDVIs5WURkVIfa6iaZeiIyLwx4i4gBLxHl52ZCCl5acQQHLt1SLa0m9/LHiIdqMXtoIJmZGvxy+LLqoXwjIVXt6x7goWqt2U6OyHwx4C0iBrxE9CAyAWrq2lNYeTBCbT/RrBref7wBHGyZSSxNxyJi1CppxyNi1HatSk6Y3qc+Hq5bydBDI6ISxklrREQlzM7GCrP6N4Sfpwve/fM0fj1yGRduJGDJoEBUdnEw9PDM3o2EFHy8IRirD11W20521ni1ax0837amOjdERNkxw5sLZniJSB87z0Xj5R+OIC45HV6uDvhqcHM0qMqFDEpCekYmvt8Xhk/+PYv45HS1T3rpSk/dyuX4RoPIksSxpKFoGPASkb4uRCdg+PJDuBCdCAdbK8wZ0BiPNqpi6GGZlb3nb2L6H0EIuR6vtutXKYcZfeujuU8FQw+NiAyAAW8RMeAlosJ2CRj701FsPxuttsd29sVrXetyJa8iuhJzB+//dQbr7y7+Ud7RFhN7+OHpFjVgzdeWyGLFMeAtGga8RFRY0g5r1l9n8PWui2q7Z31PzH2qMZzsOWVCX8lpGfh65wUs2noed9IyILHtc628MaF7XS76QURgwFtEDHiJqKh+PhSBt9ecQmpGJvw9XdQiFdXc2CKrIOTP0uYzUZj552mE30pS+1r4uGF63/qoX4W10USUhQFvETHgJaLicDjsFl78/rDqDVvRyQ5fDApEC9ab5uvijUTMWBeEbSFZZSGVXezxVu966NekCvscE1EODHiLiAEvERUXWZFtxHeHcPpqHGyty+C9xxpgYIsahh6W0UlMScfCraH4ZudFlRWX12pY+5p4pXMdOLMchIhywYC3iBjwElFxSkpNx+s/H8dfJ6+p7aHtfPB273qwsWa/WPkT9MfxK5j1VzCuxSWrfR3qVsLUPgGoXcnZ0MMjIiPGgLeIGPASUUksf7tgyznM33RObT9Uxx0Ln2kGV0dbWKrTV+IwfV0QDly8pbZrVHBUywF3rVeZ5QtE9EAMeIuIAS8RlZS/Tl7FhNXHVdeBWu5O+GpIc4vLZMYkpaqFI1bsC0OmBqpv8csdfTHi4VpcmpmICowBbxEx4CWikhR0JVbV9V6JTYaLgw0WPttMXca3hJZtqw5GYPbGYNxOSlP7HmnohbceqYeq5csaenhEZGIY8BYRA14iKmnR8SkYteIwDofdVv1l334kAMPa+ZjtpXz5PmWVtJORsWq7roczpvepj7a+7oYeGhGZKAa8RcSAl4hKQ0p6Bt5Zcwo/H76stp9qXg3vPtYA9jbmc1k/Kj4ZH/4djN+ORKptF3sbjOtWF4PaeMOWk/aIqJTiNfZ6ISIyEAlsP36yEfw8XfDBX2ew+tBlXIhOVP163Z3tYcrSMjKxbPclfLr5HBJS0tW+AYHV8EZPf1RyMe3vjYhMDzO8uWCGl4hK27aQKLzy01HEJ6eretYvBwea7KpiO89Fq/KF89GJartxNVe1SlrTGm6GHhoRmRGWNBQRA14iMoTz0QkY/t0htdpYWVtrfPJUY/Rq6AVTEXErCe+vP4MNQVn9hmV1uTd6+mFAYHVYSaEyEVExYsBbRAx4ichQYpPSMOanI9h57obaHte1LsZ28TXqyWzJaRn4Yvt5LN52HinpmbC2KoNBrb1Vra5rWcvtM0xEJYsBbxEx4CUiQ0rPyMT7f53Bt7sv6Vp3zR7QCI52xjXtQv58bAy6jvfWn8bl23fUvta1KqjyBX9P/t9JRCWLk9aIiEyYLDk8rY8EjS545/dTWH/yKi7dTMRXg5ujipH0qw2NSsCMdUG6TLSXqwPe6l0PjzbyMupsNBFZJmZ4c8EMLxEZC1l296UVh3EzMVV1blgyKBCB3oab/BWfnIYFm8+p7HN6pgZ21lYY8XBNvNzJ1+gy0ERk3uJY0lA0DHiJyJhcvp2kJrMFX4tXAeYH/RviycBqpTqGzEwN1hyNxIcbgtWiGaKLf2VMeTQAPu5OpToWIiLBgLeIGPASkbFJTEnH+NXHVM2sGPFQTUzqVU9NECtppyJjMXXtKRwJj1HbPhUdVclFJ//KJf61iYjywoC3iBjwEpExkizr/E1nsWBLqNruULcSPnu2Kco5lEwnhFuJqZi9MQQrD4ZD/lI42lljTGdfvNC+plmtBkdEpokBbxEx4CUiY7bu+BVM/OU4ktMyUbuSE74e0gI1i7GsICNTgx/3h2HOP2cReydN7evbuAom9/aHl6txTJojIopjwFs0DHiJyNidvByLEcsP4Vpcsup1u+jZZmhfx71YJslN+yMIZ67GqW3pFDGjb320qlWxGEZNRFR8GPAWEQNeIjIFUXHJGPn9YRyLiFG1vFMeqYchbX0K1RbsWmwyZv19BmuPXVHb5Rxs8HoPPzzbsoZqk0ZEZGwY8BYRA14iMhWyytlba07ityORavuZltUxo28D2NkULEhNSc/A0l2X8NmWc0hKzYDEyk+3qI7Xu/uhorN9CY+eiKjwuPAEEZGFcLC1xtwBjVXpway/g/HTgQicj07E4ueaqYBV6nGlTCEqPhmVXRzQsmYFXWeHbSFRmLHuNC7eSFTbTWuUV+ULjaqVN/B3RURUvJjhzQUzvERkirYGR2HsT0cRn5KOam5l8XxbH3yz6yKuxibrjpEV0UZ39MX2s9HYdCarxZksaDGplz/6N60Kq1Joc0ZEVBxY0lBEDHiJyFSdux6P4csPIexm0gOPtbEqo4LisV3rlFhrMyIiY4jXOBOBiMiM1PFwwa+j2qoV2fIjNb7rx7bHO48GMNglIrPHgJeIyMyci0pAakZmvsekpmfiVmJWj10iInPHgJeIyMzIBLXiPI6IyNQx4CUiMjPSjaE4jyMiMnUMeImIzIy0HpNuDHn1W5D98rgcR0RkCRjwEhGZGemzO61PgLp/b9Cr3ZbHtf14iYjMHQNeIiIz1LOBFxb/rxk8XXOWLci27JfHiYgsBVdaIyIyUxLUdgvwzHOlNSIiS8GAl4jIjElw26Z2RUMPg4jIoFjSQERERERmzSgC3kWLFsHHxwcODg5o1aoVDhw4kOexy5YtQ5kyZXLc5PO00tLS8Oabb6Jhw4ZwcnJClSpVMHjwYFy5cqWUvhsiIiIiMiYGD3hXrVqF8ePHY9q0aThy5AgaN26MHj16ICoqKs/PkfWSr169qruFhYXpHktKSlLPM2XKFPXxt99+Q0hICPr27VtK3xERERERGZMyGo1GY8gBSEa3RYsWWLhwodrOzMxE9erV8corr2DSpEm5Znhfe+01xMTEFPhrHDx4EC1btlSBcY0aNR54fFxcHFxdXREbG6uCayIiIiIyLvrEawbN8KampuLw4cPo2rXrfwOyslLbe/fuzfPzEhIS4O3trQLjfv36ISgoKN+vIy+ElD6UL18+18dTUlLUi5b9RkRERETmwaAB740bN5CRkQEPD48c+2X72rVruX6On58fli5dirVr12LFihUqI9y2bVtcvnw51+OTk5NVTe8zzzyTZ/Q/a9Ys9Q5Be5NAmoiIiIjMg8FrePXVpk0bNQmtSZMm6NChg6rRrVSpEpYsWXLfsTKB7amnnoJUbSxevDjP55w8ebLKAmtvERERJfxdEBEREZFF9OF1d3eHtbU1rl+/nmO/bHt6ehboOWxtbdG0aVOEhobmGuxK3e6WLVvyre2wt7dXNyIiIiIyPwbN8NrZ2SEwMBCbN2/W7ZMSBdmWTG5BSEnEyZMn4eXldV+we+7cOWzatAkVK7LpOhEREZGlMvhKa9KSbMiQIWjevLnqpDB//nwkJiZi6NCh6nEpX6hataqqsxUzZ85E69at4evrqzo1zJ49W2Vxhw8frgt2n3zySdWS7M8//1QBsbYeuEKFCirIJiIiIiLLYfCAd+DAgYiOjsbUqVNVYCq1uRs2bNBNZAsPD1edG7Ru376NESNGqGPd3NxUhnjPnj0ICAhQj0dGRuKPP/5Q9+W5stu6dSs6duz4wDFpO7WxWwMRERGRcdLGaQXpsGvwPrzGSDo+sFMDERERkfGTZgPVqlXL9xgGvLmQOmJZitjFxUX17y3JdyYSWMuJ4gIXpoXnzjTxvJkunjvTxPNmuuJM4NxJCBsfH48qVarkqAYwypIGYyQv2oPeKRQn+UEy1h8myh/PnWnieTNdPHemiefNdJUz8nMn6yeYZR9eIiIiIiJ9MOAlIiIiIrPGgNeAZLGLadOmcdELE8RzZ5p43kwXz51p4nkzXfZmdu44aY2IiIiIzBozvERERERk1hjwEhEREZFZY8BLRERERGaNAS8RERERmTUGvAayaNEi+Pj4wMHBAa1atcKBAwcMPSSLt2PHDvTp00et2CIr7P3+++85Hpf5nVOnToWXlxfKli2Lrl274ty5czmOuXXrFp577jnVpLt8+fJ44YUXkJCQUMrfiWWZNWsWWrRooVZGrFy5Mh577DGEhITkOCY5ORkvv/wyKlasCGdnZzzxxBO4fv16jmPCw8PxyCOPwNHRUT3PxIkTkZ6eXsrfjeVYvHgxGjVqpGtq36ZNG/z999+6x3nOTMeHH36o/s987bXXdPt4/ozT9OnT1bnKfvP397eI88aA1wBWrVqF8ePHq3YfR44cQePGjdGjRw9ERUUZemgWLTExUZ0LeTOSm48//hgLFizAF198gf3798PJyUmdN/kPQkuC3aCgIPz777/4888/VRA9cuTIUvwuLM/27dvVf9D79u1Tr3taWhq6d++uzqfWuHHjsG7dOvz888/qeFk6vH///rrHMzIy1H/gqamp2LNnD7777jssW7ZMvcGhkiGrWUqgdPjwYRw6dAidO3dGv3791O+P4DkzDQcPHsSSJUvUm5fseP6MV/369XH16lXdbdeuXZZx3qQtGZWuli1bal5++WXddkZGhqZKlSqaWbNmGXRc9B/51VizZo1uOzMzU+Pp6amZPXu2bl9MTIzG3t5e89NPP6nt06dPq887ePCg7pi///5bU6ZMGU1kZGQpfweWKyoqSp2H7du3686Tra2t5ueff9Ydc+bMGXXM3r171fZff/2lsbKy0ly7dk13zOLFizXlypXTpKSkGOC7sExubm6ar7/+mufMRMTHx2vq1Kmj+ffffzUdOnTQvPrqq2o/z5/xmjZtmqZx48a5Pmbu540Z3lIm74okoyGXw7WsrKzU9t69ew06NsrbxYsXce3atRznTdbvlnIU7XmTj1LG0Lx5c90xcrycX8kIU+mIjY1VHytUqKA+yu+bZH2znzu5hFejRo0c565hw4bw8PDQHSPZ+7i4OF3GkUqOZI1WrlypsvJS2sBzZhrkyopk+7KfJ8HzZ9zOnTunSvdq1aqlrkpKiYIlnDcbQw/A0ty4cUP95579h0XIdnBwsMHGRfmTYFfkdt60j8lHqWfKzsbGRgVe2mOoZGVmZqo6wnbt2qFBgwZqn7z2dnZ26s1Ifucut3OrfYxKxsmTJ1WAK2VBUi+4Zs0aBAQE4NixYzxnRk7eoEhJnpQ03Iu/c8arVatWqgTBz89PlTPMmDEDDz30EE6dOmX2540BLxGZVcZJ/uPOXpNGxkv+6EpwK1n5X375BUOGDFF1g2TcIiIi8Oqrr6qaeZl4TaajV69euvtSdy0BsLe3N1avXq0mY5szljSUMnd3d1hbW98361G2PT09DTYuyp/23OR33uTjvRMPZeaqdG7guS15Y8aMURMFt27dqiZEaclrL6VEMTEx+Z673M6t9jEqGZJN8vX1RWBgoOq2IZNGP/30U54zIyeXvuX/umbNmqmrWHKTNyoyqVfuS8aP5880lC9fHnXr1kVoaKjZ/94x4DXAf/Dyn/vmzZtzXIaVbbm0R8apZs2a6pc5+3mTmiWpzdWeN/ko/1HIHwOtLVu2qPMr76KpZMgcQwl25XK4vN5yrrKT3zdbW9sc507alkndWvZzJ5fXs79hkeyVtMuSS+xUOuR3JSUlhefMyHXp0kW99pKd195k7oLUg2rv8/yZhoSEBJw/f1612zT73ztDz5qzRCtXrlSz+5ctW6Zm9o8cOVJTvnz5HLMeyTAzjo8ePapu8qvxySefqPthYWHq8Q8//FCdp7Vr12pOnDih6devn6ZmzZqaO3fu6J6jZ8+emqZNm2r279+v2bVrl5rB/MwzzxjwuzJ/L730ksbV1VWzbds2zdWrV3W3pKQk3TGjRo3S1KhRQ7NlyxbNoUOHNG3atFE3rfT0dE2DBg003bt31xw7dkyzYcMGTaVKlTSTJ0820Hdl/iZNmqQ6aVy8eFH9Psm2dDT5559/1OM8Z6Yle5cGwfNnnCZMmKD+r5Tfu927d2u6du2qcXd3V91tzP28MeA1kM8++0z9UNnZ2ak2Zfv27TP0kCze1q1bVaB7723IkCG61mRTpkzReHh4qDcsXbp00YSEhOR4jps3b6oA19nZWbVpGTp0qAqkqeTkds7k9u233+qOkTclo0ePVm2vHB0dNY8//rgKirO7dOmSplevXpqyZcuqPwDyhyEtLc0A35FlGDZsmMbb21v9Hyh/MOX3SRvsCp4z0w54ef6M08CBAzVeXl7q965q1apqOzQ01CLOWxn5x9BZZiIiIiKiksIaXiIiIiIyawx4iYiIiMisMeAlIiIiIrPGgJeIiIiIzBoDXiIiIiIyawx4iYiIiMisMeAlIiIiIrPGgJeI6AF8fHwwf/58WKqOHTvitddeK/LzXLp0CWXKlFHLz+Zl27Zt6hhZpjsvy5YtQ/ny5Ys8HiKyHAx4iajAnn/+eRWMfPjhhzn2//7772q/uTp48CBGjhxZpOe4ePEinn32WVSpUgUODg6oVq0a+vXrh+DgYJi6/AJQ+bmQnw9RvXp1XL16FQ0aNIAlBfpEZHgMeIlILxKsffTRR7h9+3apf+20tDQYQqVKleDo6FikcXfr1g2xsbH47bffEBISglWrVqFhw4b5ZjLNjbW1NTw9PWFjY2PooZis1NRUQw+ByCQx4CUivXTt2lUFLbNmzcr3uF27duGhhx5C2bJlVWZv7NixSExMzDXzpyVZQskWZr/8LYFhhw4dVKD9ww8/IDMzEzNnzlQZUnt7ezRp0gQbNmzQPYf28ySw7NSpkwpUGzdujL179+qOCQsLQ58+feDm5gYnJyfUr18ff/31V4FLGuT5v/76azz++OPq+evUqYM//vgjz88PCgrC+fPn8fnnn6N169bw9vZGu3bt8N5776ltrYiICDz11FPqdahQoYLKAMv3k93SpUvVeOV79/LywpgxY3SPhYeHq89xdnZGuXLl1HNdv35d9/j06dPV6/X999+r78nV1RVPP/004uPjdcfIORo8eLB6Dnn+uXPnorjkVtIgr3vdunXVz4mcr3u/XyE/EzVq1FCvtbzmN2/evO+YtWvXolmzZurnpFatWpgxYwbS09MLfc4K4s0331Rjl+eTrzllyhTdmzL5PqysrHDo0KEcnyM/R3L+5edYnDp1Cr169VKvt4eHBwYNGoQbN27kyDLLOZZMs7u7O3r06FGkMRNZKga8RKR3lu6DDz7AZ599hsuXL+d6jAR3PXv2xBNPPIETJ06ooFUC4OzBWUFNmjQJr776Ks6cOaP+2H/66acqCJszZ456btnXt29fnDt3Lsfnvf3223j99ddVcCVByTPPPKMLgF5++WWkpKRgx44dOHnypMpYS8ChDwmoJKCUMfTu3RvPPfccbt26lWeGWIKfX375BRkZGbkeI4GSfC8uLi7YuXMndu/ercYkr6M2q7d48WI1dimvkHFLwObr66sekwBKgl0Zw/bt2/Hvv//iwoULGDhw4H3nRt5o/Pnnn+omx2YvUZk4caLaJwHkP//8o2pqjxw5gpIgAX7//v3Vmw85T8OHD1fnO7v9+/fjhRdeUD87cowExfJGITt5vSRIl5+T06dPY8mSJSpIfv/99wt9zgpCzpV8Hfma8nP51VdfYd68eeoxeUMhbw6//fbbHJ8j21IaJD8Pkt3v3LkzmjZtqgJjeeMmb1BkjNl99913sLOzUz8TX3zxRaHHS2TRNEREBTRkyBBNv3791P3WrVtrhg0bpu6vWbNGk/2/kxdeeEEzcuTIHJ+7c+dOjZWVlebOnTtqW46Xz8vO1dVV8+2336r7Fy9eVMfMnz8/xzFVqlTRvP/++zn2tWjRQjN69Ogcn/f111/rHg8KClL7zpw5o7YbNmyomT59eoG/b29vb828efN02/Jc77zzjm47ISFB7fv777/zfI6FCxdqHB0dNS4uLppOnTppZs6cqTl//rzu8e+//17j5+enyczM1O1LSUnRlC1bVrNx40bd9/7222/n+vz//POPxtraWhMeHn7f933gwAG1PW3aNDWGuLg43TETJ07UtGrVSt2Pj4/X2NnZaVavXq17/ObNm2oMr776ap7fm5wz+TpOTk733bKfZ+25OXr0qNqePHmyJiAgIMdzvfnmm+qY27dvq+1nnnlG07t37xzHDBw4UP2saHXp0kXzwQcf5DhGXk8vL68inbMOHTrk+33fa/bs2ZrAwEDd9qpVqzRubm6a5ORktX348GFNmTJl1Osg3n33XU337t1zPEdERIQaV0hIiG4MTZs2LfAYiCh3zPASUaFIVlQyT5J5vdfx48dV5ksylNqbZC8lCymTt/TRvHlz3f24uDhcuXJFlQNkJ9v3jqNRo0a6+3JpXkRFRamPUl4hWUL5vGnTpqmMn76yP7+URUgJgfb5cyOZ2WvXrqmyjDZt2uDnn39WpQmSidW+ZqGhoSprqH3NpKwhOTlZZWXlueV779KlS67PL9+/lI7ITSsgIECVR2R/bSTzKF8j+2ujHbd8Hckmt2rVSve4jMHPz++Br4c8p2Rg773lR8aV/WsJeW30PUZeOylzyf7zNmLECDVBLikpqdDn7EHkyoX8DEmJj3zNd955R5WVaD322GPqisiaNWvUtvxOSIZazoF23Fu3bs0xbn9/f9250AoMDCz0GIkoC2cOEFGhPPzwwyqInTx5srpEm11CQgJefPFFFVjeS2oxtTWVWYm3/CelSWBSGLa2trr72g4S2rpJuXQuY1+/fr26bC/1yFIm8corrxTq+bVfQ/v8+QWFcvlebhJwyxjko0xok9dMAhsJiPMqiSgOhRl3Qcj4tOUVpU1eOylXkPKIe0lNb0l871ITLiUR8nXlPEo99MqVK3PUPEsZgpRaSBmDjO3HH39UpQ/Zxy0/C/Lm8V7aN2lF+R0gov8w4CWiQpPaT5kEdW8GUCYPSV1jfgGQBHGSgdOSGtzs2bjcSEZO2npJLaNMZNOS7ZYtW+o1dsmEjho1St0kaJf6S30C3qKSYEuyeXv27NG9ZpIxrFy5svo+cyOZwc2bN6ss4b3q1aunamLlps3yyjmQOlHJ9BZE7dq1VVAodbPaNybSjePs2bM5Xu/iImO+d+LYvn377jtGxpPfMfLaSeeL0gy45bzJ5DOpFc8+GfJe8uZK2rDJhEWpIc8elMu4f/31V3Ve2bmCqGSxpIGICk3aakmWa8GCBffNXpeAQDvRSIJZmQSVfdKaTNZZuHAhjh49qibsSOB5bwYuNzKpSjJiEhxKkCOTnORryISlgpIZ7xs3blTlFTIhSy4rS2BVUmR8MqFMJq1JECqlC998843quCD7hbyOMgtftmUSloxNJoxJllw7OVC6LEgGUV5veU1l7DJ5UMgEKe35kP0HDhxQ2UUJVLOXheRHLqnLBDF5jbds2aI6CGgnWJUEOefyfcjXk3MpGVBtlw4t+f5lMpdMUpRj5Wcme1cOMXXqVCxfvlxlW6UjhpRBSLZVSgyKKjo6+r4yDZlYJl0epHxBvo6UH8g50ZYuZCc/V9KJQ34nZOKkdKPIXuYik+Zkv/R6lueRn8uhQ4fmObmRiAqHAS8RFYnUTt57WVhqJWWmv2QGpTWZzEKXoESys1oSuEkmUh6XBRmko0JBet1KADR+/HhMmDBBBXgS/EiWUAKQgpJgQoINCUakC4J0cZAMXEmRFmqSxZOATOpRJbMnl7ZlW5shlO9dukZIZlWygDI2CT6lhleb8R0yZIhqayVjlfrfRx99VNedQjLG8qZCWq1JuYkEwNIqS94Y6GP27NnqnMildnmO9u3bl1gNqXyvkuGUrhHSOk46EEgHkOwkWJTsu7xecoyUoNwbyEpJgXSckMdatGihPke6JUgGtqgkCJef3+w3GY90Bhk3bpx6EydXOeQNnrQly42cR6mNHjZsWI792qsV8vPYvXt39fMsb8ak7rqk3mQQWaoyMnPN0IMgIiIyV++++66apFiYyZFEVDz4FpKIiKgEyKQ0KQuRMozSrA8novsx4CUiIioBUu4g5SCyWtq95QxEVLpY0kBEREREZo0ZXiIiIiIyawx4iYiIiMisMeAlIiIiIrPGgJeIiIiIzBoDXiIiIiIyawx4iYiIiMisMeAlIiIiIrPGgJeIiIiIzBoDXiIiIiKCOfs/m1MY4UB/tLUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Single-layer model (neurons={num_neurons}): CV accuracy = {test_acc:.3f}\")\n",
    "\n",
    "a = 0 \n",
    "\n",
    "for num_layers in second_layer_neurons:\n",
    "    print(f\"Two-layer model ({num_neurons}, {num_layers}): CV accuracy = {cv_mean_2nd[a]:.3f}\")\n",
    "    a += 1\n",
    "\n",
    "# --- Plotting results ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(second_layer_neurons, cv_mean_2nd, marker='o', label='Two-layer MLP')\n",
    "plt.xlabel('Neurons in Second Hidden Layer')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6423452",
   "metadata": {},
   "source": [
    "# For same number of neurons (i.e., 128), the cross-validation accuracy increased from 0.447 to 0.536 after addition of a hidden layer with same number of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f76b9",
   "metadata": {},
   "source": [
    "# 6. Train and tune a pipelined version of the MLPClassifier model, using scaling or PCA. Report and discuss if and how model performance changes compared to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "448bac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading list of algorithms to train ...\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#A pipelined MLPClassifier using scaling \n",
    "\n",
    "print('Reading list of algorithms to train ...')\n",
    "models = []\n",
    "models.append(( 'mlp', MLPClassifier(hidden_layer_sizes=(128,128), activation = 'relu',  \n",
    "                    alpha=0.001,  learning_rate_init=.001 , max_iter=100, \n",
    "                    random_state=seed, verbose=1) ))\n",
    "models.append(( 'scaled_mlp', make_pipeline( MinMaxScaler(), MLPClassifier(hidden_layer_sizes=(128,128), activation = 'relu',  \n",
    "                    alpha=0.001,  learning_rate_init=.001 , max_iter=100, \n",
    "                    random_state=seed, verbose=1)  )  ))\n",
    "#models.append(( 'scaled_SVM_std', make_pipeline( StandardScaler(), svm.SVC(random_state=seed) )  ))\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "84f8787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ++ NOW WORKING ON ALGORITHM mlp ++\n",
      "Splitting data into 5 folds\n",
      "Training model and validating it on each fold\n",
      "Iteration 1, loss = 7.49637266\n",
      "Iteration 2, loss = 2.45693386\n",
      "Iteration 3, loss = 2.23661772\n",
      "Iteration 4, loss = 2.19446292\n",
      "Iteration 5, loss = 1.90162731\n",
      "Iteration 6, loss = 1.73899395\n",
      "Iteration 7, loss = 1.59862758\n",
      "Iteration 8, loss = 1.44822955\n",
      "Iteration 9, loss = 1.32945126\n",
      "Iteration 10, loss = 1.24755697\n",
      "Iteration 11, loss = 1.24503695\n",
      "Iteration 12, loss = 1.21695555\n",
      "Iteration 13, loss = 1.19429985\n",
      "Iteration 14, loss = 1.18654512\n",
      "Iteration 15, loss = 1.16824678\n",
      "Iteration 16, loss = 1.15332091\n",
      "Iteration 17, loss = 1.15215087\n",
      "Iteration 18, loss = 1.14809146\n",
      "Iteration 19, loss = 1.14765869\n",
      "Iteration 20, loss = 1.14282129\n",
      "Iteration 21, loss = 1.14166499\n",
      "Iteration 22, loss = 1.14844400\n",
      "Iteration 23, loss = 1.13302001\n",
      "Iteration 24, loss = 1.13435912\n",
      "Iteration 25, loss = 1.11468504\n",
      "Iteration 26, loss = 1.12734942\n",
      "Iteration 27, loss = 1.14005594\n",
      "Iteration 28, loss = 1.12851300\n",
      "Iteration 29, loss = 1.12991000\n",
      "Iteration 30, loss = 1.11114069\n",
      "Iteration 31, loss = 1.10541166\n",
      "Iteration 32, loss = 1.09513266\n",
      "Iteration 33, loss = 1.10500240\n",
      "Iteration 34, loss = 1.09991350\n",
      "Iteration 35, loss = 1.14727414\n",
      "Iteration 36, loss = 1.18286631\n",
      "Iteration 37, loss = 1.14336800\n",
      "Iteration 38, loss = 1.15085806\n",
      "Iteration 39, loss = 1.12225594\n",
      "Iteration 40, loss = 1.10274823\n",
      "Iteration 41, loss = 1.12645161\n",
      "Iteration 42, loss = 1.11006070\n",
      "Iteration 43, loss = 1.10525447\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.27037133\n",
      "Iteration 2, loss = 2.59019799\n",
      "Iteration 3, loss = 2.34028168\n",
      "Iteration 4, loss = 2.07467744\n",
      "Iteration 5, loss = 1.93531746\n",
      "Iteration 6, loss = 1.71732258\n",
      "Iteration 7, loss = 1.56092068\n",
      "Iteration 8, loss = 1.43063440\n",
      "Iteration 9, loss = 1.30892010\n",
      "Iteration 10, loss = 1.27301746\n",
      "Iteration 11, loss = 1.26851585\n",
      "Iteration 12, loss = 1.22564051\n",
      "Iteration 13, loss = 1.20296780\n",
      "Iteration 14, loss = 1.20615309\n",
      "Iteration 15, loss = 1.20445858\n",
      "Iteration 16, loss = 1.18514443\n",
      "Iteration 17, loss = 1.18150482\n",
      "Iteration 18, loss = 1.16826866\n",
      "Iteration 19, loss = 1.16603632\n",
      "Iteration 20, loss = 1.15252716\n",
      "Iteration 21, loss = 1.15956021\n",
      "Iteration 22, loss = 1.15412301\n",
      "Iteration 23, loss = 1.15285825\n",
      "Iteration 24, loss = 1.14445825\n",
      "Iteration 25, loss = 1.13975031\n",
      "Iteration 26, loss = 1.13861233\n",
      "Iteration 27, loss = 1.15347823\n",
      "Iteration 28, loss = 1.13347727\n",
      "Iteration 29, loss = 1.13320136\n",
      "Iteration 30, loss = 1.13598846\n",
      "Iteration 31, loss = 1.13418509\n",
      "Iteration 32, loss = 1.12906028\n",
      "Iteration 33, loss = 1.14516444\n",
      "Iteration 34, loss = 1.15754377\n",
      "Iteration 35, loss = 1.12815957\n",
      "Iteration 36, loss = 1.12677501\n",
      "Iteration 37, loss = 1.12288005\n",
      "Iteration 38, loss = 1.13500254\n",
      "Iteration 39, loss = 1.12593507\n",
      "Iteration 40, loss = 1.16050160\n",
      "Iteration 41, loss = 1.13140155\n",
      "Iteration 42, loss = 1.14759876\n",
      "Iteration 43, loss = 1.12015050\n",
      "Iteration 44, loss = 1.12803706\n",
      "Iteration 45, loss = 1.10553548\n",
      "Iteration 46, loss = 1.12103187\n",
      "Iteration 47, loss = 1.10440354\n",
      "Iteration 48, loss = 1.09329617\n",
      "Iteration 49, loss = 1.09889260\n",
      "Iteration 50, loss = 1.10117888\n",
      "Iteration 51, loss = 1.10907514\n",
      "Iteration 52, loss = 1.09719344\n",
      "Iteration 53, loss = 1.09969970\n",
      "Iteration 54, loss = 1.13111316\n",
      "Iteration 55, loss = 1.11270394\n",
      "Iteration 56, loss = 1.11594424\n",
      "Iteration 57, loss = 1.14349794\n",
      "Iteration 58, loss = 1.17725024\n",
      "Iteration 59, loss = 1.14211111\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.34766474\n",
      "Iteration 2, loss = 2.81266496\n",
      "Iteration 3, loss = 2.35999245\n",
      "Iteration 4, loss = 2.26080775\n",
      "Iteration 5, loss = 1.99915404\n",
      "Iteration 6, loss = 1.76820543\n",
      "Iteration 7, loss = 1.58634870\n",
      "Iteration 8, loss = 1.45884504\n",
      "Iteration 9, loss = 1.30178195\n",
      "Iteration 10, loss = 1.25116479\n",
      "Iteration 11, loss = 1.27477780\n",
      "Iteration 12, loss = 1.21982783\n",
      "Iteration 13, loss = 1.21504154\n",
      "Iteration 14, loss = 1.22019318\n",
      "Iteration 15, loss = 1.19857369\n",
      "Iteration 16, loss = 1.20651949\n",
      "Iteration 17, loss = 1.18606067\n",
      "Iteration 18, loss = 1.17830740\n",
      "Iteration 19, loss = 1.17802641\n",
      "Iteration 20, loss = 1.16887482\n",
      "Iteration 21, loss = 1.16013545\n",
      "Iteration 22, loss = 1.16006151\n",
      "Iteration 23, loss = 1.16528433\n",
      "Iteration 24, loss = 1.16922772\n",
      "Iteration 25, loss = 1.16653680\n",
      "Iteration 26, loss = 1.16282652\n",
      "Iteration 27, loss = 1.14877228\n",
      "Iteration 28, loss = 1.14191722\n",
      "Iteration 29, loss = 1.15097129\n",
      "Iteration 30, loss = 1.14711466\n",
      "Iteration 31, loss = 1.15554753\n",
      "Iteration 32, loss = 1.13889698\n",
      "Iteration 33, loss = 1.14653961\n",
      "Iteration 34, loss = 1.14681770\n",
      "Iteration 35, loss = 1.17007399\n",
      "Iteration 36, loss = 1.13267581\n",
      "Iteration 37, loss = 1.12376222\n",
      "Iteration 38, loss = 1.11207272\n",
      "Iteration 39, loss = 1.12409005\n",
      "Iteration 40, loss = 1.13971311\n",
      "Iteration 41, loss = 1.11130370\n",
      "Iteration 42, loss = 1.11707024\n",
      "Iteration 43, loss = 1.15041274\n",
      "Iteration 44, loss = 1.12801466\n",
      "Iteration 45, loss = 1.11271521\n",
      "Iteration 46, loss = 1.11768392\n",
      "Iteration 47, loss = 1.10416005\n",
      "Iteration 48, loss = 1.10263196\n",
      "Iteration 49, loss = 1.11132154\n",
      "Iteration 50, loss = 1.10090578\n",
      "Iteration 51, loss = 1.13874292\n",
      "Iteration 52, loss = 1.11871010\n",
      "Iteration 53, loss = 1.10110512\n",
      "Iteration 54, loss = 1.08826976\n",
      "Iteration 55, loss = 1.09785492\n",
      "Iteration 56, loss = 1.09416664\n",
      "Iteration 57, loss = 1.09329087\n",
      "Iteration 58, loss = 1.09047655\n",
      "Iteration 59, loss = 1.09005530\n",
      "Iteration 60, loss = 1.08208173\n",
      "Iteration 61, loss = 1.10836514\n",
      "Iteration 62, loss = 1.09805348\n",
      "Iteration 63, loss = 1.14331113\n",
      "Iteration 64, loss = 1.14724994\n",
      "Iteration 65, loss = 1.13122437\n",
      "Iteration 66, loss = 1.11801078\n",
      "Iteration 67, loss = 1.07542197\n",
      "Iteration 68, loss = 1.08787441\n",
      "Iteration 69, loss = 1.09701869\n",
      "Iteration 70, loss = 1.09733319\n",
      "Iteration 71, loss = 1.09075512\n",
      "Iteration 72, loss = 1.07964597\n",
      "Iteration 73, loss = 1.11016635\n",
      "Iteration 74, loss = 1.11102860\n",
      "Iteration 75, loss = 1.15657690\n",
      "Iteration 76, loss = 1.11078144\n",
      "Iteration 77, loss = 1.09488008\n",
      "Iteration 78, loss = 1.08664528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.44396212\n",
      "Iteration 2, loss = 2.61262146\n",
      "Iteration 3, loss = 2.22951312\n",
      "Iteration 4, loss = 2.18917891\n",
      "Iteration 5, loss = 1.91613973\n",
      "Iteration 6, loss = 1.74893178\n",
      "Iteration 7, loss = 1.64852466\n",
      "Iteration 8, loss = 1.48860738\n",
      "Iteration 9, loss = 1.38146846\n",
      "Iteration 10, loss = 1.33585404\n",
      "Iteration 11, loss = 1.29037145\n",
      "Iteration 12, loss = 1.23372777\n",
      "Iteration 13, loss = 1.22028473\n",
      "Iteration 14, loss = 1.20964276\n",
      "Iteration 15, loss = 1.21517990\n",
      "Iteration 16, loss = 1.21970149\n",
      "Iteration 17, loss = 1.19637096\n",
      "Iteration 18, loss = 1.19650656\n",
      "Iteration 19, loss = 1.18008511\n",
      "Iteration 20, loss = 1.15968319\n",
      "Iteration 21, loss = 1.15800984\n",
      "Iteration 22, loss = 1.14448033\n",
      "Iteration 23, loss = 1.14905300\n",
      "Iteration 24, loss = 1.15076963\n",
      "Iteration 25, loss = 1.13027387\n",
      "Iteration 26, loss = 1.14697874\n",
      "Iteration 27, loss = 1.13739474\n",
      "Iteration 28, loss = 1.12626928\n",
      "Iteration 29, loss = 1.12819029\n",
      "Iteration 30, loss = 1.13393021\n",
      "Iteration 31, loss = 1.13259697\n",
      "Iteration 32, loss = 1.13338049\n",
      "Iteration 33, loss = 1.12318877\n",
      "Iteration 34, loss = 1.12991906\n",
      "Iteration 35, loss = 1.12333908\n",
      "Iteration 36, loss = 1.10369446\n",
      "Iteration 37, loss = 1.10154677\n",
      "Iteration 38, loss = 1.10418725\n",
      "Iteration 39, loss = 1.10407358\n",
      "Iteration 40, loss = 1.09616111\n",
      "Iteration 41, loss = 1.09850365\n",
      "Iteration 42, loss = 1.10655314\n",
      "Iteration 43, loss = 1.13468277\n",
      "Iteration 44, loss = 1.10967476\n",
      "Iteration 45, loss = 1.13051686\n",
      "Iteration 46, loss = 1.10987437\n",
      "Iteration 47, loss = 1.08768376\n",
      "Iteration 48, loss = 1.09413195\n",
      "Iteration 49, loss = 1.10743996\n",
      "Iteration 50, loss = 1.09499034\n",
      "Iteration 51, loss = 1.11133916\n",
      "Iteration 52, loss = 1.08583265\n",
      "Iteration 53, loss = 1.08559167\n",
      "Iteration 54, loss = 1.08363675\n",
      "Iteration 55, loss = 1.07948444\n",
      "Iteration 56, loss = 1.07780225\n",
      "Iteration 57, loss = 1.09431150\n",
      "Iteration 58, loss = 1.10365621\n",
      "Iteration 59, loss = 1.10819452\n",
      "Iteration 60, loss = 1.11688903\n",
      "Iteration 61, loss = 1.11226165\n",
      "Iteration 62, loss = 1.13348034\n",
      "Iteration 63, loss = 1.10771144\n",
      "Iteration 64, loss = 1.17558279\n",
      "Iteration 65, loss = 1.12273215\n",
      "Iteration 66, loss = 1.14249262\n",
      "Iteration 67, loss = 1.11196810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.53049419\n",
      "Iteration 2, loss = 2.45037238\n",
      "Iteration 3, loss = 2.30013913\n",
      "Iteration 4, loss = 2.06685424\n",
      "Iteration 5, loss = 1.98425654\n",
      "Iteration 6, loss = 1.80207269\n",
      "Iteration 7, loss = 1.55063675\n",
      "Iteration 8, loss = 1.41826375\n",
      "Iteration 9, loss = 1.32095057\n",
      "Iteration 10, loss = 1.29279986\n",
      "Iteration 11, loss = 1.22090039\n",
      "Iteration 12, loss = 1.23259743\n",
      "Iteration 13, loss = 1.22368240\n",
      "Iteration 14, loss = 1.20065359\n",
      "Iteration 15, loss = 1.18938620\n",
      "Iteration 16, loss = 1.18447095\n",
      "Iteration 17, loss = 1.18011880\n",
      "Iteration 18, loss = 1.16768258\n",
      "Iteration 19, loss = 1.16499891\n",
      "Iteration 20, loss = 1.16412442\n",
      "Iteration 21, loss = 1.15911464\n",
      "Iteration 22, loss = 1.15063153\n",
      "Iteration 23, loss = 1.15811947\n",
      "Iteration 24, loss = 1.14634291\n",
      "Iteration 25, loss = 1.15881952\n",
      "Iteration 26, loss = 1.17643749\n",
      "Iteration 27, loss = 1.15662118\n",
      "Iteration 28, loss = 1.14387431\n",
      "Iteration 29, loss = 1.13549034\n",
      "Iteration 30, loss = 1.14063856\n",
      "Iteration 31, loss = 1.15100748\n",
      "Iteration 32, loss = 1.17714659\n",
      "Iteration 33, loss = 1.16418181\n",
      "Iteration 34, loss = 1.13991129\n",
      "Iteration 35, loss = 1.12966030\n",
      "Iteration 36, loss = 1.12025206\n",
      "Iteration 37, loss = 1.12729288\n",
      "Iteration 38, loss = 1.12624813\n",
      "Iteration 39, loss = 1.13064679\n",
      "Iteration 40, loss = 1.13957274\n",
      "Iteration 41, loss = 1.12356376\n",
      "Iteration 42, loss = 1.12425730\n",
      "Iteration 43, loss = 1.15303837\n",
      "Iteration 44, loss = 1.13311104\n",
      "Iteration 45, loss = 1.13397314\n",
      "Iteration 46, loss = 1.13542403\n",
      "Iteration 47, loss = 1.16590567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "algorithm mlp f1_micro results: mean = 0.489728 (std = 0.014510)\n",
      "\n",
      "\n",
      " ++ NOW WORKING ON ALGORITHM scaled_mlp ++\n",
      "Splitting data into 5 folds\n",
      "Training model and validating it on each fold\n",
      "Iteration 1, loss = 1.61789518\n",
      "Iteration 2, loss = 1.48255728\n",
      "Iteration 3, loss = 1.37989687\n",
      "Iteration 4, loss = 1.30465656\n",
      "Iteration 5, loss = 1.24875398\n",
      "Iteration 6, loss = 1.20842394\n",
      "Iteration 7, loss = 1.18291528\n",
      "Iteration 8, loss = 1.16468266\n",
      "Iteration 9, loss = 1.15235919\n",
      "Iteration 10, loss = 1.14038255\n",
      "Iteration 11, loss = 1.12967526\n",
      "Iteration 12, loss = 1.11850523\n",
      "Iteration 13, loss = 1.10759144\n",
      "Iteration 14, loss = 1.09833924\n",
      "Iteration 15, loss = 1.09060176\n",
      "Iteration 16, loss = 1.08191298\n",
      "Iteration 17, loss = 1.07615068\n",
      "Iteration 18, loss = 1.06856507\n",
      "Iteration 19, loss = 1.06468094\n",
      "Iteration 20, loss = 1.05855475\n",
      "Iteration 21, loss = 1.05517553\n",
      "Iteration 22, loss = 1.05069111\n",
      "Iteration 23, loss = 1.04758032\n",
      "Iteration 24, loss = 1.04324497\n",
      "Iteration 25, loss = 1.03887797\n",
      "Iteration 26, loss = 1.03961582\n",
      "Iteration 27, loss = 1.03407800\n",
      "Iteration 28, loss = 1.03179998\n",
      "Iteration 29, loss = 1.02808795\n",
      "Iteration 30, loss = 1.02800979\n",
      "Iteration 31, loss = 1.02336588\n",
      "Iteration 32, loss = 1.02255570\n",
      "Iteration 33, loss = 1.01852616\n",
      "Iteration 34, loss = 1.02093637\n",
      "Iteration 35, loss = 1.01985756\n",
      "Iteration 36, loss = 1.01454815\n",
      "Iteration 37, loss = 1.01196049\n",
      "Iteration 38, loss = 1.00949176\n",
      "Iteration 39, loss = 1.00620066\n",
      "Iteration 40, loss = 1.00524566\n",
      "Iteration 41, loss = 1.00346160\n",
      "Iteration 42, loss = 1.00056581\n",
      "Iteration 43, loss = 0.99879300\n",
      "Iteration 44, loss = 1.00088824\n",
      "Iteration 45, loss = 0.99677966\n",
      "Iteration 46, loss = 0.99364045\n",
      "Iteration 47, loss = 0.99418731\n",
      "Iteration 48, loss = 0.99034302\n",
      "Iteration 49, loss = 0.98846318\n",
      "Iteration 50, loss = 0.98683257\n",
      "Iteration 51, loss = 0.98405005\n",
      "Iteration 52, loss = 0.98278877\n",
      "Iteration 53, loss = 0.98171357\n",
      "Iteration 54, loss = 0.98190174\n",
      "Iteration 55, loss = 0.97838208\n",
      "Iteration 56, loss = 0.97855580\n",
      "Iteration 57, loss = 0.97605063\n",
      "Iteration 58, loss = 0.97615320\n",
      "Iteration 59, loss = 0.97261104\n",
      "Iteration 60, loss = 0.97151250\n",
      "Iteration 61, loss = 0.96899176\n",
      "Iteration 62, loss = 0.96829880\n",
      "Iteration 63, loss = 0.96786393\n",
      "Iteration 64, loss = 0.96587830\n",
      "Iteration 65, loss = 0.96427060\n",
      "Iteration 66, loss = 0.96243225\n",
      "Iteration 67, loss = 0.96244736\n",
      "Iteration 68, loss = 0.96114371\n",
      "Iteration 69, loss = 0.96020620\n",
      "Iteration 70, loss = 0.95598817\n",
      "Iteration 71, loss = 0.95845558\n",
      "Iteration 72, loss = 0.95360557\n",
      "Iteration 73, loss = 0.95594540\n",
      "Iteration 74, loss = 0.95132567\n",
      "Iteration 75, loss = 0.95155954\n",
      "Iteration 76, loss = 0.95031217\n",
      "Iteration 77, loss = 0.94943869\n",
      "Iteration 78, loss = 0.94669902\n",
      "Iteration 79, loss = 0.94482230\n",
      "Iteration 80, loss = 0.94450029\n",
      "Iteration 81, loss = 0.94331001\n",
      "Iteration 82, loss = 0.94105297\n",
      "Iteration 83, loss = 0.93984846\n",
      "Iteration 84, loss = 0.93748475\n",
      "Iteration 85, loss = 0.93765057\n",
      "Iteration 86, loss = 0.93583331\n",
      "Iteration 87, loss = 0.93397204\n",
      "Iteration 88, loss = 0.93799932\n",
      "Iteration 89, loss = 0.93098713\n",
      "Iteration 90, loss = 0.93273516\n",
      "Iteration 91, loss = 0.93759163\n",
      "Iteration 92, loss = 0.93112987\n",
      "Iteration 93, loss = 0.93104723\n",
      "Iteration 94, loss = 0.92761112\n",
      "Iteration 95, loss = 0.92534647\n",
      "Iteration 96, loss = 0.92725154\n",
      "Iteration 97, loss = 0.92241082\n",
      "Iteration 98, loss = 0.92181740\n",
      "Iteration 99, loss = 0.91960732\n",
      "Iteration 100, loss = 0.92148195\n",
      "Iteration 1, loss = 1.60582817\n",
      "Iteration 2, loss = 1.47492620\n",
      "Iteration 3, loss = 1.37444378\n",
      "Iteration 4, loss = 1.30832541\n",
      "Iteration 5, loss = 1.25634464\n",
      "Iteration 6, loss = 1.22081657\n",
      "Iteration 7, loss = 1.19190731\n",
      "Iteration 8, loss = 1.17374913\n",
      "Iteration 9, loss = 1.16018653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 1.14899994\n",
      "Iteration 11, loss = 1.13960389\n",
      "Iteration 12, loss = 1.12838055\n",
      "Iteration 13, loss = 1.11815089\n",
      "Iteration 14, loss = 1.10782336\n",
      "Iteration 15, loss = 1.09954903\n",
      "Iteration 16, loss = 1.09132053\n",
      "Iteration 17, loss = 1.08577341\n",
      "Iteration 18, loss = 1.07682437\n",
      "Iteration 19, loss = 1.07194256\n",
      "Iteration 20, loss = 1.06645431\n",
      "Iteration 21, loss = 1.06080534\n",
      "Iteration 22, loss = 1.05671608\n",
      "Iteration 23, loss = 1.05215406\n",
      "Iteration 24, loss = 1.04974032\n",
      "Iteration 25, loss = 1.04511273\n",
      "Iteration 26, loss = 1.04110561\n",
      "Iteration 27, loss = 1.03767524\n",
      "Iteration 28, loss = 1.03327628\n",
      "Iteration 29, loss = 1.03199396\n",
      "Iteration 30, loss = 1.02767217\n",
      "Iteration 31, loss = 1.02528700\n",
      "Iteration 32, loss = 1.02129780\n",
      "Iteration 33, loss = 1.02051615\n",
      "Iteration 34, loss = 1.01838126\n",
      "Iteration 35, loss = 1.01422461\n",
      "Iteration 36, loss = 1.01241012\n",
      "Iteration 37, loss = 1.00911473\n",
      "Iteration 38, loss = 1.00751782\n",
      "Iteration 39, loss = 1.00642827\n",
      "Iteration 40, loss = 1.00307288\n",
      "Iteration 41, loss = 1.00342834\n",
      "Iteration 42, loss = 0.99863214\n",
      "Iteration 43, loss = 1.00339029\n",
      "Iteration 44, loss = 0.99465142\n",
      "Iteration 45, loss = 0.99666496\n",
      "Iteration 46, loss = 0.99337997\n",
      "Iteration 47, loss = 0.99086148\n",
      "Iteration 48, loss = 0.98987570\n",
      "Iteration 49, loss = 0.98784519\n",
      "Iteration 50, loss = 0.98656289\n",
      "Iteration 51, loss = 0.98561440\n",
      "Iteration 52, loss = 0.98340600\n",
      "Iteration 53, loss = 0.98229584\n",
      "Iteration 54, loss = 0.98156907\n",
      "Iteration 55, loss = 0.98023475\n",
      "Iteration 56, loss = 0.98265720\n",
      "Iteration 57, loss = 0.98112372\n",
      "Iteration 58, loss = 0.97773559\n",
      "Iteration 59, loss = 0.97691239\n",
      "Iteration 60, loss = 0.97545294\n",
      "Iteration 61, loss = 0.97270398\n",
      "Iteration 62, loss = 0.97251039\n",
      "Iteration 63, loss = 0.97133925\n",
      "Iteration 64, loss = 0.97027325\n",
      "Iteration 65, loss = 0.96936289\n",
      "Iteration 66, loss = 0.96896917\n",
      "Iteration 67, loss = 0.96726439\n",
      "Iteration 68, loss = 0.96613532\n",
      "Iteration 69, loss = 0.96478083\n",
      "Iteration 70, loss = 0.96339512\n",
      "Iteration 71, loss = 0.96247282\n",
      "Iteration 72, loss = 0.96023631\n",
      "Iteration 73, loss = 0.95938146\n",
      "Iteration 74, loss = 0.95958612\n",
      "Iteration 75, loss = 0.95801586\n",
      "Iteration 76, loss = 0.95695290\n",
      "Iteration 77, loss = 0.95589355\n",
      "Iteration 78, loss = 0.95438884\n",
      "Iteration 79, loss = 0.95327355\n",
      "Iteration 80, loss = 0.95228012\n",
      "Iteration 81, loss = 0.95080016\n",
      "Iteration 82, loss = 0.95061674\n",
      "Iteration 83, loss = 0.94901890\n",
      "Iteration 84, loss = 0.94764306\n",
      "Iteration 85, loss = 0.94697376\n",
      "Iteration 86, loss = 0.94562123\n",
      "Iteration 87, loss = 0.94570287\n",
      "Iteration 88, loss = 0.94353531\n",
      "Iteration 89, loss = 0.94325818\n",
      "Iteration 90, loss = 0.94305748\n",
      "Iteration 91, loss = 0.94253611\n",
      "Iteration 92, loss = 0.94051016\n",
      "Iteration 93, loss = 0.94213144\n",
      "Iteration 94, loss = 0.93903020\n",
      "Iteration 95, loss = 0.93716071\n",
      "Iteration 96, loss = 0.93733680\n",
      "Iteration 97, loss = 0.93710735\n",
      "Iteration 98, loss = 0.93492706\n",
      "Iteration 99, loss = 0.93446500\n",
      "Iteration 100, loss = 0.93349497\n",
      "Iteration 1, loss = 1.60823903\n",
      "Iteration 2, loss = 1.47313246\n",
      "Iteration 3, loss = 1.37343960\n",
      "Iteration 4, loss = 1.30092702\n",
      "Iteration 5, loss = 1.24580205\n",
      "Iteration 6, loss = 1.20818776\n",
      "Iteration 7, loss = 1.18241126\n",
      "Iteration 8, loss = 1.16269751\n",
      "Iteration 9, loss = 1.14876108\n",
      "Iteration 10, loss = 1.13907631\n",
      "Iteration 11, loss = 1.12769497\n",
      "Iteration 12, loss = 1.11575120\n",
      "Iteration 13, loss = 1.10636894\n",
      "Iteration 14, loss = 1.09788658\n",
      "Iteration 15, loss = 1.08895229\n",
      "Iteration 16, loss = 1.08031494\n",
      "Iteration 17, loss = 1.07455555\n",
      "Iteration 18, loss = 1.06769804\n",
      "Iteration 19, loss = 1.06219473\n",
      "Iteration 20, loss = 1.05737331\n",
      "Iteration 21, loss = 1.05225470\n",
      "Iteration 22, loss = 1.04784518\n",
      "Iteration 23, loss = 1.04665133\n",
      "Iteration 24, loss = 1.04013749\n",
      "Iteration 25, loss = 1.03756511\n",
      "Iteration 26, loss = 1.03327673\n",
      "Iteration 27, loss = 1.03109535\n",
      "Iteration 28, loss = 1.02690895\n",
      "Iteration 29, loss = 1.02549040\n",
      "Iteration 30, loss = 1.02139735\n",
      "Iteration 31, loss = 1.02197244\n",
      "Iteration 32, loss = 1.01992034\n",
      "Iteration 33, loss = 1.01750520\n",
      "Iteration 34, loss = 1.01360203\n",
      "Iteration 35, loss = 1.01144943\n",
      "Iteration 36, loss = 1.00653189\n",
      "Iteration 37, loss = 1.00529177\n",
      "Iteration 38, loss = 1.00561704\n",
      "Iteration 39, loss = 1.00213064\n",
      "Iteration 40, loss = 0.99934469\n",
      "Iteration 41, loss = 0.99614149\n",
      "Iteration 42, loss = 0.99565113\n",
      "Iteration 43, loss = 0.99448413\n",
      "Iteration 44, loss = 0.99051484\n",
      "Iteration 45, loss = 0.98827925\n",
      "Iteration 46, loss = 0.98703357\n",
      "Iteration 47, loss = 0.98652341\n",
      "Iteration 48, loss = 0.98175614\n",
      "Iteration 49, loss = 0.98029520\n",
      "Iteration 50, loss = 0.98348583\n",
      "Iteration 51, loss = 0.97637197\n",
      "Iteration 52, loss = 0.97515320\n",
      "Iteration 53, loss = 0.97311307\n",
      "Iteration 54, loss = 0.97036204\n",
      "Iteration 55, loss = 0.96862553\n",
      "Iteration 56, loss = 0.96757593\n",
      "Iteration 57, loss = 0.96446641\n",
      "Iteration 58, loss = 0.96435546\n",
      "Iteration 59, loss = 0.96381359\n",
      "Iteration 60, loss = 0.96216505\n",
      "Iteration 61, loss = 0.95851421\n",
      "Iteration 62, loss = 0.95711378\n",
      "Iteration 63, loss = 0.95812342\n",
      "Iteration 64, loss = 0.95386379\n",
      "Iteration 65, loss = 0.95318534\n",
      "Iteration 66, loss = 0.95180097\n",
      "Iteration 67, loss = 0.95052103\n",
      "Iteration 68, loss = 0.95223252\n",
      "Iteration 69, loss = 0.94772266\n",
      "Iteration 70, loss = 0.94722521\n",
      "Iteration 71, loss = 0.94505866\n",
      "Iteration 72, loss = 0.94296876\n",
      "Iteration 73, loss = 0.94186873\n",
      "Iteration 74, loss = 0.94164236\n",
      "Iteration 75, loss = 0.93900036\n",
      "Iteration 76, loss = 0.93875851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77, loss = 0.93799118\n",
      "Iteration 78, loss = 0.93879428\n",
      "Iteration 79, loss = 0.93519132\n",
      "Iteration 80, loss = 0.93397127\n",
      "Iteration 81, loss = 0.93145178\n",
      "Iteration 82, loss = 0.93089312\n",
      "Iteration 83, loss = 0.93028909\n",
      "Iteration 84, loss = 0.92923132\n",
      "Iteration 85, loss = 0.92998909\n",
      "Iteration 86, loss = 0.92644946\n",
      "Iteration 87, loss = 0.92621271\n",
      "Iteration 88, loss = 0.92614931\n",
      "Iteration 89, loss = 0.92347603\n",
      "Iteration 90, loss = 0.92292522\n",
      "Iteration 91, loss = 0.92342401\n",
      "Iteration 92, loss = 0.92268363\n",
      "Iteration 93, loss = 0.91989289\n",
      "Iteration 94, loss = 0.91812963\n",
      "Iteration 95, loss = 0.91768391\n",
      "Iteration 96, loss = 0.91559602\n",
      "Iteration 97, loss = 0.91524817\n",
      "Iteration 98, loss = 0.91443090\n",
      "Iteration 99, loss = 0.91439488\n",
      "Iteration 100, loss = 0.91234293\n",
      "Iteration 1, loss = 1.61701239\n",
      "Iteration 2, loss = 1.47786030\n",
      "Iteration 3, loss = 1.37619022\n",
      "Iteration 4, loss = 1.29890763\n",
      "Iteration 5, loss = 1.24384699\n",
      "Iteration 6, loss = 1.20514180\n",
      "Iteration 7, loss = 1.17881392\n",
      "Iteration 8, loss = 1.15995156\n",
      "Iteration 9, loss = 1.15084293\n",
      "Iteration 10, loss = 1.14043964\n",
      "Iteration 11, loss = 1.13243380\n",
      "Iteration 12, loss = 1.12123953\n",
      "Iteration 13, loss = 1.11225083\n",
      "Iteration 14, loss = 1.10304873\n",
      "Iteration 15, loss = 1.09527016\n",
      "Iteration 16, loss = 1.08799484\n",
      "Iteration 17, loss = 1.08015894\n",
      "Iteration 18, loss = 1.07409332\n",
      "Iteration 19, loss = 1.07016022\n",
      "Iteration 20, loss = 1.06451198\n",
      "Iteration 21, loss = 1.06061762\n",
      "Iteration 22, loss = 1.05795113\n",
      "Iteration 23, loss = 1.05204178\n",
      "Iteration 24, loss = 1.04865148\n",
      "Iteration 25, loss = 1.04494457\n",
      "Iteration 26, loss = 1.04284005\n",
      "Iteration 27, loss = 1.04183536\n",
      "Iteration 28, loss = 1.03617950\n",
      "Iteration 29, loss = 1.03428708\n",
      "Iteration 30, loss = 1.03170034\n",
      "Iteration 31, loss = 1.02966458\n",
      "Iteration 32, loss = 1.02697687\n",
      "Iteration 33, loss = 1.02662590\n",
      "Iteration 34, loss = 1.02309348\n",
      "Iteration 35, loss = 1.02218360\n",
      "Iteration 36, loss = 1.01843245\n",
      "Iteration 37, loss = 1.01827479\n",
      "Iteration 38, loss = 1.01537343\n",
      "Iteration 39, loss = 1.01403541\n",
      "Iteration 40, loss = 1.01072670\n",
      "Iteration 41, loss = 1.01117241\n",
      "Iteration 42, loss = 1.00919296\n",
      "Iteration 43, loss = 1.00741626\n",
      "Iteration 44, loss = 1.00636250\n",
      "Iteration 45, loss = 1.00392349\n",
      "Iteration 46, loss = 1.00287319\n",
      "Iteration 47, loss = 1.00116888\n",
      "Iteration 48, loss = 1.00032360\n",
      "Iteration 49, loss = 0.99724829\n",
      "Iteration 50, loss = 0.99554582\n",
      "Iteration 51, loss = 0.99437949\n",
      "Iteration 52, loss = 0.99250234\n",
      "Iteration 53, loss = 0.99112205\n",
      "Iteration 54, loss = 0.98940276\n",
      "Iteration 55, loss = 0.98796670\n",
      "Iteration 56, loss = 0.98745904\n",
      "Iteration 57, loss = 0.98569445\n",
      "Iteration 58, loss = 0.98745142\n",
      "Iteration 59, loss = 0.98609624\n",
      "Iteration 60, loss = 0.98098766\n",
      "Iteration 61, loss = 0.98278383\n",
      "Iteration 62, loss = 0.97780803\n",
      "Iteration 63, loss = 0.97997286\n",
      "Iteration 64, loss = 0.97535424\n",
      "Iteration 65, loss = 0.97690951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66, loss = 0.97387107\n",
      "Iteration 67, loss = 0.97434873\n",
      "Iteration 68, loss = 0.97225765\n",
      "Iteration 69, loss = 0.97038405\n",
      "Iteration 70, loss = 0.96898657\n",
      "Iteration 71, loss = 0.96766194\n",
      "Iteration 72, loss = 0.96684526\n",
      "Iteration 73, loss = 0.96929219\n",
      "Iteration 74, loss = 0.96278558\n",
      "Iteration 75, loss = 0.96468075\n",
      "Iteration 76, loss = 0.96117320\n",
      "Iteration 77, loss = 0.96181480\n",
      "Iteration 78, loss = 0.95990594\n",
      "Iteration 79, loss = 0.95821165\n",
      "Iteration 80, loss = 0.95739161\n",
      "Iteration 81, loss = 0.95748705\n",
      "Iteration 82, loss = 0.95855544\n",
      "Iteration 83, loss = 0.95348305\n",
      "Iteration 84, loss = 0.95544378\n",
      "Iteration 85, loss = 0.95023568\n",
      "Iteration 86, loss = 0.95463460\n",
      "Iteration 87, loss = 0.94896996\n",
      "Iteration 88, loss = 0.95129011\n",
      "Iteration 89, loss = 0.94804195\n",
      "Iteration 90, loss = 0.94755278\n",
      "Iteration 91, loss = 0.94633191\n",
      "Iteration 92, loss = 0.94326930\n",
      "Iteration 93, loss = 0.94433780\n",
      "Iteration 94, loss = 0.94306484\n",
      "Iteration 95, loss = 0.94187686\n",
      "Iteration 96, loss = 0.93916974\n",
      "Iteration 97, loss = 0.93882806\n",
      "Iteration 98, loss = 0.93821835\n",
      "Iteration 99, loss = 0.93633914\n",
      "Iteration 100, loss = 0.93551852\n",
      "Iteration 1, loss = 1.61144574\n",
      "Iteration 2, loss = 1.48014831\n",
      "Iteration 3, loss = 1.38520477\n",
      "Iteration 4, loss = 1.31366331\n",
      "Iteration 5, loss = 1.26038195\n",
      "Iteration 6, loss = 1.22270586\n",
      "Iteration 7, loss = 1.19321669\n",
      "Iteration 8, loss = 1.17487766\n",
      "Iteration 9, loss = 1.16280286\n",
      "Iteration 10, loss = 1.15274585\n",
      "Iteration 11, loss = 1.14337496\n",
      "Iteration 12, loss = 1.13335202\n",
      "Iteration 13, loss = 1.12316814\n",
      "Iteration 14, loss = 1.11370426\n",
      "Iteration 15, loss = 1.10531916\n",
      "Iteration 16, loss = 1.09636057\n",
      "Iteration 17, loss = 1.08944206\n",
      "Iteration 18, loss = 1.08281008\n",
      "Iteration 19, loss = 1.07744488\n",
      "Iteration 20, loss = 1.07164459\n",
      "Iteration 21, loss = 1.06603501\n",
      "Iteration 22, loss = 1.06277569\n",
      "Iteration 23, loss = 1.05654529\n",
      "Iteration 24, loss = 1.05429410\n",
      "Iteration 25, loss = 1.05125648\n",
      "Iteration 26, loss = 1.04582610\n",
      "Iteration 27, loss = 1.04322821\n",
      "Iteration 28, loss = 1.03950533\n",
      "Iteration 29, loss = 1.03658773\n",
      "Iteration 30, loss = 1.03445322\n",
      "Iteration 31, loss = 1.03375874\n",
      "Iteration 32, loss = 1.02778971\n",
      "Iteration 33, loss = 1.02765021\n",
      "Iteration 34, loss = 1.02287407\n",
      "Iteration 35, loss = 1.02312585\n",
      "Iteration 36, loss = 1.02027738\n",
      "Iteration 37, loss = 1.01725135\n",
      "Iteration 38, loss = 1.01515112\n",
      "Iteration 39, loss = 1.01374760\n",
      "Iteration 40, loss = 1.01098624\n",
      "Iteration 41, loss = 1.00939587\n",
      "Iteration 42, loss = 1.00722482\n",
      "Iteration 43, loss = 1.00526234\n",
      "Iteration 44, loss = 1.00373914\n",
      "Iteration 45, loss = 1.00398277\n",
      "Iteration 46, loss = 0.99999821\n",
      "Iteration 47, loss = 1.00025079\n",
      "Iteration 48, loss = 0.99748525\n",
      "Iteration 49, loss = 0.99455992\n",
      "Iteration 50, loss = 0.99378556\n",
      "Iteration 51, loss = 0.99083680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.99080867\n",
      "Iteration 53, loss = 0.98772408\n",
      "Iteration 54, loss = 0.98706310\n",
      "Iteration 55, loss = 0.98386669\n",
      "Iteration 56, loss = 0.98371303\n",
      "Iteration 57, loss = 0.98090929\n",
      "Iteration 58, loss = 0.98059167\n",
      "Iteration 59, loss = 0.97856425\n",
      "Iteration 60, loss = 0.97636880\n",
      "Iteration 61, loss = 0.97471704\n",
      "Iteration 62, loss = 0.97419016\n",
      "Iteration 63, loss = 0.97138777\n",
      "Iteration 64, loss = 0.97128100\n",
      "Iteration 65, loss = 0.96971795\n",
      "Iteration 66, loss = 0.96851029\n",
      "Iteration 67, loss = 0.96607555\n",
      "Iteration 68, loss = 0.96641245\n",
      "Iteration 69, loss = 0.96472355\n",
      "Iteration 70, loss = 0.96219711\n",
      "Iteration 71, loss = 0.96479273\n",
      "Iteration 72, loss = 0.95946921\n",
      "Iteration 73, loss = 0.96218192\n",
      "Iteration 74, loss = 0.95877735\n",
      "Iteration 75, loss = 0.95711893\n",
      "Iteration 76, loss = 0.95562658\n",
      "Iteration 77, loss = 0.95456772\n",
      "Iteration 78, loss = 0.95300696\n",
      "Iteration 79, loss = 0.95129816\n",
      "Iteration 80, loss = 0.94994078\n",
      "Iteration 81, loss = 0.94913594\n",
      "Iteration 82, loss = 0.95033436\n",
      "Iteration 83, loss = 0.94750586\n",
      "Iteration 84, loss = 0.94529970\n",
      "Iteration 85, loss = 0.94395093\n",
      "Iteration 86, loss = 0.94399284\n",
      "Iteration 87, loss = 0.94212804\n",
      "Iteration 88, loss = 0.94089806\n",
      "Iteration 89, loss = 0.93947847\n",
      "Iteration 90, loss = 0.94039621\n",
      "Iteration 91, loss = 0.94119262\n",
      "Iteration 92, loss = 0.93640317\n",
      "Iteration 93, loss = 0.93682257\n",
      "Iteration 94, loss = 0.93601910\n",
      "Iteration 95, loss = 0.93314719\n",
      "Iteration 96, loss = 0.93285132\n",
      "Iteration 97, loss = 0.93223586\n",
      "Iteration 98, loss = 0.92982773\n",
      "Iteration 99, loss = 0.93007495\n",
      "Iteration 100, loss = 0.92736216\n",
      "algorithm scaled_mlp f1_micro results: mean = 0.554449 (std = 0.017960)\n",
      "\n",
      "\n",
      "done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "k4folds = 5\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:   # Select each model in turn\n",
    "    print(\" ++ NOW WORKING ON ALGORITHM %s ++\" % name)\n",
    "# create the five folds of the TRAINING data\n",
    "    print(\"Splitting data into %s folds\" % k4folds)    \n",
    "    kfold = model_selection.KFold(n_splits=k4folds, random_state=seed, shuffle=True)   \n",
    "# fit the model using four parts at a time and then validate it on the oher part that was set aside; and repeat five times.\n",
    "    print(\"Training model and validating it on each fold\") \n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring_method) \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "# some fancy footwork for printing the results\n",
    "    msg = \"algorithm %s %s results: mean = %f (std = %f)\" % (name, scoring_method, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    print('\\n')\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0db8edd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ++ NOW WORKING ON ALGORITHM mlp ++\n",
      "algorithm mlp f1_micro results: mean = 0.554449 (std = 0.017960)\n",
      "\n",
      "\n",
      " ++ NOW WORKING ON ALGORITHM scaled_mlp ++\n",
      "algorithm scaled_mlp f1_micro results: mean = 0.554449 (std = 0.017960)\n",
      "\n",
      "\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:   # Select each model in turn\n",
    "    print(\" ++ NOW WORKING ON ALGORITHM %s ++\" % name)    \n",
    "# some fancy footwork for printing the results\n",
    "    msg = \"algorithm %s %s results: mean = %f (std = %f)\" % (name, scoring_method, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    print('\\n')\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e566af9",
   "metadata": {},
   "source": [
    "# For the given data  and used method, scaling did not improve the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1b4b7b",
   "metadata": {},
   "source": [
    "# 7. Train and tune a new classifier model that is not a neural network (you can reuse one from H/W assignment #2 if you want).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9dc5639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done \n",
      "\n",
      " ++ NOW WORKING ON ALGORITHM mlp ++\n",
      "Splitting data into 5 folds\n",
      "Training model and validating it on each fold\n",
      "Iteration 1, loss = 7.49637266\n",
      "Iteration 2, loss = 2.45693386\n",
      "Iteration 3, loss = 2.23661772\n",
      "Iteration 4, loss = 2.19446292\n",
      "Iteration 5, loss = 1.90162731\n",
      "Iteration 6, loss = 1.73899395\n",
      "Iteration 7, loss = 1.59862758\n",
      "Iteration 8, loss = 1.44822955\n",
      "Iteration 9, loss = 1.32945126\n",
      "Iteration 10, loss = 1.24755697\n",
      "Iteration 11, loss = 1.24503695\n",
      "Iteration 12, loss = 1.21695555\n",
      "Iteration 13, loss = 1.19429985\n",
      "Iteration 14, loss = 1.18654512\n",
      "Iteration 15, loss = 1.16824678\n",
      "Iteration 16, loss = 1.15332091\n",
      "Iteration 17, loss = 1.15215087\n",
      "Iteration 18, loss = 1.14809146\n",
      "Iteration 19, loss = 1.14765869\n",
      "Iteration 20, loss = 1.14282129\n",
      "Iteration 21, loss = 1.14166499\n",
      "Iteration 22, loss = 1.14844400\n",
      "Iteration 23, loss = 1.13302001\n",
      "Iteration 24, loss = 1.13435912\n",
      "Iteration 25, loss = 1.11468504\n",
      "Iteration 26, loss = 1.12734942\n",
      "Iteration 27, loss = 1.14005594\n",
      "Iteration 28, loss = 1.12851300\n",
      "Iteration 29, loss = 1.12991000\n",
      "Iteration 30, loss = 1.11114069\n",
      "Iteration 31, loss = 1.10541166\n",
      "Iteration 32, loss = 1.09513266\n",
      "Iteration 33, loss = 1.10500240\n",
      "Iteration 34, loss = 1.09991350\n",
      "Iteration 35, loss = 1.14727414\n",
      "Iteration 36, loss = 1.18286631\n",
      "Iteration 37, loss = 1.14336800\n",
      "Iteration 38, loss = 1.15085806\n",
      "Iteration 39, loss = 1.12225594\n",
      "Iteration 40, loss = 1.10274823\n",
      "Iteration 41, loss = 1.12645161\n",
      "Iteration 42, loss = 1.11006070\n",
      "Iteration 43, loss = 1.10525447\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.27037133\n",
      "Iteration 2, loss = 2.59019799\n",
      "Iteration 3, loss = 2.34028168\n",
      "Iteration 4, loss = 2.07467744\n",
      "Iteration 5, loss = 1.93531746\n",
      "Iteration 6, loss = 1.71732258\n",
      "Iteration 7, loss = 1.56092068\n",
      "Iteration 8, loss = 1.43063440\n",
      "Iteration 9, loss = 1.30892010\n",
      "Iteration 10, loss = 1.27301746\n",
      "Iteration 11, loss = 1.26851585\n",
      "Iteration 12, loss = 1.22564051\n",
      "Iteration 13, loss = 1.20296780\n",
      "Iteration 14, loss = 1.20615309\n",
      "Iteration 15, loss = 1.20445858\n",
      "Iteration 16, loss = 1.18514443\n",
      "Iteration 17, loss = 1.18150482\n",
      "Iteration 18, loss = 1.16826866\n",
      "Iteration 19, loss = 1.16603632\n",
      "Iteration 20, loss = 1.15252716\n",
      "Iteration 21, loss = 1.15956021\n",
      "Iteration 22, loss = 1.15412301\n",
      "Iteration 23, loss = 1.15285825\n",
      "Iteration 24, loss = 1.14445825\n",
      "Iteration 25, loss = 1.13975031\n",
      "Iteration 26, loss = 1.13861233\n",
      "Iteration 27, loss = 1.15347823\n",
      "Iteration 28, loss = 1.13347727\n",
      "Iteration 29, loss = 1.13320136\n",
      "Iteration 30, loss = 1.13598846\n",
      "Iteration 31, loss = 1.13418509\n",
      "Iteration 32, loss = 1.12906028\n",
      "Iteration 33, loss = 1.14516444\n",
      "Iteration 34, loss = 1.15754377\n",
      "Iteration 35, loss = 1.12815957\n",
      "Iteration 36, loss = 1.12677501\n",
      "Iteration 37, loss = 1.12288005\n",
      "Iteration 38, loss = 1.13500254\n",
      "Iteration 39, loss = 1.12593507\n",
      "Iteration 40, loss = 1.16050160\n",
      "Iteration 41, loss = 1.13140155\n",
      "Iteration 42, loss = 1.14759876\n",
      "Iteration 43, loss = 1.12015050\n",
      "Iteration 44, loss = 1.12803706\n",
      "Iteration 45, loss = 1.10553548\n",
      "Iteration 46, loss = 1.12103187\n",
      "Iteration 47, loss = 1.10440354\n",
      "Iteration 48, loss = 1.09329617\n",
      "Iteration 49, loss = 1.09889260\n",
      "Iteration 50, loss = 1.10117888\n",
      "Iteration 51, loss = 1.10907514\n",
      "Iteration 52, loss = 1.09719344\n",
      "Iteration 53, loss = 1.09969970\n",
      "Iteration 54, loss = 1.13111316\n",
      "Iteration 55, loss = 1.11270394\n",
      "Iteration 56, loss = 1.11594424\n",
      "Iteration 57, loss = 1.14349794\n",
      "Iteration 58, loss = 1.17725024\n",
      "Iteration 59, loss = 1.14211111\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.34766474\n",
      "Iteration 2, loss = 2.81266496\n",
      "Iteration 3, loss = 2.35999245\n",
      "Iteration 4, loss = 2.26080775\n",
      "Iteration 5, loss = 1.99915404\n",
      "Iteration 6, loss = 1.76820543\n",
      "Iteration 7, loss = 1.58634870\n",
      "Iteration 8, loss = 1.45884504\n",
      "Iteration 9, loss = 1.30178195\n",
      "Iteration 10, loss = 1.25116479\n",
      "Iteration 11, loss = 1.27477780\n",
      "Iteration 12, loss = 1.21982783\n",
      "Iteration 13, loss = 1.21504154\n",
      "Iteration 14, loss = 1.22019318\n",
      "Iteration 15, loss = 1.19857369\n",
      "Iteration 16, loss = 1.20651949\n",
      "Iteration 17, loss = 1.18606067\n",
      "Iteration 18, loss = 1.17830740\n",
      "Iteration 19, loss = 1.17802641\n",
      "Iteration 20, loss = 1.16887482\n",
      "Iteration 21, loss = 1.16013545\n",
      "Iteration 22, loss = 1.16006151\n",
      "Iteration 23, loss = 1.16528433\n",
      "Iteration 24, loss = 1.16922772\n",
      "Iteration 25, loss = 1.16653680\n",
      "Iteration 26, loss = 1.16282652\n",
      "Iteration 27, loss = 1.14877228\n",
      "Iteration 28, loss = 1.14191722\n",
      "Iteration 29, loss = 1.15097129\n",
      "Iteration 30, loss = 1.14711466\n",
      "Iteration 31, loss = 1.15554753\n",
      "Iteration 32, loss = 1.13889698\n",
      "Iteration 33, loss = 1.14653961\n",
      "Iteration 34, loss = 1.14681770\n",
      "Iteration 35, loss = 1.17007399\n",
      "Iteration 36, loss = 1.13267581\n",
      "Iteration 37, loss = 1.12376222\n",
      "Iteration 38, loss = 1.11207272\n",
      "Iteration 39, loss = 1.12409005\n",
      "Iteration 40, loss = 1.13971311\n",
      "Iteration 41, loss = 1.11130370\n",
      "Iteration 42, loss = 1.11707024\n",
      "Iteration 43, loss = 1.15041274\n",
      "Iteration 44, loss = 1.12801466\n",
      "Iteration 45, loss = 1.11271521\n",
      "Iteration 46, loss = 1.11768392\n",
      "Iteration 47, loss = 1.10416005\n",
      "Iteration 48, loss = 1.10263196\n",
      "Iteration 49, loss = 1.11132154\n",
      "Iteration 50, loss = 1.10090578\n",
      "Iteration 51, loss = 1.13874292\n",
      "Iteration 52, loss = 1.11871010\n",
      "Iteration 53, loss = 1.10110512\n",
      "Iteration 54, loss = 1.08826976\n",
      "Iteration 55, loss = 1.09785492\n",
      "Iteration 56, loss = 1.09416664\n",
      "Iteration 57, loss = 1.09329087\n",
      "Iteration 58, loss = 1.09047655\n",
      "Iteration 59, loss = 1.09005530\n",
      "Iteration 60, loss = 1.08208173\n",
      "Iteration 61, loss = 1.10836514\n",
      "Iteration 62, loss = 1.09805348\n",
      "Iteration 63, loss = 1.14331113\n",
      "Iteration 64, loss = 1.14724994\n",
      "Iteration 65, loss = 1.13122437\n",
      "Iteration 66, loss = 1.11801078\n",
      "Iteration 67, loss = 1.07542197\n",
      "Iteration 68, loss = 1.08787441\n",
      "Iteration 69, loss = 1.09701869\n",
      "Iteration 70, loss = 1.09733319\n",
      "Iteration 71, loss = 1.09075512\n",
      "Iteration 72, loss = 1.07964597\n",
      "Iteration 73, loss = 1.11016635\n",
      "Iteration 74, loss = 1.11102860\n",
      "Iteration 75, loss = 1.15657690\n",
      "Iteration 76, loss = 1.11078144\n",
      "Iteration 77, loss = 1.09488008\n",
      "Iteration 78, loss = 1.08664528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.44396212\n",
      "Iteration 2, loss = 2.61262146\n",
      "Iteration 3, loss = 2.22951312\n",
      "Iteration 4, loss = 2.18917891\n",
      "Iteration 5, loss = 1.91613973\n",
      "Iteration 6, loss = 1.74893178\n",
      "Iteration 7, loss = 1.64852466\n",
      "Iteration 8, loss = 1.48860738\n",
      "Iteration 9, loss = 1.38146846\n",
      "Iteration 10, loss = 1.33585404\n",
      "Iteration 11, loss = 1.29037145\n",
      "Iteration 12, loss = 1.23372777\n",
      "Iteration 13, loss = 1.22028473\n",
      "Iteration 14, loss = 1.20964276\n",
      "Iteration 15, loss = 1.21517990\n",
      "Iteration 16, loss = 1.21970149\n",
      "Iteration 17, loss = 1.19637096\n",
      "Iteration 18, loss = 1.19650656\n",
      "Iteration 19, loss = 1.18008511\n",
      "Iteration 20, loss = 1.15968319\n",
      "Iteration 21, loss = 1.15800984\n",
      "Iteration 22, loss = 1.14448033\n",
      "Iteration 23, loss = 1.14905300\n",
      "Iteration 24, loss = 1.15076963\n",
      "Iteration 25, loss = 1.13027387\n",
      "Iteration 26, loss = 1.14697874\n",
      "Iteration 27, loss = 1.13739474\n",
      "Iteration 28, loss = 1.12626928\n",
      "Iteration 29, loss = 1.12819029\n",
      "Iteration 30, loss = 1.13393021\n",
      "Iteration 31, loss = 1.13259697\n",
      "Iteration 32, loss = 1.13338049\n",
      "Iteration 33, loss = 1.12318877\n",
      "Iteration 34, loss = 1.12991906\n",
      "Iteration 35, loss = 1.12333908\n",
      "Iteration 36, loss = 1.10369446\n",
      "Iteration 37, loss = 1.10154677\n",
      "Iteration 38, loss = 1.10418725\n",
      "Iteration 39, loss = 1.10407358\n",
      "Iteration 40, loss = 1.09616111\n",
      "Iteration 41, loss = 1.09850365\n",
      "Iteration 42, loss = 1.10655314\n",
      "Iteration 43, loss = 1.13468277\n",
      "Iteration 44, loss = 1.10967476\n",
      "Iteration 45, loss = 1.13051686\n",
      "Iteration 46, loss = 1.10987437\n",
      "Iteration 47, loss = 1.08768376\n",
      "Iteration 48, loss = 1.09413195\n",
      "Iteration 49, loss = 1.10743996\n",
      "Iteration 50, loss = 1.09499034\n",
      "Iteration 51, loss = 1.11133916\n",
      "Iteration 52, loss = 1.08583265\n",
      "Iteration 53, loss = 1.08559167\n",
      "Iteration 54, loss = 1.08363675\n",
      "Iteration 55, loss = 1.07948444\n",
      "Iteration 56, loss = 1.07780225\n",
      "Iteration 57, loss = 1.09431150\n",
      "Iteration 58, loss = 1.10365621\n",
      "Iteration 59, loss = 1.10819452\n",
      "Iteration 60, loss = 1.11688903\n",
      "Iteration 61, loss = 1.11226165\n",
      "Iteration 62, loss = 1.13348034\n",
      "Iteration 63, loss = 1.10771144\n",
      "Iteration 64, loss = 1.17558279\n",
      "Iteration 65, loss = 1.12273215\n",
      "Iteration 66, loss = 1.14249262\n",
      "Iteration 67, loss = 1.11196810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.53049419\n",
      "Iteration 2, loss = 2.45037238\n",
      "Iteration 3, loss = 2.30013913\n",
      "Iteration 4, loss = 2.06685424\n",
      "Iteration 5, loss = 1.98425654\n",
      "Iteration 6, loss = 1.80207269\n",
      "Iteration 7, loss = 1.55063675\n",
      "Iteration 8, loss = 1.41826375\n",
      "Iteration 9, loss = 1.32095057\n",
      "Iteration 10, loss = 1.29279986\n",
      "Iteration 11, loss = 1.22090039\n",
      "Iteration 12, loss = 1.23259743\n",
      "Iteration 13, loss = 1.22368240\n",
      "Iteration 14, loss = 1.20065359\n",
      "Iteration 15, loss = 1.18938620\n",
      "Iteration 16, loss = 1.18447095\n",
      "Iteration 17, loss = 1.18011880\n",
      "Iteration 18, loss = 1.16768258\n",
      "Iteration 19, loss = 1.16499891\n",
      "Iteration 20, loss = 1.16412442\n",
      "Iteration 21, loss = 1.15911464\n",
      "Iteration 22, loss = 1.15063153\n",
      "Iteration 23, loss = 1.15811947\n",
      "Iteration 24, loss = 1.14634291\n",
      "Iteration 25, loss = 1.15881952\n",
      "Iteration 26, loss = 1.17643749\n",
      "Iteration 27, loss = 1.15662118\n",
      "Iteration 28, loss = 1.14387431\n",
      "Iteration 29, loss = 1.13549034\n",
      "Iteration 30, loss = 1.14063856\n",
      "Iteration 31, loss = 1.15100748\n",
      "Iteration 32, loss = 1.17714659\n",
      "Iteration 33, loss = 1.16418181\n",
      "Iteration 34, loss = 1.13991129\n",
      "Iteration 35, loss = 1.12966030\n",
      "Iteration 36, loss = 1.12025206\n",
      "Iteration 37, loss = 1.12729288\n",
      "Iteration 38, loss = 1.12624813\n",
      "Iteration 39, loss = 1.13064679\n",
      "Iteration 40, loss = 1.13957274\n",
      "Iteration 41, loss = 1.12356376\n",
      "Iteration 42, loss = 1.12425730\n",
      "Iteration 43, loss = 1.15303837\n",
      "Iteration 44, loss = 1.13311104\n",
      "Iteration 45, loss = 1.13397314\n",
      "Iteration 46, loss = 1.13542403\n",
      "Iteration 47, loss = 1.16590567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "\n",
      " ++ NOW WORKING ON ALGORITHM scaled_mlp ++\n",
      "Splitting data into 5 folds\n",
      "Training model and validating it on each fold\n",
      "Iteration 1, loss = 1.61789518\n",
      "Iteration 2, loss = 1.48255728\n",
      "Iteration 3, loss = 1.37989687\n",
      "Iteration 4, loss = 1.30465656\n",
      "Iteration 5, loss = 1.24875398\n",
      "Iteration 6, loss = 1.20842394\n",
      "Iteration 7, loss = 1.18291528\n",
      "Iteration 8, loss = 1.16468266\n",
      "Iteration 9, loss = 1.15235919\n",
      "Iteration 10, loss = 1.14038255\n",
      "Iteration 11, loss = 1.12967526\n",
      "Iteration 12, loss = 1.11850523\n",
      "Iteration 13, loss = 1.10759144\n",
      "Iteration 14, loss = 1.09833924\n",
      "Iteration 15, loss = 1.09060176\n",
      "Iteration 16, loss = 1.08191298\n",
      "Iteration 17, loss = 1.07615068\n",
      "Iteration 18, loss = 1.06856507\n",
      "Iteration 19, loss = 1.06468094\n",
      "Iteration 20, loss = 1.05855475\n",
      "Iteration 21, loss = 1.05517553\n",
      "Iteration 22, loss = 1.05069111\n",
      "Iteration 23, loss = 1.04758032\n",
      "Iteration 24, loss = 1.04324497\n",
      "Iteration 25, loss = 1.03887797\n",
      "Iteration 26, loss = 1.03961582\n",
      "Iteration 27, loss = 1.03407800\n",
      "Iteration 28, loss = 1.03179998\n",
      "Iteration 29, loss = 1.02808795\n",
      "Iteration 30, loss = 1.02800979\n",
      "Iteration 31, loss = 1.02336588\n",
      "Iteration 32, loss = 1.02255570\n",
      "Iteration 33, loss = 1.01852616\n",
      "Iteration 34, loss = 1.02093637\n",
      "Iteration 35, loss = 1.01985756\n",
      "Iteration 36, loss = 1.01454815\n",
      "Iteration 37, loss = 1.01196049\n",
      "Iteration 38, loss = 1.00949176\n",
      "Iteration 39, loss = 1.00620066\n",
      "Iteration 40, loss = 1.00524566\n",
      "Iteration 41, loss = 1.00346160\n",
      "Iteration 42, loss = 1.00056581\n",
      "Iteration 43, loss = 0.99879300\n",
      "Iteration 44, loss = 1.00088824\n",
      "Iteration 45, loss = 0.99677966\n",
      "Iteration 46, loss = 0.99364045\n",
      "Iteration 47, loss = 0.99418731\n",
      "Iteration 48, loss = 0.99034302\n",
      "Iteration 49, loss = 0.98846318\n",
      "Iteration 50, loss = 0.98683257\n",
      "Iteration 51, loss = 0.98405005\n",
      "Iteration 52, loss = 0.98278877\n",
      "Iteration 53, loss = 0.98171357\n",
      "Iteration 54, loss = 0.98190174\n",
      "Iteration 55, loss = 0.97838208\n",
      "Iteration 56, loss = 0.97855580\n",
      "Iteration 57, loss = 0.97605063\n",
      "Iteration 58, loss = 0.97615320\n",
      "Iteration 59, loss = 0.97261104\n",
      "Iteration 60, loss = 0.97151250\n",
      "Iteration 61, loss = 0.96899176\n",
      "Iteration 62, loss = 0.96829880\n",
      "Iteration 63, loss = 0.96786393\n",
      "Iteration 64, loss = 0.96587830\n",
      "Iteration 65, loss = 0.96427060\n",
      "Iteration 66, loss = 0.96243225\n",
      "Iteration 67, loss = 0.96244736\n",
      "Iteration 68, loss = 0.96114371\n",
      "Iteration 69, loss = 0.96020620\n",
      "Iteration 70, loss = 0.95598817\n",
      "Iteration 71, loss = 0.95845558\n",
      "Iteration 72, loss = 0.95360557\n",
      "Iteration 73, loss = 0.95594540\n",
      "Iteration 74, loss = 0.95132567\n",
      "Iteration 75, loss = 0.95155954\n",
      "Iteration 76, loss = 0.95031217\n",
      "Iteration 77, loss = 0.94943869\n",
      "Iteration 78, loss = 0.94669902\n",
      "Iteration 79, loss = 0.94482230\n",
      "Iteration 80, loss = 0.94450029\n",
      "Iteration 81, loss = 0.94331001\n",
      "Iteration 82, loss = 0.94105297\n",
      "Iteration 83, loss = 0.93984846\n",
      "Iteration 84, loss = 0.93748475\n",
      "Iteration 85, loss = 0.93765057\n",
      "Iteration 86, loss = 0.93583331\n",
      "Iteration 87, loss = 0.93397204\n",
      "Iteration 88, loss = 0.93799932\n",
      "Iteration 89, loss = 0.93098713\n",
      "Iteration 90, loss = 0.93273516\n",
      "Iteration 91, loss = 0.93759163\n",
      "Iteration 92, loss = 0.93112987\n",
      "Iteration 93, loss = 0.93104723\n",
      "Iteration 94, loss = 0.92761112\n",
      "Iteration 95, loss = 0.92534647\n",
      "Iteration 96, loss = 0.92725154\n",
      "Iteration 97, loss = 0.92241082\n",
      "Iteration 98, loss = 0.92181740\n",
      "Iteration 99, loss = 0.91960732\n",
      "Iteration 100, loss = 0.92148195\n",
      "Iteration 1, loss = 1.60582817\n",
      "Iteration 2, loss = 1.47492620\n",
      "Iteration 3, loss = 1.37444378\n",
      "Iteration 4, loss = 1.30832541\n",
      "Iteration 5, loss = 1.25634464\n",
      "Iteration 6, loss = 1.22081657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 1.19190731\n",
      "Iteration 8, loss = 1.17374913\n",
      "Iteration 9, loss = 1.16018653\n",
      "Iteration 10, loss = 1.14899994\n",
      "Iteration 11, loss = 1.13960389\n",
      "Iteration 12, loss = 1.12838055\n",
      "Iteration 13, loss = 1.11815089\n",
      "Iteration 14, loss = 1.10782336\n",
      "Iteration 15, loss = 1.09954903\n",
      "Iteration 16, loss = 1.09132053\n",
      "Iteration 17, loss = 1.08577341\n",
      "Iteration 18, loss = 1.07682437\n",
      "Iteration 19, loss = 1.07194256\n",
      "Iteration 20, loss = 1.06645431\n",
      "Iteration 21, loss = 1.06080534\n",
      "Iteration 22, loss = 1.05671608\n",
      "Iteration 23, loss = 1.05215406\n",
      "Iteration 24, loss = 1.04974032\n",
      "Iteration 25, loss = 1.04511273\n",
      "Iteration 26, loss = 1.04110561\n",
      "Iteration 27, loss = 1.03767524\n",
      "Iteration 28, loss = 1.03327628\n",
      "Iteration 29, loss = 1.03199396\n",
      "Iteration 30, loss = 1.02767217\n",
      "Iteration 31, loss = 1.02528700\n",
      "Iteration 32, loss = 1.02129780\n",
      "Iteration 33, loss = 1.02051615\n",
      "Iteration 34, loss = 1.01838126\n",
      "Iteration 35, loss = 1.01422461\n",
      "Iteration 36, loss = 1.01241012\n",
      "Iteration 37, loss = 1.00911473\n",
      "Iteration 38, loss = 1.00751782\n",
      "Iteration 39, loss = 1.00642827\n",
      "Iteration 40, loss = 1.00307288\n",
      "Iteration 41, loss = 1.00342834\n",
      "Iteration 42, loss = 0.99863214\n",
      "Iteration 43, loss = 1.00339029\n",
      "Iteration 44, loss = 0.99465142\n",
      "Iteration 45, loss = 0.99666496\n",
      "Iteration 46, loss = 0.99337997\n",
      "Iteration 47, loss = 0.99086148\n",
      "Iteration 48, loss = 0.98987570\n",
      "Iteration 49, loss = 0.98784519\n",
      "Iteration 50, loss = 0.98656289\n",
      "Iteration 51, loss = 0.98561440\n",
      "Iteration 52, loss = 0.98340600\n",
      "Iteration 53, loss = 0.98229584\n",
      "Iteration 54, loss = 0.98156907\n",
      "Iteration 55, loss = 0.98023475\n",
      "Iteration 56, loss = 0.98265720\n",
      "Iteration 57, loss = 0.98112372\n",
      "Iteration 58, loss = 0.97773559\n",
      "Iteration 59, loss = 0.97691239\n",
      "Iteration 60, loss = 0.97545294\n",
      "Iteration 61, loss = 0.97270398\n",
      "Iteration 62, loss = 0.97251039\n",
      "Iteration 63, loss = 0.97133925\n",
      "Iteration 64, loss = 0.97027325\n",
      "Iteration 65, loss = 0.96936289\n",
      "Iteration 66, loss = 0.96896917\n",
      "Iteration 67, loss = 0.96726439\n",
      "Iteration 68, loss = 0.96613532\n",
      "Iteration 69, loss = 0.96478083\n",
      "Iteration 70, loss = 0.96339512\n",
      "Iteration 71, loss = 0.96247282\n",
      "Iteration 72, loss = 0.96023631\n",
      "Iteration 73, loss = 0.95938146\n",
      "Iteration 74, loss = 0.95958612\n",
      "Iteration 75, loss = 0.95801586\n",
      "Iteration 76, loss = 0.95695290\n",
      "Iteration 77, loss = 0.95589355\n",
      "Iteration 78, loss = 0.95438884\n",
      "Iteration 79, loss = 0.95327355\n",
      "Iteration 80, loss = 0.95228012\n",
      "Iteration 81, loss = 0.95080016\n",
      "Iteration 82, loss = 0.95061674\n",
      "Iteration 83, loss = 0.94901890\n",
      "Iteration 84, loss = 0.94764306\n",
      "Iteration 85, loss = 0.94697376\n",
      "Iteration 86, loss = 0.94562123\n",
      "Iteration 87, loss = 0.94570287\n",
      "Iteration 88, loss = 0.94353531\n",
      "Iteration 89, loss = 0.94325818\n",
      "Iteration 90, loss = 0.94305748\n",
      "Iteration 91, loss = 0.94253611\n",
      "Iteration 92, loss = 0.94051016\n",
      "Iteration 93, loss = 0.94213144\n",
      "Iteration 94, loss = 0.93903020\n",
      "Iteration 95, loss = 0.93716071\n",
      "Iteration 96, loss = 0.93733680\n",
      "Iteration 97, loss = 0.93710735\n",
      "Iteration 98, loss = 0.93492706\n",
      "Iteration 99, loss = 0.93446500\n",
      "Iteration 100, loss = 0.93349497\n",
      "Iteration 1, loss = 1.60823903\n",
      "Iteration 2, loss = 1.47313246\n",
      "Iteration 3, loss = 1.37343960\n",
      "Iteration 4, loss = 1.30092702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 1.24580205\n",
      "Iteration 6, loss = 1.20818776\n",
      "Iteration 7, loss = 1.18241126\n",
      "Iteration 8, loss = 1.16269751\n",
      "Iteration 9, loss = 1.14876108\n",
      "Iteration 10, loss = 1.13907631\n",
      "Iteration 11, loss = 1.12769497\n",
      "Iteration 12, loss = 1.11575120\n",
      "Iteration 13, loss = 1.10636894\n",
      "Iteration 14, loss = 1.09788658\n",
      "Iteration 15, loss = 1.08895229\n",
      "Iteration 16, loss = 1.08031494\n",
      "Iteration 17, loss = 1.07455555\n",
      "Iteration 18, loss = 1.06769804\n",
      "Iteration 19, loss = 1.06219473\n",
      "Iteration 20, loss = 1.05737331\n",
      "Iteration 21, loss = 1.05225470\n",
      "Iteration 22, loss = 1.04784518\n",
      "Iteration 23, loss = 1.04665133\n",
      "Iteration 24, loss = 1.04013749\n",
      "Iteration 25, loss = 1.03756511\n",
      "Iteration 26, loss = 1.03327673\n",
      "Iteration 27, loss = 1.03109535\n",
      "Iteration 28, loss = 1.02690895\n",
      "Iteration 29, loss = 1.02549040\n",
      "Iteration 30, loss = 1.02139735\n",
      "Iteration 31, loss = 1.02197244\n",
      "Iteration 32, loss = 1.01992034\n",
      "Iteration 33, loss = 1.01750520\n",
      "Iteration 34, loss = 1.01360203\n",
      "Iteration 35, loss = 1.01144943\n",
      "Iteration 36, loss = 1.00653189\n",
      "Iteration 37, loss = 1.00529177\n",
      "Iteration 38, loss = 1.00561704\n",
      "Iteration 39, loss = 1.00213064\n",
      "Iteration 40, loss = 0.99934469\n",
      "Iteration 41, loss = 0.99614149\n",
      "Iteration 42, loss = 0.99565113\n",
      "Iteration 43, loss = 0.99448413\n",
      "Iteration 44, loss = 0.99051484\n",
      "Iteration 45, loss = 0.98827925\n",
      "Iteration 46, loss = 0.98703357\n",
      "Iteration 47, loss = 0.98652341\n",
      "Iteration 48, loss = 0.98175614\n",
      "Iteration 49, loss = 0.98029520\n",
      "Iteration 50, loss = 0.98348583\n",
      "Iteration 51, loss = 0.97637197\n",
      "Iteration 52, loss = 0.97515320\n",
      "Iteration 53, loss = 0.97311307\n",
      "Iteration 54, loss = 0.97036204\n",
      "Iteration 55, loss = 0.96862553\n",
      "Iteration 56, loss = 0.96757593\n",
      "Iteration 57, loss = 0.96446641\n",
      "Iteration 58, loss = 0.96435546\n",
      "Iteration 59, loss = 0.96381359\n",
      "Iteration 60, loss = 0.96216505\n",
      "Iteration 61, loss = 0.95851421\n",
      "Iteration 62, loss = 0.95711378\n",
      "Iteration 63, loss = 0.95812342\n",
      "Iteration 64, loss = 0.95386379\n",
      "Iteration 65, loss = 0.95318534\n",
      "Iteration 66, loss = 0.95180097\n",
      "Iteration 67, loss = 0.95052103\n",
      "Iteration 68, loss = 0.95223252\n",
      "Iteration 69, loss = 0.94772266\n",
      "Iteration 70, loss = 0.94722521\n",
      "Iteration 71, loss = 0.94505866\n",
      "Iteration 72, loss = 0.94296876\n",
      "Iteration 73, loss = 0.94186873\n",
      "Iteration 74, loss = 0.94164236\n",
      "Iteration 75, loss = 0.93900036\n",
      "Iteration 76, loss = 0.93875851\n",
      "Iteration 77, loss = 0.93799118\n",
      "Iteration 78, loss = 0.93879428\n",
      "Iteration 79, loss = 0.93519132\n",
      "Iteration 80, loss = 0.93397127\n",
      "Iteration 81, loss = 0.93145178\n",
      "Iteration 82, loss = 0.93089312\n",
      "Iteration 83, loss = 0.93028909\n",
      "Iteration 84, loss = 0.92923132\n",
      "Iteration 85, loss = 0.92998909\n",
      "Iteration 86, loss = 0.92644946\n",
      "Iteration 87, loss = 0.92621271\n",
      "Iteration 88, loss = 0.92614931\n",
      "Iteration 89, loss = 0.92347603\n",
      "Iteration 90, loss = 0.92292522\n",
      "Iteration 91, loss = 0.92342401\n",
      "Iteration 92, loss = 0.92268363\n",
      "Iteration 93, loss = 0.91989289\n",
      "Iteration 94, loss = 0.91812963\n",
      "Iteration 95, loss = 0.91768391\n",
      "Iteration 96, loss = 0.91559602\n",
      "Iteration 97, loss = 0.91524817\n",
      "Iteration 98, loss = 0.91443090\n",
      "Iteration 99, loss = 0.91439488\n",
      "Iteration 100, loss = 0.91234293\n",
      "Iteration 1, loss = 1.61701239\n",
      "Iteration 2, loss = 1.47786030\n",
      "Iteration 3, loss = 1.37619022\n",
      "Iteration 4, loss = 1.29890763\n",
      "Iteration 5, loss = 1.24384699\n",
      "Iteration 6, loss = 1.20514180\n",
      "Iteration 7, loss = 1.17881392\n",
      "Iteration 8, loss = 1.15995156\n",
      "Iteration 9, loss = 1.15084293\n",
      "Iteration 10, loss = 1.14043964\n",
      "Iteration 11, loss = 1.13243380\n",
      "Iteration 12, loss = 1.12123953\n",
      "Iteration 13, loss = 1.11225083\n",
      "Iteration 14, loss = 1.10304873\n",
      "Iteration 15, loss = 1.09527016\n",
      "Iteration 16, loss = 1.08799484\n",
      "Iteration 17, loss = 1.08015894\n",
      "Iteration 18, loss = 1.07409332\n",
      "Iteration 19, loss = 1.07016022\n",
      "Iteration 20, loss = 1.06451198\n",
      "Iteration 21, loss = 1.06061762\n",
      "Iteration 22, loss = 1.05795113\n",
      "Iteration 23, loss = 1.05204178\n",
      "Iteration 24, loss = 1.04865148\n",
      "Iteration 25, loss = 1.04494457\n",
      "Iteration 26, loss = 1.04284005\n",
      "Iteration 27, loss = 1.04183536\n",
      "Iteration 28, loss = 1.03617950\n",
      "Iteration 29, loss = 1.03428708\n",
      "Iteration 30, loss = 1.03170034\n",
      "Iteration 31, loss = 1.02966458\n",
      "Iteration 32, loss = 1.02697687\n",
      "Iteration 33, loss = 1.02662590\n",
      "Iteration 34, loss = 1.02309348\n",
      "Iteration 35, loss = 1.02218360\n",
      "Iteration 36, loss = 1.01843245\n",
      "Iteration 37, loss = 1.01827479\n",
      "Iteration 38, loss = 1.01537343\n",
      "Iteration 39, loss = 1.01403541\n",
      "Iteration 40, loss = 1.01072670\n",
      "Iteration 41, loss = 1.01117241\n",
      "Iteration 42, loss = 1.00919296\n",
      "Iteration 43, loss = 1.00741626\n",
      "Iteration 44, loss = 1.00636250\n",
      "Iteration 45, loss = 1.00392349\n",
      "Iteration 46, loss = 1.00287319\n",
      "Iteration 47, loss = 1.00116888\n",
      "Iteration 48, loss = 1.00032360\n",
      "Iteration 49, loss = 0.99724829\n",
      "Iteration 50, loss = 0.99554582\n",
      "Iteration 51, loss = 0.99437949\n",
      "Iteration 52, loss = 0.99250234\n",
      "Iteration 53, loss = 0.99112205\n",
      "Iteration 54, loss = 0.98940276\n",
      "Iteration 55, loss = 0.98796670\n",
      "Iteration 56, loss = 0.98745904\n",
      "Iteration 57, loss = 0.98569445\n",
      "Iteration 58, loss = 0.98745142\n",
      "Iteration 59, loss = 0.98609624\n",
      "Iteration 60, loss = 0.98098766\n",
      "Iteration 61, loss = 0.98278383\n",
      "Iteration 62, loss = 0.97780803\n",
      "Iteration 63, loss = 0.97997286\n",
      "Iteration 64, loss = 0.97535424\n",
      "Iteration 65, loss = 0.97690951\n",
      "Iteration 66, loss = 0.97387107\n",
      "Iteration 67, loss = 0.97434873\n",
      "Iteration 68, loss = 0.97225765\n",
      "Iteration 69, loss = 0.97038405\n",
      "Iteration 70, loss = 0.96898657\n",
      "Iteration 71, loss = 0.96766194\n",
      "Iteration 72, loss = 0.96684526\n",
      "Iteration 73, loss = 0.96929219\n",
      "Iteration 74, loss = 0.96278558\n",
      "Iteration 75, loss = 0.96468075\n",
      "Iteration 76, loss = 0.96117320\n",
      "Iteration 77, loss = 0.96181480\n",
      "Iteration 78, loss = 0.95990594\n",
      "Iteration 79, loss = 0.95821165\n",
      "Iteration 80, loss = 0.95739161\n",
      "Iteration 81, loss = 0.95748705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 82, loss = 0.95855544\n",
      "Iteration 83, loss = 0.95348305\n",
      "Iteration 84, loss = 0.95544378\n",
      "Iteration 85, loss = 0.95023568\n",
      "Iteration 86, loss = 0.95463460\n",
      "Iteration 87, loss = 0.94896996\n",
      "Iteration 88, loss = 0.95129011\n",
      "Iteration 89, loss = 0.94804195\n",
      "Iteration 90, loss = 0.94755278\n",
      "Iteration 91, loss = 0.94633191\n",
      "Iteration 92, loss = 0.94326930\n",
      "Iteration 93, loss = 0.94433780\n",
      "Iteration 94, loss = 0.94306484\n",
      "Iteration 95, loss = 0.94187686\n",
      "Iteration 96, loss = 0.93916974\n",
      "Iteration 97, loss = 0.93882806\n",
      "Iteration 98, loss = 0.93821835\n",
      "Iteration 99, loss = 0.93633914\n",
      "Iteration 100, loss = 0.93551852\n",
      "Iteration 1, loss = 1.61144574\n",
      "Iteration 2, loss = 1.48014831\n",
      "Iteration 3, loss = 1.38520477\n",
      "Iteration 4, loss = 1.31366331\n",
      "Iteration 5, loss = 1.26038195\n",
      "Iteration 6, loss = 1.22270586\n",
      "Iteration 7, loss = 1.19321669\n",
      "Iteration 8, loss = 1.17487766\n",
      "Iteration 9, loss = 1.16280286\n",
      "Iteration 10, loss = 1.15274585\n",
      "Iteration 11, loss = 1.14337496\n",
      "Iteration 12, loss = 1.13335202\n",
      "Iteration 13, loss = 1.12316814\n",
      "Iteration 14, loss = 1.11370426\n",
      "Iteration 15, loss = 1.10531916\n",
      "Iteration 16, loss = 1.09636057\n",
      "Iteration 17, loss = 1.08944206\n",
      "Iteration 18, loss = 1.08281008\n",
      "Iteration 19, loss = 1.07744488\n",
      "Iteration 20, loss = 1.07164459\n",
      "Iteration 21, loss = 1.06603501\n",
      "Iteration 22, loss = 1.06277569\n",
      "Iteration 23, loss = 1.05654529\n",
      "Iteration 24, loss = 1.05429410\n",
      "Iteration 25, loss = 1.05125648\n",
      "Iteration 26, loss = 1.04582610\n",
      "Iteration 27, loss = 1.04322821\n",
      "Iteration 28, loss = 1.03950533\n",
      "Iteration 29, loss = 1.03658773\n",
      "Iteration 30, loss = 1.03445322\n",
      "Iteration 31, loss = 1.03375874\n",
      "Iteration 32, loss = 1.02778971\n",
      "Iteration 33, loss = 1.02765021\n",
      "Iteration 34, loss = 1.02287407\n",
      "Iteration 35, loss = 1.02312585\n",
      "Iteration 36, loss = 1.02027738\n",
      "Iteration 37, loss = 1.01725135\n",
      "Iteration 38, loss = 1.01515112\n",
      "Iteration 39, loss = 1.01374760\n",
      "Iteration 40, loss = 1.01098624\n",
      "Iteration 41, loss = 1.00939587\n",
      "Iteration 42, loss = 1.00722482\n",
      "Iteration 43, loss = 1.00526234\n",
      "Iteration 44, loss = 1.00373914\n",
      "Iteration 45, loss = 1.00398277\n",
      "Iteration 46, loss = 0.99999821\n",
      "Iteration 47, loss = 1.00025079\n",
      "Iteration 48, loss = 0.99748525\n",
      "Iteration 49, loss = 0.99455992\n",
      "Iteration 50, loss = 0.99378556\n",
      "Iteration 51, loss = 0.99083680\n",
      "Iteration 52, loss = 0.99080867\n",
      "Iteration 53, loss = 0.98772408\n",
      "Iteration 54, loss = 0.98706310\n",
      "Iteration 55, loss = 0.98386669\n",
      "Iteration 56, loss = 0.98371303\n",
      "Iteration 57, loss = 0.98090929\n",
      "Iteration 58, loss = 0.98059167\n",
      "Iteration 59, loss = 0.97856425\n",
      "Iteration 60, loss = 0.97636880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61, loss = 0.97471704\n",
      "Iteration 62, loss = 0.97419016\n",
      "Iteration 63, loss = 0.97138777\n",
      "Iteration 64, loss = 0.97128100\n",
      "Iteration 65, loss = 0.96971795\n",
      "Iteration 66, loss = 0.96851029\n",
      "Iteration 67, loss = 0.96607555\n",
      "Iteration 68, loss = 0.96641245\n",
      "Iteration 69, loss = 0.96472355\n",
      "Iteration 70, loss = 0.96219711\n",
      "Iteration 71, loss = 0.96479273\n",
      "Iteration 72, loss = 0.95946921\n",
      "Iteration 73, loss = 0.96218192\n",
      "Iteration 74, loss = 0.95877735\n",
      "Iteration 75, loss = 0.95711893\n",
      "Iteration 76, loss = 0.95562658\n",
      "Iteration 77, loss = 0.95456772\n",
      "Iteration 78, loss = 0.95300696\n",
      "Iteration 79, loss = 0.95129816\n",
      "Iteration 80, loss = 0.94994078\n",
      "Iteration 81, loss = 0.94913594\n",
      "Iteration 82, loss = 0.95033436\n",
      "Iteration 83, loss = 0.94750586\n",
      "Iteration 84, loss = 0.94529970\n",
      "Iteration 85, loss = 0.94395093\n",
      "Iteration 86, loss = 0.94399284\n",
      "Iteration 87, loss = 0.94212804\n",
      "Iteration 88, loss = 0.94089806\n",
      "Iteration 89, loss = 0.93947847\n",
      "Iteration 90, loss = 0.94039621\n",
      "Iteration 91, loss = 0.94119262\n",
      "Iteration 92, loss = 0.93640317\n",
      "Iteration 93, loss = 0.93682257\n",
      "Iteration 94, loss = 0.93601910\n",
      "Iteration 95, loss = 0.93314719\n",
      "Iteration 96, loss = 0.93285132\n",
      "Iteration 97, loss = 0.93223586\n",
      "Iteration 98, loss = 0.92982773\n",
      "Iteration 99, loss = 0.93007495\n",
      "Iteration 100, loss = 0.92736216\n",
      "\n",
      "\n",
      " ++ NOW WORKING ON ALGORITHM raw_SVM ++\n",
      "Splitting data into 5 folds\n",
      "Training model and validating it on each fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ++ NOW WORKING ON ALGORITHM scaled_SVM ++\n",
      "Splitting data into 5 folds\n",
      "Training model and validating it on each fold\n",
      "\n",
      "\n",
      " ++ NOW WORKING ON ALGORITHM scaled_SVM_std ++\n",
      "Splitting data into 5 folds\n",
      "Training model and validating it on each fold\n",
      "\n",
      "\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#adding non-neural network classifiers from previous homework\n",
    "models.append(( 'raw_SVM', svm.SVC(random_state=seed) ))\n",
    "models.append(( 'scaled_SVM', make_pipeline( MinMaxScaler(), svm.SVC(random_state=seed) )  ))\n",
    "models.append(( 'scaled_SVM_std', make_pipeline( StandardScaler(), svm.SVC(random_state=seed) )  ))\n",
    "print('done \\n')\n",
    "\n",
    "k4folds = 5\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:   # Select each model in turn\n",
    "    print(\" ++ NOW WORKING ON ALGORITHM %s ++\" % name)\n",
    "# create the five folds of the TRAINING data\n",
    "    print(\"Splitting data into %s folds\" % k4folds)    \n",
    "    kfold = model_selection.KFold(n_splits=k4folds, random_state=seed, shuffle=True)   \n",
    "# fit the model using four parts at a time and then validate it on the oher part that was set aside; and repeat five times.\n",
    "    print(\"Training model and validating it on each fold\") \n",
    "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring_method) \n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('\\n')\n",
    "print('done \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "63b30c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.48717949, 0.51794872, 0.48205128, 0.47692308, 0.48453608]), array([0.54358974, 0.54358974, 0.53846154, 0.55897436, 0.58762887]), array([0.48205128, 0.41538462, 0.49230769, 0.48717949, 0.48453608]), array([0.55384615, 0.52307692, 0.54358974, 0.54871795, 0.54639175]), array([0.55384615, 0.54871795, 0.56410256, 0.57435897, 0.54639175])]\n",
      " ---- ALGORITHM: mlp ----\n",
      "algorithm mlp f1_micro results: mean = 0.559122 (std = 0.019921)\n",
      "\n",
      "\n",
      " ---- ALGORITHM: scaled_mlp ----\n",
      "algorithm scaled_mlp f1_micro results: mean = 0.559122 (std = 0.019921)\n",
      "\n",
      "\n",
      " ---- ALGORITHM: raw_SVM ----\n",
      "algorithm raw_SVM f1_micro results: mean = 0.559122 (std = 0.019921)\n",
      "\n",
      "\n",
      " ---- ALGORITHM: scaled_SVM ----\n",
      "algorithm scaled_SVM f1_micro results: mean = 0.559122 (std = 0.019921)\n",
      "\n",
      "\n",
      " ---- ALGORITHM: scaled_SVM_std ----\n",
      "algorithm scaled_SVM_std f1_micro results: mean = 0.559122 (std = 0.019921)\n",
      "\n",
      "\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "print(results)\n",
    "for name, model in models:   # Select each model in turn\n",
    "    print(\" ---- ALGORITHM: %s ----\" % name)    \n",
    "    msg = \"algorithm %s %s results: mean = %f (std = %f)\" % (name, scoring_method, cv_results.mean(), cv_results.std())\n",
    "    a += 1\n",
    "    print(msg)\n",
    "    print('\\n')\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539e402",
   "metadata": {},
   "source": [
    "# 8. Test this new modelâ€™s performance on the summative test set and compare to the best MLPClassifier. Report and discuss your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "761adc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.88056398\n",
      "Iteration 2, loss = 1.72539666\n",
      "Iteration 3, loss = 1.33297863\n",
      "Iteration 4, loss = 1.24983255\n",
      "Iteration 5, loss = 1.23448205\n",
      "Iteration 6, loss = 1.22568178\n",
      "Iteration 7, loss = 1.21670675\n",
      "Iteration 8, loss = 1.19599367\n",
      "Iteration 9, loss = 1.20748454\n",
      "Iteration 10, loss = 1.20846033\n",
      "Iteration 11, loss = 1.20154790\n",
      "Iteration 12, loss = 1.19597495\n",
      "Iteration 13, loss = 1.18113882\n",
      "Iteration 14, loss = 1.19383099\n",
      "Iteration 15, loss = 1.16621283\n",
      "Iteration 16, loss = 1.18091341\n",
      "Iteration 17, loss = 1.20023963\n",
      "Iteration 18, loss = 1.19225937\n",
      "Iteration 19, loss = 1.16913873\n",
      "Iteration 20, loss = 1.18554481\n",
      "Iteration 21, loss = 1.22027805\n",
      "Iteration 22, loss = 1.16835136\n",
      "Iteration 23, loss = 1.14982606\n",
      "Iteration 24, loss = 1.16404695\n",
      "Iteration 25, loss = 1.20016463\n",
      "Iteration 26, loss = 1.17241732\n",
      "Iteration 27, loss = 1.15121819\n",
      "Iteration 28, loss = 1.16669467\n",
      "Iteration 29, loss = 1.16530123\n",
      "Iteration 30, loss = 1.16649919\n",
      "Iteration 31, loss = 1.15717559\n",
      "Iteration 32, loss = 1.15998502\n",
      "Iteration 33, loss = 1.13721276\n",
      "Iteration 34, loss = 1.15141052\n",
      "Iteration 35, loss = 1.13600141\n",
      "Iteration 36, loss = 1.15336115\n",
      "Iteration 37, loss = 1.12767655\n",
      "Iteration 38, loss = 1.15526427\n",
      "Iteration 39, loss = 1.12582973\n",
      "Iteration 40, loss = 1.14229497\n",
      "Iteration 41, loss = 1.15206086\n",
      "Iteration 42, loss = 1.14892911\n",
      "Iteration 43, loss = 1.19068321\n",
      "Iteration 44, loss = 1.15753405\n",
      "Iteration 45, loss = 1.12343174\n",
      "Iteration 46, loss = 1.15435622\n",
      "Iteration 47, loss = 1.15053202\n",
      "Iteration 48, loss = 1.13829521\n",
      "Iteration 49, loss = 1.12669550\n",
      "Iteration 50, loss = 1.16917288\n",
      "Iteration 51, loss = 1.15888339\n",
      "Iteration 52, loss = 1.14417823\n",
      "Iteration 53, loss = 1.11159891\n",
      "Iteration 54, loss = 1.10452390\n",
      "Iteration 55, loss = 1.13251153\n",
      "Iteration 56, loss = 1.11311823\n",
      "Iteration 57, loss = 1.13742416\n",
      "Iteration 58, loss = 1.11541708\n",
      "Iteration 59, loss = 1.13754078\n",
      "Iteration 60, loss = 1.13360561\n",
      "Iteration 61, loss = 1.13201597\n",
      "Iteration 62, loss = 1.13830388\n",
      "Iteration 63, loss = 1.15478053\n",
      "Iteration 64, loss = 1.15072065\n",
      "Iteration 65, loss = 1.12845327\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.84548291\n",
      "Iteration 2, loss = 1.78529200\n",
      "Iteration 3, loss = 1.32280916\n",
      "Iteration 4, loss = 1.25406647\n",
      "Iteration 5, loss = 1.25056632\n",
      "Iteration 6, loss = 1.23177022\n",
      "Iteration 7, loss = 1.23068309\n",
      "Iteration 8, loss = 1.23036958\n",
      "Iteration 9, loss = 1.20898278\n",
      "Iteration 10, loss = 1.19848709\n",
      "Iteration 11, loss = 1.20543252\n",
      "Iteration 12, loss = 1.19770622\n",
      "Iteration 13, loss = 1.19762409\n",
      "Iteration 14, loss = 1.24268511\n",
      "Iteration 15, loss = 1.20439566\n",
      "Iteration 16, loss = 1.21235616\n",
      "Iteration 17, loss = 1.25731378\n",
      "Iteration 18, loss = 1.20511139\n",
      "Iteration 19, loss = 1.17451986\n",
      "Iteration 20, loss = 1.18428868\n",
      "Iteration 21, loss = 1.21779024\n",
      "Iteration 22, loss = 1.19172813\n",
      "Iteration 23, loss = 1.16721570\n",
      "Iteration 24, loss = 1.20847800\n",
      "Iteration 25, loss = 1.19429464\n",
      "Iteration 26, loss = 1.16242785\n",
      "Iteration 27, loss = 1.16384481\n",
      "Iteration 28, loss = 1.16182445\n",
      "Iteration 29, loss = 1.16664203\n",
      "Iteration 30, loss = 1.18573266\n",
      "Iteration 31, loss = 1.16759006\n",
      "Iteration 32, loss = 1.13709453\n",
      "Iteration 33, loss = 1.15559912\n",
      "Iteration 34, loss = 1.15355109\n",
      "Iteration 35, loss = 1.14539184\n",
      "Iteration 36, loss = 1.13064547\n",
      "Iteration 37, loss = 1.14503549\n",
      "Iteration 38, loss = 1.15422215\n",
      "Iteration 39, loss = 1.14005688\n",
      "Iteration 40, loss = 1.14244297\n",
      "Iteration 41, loss = 1.15490537\n",
      "Iteration 42, loss = 1.18865637\n",
      "Iteration 43, loss = 1.18615473\n",
      "Iteration 44, loss = 1.15894797\n",
      "Iteration 45, loss = 1.11853231\n",
      "Iteration 46, loss = 1.15078859\n",
      "Iteration 47, loss = 1.16252205\n",
      "Iteration 48, loss = 1.16195702\n",
      "Iteration 49, loss = 1.13660434\n",
      "Iteration 50, loss = 1.14625681\n",
      "Iteration 51, loss = 1.14666400\n",
      "Iteration 52, loss = 1.12097292\n",
      "Iteration 53, loss = 1.13061541\n",
      "Iteration 54, loss = 1.15278078\n",
      "Iteration 55, loss = 1.18076008\n",
      "Iteration 56, loss = 1.13200427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.76355174\n",
      "Iteration 2, loss = 1.76462098\n",
      "Iteration 3, loss = 1.28992236\n",
      "Iteration 4, loss = 1.23618101\n",
      "Iteration 5, loss = 1.23195561\n",
      "Iteration 6, loss = 1.22039070\n",
      "Iteration 7, loss = 1.23208798\n",
      "Iteration 8, loss = 1.21152078\n",
      "Iteration 9, loss = 1.18408061\n",
      "Iteration 10, loss = 1.18935428\n",
      "Iteration 11, loss = 1.20141354\n",
      "Iteration 12, loss = 1.19718985\n",
      "Iteration 13, loss = 1.19644668\n",
      "Iteration 14, loss = 1.17714640\n",
      "Iteration 15, loss = 1.17016452\n",
      "Iteration 16, loss = 1.18308733\n",
      "Iteration 17, loss = 1.19205281\n",
      "Iteration 18, loss = 1.21604191\n",
      "Iteration 19, loss = 1.19577248\n",
      "Iteration 20, loss = 1.17263068\n",
      "Iteration 21, loss = 1.15931615\n",
      "Iteration 22, loss = 1.14646088\n",
      "Iteration 23, loss = 1.17302869\n",
      "Iteration 24, loss = 1.19625087\n",
      "Iteration 25, loss = 1.16369930\n",
      "Iteration 26, loss = 1.13471144\n",
      "Iteration 27, loss = 1.15138154\n",
      "Iteration 28, loss = 1.15175217\n",
      "Iteration 29, loss = 1.16932233\n",
      "Iteration 30, loss = 1.17552241\n",
      "Iteration 31, loss = 1.13772413\n",
      "Iteration 32, loss = 1.12755614\n",
      "Iteration 33, loss = 1.17470365\n",
      "Iteration 34, loss = 1.15707139\n",
      "Iteration 35, loss = 1.14150144\n",
      "Iteration 36, loss = 1.12775446\n",
      "Iteration 37, loss = 1.14327828\n",
      "Iteration 38, loss = 1.17938037\n",
      "Iteration 39, loss = 1.13356530\n",
      "Iteration 40, loss = 1.12874831\n",
      "Iteration 41, loss = 1.14177753\n",
      "Iteration 42, loss = 1.12115418\n",
      "Iteration 43, loss = 1.13151922\n",
      "Iteration 44, loss = 1.15249510\n",
      "Iteration 45, loss = 1.14132622\n",
      "Iteration 46, loss = 1.17423028\n",
      "Iteration 47, loss = 1.12800215\n",
      "Iteration 48, loss = 1.17024760\n",
      "Iteration 49, loss = 1.12546405\n",
      "Iteration 50, loss = 1.11204619\n",
      "Iteration 51, loss = 1.11592657\n",
      "Iteration 52, loss = 1.09843240\n",
      "Iteration 53, loss = 1.14063284\n",
      "Iteration 54, loss = 1.10890630\n",
      "Iteration 55, loss = 1.11384532\n",
      "Iteration 56, loss = 1.11170848\n",
      "Iteration 57, loss = 1.14609533\n",
      "Iteration 58, loss = 1.11138224\n",
      "Iteration 59, loss = 1.10695529\n",
      "Iteration 60, loss = 1.15050446\n",
      "Iteration 61, loss = 1.14425103\n",
      "Iteration 62, loss = 1.10565132\n",
      "Iteration 63, loss = 1.11531276\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.75733183\n",
      "Iteration 2, loss = 1.76224838\n",
      "Iteration 3, loss = 1.32837788\n",
      "Iteration 4, loss = 1.27091393\n",
      "Iteration 5, loss = 1.23730240\n",
      "Iteration 6, loss = 1.23225887\n",
      "Iteration 7, loss = 1.21714268\n",
      "Iteration 8, loss = 1.23255671\n",
      "Iteration 9, loss = 1.20079927\n",
      "Iteration 10, loss = 1.18092546\n",
      "Iteration 11, loss = 1.17955295\n",
      "Iteration 12, loss = 1.18823641\n",
      "Iteration 13, loss = 1.17881948\n",
      "Iteration 14, loss = 1.18908325\n",
      "Iteration 15, loss = 1.16843883\n",
      "Iteration 16, loss = 1.16997370\n",
      "Iteration 17, loss = 1.17628869\n",
      "Iteration 18, loss = 1.18909930\n",
      "Iteration 19, loss = 1.16088977\n",
      "Iteration 20, loss = 1.16279953\n",
      "Iteration 21, loss = 1.15683232\n",
      "Iteration 22, loss = 1.15354648\n",
      "Iteration 23, loss = 1.18372234\n",
      "Iteration 24, loss = 1.18029207\n",
      "Iteration 25, loss = 1.15702964\n",
      "Iteration 26, loss = 1.16464447\n",
      "Iteration 27, loss = 1.14818384\n",
      "Iteration 28, loss = 1.15318274\n",
      "Iteration 29, loss = 1.16760262\n",
      "Iteration 30, loss = 1.16097936\n",
      "Iteration 31, loss = 1.16430269\n",
      "Iteration 32, loss = 1.16284557\n",
      "Iteration 33, loss = 1.18566386\n",
      "Iteration 34, loss = 1.15182689\n",
      "Iteration 35, loss = 1.17477096\n",
      "Iteration 36, loss = 1.13901116\n",
      "Iteration 37, loss = 1.13194382\n",
      "Iteration 38, loss = 1.12595835\n",
      "Iteration 39, loss = 1.13015767\n",
      "Iteration 40, loss = 1.15307050\n",
      "Iteration 41, loss = 1.15239208\n",
      "Iteration 42, loss = 1.13181328\n",
      "Iteration 43, loss = 1.13341925\n",
      "Iteration 44, loss = 1.16432563\n",
      "Iteration 45, loss = 1.13617429\n",
      "Iteration 46, loss = 1.13654891\n",
      "Iteration 47, loss = 1.15379307\n",
      "Iteration 48, loss = 1.12951274\n",
      "Iteration 49, loss = 1.12072928\n",
      "Iteration 50, loss = 1.13447489\n",
      "Iteration 51, loss = 1.12411970\n",
      "Iteration 52, loss = 1.13861231\n",
      "Iteration 53, loss = 1.14573222\n",
      "Iteration 54, loss = 1.11368912\n",
      "Iteration 55, loss = 1.12787832\n",
      "Iteration 56, loss = 1.12025303\n",
      "Iteration 57, loss = 1.13803630\n",
      "Iteration 58, loss = 1.13418758\n",
      "Iteration 59, loss = 1.11744346\n",
      "Iteration 60, loss = 1.15262164\n",
      "Iteration 61, loss = 1.14691396\n",
      "Iteration 62, loss = 1.11153668\n",
      "Iteration 63, loss = 1.11408844\n",
      "Iteration 64, loss = 1.15392842\n",
      "Iteration 65, loss = 1.14528740\n",
      "Iteration 66, loss = 1.13356746\n",
      "Iteration 67, loss = 1.09668758\n",
      "Iteration 68, loss = 1.10451752\n",
      "Iteration 69, loss = 1.09014640\n",
      "Iteration 70, loss = 1.10052283\n",
      "Iteration 71, loss = 1.12232966\n",
      "Iteration 72, loss = 1.09710513\n",
      "Iteration 73, loss = 1.09989609\n",
      "Iteration 74, loss = 1.13988388\n",
      "Iteration 75, loss = 1.13506311\n",
      "Iteration 76, loss = 1.12236656\n",
      "Iteration 77, loss = 1.17160137\n",
      "Iteration 78, loss = 1.17826989\n",
      "Iteration 79, loss = 1.12644320\n",
      "Iteration 80, loss = 1.10889992\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.90402327\n",
      "Iteration 2, loss = 1.77737929\n",
      "Iteration 3, loss = 1.32810445\n",
      "Iteration 4, loss = 1.27818497\n",
      "Iteration 5, loss = 1.24674618\n",
      "Iteration 6, loss = 1.23872398\n",
      "Iteration 7, loss = 1.22140140\n",
      "Iteration 8, loss = 1.19301818\n",
      "Iteration 9, loss = 1.19977334\n",
      "Iteration 10, loss = 1.21590643\n",
      "Iteration 11, loss = 1.17877985\n",
      "Iteration 12, loss = 1.19229418\n",
      "Iteration 13, loss = 1.18577129\n",
      "Iteration 14, loss = 1.17696196\n",
      "Iteration 15, loss = 1.21536620\n",
      "Iteration 16, loss = 1.18953010\n",
      "Iteration 17, loss = 1.19553719\n",
      "Iteration 18, loss = 1.21512223\n",
      "Iteration 19, loss = 1.20765581\n",
      "Iteration 20, loss = 1.15913974\n",
      "Iteration 21, loss = 1.16745867\n",
      "Iteration 22, loss = 1.20082889\n",
      "Iteration 23, loss = 1.18383366\n",
      "Iteration 24, loss = 1.19504066\n",
      "Iteration 25, loss = 1.18491607\n",
      "Iteration 26, loss = 1.16160225\n",
      "Iteration 27, loss = 1.15385699\n",
      "Iteration 28, loss = 1.16305314\n",
      "Iteration 29, loss = 1.17914807\n",
      "Iteration 30, loss = 1.20247943\n",
      "Iteration 31, loss = 1.18971066\n",
      "Iteration 32, loss = 1.19785430\n",
      "Iteration 33, loss = 1.14344604\n",
      "Iteration 34, loss = 1.13929566\n",
      "Iteration 35, loss = 1.14146549\n",
      "Iteration 36, loss = 1.15926795\n",
      "Iteration 37, loss = 1.16860797\n",
      "Iteration 38, loss = 1.13031771\n",
      "Iteration 39, loss = 1.13147524\n",
      "Iteration 40, loss = 1.14598720\n",
      "Iteration 41, loss = 1.16186251\n",
      "Iteration 42, loss = 1.14517727\n",
      "Iteration 43, loss = 1.13894471\n",
      "Iteration 44, loss = 1.13629918\n",
      "Iteration 45, loss = 1.20715622\n",
      "Iteration 46, loss = 1.21526638\n",
      "Iteration 47, loss = 1.18704608\n",
      "Iteration 48, loss = 1.17132247\n",
      "Iteration 49, loss = 1.12617811\n",
      "Iteration 50, loss = 1.11610733\n",
      "Iteration 51, loss = 1.13329235\n",
      "Iteration 52, loss = 1.12419607\n",
      "Iteration 53, loss = 1.14463584\n",
      "Iteration 54, loss = 1.14515387\n",
      "Iteration 55, loss = 1.12626544\n",
      "Iteration 56, loss = 1.12104036\n",
      "Iteration 57, loss = 1.12062476\n",
      "Iteration 58, loss = 1.13415365\n",
      "Iteration 59, loss = 1.13106695\n",
      "Iteration 60, loss = 1.14091516\n",
      "Iteration 61, loss = 1.14250532\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "\n",
      "Iteration 1, loss = 1.46497626\n",
      "Iteration 2, loss = 1.26372725\n",
      "Iteration 3, loss = 1.21491745\n",
      "Iteration 4, loss = 1.18341244\n",
      "Iteration 5, loss = 1.15759525\n",
      "Iteration 6, loss = 1.13993527\n",
      "Iteration 7, loss = 1.12551720\n",
      "Iteration 8, loss = 1.11636039\n",
      "Iteration 9, loss = 1.10780480\n",
      "Iteration 10, loss = 1.10128950\n",
      "Iteration 11, loss = 1.09361369\n",
      "Iteration 12, loss = 1.08909061\n",
      "Iteration 13, loss = 1.08323687\n",
      "Iteration 14, loss = 1.07821038\n",
      "Iteration 15, loss = 1.07467320\n",
      "Iteration 16, loss = 1.07440538\n",
      "Iteration 17, loss = 1.07208686\n",
      "Iteration 18, loss = 1.06665745\n",
      "Iteration 19, loss = 1.06677215\n",
      "Iteration 20, loss = 1.06101989\n",
      "Iteration 21, loss = 1.06520040\n",
      "Iteration 22, loss = 1.05662950\n",
      "Iteration 23, loss = 1.05833453\n",
      "Iteration 24, loss = 1.05475022\n",
      "Iteration 25, loss = 1.05309911\n",
      "Iteration 26, loss = 1.05055282\n",
      "Iteration 27, loss = 1.04784071\n",
      "Iteration 28, loss = 1.04897249\n",
      "Iteration 29, loss = 1.05095078\n",
      "Iteration 30, loss = 1.04685261\n",
      "Iteration 31, loss = 1.04914545\n",
      "Iteration 32, loss = 1.04470215\n",
      "Iteration 33, loss = 1.03997532\n",
      "Iteration 34, loss = 1.04071522\n",
      "Iteration 35, loss = 1.03872878\n",
      "Iteration 36, loss = 1.03891126\n",
      "Iteration 37, loss = 1.03794002\n",
      "Iteration 38, loss = 1.03891766\n",
      "Iteration 39, loss = 1.03362346\n",
      "Iteration 40, loss = 1.03133663\n",
      "Iteration 41, loss = 1.03110569\n",
      "Iteration 42, loss = 1.03362132\n",
      "Iteration 43, loss = 1.03069184\n",
      "Iteration 44, loss = 1.03027165\n",
      "Iteration 45, loss = 1.02887331\n",
      "Iteration 46, loss = 1.02966777\n",
      "Iteration 47, loss = 1.02978810\n",
      "Iteration 48, loss = 1.02815041\n",
      "Iteration 49, loss = 1.02528182\n",
      "Iteration 50, loss = 1.02314872\n",
      "Iteration 51, loss = 1.02128615\n",
      "Iteration 52, loss = 1.01930304\n",
      "Iteration 53, loss = 1.02101722\n",
      "Iteration 54, loss = 1.01827416\n",
      "Iteration 55, loss = 1.01885105\n",
      "Iteration 56, loss = 1.01586441\n",
      "Iteration 57, loss = 1.01593651\n",
      "Iteration 58, loss = 1.01347571\n",
      "Iteration 59, loss = 1.01534907\n",
      "Iteration 60, loss = 1.01635473\n",
      "Iteration 61, loss = 1.01195864\n",
      "Iteration 62, loss = 1.01074552\n",
      "Iteration 63, loss = 1.00873119\n",
      "Iteration 64, loss = 1.00749894\n",
      "Iteration 65, loss = 1.00852170\n",
      "Iteration 66, loss = 1.00671880\n",
      "Iteration 67, loss = 1.00557119\n",
      "Iteration 68, loss = 1.00698912\n",
      "Iteration 69, loss = 1.00930767\n",
      "Iteration 70, loss = 1.00333946\n",
      "Iteration 71, loss = 1.00250398\n",
      "Iteration 72, loss = 0.99943600\n",
      "Iteration 73, loss = 0.99977423\n",
      "Iteration 74, loss = 1.00006019\n",
      "Iteration 75, loss = 0.99833855\n",
      "Iteration 76, loss = 1.00009323\n",
      "Iteration 77, loss = 0.99719128\n",
      "Iteration 78, loss = 0.99536440\n",
      "Iteration 79, loss = 0.99535451\n",
      "Iteration 80, loss = 0.99440078\n",
      "Iteration 81, loss = 0.99801598\n",
      "Iteration 82, loss = 0.99443393\n",
      "Iteration 83, loss = 0.99167863\n",
      "Iteration 84, loss = 0.99099081\n",
      "Iteration 85, loss = 0.99033449\n",
      "Iteration 86, loss = 0.99202615\n",
      "Iteration 87, loss = 0.99473646\n",
      "Iteration 88, loss = 0.98543232\n",
      "Iteration 89, loss = 0.98482829\n",
      "Iteration 90, loss = 0.98808168\n",
      "Iteration 91, loss = 0.98745985\n",
      "Iteration 92, loss = 0.98745458\n",
      "Iteration 93, loss = 0.98426359\n",
      "Iteration 94, loss = 0.98014795\n",
      "Iteration 95, loss = 0.98214921\n",
      "Iteration 96, loss = 0.98650211\n",
      "Iteration 97, loss = 0.98348045\n",
      "Iteration 98, loss = 0.97904166\n",
      "Iteration 99, loss = 0.97578272\n",
      "Iteration 100, loss = 0.97948476\n",
      "Iteration 1, loss = 1.46655381\n",
      "Iteration 2, loss = 1.26700649\n",
      "Iteration 3, loss = 1.21952094\n",
      "Iteration 4, loss = 1.18729895\n",
      "Iteration 5, loss = 1.16007069\n",
      "Iteration 6, loss = 1.14495022\n",
      "Iteration 7, loss = 1.13009985\n",
      "Iteration 8, loss = 1.12087235\n",
      "Iteration 9, loss = 1.11121480\n",
      "Iteration 10, loss = 1.10265752\n",
      "Iteration 11, loss = 1.09846646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 1.09763903\n",
      "Iteration 13, loss = 1.08758158\n",
      "Iteration 14, loss = 1.08269713\n",
      "Iteration 15, loss = 1.07784371\n",
      "Iteration 16, loss = 1.07481740\n",
      "Iteration 17, loss = 1.07243903\n",
      "Iteration 18, loss = 1.07064386\n",
      "Iteration 19, loss = 1.06619363\n",
      "Iteration 20, loss = 1.05987516\n",
      "Iteration 21, loss = 1.05886882\n",
      "Iteration 22, loss = 1.05599347\n",
      "Iteration 23, loss = 1.05864418\n",
      "Iteration 24, loss = 1.05268381\n",
      "Iteration 25, loss = 1.05174600\n",
      "Iteration 26, loss = 1.05053276\n",
      "Iteration 27, loss = 1.04704122\n",
      "Iteration 28, loss = 1.04818675\n",
      "Iteration 29, loss = 1.04602836\n",
      "Iteration 30, loss = 1.04353172\n",
      "Iteration 31, loss = 1.04255951\n",
      "Iteration 32, loss = 1.04018003\n",
      "Iteration 33, loss = 1.03905451\n",
      "Iteration 34, loss = 1.03923081\n",
      "Iteration 35, loss = 1.03531748\n",
      "Iteration 36, loss = 1.03522898\n",
      "Iteration 37, loss = 1.03717799\n",
      "Iteration 38, loss = 1.03400819\n",
      "Iteration 39, loss = 1.03165351\n",
      "Iteration 40, loss = 1.02943216\n",
      "Iteration 41, loss = 1.03022974\n",
      "Iteration 42, loss = 1.03019033\n",
      "Iteration 43, loss = 1.02850219\n",
      "Iteration 44, loss = 1.02705632\n",
      "Iteration 45, loss = 1.02439280\n",
      "Iteration 46, loss = 1.02295464\n",
      "Iteration 47, loss = 1.02419461\n",
      "Iteration 48, loss = 1.02634964\n",
      "Iteration 49, loss = 1.02270441\n",
      "Iteration 50, loss = 1.02231563\n",
      "Iteration 51, loss = 1.02078417\n",
      "Iteration 52, loss = 1.01752643\n",
      "Iteration 53, loss = 1.01880310\n",
      "Iteration 54, loss = 1.01915665\n",
      "Iteration 55, loss = 1.01593518\n",
      "Iteration 56, loss = 1.01640319\n",
      "Iteration 57, loss = 1.01470700\n",
      "Iteration 58, loss = 1.01065460\n",
      "Iteration 59, loss = 1.01344506\n",
      "Iteration 60, loss = 1.01407930\n",
      "Iteration 61, loss = 1.00927053\n",
      "Iteration 62, loss = 1.00781416\n",
      "Iteration 63, loss = 1.00806782\n",
      "Iteration 64, loss = 1.00641042\n",
      "Iteration 65, loss = 1.00596662\n",
      "Iteration 66, loss = 1.00556918\n",
      "Iteration 67, loss = 1.00428410\n",
      "Iteration 68, loss = 1.00224664\n",
      "Iteration 69, loss = 1.00764289\n",
      "Iteration 70, loss = 1.00242573\n",
      "Iteration 71, loss = 0.99932657\n",
      "Iteration 72, loss = 1.00033688\n",
      "Iteration 73, loss = 0.99654764\n",
      "Iteration 74, loss = 0.99952966\n",
      "Iteration 75, loss = 0.99495263\n",
      "Iteration 76, loss = 1.00225273\n",
      "Iteration 77, loss = 1.00049572\n",
      "Iteration 78, loss = 0.99532691\n",
      "Iteration 79, loss = 0.99389711\n",
      "Iteration 80, loss = 0.99221589\n",
      "Iteration 81, loss = 0.99155352\n",
      "Iteration 82, loss = 0.99016676\n",
      "Iteration 83, loss = 0.98884844\n",
      "Iteration 84, loss = 0.99058783\n",
      "Iteration 85, loss = 0.98962187\n",
      "Iteration 86, loss = 0.98990310\n",
      "Iteration 87, loss = 0.99093132\n",
      "Iteration 88, loss = 0.98472727\n",
      "Iteration 89, loss = 0.98306571\n",
      "Iteration 90, loss = 0.98398316\n",
      "Iteration 91, loss = 0.98440548\n",
      "Iteration 92, loss = 0.98368674\n",
      "Iteration 93, loss = 0.98309315\n",
      "Iteration 94, loss = 0.98217184\n",
      "Iteration 95, loss = 0.97822992\n",
      "Iteration 96, loss = 0.98036860\n",
      "Iteration 97, loss = 0.97628213\n",
      "Iteration 98, loss = 0.97577275\n",
      "Iteration 99, loss = 0.97671379\n",
      "Iteration 100, loss = 0.97967124\n",
      "Iteration 1, loss = 1.45964355\n",
      "Iteration 2, loss = 1.25920676\n",
      "Iteration 3, loss = 1.20801815\n",
      "Iteration 4, loss = 1.17041763\n",
      "Iteration 5, loss = 1.14252094\n",
      "Iteration 6, loss = 1.12628912\n",
      "Iteration 7, loss = 1.11555110\n",
      "Iteration 8, loss = 1.10302818\n",
      "Iteration 9, loss = 1.09634758\n",
      "Iteration 10, loss = 1.08798783\n",
      "Iteration 11, loss = 1.08112216\n",
      "Iteration 12, loss = 1.07739967\n",
      "Iteration 13, loss = 1.06868036\n",
      "Iteration 14, loss = 1.06562926\n",
      "Iteration 15, loss = 1.06118727\n",
      "Iteration 16, loss = 1.05626151\n",
      "Iteration 17, loss = 1.05316408\n",
      "Iteration 18, loss = 1.05198026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 1.04982771\n",
      "Iteration 20, loss = 1.04598015\n",
      "Iteration 21, loss = 1.04114336\n",
      "Iteration 22, loss = 1.04322705\n",
      "Iteration 23, loss = 1.03990531\n",
      "Iteration 24, loss = 1.03651942\n",
      "Iteration 25, loss = 1.03439118\n",
      "Iteration 26, loss = 1.03304933\n",
      "Iteration 27, loss = 1.03158572\n",
      "Iteration 28, loss = 1.03100674\n",
      "Iteration 29, loss = 1.03016363\n",
      "Iteration 30, loss = 1.03037425\n",
      "Iteration 31, loss = 1.02403849\n",
      "Iteration 32, loss = 1.02476171\n",
      "Iteration 33, loss = 1.02420290\n",
      "Iteration 34, loss = 1.02122401\n",
      "Iteration 35, loss = 1.01978432\n",
      "Iteration 36, loss = 1.01946350\n",
      "Iteration 37, loss = 1.01823498\n",
      "Iteration 38, loss = 1.01647574\n",
      "Iteration 39, loss = 1.01719654\n",
      "Iteration 40, loss = 1.01395807\n",
      "Iteration 41, loss = 1.01999749\n",
      "Iteration 42, loss = 1.01228367\n",
      "Iteration 43, loss = 1.01257330\n",
      "Iteration 44, loss = 1.00920247\n",
      "Iteration 45, loss = 1.00976877\n",
      "Iteration 46, loss = 1.00753491\n",
      "Iteration 47, loss = 1.00581849\n",
      "Iteration 48, loss = 1.00523170\n",
      "Iteration 49, loss = 1.00371407\n",
      "Iteration 50, loss = 1.00662405\n",
      "Iteration 51, loss = 1.00695995\n",
      "Iteration 52, loss = 1.00004703\n",
      "Iteration 53, loss = 0.99988521\n",
      "Iteration 54, loss = 0.99881834\n",
      "Iteration 55, loss = 0.99725537\n",
      "Iteration 56, loss = 1.00070082\n",
      "Iteration 57, loss = 1.00035056\n",
      "Iteration 58, loss = 0.99169139\n",
      "Iteration 59, loss = 0.99400085\n",
      "Iteration 60, loss = 0.99429982\n",
      "Iteration 61, loss = 0.99229224\n",
      "Iteration 62, loss = 0.99025195\n",
      "Iteration 63, loss = 0.98926123\n",
      "Iteration 64, loss = 0.98644738\n",
      "Iteration 65, loss = 0.98519405\n",
      "Iteration 66, loss = 0.98655064\n",
      "Iteration 67, loss = 0.98763798\n",
      "Iteration 68, loss = 0.98425880\n",
      "Iteration 69, loss = 0.98391294\n",
      "Iteration 70, loss = 0.98483793\n",
      "Iteration 71, loss = 0.98264042\n",
      "Iteration 72, loss = 0.98860881\n",
      "Iteration 73, loss = 0.98327105\n",
      "Iteration 74, loss = 0.98489911\n",
      "Iteration 75, loss = 0.97764056\n",
      "Iteration 76, loss = 0.98494379\n",
      "Iteration 77, loss = 0.98653890\n",
      "Iteration 78, loss = 0.97725442\n",
      "Iteration 79, loss = 0.98132761\n",
      "Iteration 80, loss = 0.97683426\n",
      "Iteration 81, loss = 0.97590967\n",
      "Iteration 82, loss = 0.97280719\n",
      "Iteration 83, loss = 0.97305423\n",
      "Iteration 84, loss = 0.96982061\n",
      "Iteration 85, loss = 0.97261759\n",
      "Iteration 86, loss = 0.97175081\n",
      "Iteration 87, loss = 0.97308812\n",
      "Iteration 88, loss = 0.96969746\n",
      "Iteration 89, loss = 0.97486815\n",
      "Iteration 90, loss = 0.96714682\n",
      "Iteration 91, loss = 0.96769523\n",
      "Iteration 92, loss = 0.96965575\n",
      "Iteration 93, loss = 0.96917089\n",
      "Iteration 94, loss = 0.96501060\n",
      "Iteration 95, loss = 0.96215659\n",
      "Iteration 96, loss = 0.96338296\n",
      "Iteration 97, loss = 0.96234927\n",
      "Iteration 98, loss = 0.96428429\n",
      "Iteration 99, loss = 0.96383975\n",
      "Iteration 100, loss = 0.96363346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.45938765\n",
      "Iteration 2, loss = 1.26214550\n",
      "Iteration 3, loss = 1.21272090\n",
      "Iteration 4, loss = 1.17740466\n",
      "Iteration 5, loss = 1.15011427\n",
      "Iteration 6, loss = 1.13398901\n",
      "Iteration 7, loss = 1.12093675\n",
      "Iteration 8, loss = 1.10635116\n",
      "Iteration 9, loss = 1.09767872\n",
      "Iteration 10, loss = 1.08843243\n",
      "Iteration 11, loss = 1.08181269\n",
      "Iteration 12, loss = 1.07862770\n",
      "Iteration 13, loss = 1.06891594\n",
      "Iteration 14, loss = 1.06703049\n",
      "Iteration 15, loss = 1.06324012\n",
      "Iteration 16, loss = 1.05793116\n",
      "Iteration 17, loss = 1.05462861\n",
      "Iteration 18, loss = 1.05207880\n",
      "Iteration 19, loss = 1.05327465\n",
      "Iteration 20, loss = 1.04863943\n",
      "Iteration 21, loss = 1.04508725\n",
      "Iteration 22, loss = 1.04523565\n",
      "Iteration 23, loss = 1.04039265\n",
      "Iteration 24, loss = 1.03872811\n",
      "Iteration 25, loss = 1.03655510\n",
      "Iteration 26, loss = 1.03518650\n",
      "Iteration 27, loss = 1.03205054\n",
      "Iteration 28, loss = 1.03190877\n",
      "Iteration 29, loss = 1.03054222\n",
      "Iteration 30, loss = 1.03066488\n",
      "Iteration 31, loss = 1.02792666\n",
      "Iteration 32, loss = 1.02780787\n",
      "Iteration 33, loss = 1.02590786\n",
      "Iteration 34, loss = 1.02688213\n",
      "Iteration 35, loss = 1.02152038\n",
      "Iteration 36, loss = 1.02073942\n",
      "Iteration 37, loss = 1.01974427\n",
      "Iteration 38, loss = 1.01955755\n",
      "Iteration 39, loss = 1.01699552\n",
      "Iteration 40, loss = 1.01715265\n",
      "Iteration 41, loss = 1.01854058\n",
      "Iteration 42, loss = 1.01258331\n",
      "Iteration 43, loss = 1.01197755\n",
      "Iteration 44, loss = 1.01037519\n",
      "Iteration 45, loss = 1.01040703\n",
      "Iteration 46, loss = 1.01220042\n",
      "Iteration 47, loss = 1.01044302\n",
      "Iteration 48, loss = 1.00930612\n",
      "Iteration 49, loss = 1.00937147\n",
      "Iteration 50, loss = 1.01185596\n",
      "Iteration 51, loss = 1.00894995\n",
      "Iteration 52, loss = 1.00337476\n",
      "Iteration 53, loss = 1.00181169\n",
      "Iteration 54, loss = 0.99975147\n",
      "Iteration 55, loss = 1.00140611\n",
      "Iteration 56, loss = 0.99937896\n",
      "Iteration 57, loss = 0.99805811\n",
      "Iteration 58, loss = 0.99743185\n",
      "Iteration 59, loss = 0.99848579\n",
      "Iteration 60, loss = 0.99755072\n",
      "Iteration 61, loss = 0.99700232\n",
      "Iteration 62, loss = 0.99917487\n",
      "Iteration 63, loss = 0.99490598\n",
      "Iteration 64, loss = 0.99323886\n",
      "Iteration 65, loss = 0.99107952\n",
      "Iteration 66, loss = 0.99122652\n",
      "Iteration 67, loss = 0.99131859\n",
      "Iteration 68, loss = 0.99007988\n",
      "Iteration 69, loss = 0.99233514\n",
      "Iteration 70, loss = 0.99099327\n",
      "Iteration 71, loss = 0.98782749\n",
      "Iteration 72, loss = 0.98849871\n",
      "Iteration 73, loss = 0.98566024\n",
      "Iteration 74, loss = 0.99128026\n",
      "Iteration 75, loss = 0.98704788\n",
      "Iteration 76, loss = 0.98826263\n",
      "Iteration 77, loss = 0.98695351\n",
      "Iteration 78, loss = 0.98149199\n",
      "Iteration 79, loss = 0.98217042\n",
      "Iteration 80, loss = 0.98065249\n",
      "Iteration 81, loss = 0.98060797\n",
      "Iteration 82, loss = 0.98313034\n",
      "Iteration 83, loss = 0.97889896\n",
      "Iteration 84, loss = 0.97615811\n",
      "Iteration 85, loss = 0.97671321\n",
      "Iteration 86, loss = 0.97796707\n",
      "Iteration 87, loss = 0.97573830\n",
      "Iteration 88, loss = 0.97569988\n",
      "Iteration 89, loss = 0.97693071\n",
      "Iteration 90, loss = 0.97455588\n",
      "Iteration 91, loss = 0.97532431\n",
      "Iteration 92, loss = 0.97453561\n",
      "Iteration 93, loss = 0.96976798\n",
      "Iteration 94, loss = 0.97050226\n",
      "Iteration 95, loss = 0.96795834\n",
      "Iteration 96, loss = 0.96810650\n",
      "Iteration 97, loss = 0.96590406\n",
      "Iteration 98, loss = 0.96765357\n",
      "Iteration 99, loss = 0.96527219\n",
      "Iteration 100, loss = 0.96973615\n",
      "Iteration 1, loss = 1.46241913\n",
      "Iteration 2, loss = 1.26036662\n",
      "Iteration 3, loss = 1.21811001\n",
      "Iteration 4, loss = 1.18267843\n",
      "Iteration 5, loss = 1.15873701\n",
      "Iteration 6, loss = 1.14006815\n",
      "Iteration 7, loss = 1.12519377\n",
      "Iteration 8, loss = 1.11472294\n",
      "Iteration 9, loss = 1.10683380\n",
      "Iteration 10, loss = 1.10026980\n",
      "Iteration 11, loss = 1.09110890\n",
      "Iteration 12, loss = 1.08763024\n",
      "Iteration 13, loss = 1.09048114\n",
      "Iteration 14, loss = 1.08366854\n",
      "Iteration 15, loss = 1.07402160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 1.07461338\n",
      "Iteration 17, loss = 1.06926947\n",
      "Iteration 18, loss = 1.06506730\n",
      "Iteration 19, loss = 1.06584761\n",
      "Iteration 20, loss = 1.06261279\n",
      "Iteration 21, loss = 1.05927926\n",
      "Iteration 22, loss = 1.05671277\n",
      "Iteration 23, loss = 1.05478382\n",
      "Iteration 24, loss = 1.05417810\n",
      "Iteration 25, loss = 1.05227829\n",
      "Iteration 26, loss = 1.05346341\n",
      "Iteration 27, loss = 1.05342228\n",
      "Iteration 28, loss = 1.04717705\n",
      "Iteration 29, loss = 1.04827009\n",
      "Iteration 30, loss = 1.04217399\n",
      "Iteration 31, loss = 1.04353384\n",
      "Iteration 32, loss = 1.04627007\n",
      "Iteration 33, loss = 1.04399974\n",
      "Iteration 34, loss = 1.03854438\n",
      "Iteration 35, loss = 1.03774134\n",
      "Iteration 36, loss = 1.03994971\n",
      "Iteration 37, loss = 1.04608634\n",
      "Iteration 38, loss = 1.03669977\n",
      "Iteration 39, loss = 1.03383220\n",
      "Iteration 40, loss = 1.03370303\n",
      "Iteration 41, loss = 1.02834822\n",
      "Iteration 42, loss = 1.03243256\n",
      "Iteration 43, loss = 1.02799565\n",
      "Iteration 44, loss = 1.02555962\n",
      "Iteration 45, loss = 1.02547931\n",
      "Iteration 46, loss = 1.02306703\n",
      "Iteration 47, loss = 1.02402789\n",
      "Iteration 48, loss = 1.02260724\n",
      "Iteration 49, loss = 1.02256065\n",
      "Iteration 50, loss = 1.02021713\n",
      "Iteration 51, loss = 1.01912190\n",
      "Iteration 52, loss = 1.01730375\n",
      "Iteration 53, loss = 1.01875837\n",
      "Iteration 54, loss = 1.01623652\n",
      "Iteration 55, loss = 1.01405003\n",
      "Iteration 56, loss = 1.01261514\n",
      "Iteration 57, loss = 1.01449555\n",
      "Iteration 58, loss = 1.01205162\n",
      "Iteration 59, loss = 1.01141753\n",
      "Iteration 60, loss = 1.01007601\n",
      "Iteration 61, loss = 1.00801130\n",
      "Iteration 62, loss = 1.01181112\n",
      "Iteration 63, loss = 1.00664072\n",
      "Iteration 64, loss = 1.00781624\n",
      "Iteration 65, loss = 1.00426255\n",
      "Iteration 66, loss = 1.00506832\n",
      "Iteration 67, loss = 1.00265147\n",
      "Iteration 68, loss = 1.00357709\n",
      "Iteration 69, loss = 1.00111568\n",
      "Iteration 70, loss = 1.00177171\n",
      "Iteration 71, loss = 1.00059968\n",
      "Iteration 72, loss = 0.99980659\n",
      "Iteration 73, loss = 0.99747378\n",
      "Iteration 74, loss = 0.99589404\n",
      "Iteration 75, loss = 0.99831802\n",
      "Iteration 76, loss = 0.99481655\n",
      "Iteration 77, loss = 0.99591905\n",
      "Iteration 78, loss = 0.99853128\n",
      "Iteration 79, loss = 0.99463839\n",
      "Iteration 80, loss = 0.99974646\n",
      "Iteration 81, loss = 0.99662967\n",
      "Iteration 82, loss = 0.99424124\n",
      "Iteration 83, loss = 0.99097492\n",
      "Iteration 84, loss = 0.98775485\n",
      "Iteration 85, loss = 0.99422383\n",
      "Iteration 86, loss = 0.99114239\n",
      "Iteration 87, loss = 0.99060977\n",
      "Iteration 88, loss = 0.99302286\n",
      "Iteration 89, loss = 0.99031217\n",
      "Iteration 90, loss = 0.98384261\n",
      "Iteration 91, loss = 0.98363894\n",
      "Iteration 92, loss = 0.98467154\n",
      "Iteration 93, loss = 0.98401545\n",
      "Iteration 94, loss = 0.98361358\n",
      "Iteration 95, loss = 0.98163541\n",
      "Iteration 96, loss = 0.98130449\n",
      "Iteration 97, loss = 0.98374800\n",
      "Iteration 98, loss = 0.98343412\n",
      "Iteration 99, loss = 0.98118762\n",
      "Iteration 100, loss = 0.98204455\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbhendal/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "k4folds = 5\n",
    "final_results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:   # Select each model in turn\n",
    "    #print(\" ++ NOW WORKING ON ALGORITHM %s ++\" % name)\n",
    "    kfold = model_selection.KFold(n_splits=k4folds, random_state=seed, shuffle=True)\n",
    "    cv_results = model_selection.cross_val_score(model, X_test, y_test, scoring=scoring_method) \n",
    "    final_results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('\\n')\n",
    "print('done \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7faef8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- ALGORITHM: mlp ----\n",
      "algorithm mlp f1_micro results: mean = 0.559122 (std = 0.019921)\n",
      "\n",
      "\n",
      " ---- ALGORITHM: scaled_mlp ----\n",
      "algorithm scaled_mlp f1_micro results: mean = 0.559122 (std = 0.019921)\n",
      "\n",
      "\n",
      " ---- ALGORITHM: raw_SVM ----\n",
      "algorithm raw_SVM f1_micro results: mean = 0.559122 (std = 0.019921)\n",
      "\n",
      "\n",
      " ---- ALGORITHM: scaled_SVM ----\n",
      "algorithm scaled_SVM f1_micro results: mean = 0.559122 (std = 0.019921)\n",
      "\n",
      "\n",
      " ---- ALGORITHM: scaled_SVM_std ----\n",
      "algorithm scaled_SVM_std f1_micro results: mean = 0.559122 (std = 0.019921)\n",
      "\n",
      "\n",
      "done \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:   # Select each model in turn\n",
    "    print(\" ---- ALGORITHM: %s ----\" % name)    \n",
    "    msg = \"algorithm %s %s results: mean = %f (std = %f)\" % (name, scoring_method, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    print('\\n')\n",
    "print('done \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
